{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEsaesKwwFnf",
        "outputId": "c020eac5-a8d3-42da-a1c8-a1ef30ecce3b"
      },
      "outputs": [],
      "source": [
        "# ! pip install keras-rl2\n",
        "# ! pip install chess\n",
        "# ! pip install python-chess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ocqViokxFxM",
        "outputId": "a32fdb04-daf7-474b-ab41-be5bee6850bd"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INImGNcRyKhX",
        "outputId": "afe25e4d-72d3-4bc4-88df-4bb6be1b84d1"
      },
      "outputs": [],
      "source": [
        "# ls drive/MyDrive/Data/Chess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-12-15 10:05:57.590584: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2021-12-15 10:05:57.590619: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten,\\\n",
        "     Input,BatchNormalization, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "# import gym_chess\n",
        "\n",
        "import chess\n",
        "from sys import platform\n",
        "import os\n",
        "import chess.engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 9656455395442982541\n",
            "]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-12-15 10:06:00.531582: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-12-15 10:06:00.532663: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2021-12-15 10:06:00.532692: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2021-12-15 10:06:00.532723: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LoG): /proc/driver/nvidia/version does not exist\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "# os.system('chmod +x stockfish_14.1_linux_x64')\n",
        "# engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish_14.1_linux_x64\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "if platform == \"linux\" or platform == \"linux2\":\n",
        "    os.system('chmod +x ../stockfish/stockfish_14.1_linux_x64')\n",
        "    engine = chess.engine.SimpleEngine.popen_uci(r\"../stockfish/stockfish_14.1_linux_x64\")\n",
        "elif platform == \"win32\":\n",
        "    engine = chess.engine.SimpleEngine.popen_uci(r\"../stockfish/stockfish_14.1_win_32bit.exe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_move(env):\n",
        "    result = engine.play(env.env, chess.engine.Limit(time=0.05))\n",
        "    return result.move"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "STATE_SHAPE = (65, )\n",
        "NB_ACTIONS = 4096"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChessEnv:\n",
        "    '''\n",
        "    state - obser: ndarray - (65,): [:65] is flatten from int_board; [65] is color of bot; 1 is white and -1 is black\n",
        "    step: int. step_range = (0, 4096) , is encoded from square A to square B (64 x 64 val)\n",
        "    reward: int\n",
        "    '''\n",
        "\n",
        "    mapped = {\n",
        "            'P': 10,     # White Pawn\n",
        "            'p': -10,    # Black Pawn\n",
        "            'N': 20,     # White Knight\n",
        "            'n': -20,    # Black Knight\n",
        "            'B': 30,     # White Bishop\n",
        "            'b': -30,    # Black Bishop\n",
        "            'R': 40,     # White Rook\n",
        "            'r': -40,    # Black Rook\n",
        "            'Q': 50,     # White Queen\n",
        "            'q': -50,    # Black Queen\n",
        "            'K': 900,     # White King\n",
        "            'k': -900     # Black King\n",
        "    }\n",
        "    # state_shape = (8, 8)\n",
        "    # nb_actions = 4096\n",
        "    model = None\n",
        "    \n",
        "    def __init__(self, model: Sequential, neg_r_each_step = -1) -> None:\n",
        "        self.env = chess.Board()\n",
        "        self.model = model\n",
        "        self.state = self.reset()\n",
        "        # [-1] = 1 -> white, -1 -> black\n",
        "        self.bot_color = self.env.turn * 2 - 1\n",
        "        self.neg_r_each_step = neg_r_each_step\n",
        "\n",
        "    def is_draw(self):\n",
        "        if self.env.is_stalemate():\n",
        "            print(\"statlemate\")\n",
        "            return True\n",
        "        if self.env.is_fivefold_repetition():\n",
        "            print(\"fivefold repetition\")\n",
        "            return True\n",
        "        if self.env.is_seventyfive_moves():\n",
        "            print(\"75 moves\")\n",
        "            return True\n",
        "        if self.env.is_insufficient_material():\n",
        "            print(\"Insufficient Material\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def is_checkmate(self):\n",
        "        # If There is checkmate then it will be TRUE else FALSE.It will be a boolean value.\n",
        "        return self.env.is_checkmate()\n",
        "\n",
        "    def convert_board_to_int(self):\n",
        "        epd_string = self.env.epd()\n",
        "        list_int = np.empty((0, ))\n",
        "        for i in epd_string:\n",
        "            if i == \" \":\n",
        "                list_int = list_int.reshape((8, 8))\n",
        "                return list_int\n",
        "            elif i != \"/\":\n",
        "                if i in self.mapped:\n",
        "                    list_int = np.append(list_int, self.mapped[i])\n",
        "                else:\n",
        "                    for counter in range(0, int(i)):\n",
        "                        list_int = np.append(list_int, 0)\n",
        "        list_int = list_int.reshape((8, 8))\n",
        "        return list_int\n",
        "\n",
        "    def get_state(self) -> np.ndarray:\n",
        "        return np.append(self.convert_board_to_int().reshape(64,), self.env.turn * 2 - 1)\n",
        "\n",
        "    def legal_moves(self):\n",
        "        return list(self.env.legal_moves)\n",
        "\n",
        "    def encodeMove(self, move_uci:str):\n",
        "        if len(move_uci) != 4:\n",
        "            raise ValueError()\n",
        "        a, b = chess.parse_square(move_uci[:2]), chess.parse_square(move_uci[2:])\n",
        "        return a * 64 + b\n",
        "\n",
        "    def decodeMove(self, move_int:int):\n",
        "        a, b = move_int//64, move_int%64\n",
        "        # a, b = chess.square_name(a), chess.square_name(b)\n",
        "\n",
        "        move = self.env.find_move(from_square= a,to_square= b)\n",
        "        return move\n",
        "\n",
        "    def render(self):\n",
        "        print(self.env.unicode())\n",
        "\n",
        "    def reset(self):\n",
        "        # random state\n",
        "        redo = True\n",
        "        num_sample_steps = 0\n",
        "        while redo:\n",
        "            redo = False\n",
        "            self.env = chess.Board()\n",
        "            num_sample_steps = np.random.randint(0, 50)\n",
        "            for i in range (num_sample_steps):\n",
        "                lg_move = self.legal_moves()\n",
        "                if len(lg_move) != 0:\n",
        "                    move = np.random.choice(self.legal_moves())\n",
        "                    self.env.push(move)\n",
        "                else:\n",
        "                    redo = True\n",
        "                    break\n",
        "\n",
        "        Q_val = self.model.predict(self.get_state().reshape((1, 1) + STATE_SHAPE)).reshape(-1, )\n",
        "        print('Val:', min(Q_val), max(Q_val))\n",
        "        return self.get_state()\n",
        "\n",
        "    def step(self, action: int):\n",
        "        reward = 0\n",
        "        done = True\n",
        "        \n",
        "        try:\n",
        "            # move in legal move\n",
        "            move = self.decodeMove(action)\n",
        "\n",
        "            # neg reward each step\n",
        "            reward = self.neg_r_each_step\n",
        "\n",
        "            # location to_square\n",
        "            to_r, to_c = move.to_square//8, move.to_square%8\n",
        "            reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
        "\n",
        "            # action\n",
        "            self.env.push(move)\n",
        "            self.state = self.get_state()\n",
        "\n",
        "            # check end game\n",
        "            if self.is_checkmate():\n",
        "                reward += self.mapped['K']\n",
        "                done = True\n",
        "                print('Win')\n",
        "            elif self.is_draw():\n",
        "                reward += 300\n",
        "                done = True\n",
        "\n",
        "            # opponent's turn   \n",
        "            else:\n",
        "                done = False\n",
        "\n",
        "                move = find_move(self)\n",
        "\n",
        "                # location to_square\n",
        "                to_r, to_c = move.to_square//8, move.to_square%8\n",
        "                reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
        "\n",
        "                # action\n",
        "                self.env.push(move)\n",
        "                self.state = self.get_state()\n",
        "\n",
        "                # check end game\n",
        "                if self.is_checkmate():\n",
        "                    reward -= self.mapped['K']\n",
        "                    done = True\n",
        "                    print(\"Lose\")\n",
        "                elif self.is_draw():\n",
        "                    reward += 300\n",
        "                    done = True\n",
        "\n",
        "        except:\n",
        "            # wrong move\n",
        "            reward = -5000\n",
        "            done = True\n",
        "            print('wrong_move')\n",
        "\n",
        "        return self.state, reward, done, {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "WARNING:tensorflow:From /home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/layers/normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 65)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               8448      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4096)              528384    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 4096)              0         \n",
            "=================================================================\n",
            "Total params: 554,368\n",
            "Trainable params: 553,856\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# model\n",
        "model = Sequential()\n",
        "model.add(Input((1, ) + STATE_SHAPE))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(NB_ACTIONS))\n",
        "model.add(Activation('linear'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val: -57.429234 60.385868\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
            "2021-12-15 10:06:01.283775: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 1999965000 Hz\n"
          ]
        }
      ],
      "source": [
        "env = ChessEnv(model, neg_r_each_step=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_weights('chess_model_sf.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 50000 steps ...\n",
            "Val: -6383.884 -3991.3992\n",
            "wrong_move\n",
            "     1/50000: episode: 1, duration: 0.158s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6407.9067 -3951.5598\n",
            "wrong_move\n",
            "     2/50000: episode: 2, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5986.904 -4049.8723\n",
            "wrong_move\n",
            "     3/50000: episode: 3, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6337.3354 -3997.6877\n",
            "wrong_move\n",
            "     4/50000: episode: 4, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7418.108 1664.1611\n",
            "wrong_move\n",
            "     5/50000: episode: 5, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6533.347 -2772.2083\n",
            "wrong_move\n",
            "     6/50000: episode: 6, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6107.0063 -611.12067\n",
            "wrong_move\n",
            "    11/50000: episode: 7, duration: 0.243s, episode steps:   5, steps per second:  21, episode reward: -5144.000, mean reward: -1028.800 [-5000.000, -21.000], mean action: 233.800 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5559.4766 -2654.8762\n",
            "wrong_move\n",
            "    15/50000: episode: 8, duration: 0.180s, episode steps:   4, steps per second:  22, episode reward: -5053.000, mean reward: -1263.250 [-5000.000, -1.000], mean action: 630.750 [196.000, 1809.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8918.611 -2379.4595\n",
            "wrong_move\n",
            "    16/50000: episode: 9, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5529.428 1130.9841\n",
            "wrong_move\n",
            "    18/50000: episode: 10, duration: 0.069s, episode steps:   2, steps per second:  29, episode reward: -5951.000, mean reward: -2975.500 [-5000.000, -951.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6121.0386 -3479.2654\n",
            "wrong_move\n",
            "    19/50000: episode: 11, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6234.7734 -4012.1775\n",
            "wrong_move\n",
            "    20/50000: episode: 12, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6523.218 -3998.722\n",
            "wrong_move\n",
            "    21/50000: episode: 13, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5720.6284 209.38115\n",
            "wrong_move\n",
            "    22/50000: episode: 14, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6138.0483 -2488.1204\n",
            "wrong_move\n",
            "    23/50000: episode: 15, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -11179.939 5814.3286\n",
            "wrong_move\n",
            "    24/50000: episode: 16, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6279.938 -4054.1897\n",
            "wrong_move\n",
            "    25/50000: episode: 17, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5738.011 1855.0135\n",
            "wrong_move\n",
            "    27/50000: episode: 18, duration: 0.088s, episode steps:   2, steps per second:  23, episode reward: -5921.000, mean reward: -2960.500 [-5000.000, -921.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6155.0054 -3655.3801\n",
            "wrong_move\n",
            "    28/50000: episode: 19, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8540.594 -2345.0994\n",
            "wrong_move\n",
            "    29/50000: episode: 20, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9579.282 -2350.3965\n",
            "wrong_move\n",
            "    30/50000: episode: 21, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6307.9707 -4013.9048\n",
            "wrong_move\n",
            "    31/50000: episode: 22, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6188.228 -3112.5466\n",
            "wrong_move\n",
            "    43/50000: episode: 23, duration: 0.692s, episode steps:  12, steps per second:  17, episode reward: -5931.000, mean reward: -494.250 [-5000.000, -1.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7138.056 -2817.0703\n",
            "wrong_move\n",
            "    44/50000: episode: 24, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10410.4 -1826.1011\n",
            "wrong_move\n",
            "    45/50000: episode: 25, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5705.856 749.6623\n",
            "wrong_move\n",
            "    46/50000: episode: 26, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8072.456 -3978.4011\n",
            "wrong_move\n",
            "    47/50000: episode: 27, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6334.5884 -1096.1793\n",
            "wrong_move\n",
            "    50/50000: episode: 28, duration: 0.153s, episode steps:   3, steps per second:  20, episode reward: -5952.000, mean reward: -1984.000 [-5000.000, -51.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7563.117 -3027.99\n",
            "wrong_move\n",
            "    51/50000: episode: 29, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6383.884 -3991.3992\n",
            "wrong_move\n",
            "    52/50000: episode: 30, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6390.3867 -3933.734\n",
            "wrong_move\n",
            "    53/50000: episode: 31, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8691.587 -2623.6897\n",
            "wrong_move\n",
            "    55/50000: episode: 32, duration: 0.075s, episode steps:   2, steps per second:  27, episode reward: -5901.000, mean reward: -2950.500 [-5000.000, -901.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6031.522 -1810.2454\n",
            "wrong_move\n",
            "    56/50000: episode: 33, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6483.775 -3924.2441\n",
            "wrong_move\n",
            "    57/50000: episode: 34, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6359.755 -992.15234\n",
            "wrong_move\n",
            "    58/50000: episode: 35, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6295.7646 -4022.3237\n",
            "wrong_move\n",
            "    59/50000: episode: 36, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6149.604 -4046.0386\n",
            "wrong_move\n",
            "    60/50000: episode: 37, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8772.32 -2454.4958\n",
            "wrong_move\n",
            "    61/50000: episode: 38, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6558.167 -2622.3472\n",
            "wrong_move\n",
            "    62/50000: episode: 39, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6006.5483 -2155.0276\n",
            "wrong_move\n",
            "    68/50000: episode: 40, duration: 0.327s, episode steps:   6, steps per second:  18, episode reward: -5095.000, mean reward: -849.167 [-5000.000, -1.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10413.416 -1806.0961\n",
            "wrong_move\n",
            "    69/50000: episode: 41, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8119.698 -1145.7764\n",
            "wrong_move\n",
            "    70/50000: episode: 42, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5846.646 -4100.822\n",
            "wrong_move\n",
            "    71/50000: episode: 43, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6172.544 -1358.2162\n",
            "wrong_move\n",
            "    74/50000: episode: 44, duration: 0.124s, episode steps:   3, steps per second:  24, episode reward: -5962.000, mean reward: -1987.333 [-5000.000, -21.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5554.264 -371.26984\n",
            "wrong_move\n",
            "    75/50000: episode: 45, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6158.698 -1977.7095\n",
            "wrong_move\n",
            "    76/50000: episode: 46, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7102.39 -1573.7585\n",
            "wrong_move\n",
            "    77/50000: episode: 47, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7337.8926 2014.7443\n",
            "wrong_move\n",
            "    79/50000: episode: 48, duration: 0.086s, episode steps:   2, steps per second:  23, episode reward: -5031.000, mean reward: -2515.500 [-5000.000, -31.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9368.852 -2450.4895\n",
            "wrong_move\n",
            "    80/50000: episode: 49, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6054.616 -4067.972\n",
            "wrong_move\n",
            "    81/50000: episode: 50, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9830.475 -2158.6997\n",
            "wrong_move\n",
            "    82/50000: episode: 51, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9413.846 -2406.8748\n",
            "wrong_move\n",
            "    83/50000: episode: 52, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6784.432 12910.13\n",
            "wrong_move\n",
            "    84/50000: episode: 53, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9328.445 -2377.3823\n",
            "wrong_move\n",
            "    85/50000: episode: 54, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5220.452 376.4145\n",
            "wrong_move\n",
            "    87/50000: episode: 55, duration: 0.102s, episode steps:   2, steps per second:  20, episode reward: -5051.000, mean reward: -2525.500 [-5000.000, -51.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9329.555 -2332.502\n",
            "wrong_move\n",
            "    88/50000: episode: 56, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5911.144 -2566.9182\n",
            "wrong_move\n",
            "    89/50000: episode: 57, duration: 0.043s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6198.1807 -4033.3171\n",
            "wrong_move\n",
            "    90/50000: episode: 58, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6085.0796 -3266.1917\n",
            "wrong_move\n",
            "    91/50000: episode: 59, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -15778.656 369819.56\n",
            "wrong_move\n",
            "    92/50000: episode: 60, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5716.931 -821.5557\n",
            "wrong_move\n",
            "    93/50000: episode: 61, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9287.916 -2489.4448\n",
            "wrong_move\n",
            "    94/50000: episode: 62, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6773.4155 -3766.6582\n",
            "wrong_move\n",
            "    95/50000: episode: 63, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6264.605 -4026.1577\n",
            "wrong_move\n",
            "    96/50000: episode: 64, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5849.061 -756.84503\n",
            "wrong_move\n",
            "   101/50000: episode: 65, duration: 0.274s, episode steps:   5, steps per second:  18, episode reward: -5944.000, mean reward: -1188.800 [-5000.000, -1.000], mean action: 233.800 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6192.52 -3188.0173\n",
            "wrong_move\n",
            "   102/50000: episode: 66, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9962.585 -2252.0317\n",
            "wrong_move\n",
            "   104/50000: episode: 67, duration: 0.096s, episode steps:   2, steps per second:  21, episode reward: -5921.000, mean reward: -2960.500 [-5000.000, -921.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10157.406 1613.032\n",
            "wrong_move\n",
            "   105/50000: episode: 68, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5776.9556 -3324.989\n",
            "wrong_move\n",
            "   106/50000: episode: 69, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6400.2427 -4000.5273\n",
            "wrong_move\n",
            "   107/50000: episode: 70, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5351.627 -3727.2725\n",
            "wrong_move\n",
            "   108/50000: episode: 71, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10330.177 -2205.9824\n",
            "wrong_move\n",
            "   109/50000: episode: 72, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6388.646 -4010.9888\n",
            "wrong_move\n",
            "   110/50000: episode: 73, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6135.738 -4080.2476\n",
            "wrong_move\n",
            "   111/50000: episode: 74, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5740.266 -2791.5227\n",
            "wrong_move\n",
            "   112/50000: episode: 75, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6330.0415 -4011.113\n",
            "wrong_move\n",
            "   113/50000: episode: 76, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6333.611 -3247.0195\n",
            "wrong_move\n",
            "   120/50000: episode: 77, duration: 0.377s, episode steps:   7, steps per second:  19, episode reward: -5126.000, mean reward: -732.286 [-5000.000, -1.000], mean action: 232.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10087.605 -2167.2512\n",
            "wrong_move\n",
            "   121/50000: episode: 78, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6316.547 -4031.7896\n",
            "wrong_move\n",
            "   122/50000: episode: 79, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6343.517 -3925.1938\n",
            "wrong_move\n",
            "   123/50000: episode: 80, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5908.0864 -3470.927\n",
            "wrong_move\n",
            "   124/50000: episode: 81, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9886.788 5810.734\n",
            "wrong_move\n",
            "   127/50000: episode: 82, duration: 0.133s, episode steps:   3, steps per second:  23, episode reward: -5052.000, mean reward: -1684.000 [-5000.000, -21.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5785.9614 1153.9485\n",
            "wrong_move\n",
            "   128/50000: episode: 83, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6383.884 -3991.3992\n",
            "wrong_move\n",
            "   129/50000: episode: 84, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10945.209 4025.0002\n",
            "wrong_move\n",
            "   130/50000: episode: 85, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8015.846 -3805.393\n",
            "wrong_move\n",
            "   131/50000: episode: 86, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5648.739 339.2414\n",
            "wrong_move\n",
            "   137/50000: episode: 87, duration: 0.350s, episode steps:   6, steps per second:  17, episode reward: -5085.000, mean reward: -847.500 [-5000.000, -1.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5899.986 -4022.8928\n",
            "wrong_move\n",
            "   138/50000: episode: 88, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6257.61 -4030.5627\n",
            "wrong_move\n",
            "   139/50000: episode: 89, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6431.6973 275.54852\n",
            "wrong_move\n",
            "   140/50000: episode: 90, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9663.25 -2265.7615\n",
            "wrong_move\n",
            "   141/50000: episode: 91, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6872.658 -2899.521\n",
            "wrong_move\n",
            "   142/50000: episode: 92, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6144.497 -2843.6453\n",
            "wrong_move\n",
            "   143/50000: episode: 93, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6319.879 -3969.8997\n",
            "wrong_move\n",
            "   144/50000: episode: 94, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6324.799 -551.24225\n",
            "wrong_move\n",
            "   154/50000: episode: 95, duration: 0.514s, episode steps:  10, steps per second:  19, episode reward: -6049.000, mean reward: -604.900 [-5000.000, -1.000], mean action: 233.800 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5430.0312 -1103.0856\n",
            "wrong_move\n",
            "   162/50000: episode: 96, duration: 0.405s, episode steps:   8, steps per second:  20, episode reward: -5997.000, mean reward: -749.625 [-5000.000, -1.000], mean action: 235.375 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6161.094 -3780.366\n",
            "wrong_move\n",
            "   163/50000: episode: 97, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7767.356 -2889.3425\n",
            "wrong_move\n",
            "   164/50000: episode: 98, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 103.000 [103.000, 103.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6316.485 -964.80676\n",
            "wrong_move\n",
            "   165/50000: episode: 99, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9803.509 -2141.3135\n",
            "wrong_move\n",
            "   167/50000: episode: 100, duration: 0.068s, episode steps:   2, steps per second:  29, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6924.198 -2451.0857\n",
            "wrong_move\n",
            "   168/50000: episode: 101, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10083.767 -2053.9575\n",
            "wrong_move\n",
            "   170/50000: episode: 102, duration: 0.069s, episode steps:   2, steps per second:  29, episode reward: -5911.000, mean reward: -2955.500 [-5000.000, -911.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6195.7886 -700.7526\n",
            "wrong_move\n",
            "   171/50000: episode: 103, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6352.579 -3986.0537\n",
            "wrong_move\n",
            "   172/50000: episode: 104, duration: 0.007s, episode steps:   1, steps per second: 134, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6188.1963 -4044.4844\n",
            "wrong_move\n",
            "   173/50000: episode: 105, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8680.071 1591.7001\n",
            "wrong_move\n",
            "   175/50000: episode: 106, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward: -5021.000, mean reward: -2510.500 [-5000.000, -21.000], mean action: 226.000 [193.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6287.898 -3986.6326\n",
            "wrong_move\n",
            "   176/50000: episode: 107, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -11204.217 1274.9885\n",
            "wrong_move\n",
            "   177/50000: episode: 108, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5799.3296 385.87665\n",
            "wrong_move\n",
            "   178/50000: episode: 109, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5799.194 -1413.0277\n",
            "wrong_move\n",
            "   179/50000: episode: 110, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6204.0635 1638.3286\n",
            "wrong_move\n",
            "   181/50000: episode: 111, duration: 0.076s, episode steps:   2, steps per second:  26, episode reward: -5911.000, mean reward: -2955.500 [-5000.000, -911.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6608.3794 -2223.302\n",
            "wrong_move\n",
            "   182/50000: episode: 112, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6178.187 -4096.2124\n",
            "wrong_move\n",
            "   183/50000: episode: 113, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9457.051 -2357.7715\n",
            "wrong_move\n",
            "   184/50000: episode: 114, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5950.791 -2076.8718\n",
            "wrong_move\n",
            "   185/50000: episode: 115, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6200.653 -352.32733\n",
            "wrong_move\n",
            "   191/50000: episode: 116, duration: 0.324s, episode steps:   6, steps per second:  18, episode reward: -6015.000, mean reward: -1002.500 [-5000.000, -1.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6142.3228 -4002.4077\n",
            "wrong_move\n",
            "   192/50000: episode: 117, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9956.029 -1911.6461\n",
            "wrong_move\n",
            "   193/50000: episode: 118, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6332.4116 -3974.8542\n",
            "wrong_move\n",
            "   194/50000: episode: 119, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7915.9355 -2500.0408\n",
            "wrong_move\n",
            "   195/50000: episode: 120, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5721.3525 -720.92914\n",
            "wrong_move\n",
            "   198/50000: episode: 121, duration: 0.169s, episode steps:   3, steps per second:  18, episode reward: -5942.000, mean reward: -1980.667 [-5000.000, -31.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6115.846 -4010.5647\n",
            "wrong_move\n",
            "   199/50000: episode: 122, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6138.311 -1496.9302\n",
            "wrong_move\n",
            "   200/50000: episode: 123, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5592.076 -2931.1106\n",
            "wrong_move\n",
            "   201/50000: episode: 124, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6215.051 -1638.4838\n",
            "wrong_move\n",
            "   202/50000: episode: 125, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10235.653 -2532.2224\n",
            "wrong_move\n",
            "   203/50000: episode: 126, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6168.8823 -2236.7542\n",
            "wrong_move\n",
            "   204/50000: episode: 127, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5439.083 -1382.6431\n",
            "wrong_move\n",
            "   207/50000: episode: 128, duration: 0.142s, episode steps:   3, steps per second:  21, episode reward: -5052.000, mean reward: -1684.000 [-5000.000, -1.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6447.3384 -2094.738\n",
            "wrong_move\n",
            "   208/50000: episode: 129, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7316.059 -2658.1023\n",
            "wrong_move\n",
            "   211/50000: episode: 130, duration: 0.160s, episode steps:   3, steps per second:  19, episode reward: -5952.000, mean reward: -1984.000 [-5000.000, -1.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8883.997 -2383.1655\n",
            "wrong_move\n",
            "   212/50000: episode: 131, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9731.555 -809.1794\n",
            "wrong_move\n",
            "   213/50000: episode: 132, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10480.078 -1775.4585\n",
            "wrong_move\n",
            "   214/50000: episode: 133, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5992.894 -699.55133\n",
            "wrong_move\n",
            "   215/50000: episode: 134, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6446.681 -3473.3125\n",
            "wrong_move\n",
            "   216/50000: episode: 135, duration: 0.027s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7621.171 -2316.9338\n",
            "wrong_move\n",
            "   221/50000: episode: 136, duration: 0.287s, episode steps:   5, steps per second:  17, episode reward: -5084.000, mean reward: -1016.800 [-5000.000, -1.000], mean action: 233.800 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10825.951 -1344.6404\n",
            "wrong_move\n",
            "   222/50000: episode: 137, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8855.499 -2327.0503\n",
            "wrong_move\n",
            "   223/50000: episode: 138, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -11166.451 -1377.7598\n",
            "wrong_move\n",
            "   224/50000: episode: 139, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6240.231 -3957.4114\n",
            "wrong_move\n",
            "   225/50000: episode: 140, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5501.8735 -81.28108\n",
            "wrong_move\n",
            "   226/50000: episode: 141, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5719.8784 1298.2225\n",
            "wrong_move\n",
            "   228/50000: episode: 142, duration: 0.105s, episode steps:   2, steps per second:  19, episode reward: -5051.000, mean reward: -2525.500 [-5000.000, -51.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6057.324 -3633.0012\n",
            "wrong_move\n",
            "   229/50000: episode: 143, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6656.154 -3920.7407\n",
            "wrong_move\n",
            "   230/50000: episode: 144, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6470.268 -3965.72\n",
            "wrong_move\n",
            "   231/50000: episode: 145, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9486.137 -1058.3259\n",
            "wrong_move\n",
            "   232/50000: episode: 146, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7976.48 -3956.2385\n",
            "wrong_move\n",
            "   233/50000: episode: 147, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9886.398 -1331.5234\n",
            "wrong_move\n",
            "   235/50000: episode: 148, duration: 0.116s, episode steps:   2, steps per second:  17, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6392.237 -2896.066\n",
            "wrong_move\n",
            "   236/50000: episode: 149, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7352.979 4003.2441\n",
            "wrong_move\n",
            "   238/50000: episode: 150, duration: 0.112s, episode steps:   2, steps per second:  18, episode reward: -5051.000, mean reward: -2525.500 [-5000.000, -51.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8334.373 -2483.1567\n",
            "wrong_move\n",
            "   239/50000: episode: 151, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5704.2993 248.273\n",
            "wrong_move\n",
            "   240/50000: episode: 152, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6452.689 -283.31482\n",
            "wrong_move\n",
            "   241/50000: episode: 153, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9708.584 -2251.9138\n",
            "wrong_move\n",
            "   242/50000: episode: 154, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9534.301 -2484.469\n",
            "wrong_move\n",
            "   250/50000: episode: 155, duration: 0.429s, episode steps:   8, steps per second:  19, episode reward: -5977.000, mean reward: -747.125 [-5000.000, -1.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6109.3726 -2588.8054\n",
            "wrong_move\n",
            "   251/50000: episode: 156, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6158.316 -2859.358\n",
            "wrong_move\n",
            "   252/50000: episode: 157, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6380.6396 -4005.1082\n",
            "wrong_move\n",
            "   253/50000: episode: 158, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9271.207 4215.8037\n",
            "wrong_move\n",
            "   254/50000: episode: 159, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8971.303 -2726.6482\n",
            "wrong_move\n",
            "   255/50000: episode: 160, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10564.695 -1823.4963\n",
            "wrong_move\n",
            "   256/50000: episode: 161, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7832.201 -2852.5828\n",
            "wrong_move\n",
            "   263/50000: episode: 162, duration: 0.395s, episode steps:   7, steps per second:  18, episode reward: -5996.000, mean reward: -856.571 [-5000.000, -1.000], mean action: 232.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8854.245 -2380.3308\n",
            "wrong_move\n",
            "   264/50000: episode: 163, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5671.4863 -3447.3887\n",
            "wrong_move\n",
            "   265/50000: episode: 164, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6356.484 -3694.306\n",
            "wrong_move\n",
            "   266/50000: episode: 165, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6463.8667 -4000.5679\n",
            "wrong_move\n",
            "   267/50000: episode: 166, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6384.7812 -1989.5598\n",
            "wrong_move\n",
            "   268/50000: episode: 167, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5891.4634 753.6098\n",
            "wrong_move\n",
            "   269/50000: episode: 168, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7420.509 13085.956\n",
            "wrong_move\n",
            "   270/50000: episode: 169, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5515.1562 -2305.8357\n",
            "wrong_move\n",
            "   271/50000: episode: 170, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6110.002 -1729.3625\n",
            "wrong_move\n",
            "   272/50000: episode: 171, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6320.434 -4041.7407\n",
            "wrong_move\n",
            "   273/50000: episode: 172, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9940.267 -1444.6729\n",
            "wrong_move\n",
            "   274/50000: episode: 173, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9364.313 -2288.65\n",
            "wrong_move\n",
            "   277/50000: episode: 174, duration: 0.160s, episode steps:   3, steps per second:  19, episode reward: -5962.000, mean reward: -1987.333 [-5000.000, -21.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6354.29 -3354.9941\n",
            "wrong_move\n",
            "   278/50000: episode: 175, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7603.563 2279.5618\n",
            "wrong_move\n",
            "   284/50000: episode: 176, duration: 0.294s, episode steps:   6, steps per second:  20, episode reward: -5105.000, mean reward: -850.833 [-5000.000, -1.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5639.577 -16.416456\n",
            "wrong_move\n",
            "   288/50000: episode: 177, duration: 0.178s, episode steps:   4, steps per second:  22, episode reward: -6003.000, mean reward: -1500.750 [-5000.000, -41.000], mean action: 243.250 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6405.602 -3980.7058\n",
            "wrong_move\n",
            "   289/50000: episode: 178, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8840.659 289.18402\n",
            "wrong_move\n",
            "   295/50000: episode: 179, duration: 0.289s, episode steps:   6, steps per second:  21, episode reward: -5995.000, mean reward: -999.167 [-5000.000, -1.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7467.0713 -4154.9556\n",
            "wrong_move\n",
            "   296/50000: episode: 180, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6275.191 -2559.7043\n",
            "wrong_move\n",
            "   297/50000: episode: 181, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6375.1724 -3977.4646\n",
            "wrong_move\n",
            "   298/50000: episode: 182, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10631.777 -1367.54\n",
            "wrong_move\n",
            "   299/50000: episode: 183, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6413.7114 -3617.266\n",
            "wrong_move\n",
            "   300/50000: episode: 184, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6325.9272 -4010.2832\n",
            "wrong_move\n",
            "   301/50000: episode: 185, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5916.9434 -4149.098\n",
            "wrong_move\n",
            "   302/50000: episode: 186, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7102.5225 2785.6926\n",
            "wrong_move\n",
            "   303/50000: episode: 187, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9341.806 -198.22096\n",
            "wrong_move\n",
            "   304/50000: episode: 188, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6429.6606 -4002.9788\n",
            "wrong_move\n",
            "   305/50000: episode: 189, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6350.858 -3997.427\n",
            "wrong_move\n",
            "   306/50000: episode: 190, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7226.197 -1199.778\n",
            "wrong_move\n",
            "   307/50000: episode: 191, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10179.667 -1281.69\n",
            "wrong_move\n",
            "   308/50000: episode: 192, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9357.176 -688.83136\n",
            "wrong_move\n",
            "   309/50000: episode: 193, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6129.4814 -4037.051\n",
            "wrong_move\n",
            "   310/50000: episode: 194, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9276.831 -2774.5872\n",
            "wrong_move\n",
            "   312/50000: episode: 195, duration: 0.094s, episode steps:   2, steps per second:  21, episode reward: -5041.000, mean reward: -2520.500 [-5000.000, -41.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6251.1206 -4026.4492\n",
            "wrong_move\n",
            "   313/50000: episode: 196, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6278.267 -2448.9614\n",
            "wrong_move\n",
            "   314/50000: episode: 197, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6440.1406 -3952.022\n",
            "wrong_move\n",
            "   315/50000: episode: 198, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7193.56 -2619.1042\n",
            "wrong_move\n",
            "   316/50000: episode: 199, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6118.7915 -1479.8369\n",
            "wrong_move\n",
            "   317/50000: episode: 200, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6420.514 -3972.6755\n",
            "wrong_move\n",
            "   318/50000: episode: 201, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6500.6045 -3924.0928\n",
            "wrong_move\n",
            "   319/50000: episode: 202, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6329.39 -4001.2603\n",
            "wrong_move\n",
            "   320/50000: episode: 203, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -4973.391 -2757.925\n",
            "wrong_move\n",
            "   321/50000: episode: 204, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9565.896 -2795.3335\n",
            "wrong_move\n",
            "   322/50000: episode: 205, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -16373.41 -2268.6074\n",
            "wrong_move\n",
            "   323/50000: episode: 206, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5913.092 -3519.8022\n",
            "wrong_move\n",
            "   329/50000: episode: 207, duration: 0.299s, episode steps:   6, steps per second:  20, episode reward: -5125.000, mean reward: -854.167 [-5000.000, -1.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6585.589 -2747.7366\n",
            "wrong_move\n",
            "   330/50000: episode: 208, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5275.188 -3441.751\n",
            "wrong_move\n",
            "   331/50000: episode: 209, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5300.522 -710.76807\n",
            "wrong_move\n",
            "   333/50000: episode: 210, duration: 0.074s, episode steps:   2, steps per second:  27, episode reward: -5931.000, mean reward: -2965.500 [-5000.000, -931.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6148.021 -4038.0369\n",
            "wrong_move\n",
            "   334/50000: episode: 211, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6383.884 -3991.3992\n",
            "wrong_move\n",
            "   335/50000: episode: 212, duration: 0.007s, episode steps:   1, steps per second: 145, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5939.8013 2619.2068\n",
            "wrong_move\n",
            "   337/50000: episode: 213, duration: 0.072s, episode steps:   2, steps per second:  28, episode reward: -5941.000, mean reward: -2970.500 [-5000.000, -941.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -11067.171 4494.312\n",
            "wrong_move\n",
            "   338/50000: episode: 214, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9594.055 -2329.2522\n",
            "wrong_move\n",
            "   339/50000: episode: 215, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5736.3076 -4194.314\n",
            "wrong_move\n",
            "   340/50000: episode: 216, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9968.246 -2183.839\n",
            "wrong_move\n",
            "   341/50000: episode: 217, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6420.1807 -2895.7744\n",
            "wrong_move\n",
            "   342/50000: episode: 218, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7461.527 877.8604\n",
            "wrong_move\n",
            "   344/50000: episode: 219, duration: 0.098s, episode steps:   2, steps per second:  20, episode reward: -5951.000, mean reward: -2975.500 [-5000.000, -951.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6385.3223 -3346.733\n",
            "wrong_move\n",
            "   345/50000: episode: 220, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6336.446 -3988.0752\n",
            "wrong_move\n",
            "   346/50000: episode: 221, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6338.7607 -4002.0034\n",
            "wrong_move\n",
            "   347/50000: episode: 222, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5829.576 -4076.9468\n",
            "wrong_move\n",
            "   348/50000: episode: 223, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8597.644 -1688.0258\n",
            "wrong_move\n",
            "   349/50000: episode: 224, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7950.2705 -3133.2993\n",
            "wrong_move\n",
            "   350/50000: episode: 225, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6623.7954 -2099.889\n",
            "wrong_move\n",
            "   351/50000: episode: 226, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -13755.1045 8656.372\n",
            "wrong_move\n",
            "   352/50000: episode: 227, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5783.238 -4044.9604\n",
            "wrong_move\n",
            "   353/50000: episode: 228, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9920.94 -2217.2483\n",
            "wrong_move\n",
            "   354/50000: episode: 229, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5716.0884 -4125.217\n",
            "wrong_move\n",
            "   355/50000: episode: 230, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6179.333 -4051.4797\n",
            "wrong_move\n",
            "   356/50000: episode: 231, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9899.824 -2111.4731\n",
            "wrong_move\n",
            "   357/50000: episode: 232, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6337.497 -3980.8752\n",
            "wrong_move\n",
            "   358/50000: episode: 233, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5921.5537 681.8079\n",
            "wrong_move\n",
            "   359/50000: episode: 234, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6317.7407 -4005.7917\n",
            "wrong_move\n",
            "   360/50000: episode: 235, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9365.384 -2594.4224\n",
            "wrong_move\n",
            "   366/50000: episode: 236, duration: 0.384s, episode steps:   6, steps per second:  16, episode reward: -5985.000, mean reward: -997.500 [-5000.000, -1.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10339.946 -1802.5286\n",
            "wrong_move\n",
            "   367/50000: episode: 237, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8669.849 -2394.5623\n",
            "wrong_move\n",
            "   368/50000: episode: 238, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7249.041 -3618.9783\n",
            "wrong_move\n",
            "   374/50000: episode: 239, duration: 0.348s, episode steps:   6, steps per second:  17, episode reward: -6005.000, mean reward: -1000.833 [-5000.000, -1.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6195.38 -4050.5032\n",
            "wrong_move\n",
            "   375/50000: episode: 240, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6388.637 -3975.348\n",
            "wrong_move\n",
            "   376/50000: episode: 241, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6342.88 -2751.7507\n",
            "wrong_move\n",
            "   377/50000: episode: 242, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10811.933 -1872.9934\n",
            "wrong_move\n",
            "   378/50000: episode: 243, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6102.2935 -1319.3167\n",
            "wrong_move\n",
            "   379/50000: episode: 244, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6063.845 -4076.934\n",
            "wrong_move\n",
            "   380/50000: episode: 245, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9696.322 -2821.1858\n",
            "wrong_move\n",
            "   381/50000: episode: 246, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6147.962 -4073.6384\n",
            "wrong_move\n",
            "   382/50000: episode: 247, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7234.029 2408.3674\n",
            "wrong_move\n",
            "   383/50000: episode: 248, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7537.274 212.01408\n",
            "Lose\n",
            "   393/50000: episode: 249, duration: 0.556s, episode steps:  10, steps per second:  18, episode reward: -2010.000, mean reward: -201.000 [-951.000, -1.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6186.4683 -4033.4944\n",
            "wrong_move\n",
            "   394/50000: episode: 250, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6504.438 -3868.7512\n",
            "wrong_move\n",
            "   395/50000: episode: 251, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5651.8633 -1829.6611\n",
            "wrong_move\n",
            "   396/50000: episode: 252, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6302.7915 -2736.323\n",
            "wrong_move\n",
            "   397/50000: episode: 253, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6319.309 -3759.3733\n",
            "wrong_move\n",
            "   398/50000: episode: 254, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5845.833 -1430.5405\n",
            "wrong_move\n",
            "   404/50000: episode: 255, duration: 0.299s, episode steps:   6, steps per second:  20, episode reward: -5045.000, mean reward: -840.833 [-5000.000, -1.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6237.9077 -1081.3003\n",
            "wrong_move\n",
            "   408/50000: episode: 256, duration: 0.177s, episode steps:   4, steps per second:  23, episode reward: -5933.000, mean reward: -1483.250 [-5000.000, -1.000], mean action: 243.250 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8960.046 -2490.3972\n",
            "wrong_move\n",
            "   409/50000: episode: 257, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -13926.125 14636.908\n",
            "wrong_move\n",
            "   410/50000: episode: 258, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6347.33 -3783.2537\n",
            "wrong_move\n",
            "   411/50000: episode: 259, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6086.8296 -1179.3137\n",
            "wrong_move\n",
            "   412/50000: episode: 260, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6203.9346 -4057.1292\n",
            "wrong_move\n",
            "   413/50000: episode: 261, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7754.8413 4501.4727\n",
            "wrong_move\n",
            "   415/50000: episode: 262, duration: 0.111s, episode steps:   2, steps per second:  18, episode reward: -5931.000, mean reward: -2965.500 [-5000.000, -931.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6046.158 -4054.9602\n",
            "wrong_move\n",
            "   416/50000: episode: 263, duration: 0.027s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7760.035 -3273.3413\n",
            "wrong_move\n",
            "   417/50000: episode: 264, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9291.394 -1140.4917\n",
            "wrong_move\n",
            "   418/50000: episode: 265, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5861.0977 461.60223\n",
            "wrong_move\n",
            "   419/50000: episode: 266, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6323.4116 -3993.9873\n",
            "wrong_move\n",
            "   420/50000: episode: 267, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8558.499 -2536.7915\n",
            "wrong_move\n",
            "   421/50000: episode: 268, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5532.6567 -1342.1958\n",
            "wrong_move\n",
            "   422/50000: episode: 269, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6375.9097 -1616.3226\n",
            "wrong_move\n",
            "   423/50000: episode: 270, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6510.93 -3495.8655\n",
            "wrong_move\n",
            "   424/50000: episode: 271, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6041.18 -3649.542\n",
            "wrong_move\n",
            "   425/50000: episode: 272, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5909.464 -1479.3015\n",
            "wrong_move\n",
            "   431/50000: episode: 273, duration: 0.308s, episode steps:   6, steps per second:  19, episode reward: -5935.000, mean reward: -989.167 [-5000.000, -1.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10433.449 -527.298\n",
            "wrong_move\n",
            "   432/50000: episode: 274, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6423.1704 -4002.846\n",
            "wrong_move\n",
            "   433/50000: episode: 275, duration: 0.007s, episode steps:   1, steps per second: 154, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -12340.408 596.2645\n",
            "wrong_move\n",
            "   434/50000: episode: 276, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6120.6772 -3153.7551\n",
            "wrong_move\n",
            "   435/50000: episode: 277, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10225.844 -2048.5342\n",
            "wrong_move\n",
            "   436/50000: episode: 278, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9188.608 -2435.2473\n",
            "wrong_move\n",
            "   437/50000: episode: 279, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8042.904 -3667.3262\n",
            "wrong_move\n",
            "   438/50000: episode: 280, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -11108873.0 65380200.0\n",
            "wrong_move\n",
            "   439/50000: episode: 281, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5308.974 -1237.3778\n",
            "wrong_move\n",
            "   443/50000: episode: 282, duration: 0.235s, episode steps:   4, steps per second:  17, episode reward: -5943.000, mean reward: -1485.750 [-5000.000, -21.000], mean action: 243.250 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9045.8 -2698.6963\n",
            "wrong_move\n",
            "   444/50000: episode: 283, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6175.1587 -3415.3396\n",
            "wrong_move\n",
            "   445/50000: episode: 284, duration: 0.009s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6193.605 -353.66345\n",
            "wrong_move\n",
            "   447/50000: episode: 285, duration: 0.071s, episode steps:   2, steps per second:  28, episode reward: -5951.000, mean reward: -2975.500 [-5000.000, -951.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6562.5435 -3947.411\n",
            "wrong_move\n",
            "   448/50000: episode: 286, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6608.2515 -3905.6538\n",
            "wrong_move\n",
            "   449/50000: episode: 287, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6217.223 -4027.5234\n",
            "wrong_move\n",
            "   450/50000: episode: 288, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9280.359 1263.9894\n",
            "wrong_move\n",
            "   451/50000: episode: 289, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6385.8784 -4001.0789\n",
            "wrong_move\n",
            "   452/50000: episode: 290, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9250.547 -2608.2527\n",
            "wrong_move\n",
            "   453/50000: episode: 291, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8692.799 1636.2715\n",
            "wrong_move\n",
            "   465/50000: episode: 292, duration: 0.656s, episode steps:  12, steps per second:  18, episode reward: -6061.000, mean reward: -505.083 [-5000.000, -1.000], mean action: 232.750 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9425.17 -2272.5796\n",
            "wrong_move\n",
            "   468/50000: episode: 293, duration: 0.126s, episode steps:   3, steps per second:  24, episode reward: -5062.000, mean reward: -1687.333 [-5000.000, -11.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6163.602 -1199.0994\n",
            "wrong_move\n",
            "   475/50000: episode: 294, duration: 0.354s, episode steps:   7, steps per second:  20, episode reward: -5106.000, mean reward: -729.429 [-5000.000, -1.000], mean action: 232.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -4882.509 -863.7392\n",
            "wrong_move\n",
            "   481/50000: episode: 295, duration: 0.294s, episode steps:   6, steps per second:  20, episode reward: -5095.000, mean reward: -849.167 [-5000.000, -1.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6280.228 -1749.0732\n",
            "wrong_move\n",
            "   482/50000: episode: 296, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6106.503 -2949.6677\n",
            "wrong_move\n",
            "   495/50000: episode: 297, duration: 0.662s, episode steps:  13, steps per second:  20, episode reward: -5992.000, mean reward: -460.923 [-5000.000, -1.000], mean action: 229.923 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6351.664 -4006.8542\n",
            "wrong_move\n",
            "   496/50000: episode: 298, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6345.296 -3818.781\n",
            "wrong_move\n",
            "   497/50000: episode: 299, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6182.558 -427.45343\n",
            "wrong_move\n",
            "   498/50000: episode: 300, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6544.605 -841.34546\n",
            "wrong_move\n",
            "   499/50000: episode: 301, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6387.485 -3429.1047\n",
            "wrong_move\n",
            "   500/50000: episode: 302, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5643.113 -713.1873\n",
            "wrong_move\n",
            "   501/50000: episode: 303, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5768.6978 -769.9386\n",
            "wrong_move\n",
            "   502/50000: episode: 304, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7691.241 -2264.6296\n",
            "wrong_move\n",
            "   503/50000: episode: 305, duration: 0.010s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7255.81 4149.899\n",
            "wrong_move\n",
            "   504/50000: episode: 306, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6432.122 -3934.496\n",
            "wrong_move\n",
            "   505/50000: episode: 307, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6383.884 -3991.3992\n",
            "wrong_move\n",
            "   506/50000: episode: 308, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5615.652 -3042.9253\n",
            "wrong_move\n",
            "   507/50000: episode: 309, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9407.28 -2317.504\n",
            "wrong_move\n",
            "   508/50000: episode: 310, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -13409.06 -1131.0085\n",
            "wrong_move\n",
            "   510/50000: episode: 311, duration: 0.110s, episode steps:   2, steps per second:  18, episode reward: -5951.000, mean reward: -2975.500 [-5000.000, -951.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5440.8345 33.272507\n",
            "wrong_move\n",
            "   511/50000: episode: 312, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9864.866 -2178.1318\n",
            "wrong_move\n",
            "   512/50000: episode: 313, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6627.6255 -3922.2788\n",
            "wrong_move\n",
            "   513/50000: episode: 314, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10340.114 -2090.1746\n",
            "wrong_move\n",
            "   514/50000: episode: 315, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9320.56 -2352.1848\n",
            "wrong_move\n",
            "   515/50000: episode: 316, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6351.235 -3998.753\n",
            "wrong_move\n",
            "   516/50000: episode: 317, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8701.202 -2305.218\n",
            "wrong_move\n",
            "   517/50000: episode: 318, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6329.255 -4026.2356\n",
            "wrong_move\n",
            "   518/50000: episode: 319, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6059.9175 -3999.5535\n",
            "wrong_move\n",
            "   519/50000: episode: 320, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 814.000 [814.000, 814.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10526.274 -1529.4285\n",
            "wrong_move\n",
            "   527/50000: episode: 321, duration: 0.460s, episode steps:   8, steps per second:  17, episode reward: -5067.000, mean reward: -633.375 [-5000.000,  9.000], mean action: 578.875 [196.000, 3070.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7586.811 -2548.8562\n",
            "wrong_move\n",
            "   528/50000: episode: 322, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9607.367 -2599.8025\n",
            "wrong_move\n",
            "   530/50000: episode: 323, duration: 0.079s, episode steps:   2, steps per second:  25, episode reward: -5901.000, mean reward: -2950.500 [-5000.000, -901.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9822.069 -2244.782\n",
            "wrong_move\n",
            "   531/50000: episode: 324, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5683.798 -533.31573\n",
            "wrong_move\n",
            "   535/50000: episode: 325, duration: 0.233s, episode steps:   4, steps per second:  17, episode reward: -5013.000, mean reward: -1253.250 [-5000.000, -1.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9385.758 -2308.611\n",
            "wrong_move\n",
            "   536/50000: episode: 326, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9307.727 -2462.0427\n",
            "wrong_move\n",
            "   537/50000: episode: 327, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6230.1187 -1275.3474\n",
            "wrong_move\n",
            "   538/50000: episode: 328, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6324.6675 -3166.1462\n",
            "wrong_move\n",
            "   540/50000: episode: 329, duration: 0.088s, episode steps:   2, steps per second:  23, episode reward: -5951.000, mean reward: -2975.500 [-5000.000, -951.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6454.049 -4048.3638\n",
            "wrong_move\n",
            "   541/50000: episode: 330, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6453.1895 -3874.7708\n",
            "wrong_move\n",
            "   542/50000: episode: 331, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10628.841 2.7290516\n",
            "wrong_move\n",
            "   543/50000: episode: 332, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6443.0874 -3989.683\n",
            "wrong_move\n",
            "   544/50000: episode: 333, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8062.795 5440.448\n",
            "wrong_move\n",
            "   545/50000: episode: 334, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8815.62 1644.0803\n",
            "wrong_move\n",
            "   551/50000: episode: 335, duration: 0.333s, episode steps:   6, steps per second:  18, episode reward: -6035.000, mean reward: -1005.833 [-5000.000, -11.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5579.3823 -442.63483\n",
            "wrong_move\n",
            "   556/50000: episode: 336, duration: 0.230s, episode steps:   5, steps per second:  22, episode reward: -6004.000, mean reward: -1200.800 [-5000.000, -21.000], mean action: 233.800 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6531.9824 -2624.636\n",
            "wrong_move\n",
            "   557/50000: episode: 337, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7960.833 -2578.1646\n",
            "wrong_move\n",
            "   558/50000: episode: 338, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5189.042 -2631.167\n",
            "wrong_move\n",
            "   559/50000: episode: 339, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6383.884 -3991.3992\n",
            "wrong_move\n",
            "   560/50000: episode: 340, duration: 0.007s, episode steps:   1, steps per second: 150, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9379.972 -2557.333\n",
            "wrong_move\n",
            "   561/50000: episode: 341, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8526.968 -2789.0583\n",
            "wrong_move\n",
            "   562/50000: episode: 342, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9272.197 -2406.4338\n",
            "wrong_move\n",
            "   563/50000: episode: 343, duration: 0.010s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9429.248 -2385.558\n",
            "wrong_move\n",
            "   564/50000: episode: 344, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5862.466 -4291.636\n",
            "wrong_move\n",
            "   565/50000: episode: 345, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9516.222 -2174.447\n",
            "wrong_move\n",
            "   566/50000: episode: 346, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6453.3486 -3990.0886\n",
            "wrong_move\n",
            "   567/50000: episode: 347, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6239.297 -3773.6887\n",
            "wrong_move\n",
            "   568/50000: episode: 348, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5716.447 -2540.2527\n",
            "wrong_move\n",
            "   573/50000: episode: 349, duration: 0.275s, episode steps:   5, steps per second:  18, episode reward: -5104.000, mean reward: -1020.800 [-5000.000, -1.000], mean action: 639.800 [196.000, 2289.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6214.8496 746.45404\n",
            "wrong_move\n",
            "   574/50000: episode: 350, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6187.2524 -2966.4148\n",
            "wrong_move\n",
            "   578/50000: episode: 351, duration: 0.244s, episode steps:   4, steps per second:  16, episode reward: -5063.000, mean reward: -1265.750 [-5000.000, -1.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9611.297 -2434.8982\n",
            "wrong_move\n",
            "   579/50000: episode: 352, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6823.203 -3332.2942\n",
            "wrong_move\n",
            "   580/50000: episode: 353, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5571.8696 -1989.5215\n",
            "wrong_move\n",
            "   581/50000: episode: 354, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6338.697 -3980.2285\n",
            "wrong_move\n",
            "   582/50000: episode: 355, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6398.107 -4002.8994\n",
            "wrong_move\n",
            "   583/50000: episode: 356, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8832.903 -2320.4358\n",
            "wrong_move\n",
            "   584/50000: episode: 357, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6268.269 -186.8929\n",
            "wrong_move\n",
            "   585/50000: episode: 358, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6283.0986 -4041.325\n",
            "wrong_move\n",
            "   586/50000: episode: 359, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6299.6704 -4031.6858\n",
            "wrong_move\n",
            "   587/50000: episode: 360, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6103.683 -3788.7678\n",
            "Lose\n",
            "   594/50000: episode: 361, duration: 0.389s, episode steps:   7, steps per second:  18, episode reward: -1907.000, mean reward: -272.429 [-931.000, -1.000], mean action: 232.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6298.7065 -3779.8625\n",
            "wrong_move\n",
            "   595/50000: episode: 362, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6254.1094 -3463.7678\n",
            "wrong_move\n",
            "   596/50000: episode: 363, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -12269.981 -2774.7495\n",
            "wrong_move\n",
            "   597/50000: episode: 364, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6426.483 -3951.6946\n",
            "wrong_move\n",
            "   598/50000: episode: 365, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10490.046 -2532.062\n",
            "wrong_move\n",
            "   599/50000: episode: 366, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8649.502 -2301.0024\n",
            "wrong_move\n",
            "   600/50000: episode: 367, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6419.8765 -3942.0366\n",
            "wrong_move\n",
            "   601/50000: episode: 368, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8077.8794 -2269.3423\n",
            "wrong_move\n",
            "   602/50000: episode: 369, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6383.932 -4027.8276\n",
            "wrong_move\n",
            "   603/50000: episode: 370, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10323.13 -1995.4246\n",
            "wrong_move\n",
            "   604/50000: episode: 371, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6383.884 -3991.3992\n",
            "wrong_move\n",
            "   605/50000: episode: 372, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6297.138 -2702.377\n",
            "wrong_move\n",
            "   606/50000: episode: 373, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9199.902 -2542.6606\n",
            "wrong_move\n",
            "   608/50000: episode: 374, duration: 0.109s, episode steps:   2, steps per second:  18, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9707.981 -2714.988\n",
            "wrong_move\n",
            "   609/50000: episode: 375, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10257.257 -2039.4485\n",
            "wrong_move\n",
            "   610/50000: episode: 376, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -11059.361 2203.0024\n",
            "wrong_move\n",
            "   611/50000: episode: 377, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5123.2095 -2165.4524\n",
            "wrong_move\n",
            "   612/50000: episode: 378, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9479.15 -2461.3337\n",
            "wrong_move\n",
            "   614/50000: episode: 379, duration: 0.096s, episode steps:   2, steps per second:  21, episode reward: -5091.000, mean reward: -2545.500 [-5000.000, -91.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9970.81 -2119.6099\n",
            "wrong_move\n",
            "   616/50000: episode: 380, duration: 0.098s, episode steps:   2, steps per second:  20, episode reward: -5061.000, mean reward: -2530.500 [-5000.000, -61.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6036.225 -4035.3276\n",
            "wrong_move\n",
            "   617/50000: episode: 381, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5901.9277 -809.75555\n",
            "wrong_move\n",
            "   623/50000: episode: 382, duration: 0.290s, episode steps:   6, steps per second:  21, episode reward: -5135.000, mean reward: -855.833 [-5000.000, -1.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8606.014 -2447.4006\n",
            "wrong_move\n",
            "   624/50000: episode: 383, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6172.579 -12.625685\n",
            "wrong_move\n",
            "   625/50000: episode: 384, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6231.674 775.9196\n",
            "wrong_move\n",
            "   629/50000: episode: 385, duration: 0.184s, episode steps:   4, steps per second:  22, episode reward: -5993.000, mean reward: -1498.250 [-5000.000, -41.000], mean action: 243.250 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5778.3584 -2033.4158\n",
            "wrong_move\n",
            "   630/50000: episode: 386, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8195.83 -2192.3438\n",
            "wrong_move\n",
            "   631/50000: episode: 387, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -41487164.0 686358800.0\n",
            "wrong_move\n",
            "   632/50000: episode: 388, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6381.142 -3996.6477\n",
            "wrong_move\n",
            "   633/50000: episode: 389, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9495.065 -1494.6931\n",
            "wrong_move\n",
            "   634/50000: episode: 390, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9237.783 -2281.9893\n",
            "wrong_move\n",
            "   635/50000: episode: 391, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8793.012 -2434.7803\n",
            "wrong_move\n",
            "   636/50000: episode: 392, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5878.44 -1202.035\n",
            "wrong_move\n",
            "   644/50000: episode: 393, duration: 0.409s, episode steps:   8, steps per second:  20, episode reward: -5977.000, mean reward: -747.125 [-5000.000, -1.000], mean action: 235.375 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6340.0854 -3998.747\n",
            "wrong_move\n",
            "   645/50000: episode: 394, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6309.9204 -4003.549\n",
            "wrong_move\n",
            "   646/50000: episode: 395, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6283.435 -3990.4683\n",
            "wrong_move\n",
            "   647/50000: episode: 396, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8174.2856 -1740.0122\n",
            "wrong_move\n",
            "   648/50000: episode: 397, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9440.62 -2342.3694\n",
            "wrong_move\n",
            "   649/50000: episode: 398, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10421.014 -2024.5918\n",
            "wrong_move\n",
            "   650/50000: episode: 399, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6267.6816 -4026.134\n",
            "wrong_move\n",
            "   651/50000: episode: 400, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6104.3486 211.8781\n",
            "wrong_move\n",
            "   653/50000: episode: 401, duration: 0.069s, episode steps:   2, steps per second:  29, episode reward: -5931.000, mean reward: -2965.500 [-5000.000, -931.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -4976.2896 -709.6242\n",
            "wrong_move\n",
            "   661/50000: episode: 402, duration: 0.402s, episode steps:   8, steps per second:  20, episode reward: -5997.000, mean reward: -749.625 [-5000.000, -1.000], mean action: 235.375 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5854.151 -1931.7413\n",
            "wrong_move\n",
            "   662/50000: episode: 403, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7878.1084 -3859.5923\n",
            "wrong_move\n",
            "   663/50000: episode: 404, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -13953.478 10698.851\n",
            "wrong_move\n",
            "   664/50000: episode: 405, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6429.8774 -3994.6682\n",
            "wrong_move\n",
            "   665/50000: episode: 406, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8206.071 -2199.0522\n",
            "wrong_move\n",
            "   666/50000: episode: 407, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1591.000 [1591.000, 1591.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8891.029 -2391.2502\n",
            "wrong_move\n",
            "   667/50000: episode: 408, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6001.772 -3914.937\n",
            "wrong_move\n",
            "   668/50000: episode: 409, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3543.000 [3543.000, 3543.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6373.4805 -3986.6702\n",
            "wrong_move\n",
            "   669/50000: episode: 410, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5831.058 1084.7682\n",
            "wrong_move\n",
            "   670/50000: episode: 411, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6051.5117 -3983.6687\n",
            "wrong_move\n",
            "   671/50000: episode: 412, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6314.9453 -3976.294\n",
            "wrong_move\n",
            "   672/50000: episode: 413, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6427.438 -4000.9739\n",
            "wrong_move\n",
            "   673/50000: episode: 414, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9522.468 -2291.5137\n",
            "wrong_move\n",
            "   674/50000: episode: 415, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5482.66 -73.37337\n",
            "wrong_move\n",
            "   675/50000: episode: 416, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8958.07 -882.8216\n",
            "wrong_move\n",
            "   676/50000: episode: 417, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8610.638 -2108.3179\n",
            "wrong_move\n",
            "   677/50000: episode: 418, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6426.6167 -3987.3042\n",
            "wrong_move\n",
            "   678/50000: episode: 419, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6336.303 -4004.853\n",
            "wrong_move\n",
            "   679/50000: episode: 420, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6099.142 -4091.7788\n",
            "wrong_move\n",
            "   680/50000: episode: 421, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6236.1772 -2515.3625\n",
            "wrong_move\n",
            "   681/50000: episode: 422, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8640.308 8006.1113\n",
            "wrong_move\n",
            "   682/50000: episode: 423, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5538.3013 -611.0815\n",
            "wrong_move\n",
            "   690/50000: episode: 424, duration: 0.411s, episode steps:   8, steps per second:  19, episode reward: -6007.000, mean reward: -750.875 [-5000.000, -1.000], mean action: 235.375 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6189.5703 -3518.2249\n",
            "wrong_move\n",
            "   691/50000: episode: 425, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5751.0825 786.3979\n",
            "wrong_move\n",
            "   692/50000: episode: 426, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9238.85 -2442.2932\n",
            "wrong_move\n",
            "   694/50000: episode: 427, duration: 0.065s, episode steps:   2, steps per second:  31, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7330.745 -68.02217\n",
            "wrong_move\n",
            "   695/50000: episode: 428, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5169.176 -3800.7344\n",
            "wrong_move\n",
            "   696/50000: episode: 429, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7323.312 586.9461\n",
            "wrong_move\n",
            "   697/50000: episode: 430, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8064.6797 97.06673\n",
            "wrong_move\n",
            "   698/50000: episode: 431, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6189.892 -3195.1057\n",
            "wrong_move\n",
            "   699/50000: episode: 432, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6186.6226 -4038.9119\n",
            "wrong_move\n",
            "   700/50000: episode: 433, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6146.7124 -2328.8862\n",
            "wrong_move\n",
            "   701/50000: episode: 434, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8541.774 -2454.979\n",
            "wrong_move\n",
            "   702/50000: episode: 435, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6265.53 -3977.0466\n",
            "wrong_move\n",
            "   703/50000: episode: 436, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6193.582 -4034.1162\n",
            "wrong_move\n",
            "   704/50000: episode: 437, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6506.5825 -2992.065\n",
            "wrong_move\n",
            "   705/50000: episode: 438, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5468.1196 -2136.181\n",
            "wrong_move\n",
            "   706/50000: episode: 439, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6383.884 -3991.3992\n",
            "wrong_move\n",
            "   707/50000: episode: 440, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10035.552 -1860.3983\n",
            "wrong_move\n",
            "   709/50000: episode: 441, duration: 0.081s, episode steps:   2, steps per second:  25, episode reward: -5061.000, mean reward: -2530.500 [-5000.000, -61.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6513.004 -3980.6562\n",
            "wrong_move\n",
            "   710/50000: episode: 442, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10380.288 -1941.36\n",
            "wrong_move\n",
            "   711/50000: episode: 443, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5549.9146 2410.6262\n",
            "wrong_move\n",
            "   713/50000: episode: 444, duration: 0.105s, episode steps:   2, steps per second:  19, episode reward: -5051.000, mean reward: -2525.500 [-5000.000, -51.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8669.378 -882.7153\n",
            "wrong_move\n",
            "   714/50000: episode: 445, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8735.478 -2315.1267\n",
            "wrong_move\n",
            "   715/50000: episode: 446, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6273.825 -3297.3137\n",
            "wrong_move\n",
            "   716/50000: episode: 447, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6249.9937 -4025.3691\n",
            "wrong_move\n",
            "   717/50000: episode: 448, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9242.511 -2537.8132\n",
            "wrong_move\n",
            "   718/50000: episode: 449, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8336.658 743.2713\n",
            "wrong_move\n",
            "   719/50000: episode: 450, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6274.6924 -4011.686\n",
            "wrong_move\n",
            "   720/50000: episode: 451, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9120.244 -2271.2053\n",
            "wrong_move\n",
            "   721/50000: episode: 452, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6243.6743 -4030.519\n",
            "wrong_move\n",
            "   722/50000: episode: 453, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6400.095 -3986.9873\n",
            "wrong_move\n",
            "   723/50000: episode: 454, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10264.996 -1660.331\n",
            "wrong_move\n",
            "   724/50000: episode: 455, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6210.0737 -4043.3372\n",
            "wrong_move\n",
            "   725/50000: episode: 456, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6397.492 -3980.0723\n",
            "wrong_move\n",
            "   726/50000: episode: 457, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6281.1006 -3954.406\n",
            "wrong_move\n",
            "   727/50000: episode: 458, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6367.647 -4008.443\n",
            "wrong_move\n",
            "   728/50000: episode: 459, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -16392.834 -2933.8904\n",
            "wrong_move\n",
            "   729/50000: episode: 460, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10482.009 -1820.5754\n",
            "wrong_move\n",
            "   730/50000: episode: 461, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6408.9272 -4002.0854\n",
            "wrong_move\n",
            "   731/50000: episode: 462, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6093.1963 -3264.7659\n",
            "wrong_move\n",
            "   739/50000: episode: 463, duration: 0.441s, episode steps:   8, steps per second:  18, episode reward: -5947.000, mean reward: -743.375 [-5000.000, -1.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5980.396 -2718.5417\n",
            "wrong_move\n",
            "   740/50000: episode: 464, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5473.955 3140.523\n",
            "wrong_move\n",
            "   741/50000: episode: 465, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6391.582 -3994.7805\n",
            "wrong_move\n",
            "   742/50000: episode: 466, duration: 0.007s, episode steps:   1, steps per second: 145, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5089.0405 4782.9995\n",
            "wrong_move\n",
            "   744/50000: episode: 467, duration: 0.075s, episode steps:   2, steps per second:  27, episode reward: -5951.000, mean reward: -2975.500 [-5000.000, -951.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5598.3096 -2182.3076\n",
            "wrong_move\n",
            "   745/50000: episode: 468, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5474.1816 -4300.9985\n",
            "wrong_move\n",
            "   746/50000: episode: 469, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5957.914 -106.14058\n",
            "wrong_move\n",
            "   747/50000: episode: 470, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5030.0635 -3604.154\n",
            "wrong_move\n",
            "   748/50000: episode: 471, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6212.5054 -3925.1602\n",
            "wrong_move\n",
            "   749/50000: episode: 472, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -340940.62 1866931.2\n",
            "wrong_move\n",
            "   750/50000: episode: 473, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10636.77 -1765.6827\n",
            "wrong_move\n",
            "   751/50000: episode: 474, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6474.0796 -312.38055\n",
            "wrong_move\n",
            "   752/50000: episode: 475, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10341.737 -1957.3593\n",
            "wrong_move\n",
            "   755/50000: episode: 476, duration: 0.123s, episode steps:   3, steps per second:  24, episode reward: -5012.000, mean reward: -1670.667 [-5000.000, -1.000], mean action: 217.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6261.009 -4041.23\n",
            "wrong_move\n",
            "   756/50000: episode: 477, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5614.519 110.23357\n",
            "wrong_move\n",
            "   757/50000: episode: 478, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -11347.179 -1634.4164\n",
            "wrong_move\n",
            "   758/50000: episode: 479, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5909.563 -91.70967\n",
            "wrong_move\n",
            "   760/50000: episode: 480, duration: 0.079s, episode steps:   2, steps per second:  25, episode reward: -5061.000, mean reward: -2530.500 [-5000.000, -61.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6121.0947 -1231.2206\n",
            "wrong_move\n",
            "   761/50000: episode: 481, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5523.551 737.20795\n",
            "wrong_move\n",
            "   762/50000: episode: 482, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9364.812 -2333.7952\n",
            "wrong_move\n",
            "   763/50000: episode: 483, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9894.661 -546.58295\n",
            "wrong_move\n",
            "   764/50000: episode: 484, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6207.5894 -3985.111\n",
            "wrong_move\n",
            "   765/50000: episode: 485, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7520.864 -1824.1356\n",
            "wrong_move\n",
            "   769/50000: episode: 486, duration: 0.196s, episode steps:   4, steps per second:  20, episode reward: -5953.000, mean reward: -1488.250 [-5000.000, -1.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6908.4937 -1836.6381\n",
            "wrong_move\n",
            "   774/50000: episode: 487, duration: 0.235s, episode steps:   5, steps per second:  21, episode reward: -5994.000, mean reward: -1198.800 [-5000.000, -1.000], mean action: 233.800 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5604.9614 3319.4856\n",
            "wrong_move\n",
            "   775/50000: episode: 488, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6102.131 -3972.6016\n",
            "wrong_move\n",
            "   776/50000: episode: 489, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -11890.002 -2349.5586\n",
            "wrong_move\n",
            "   777/50000: episode: 490, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6367.7505 -3384.2888\n",
            "wrong_move\n",
            "   778/50000: episode: 491, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6078.8335 -4067.8708\n",
            "wrong_move\n",
            "   779/50000: episode: 492, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6149.3545 -4078.1953\n",
            "wrong_move\n",
            "   780/50000: episode: 493, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3383.000 [3383.000, 3383.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6248.54 -3862.3784\n",
            "wrong_move\n",
            "   781/50000: episode: 494, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6477.98 -3995.0593\n",
            "wrong_move\n",
            "   782/50000: episode: 495, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9323.32 -2491.8967\n",
            "wrong_move\n",
            "   783/50000: episode: 496, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5967.906 -3810.486\n",
            "wrong_move\n",
            "   784/50000: episode: 497, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6896.2227 -2598.0232\n",
            "wrong_move\n",
            "   788/50000: episode: 498, duration: 0.223s, episode steps:   4, steps per second:  18, episode reward: -5943.000, mean reward: -1485.750 [-5000.000, -1.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9184.636 -3464.829\n",
            "wrong_move\n",
            "   789/50000: episode: 499, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6383.884 -3991.3992\n",
            "wrong_move\n",
            "   790/50000: episode: 500, duration: 0.006s, episode steps:   1, steps per second: 155, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10186.319 -2055.926\n",
            "wrong_move\n",
            "   791/50000: episode: 501, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10062.379 -2194.6946\n",
            "wrong_move\n",
            "   792/50000: episode: 502, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9416.344 -2852.016\n",
            "wrong_move\n",
            "   793/50000: episode: 503, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6238.069 -4042.7083\n",
            "wrong_move\n",
            "   794/50000: episode: 504, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10231.009 -2502.382\n",
            "wrong_move\n",
            "   795/50000: episode: 505, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7490.027 -3038.1511\n",
            "wrong_move\n",
            "   796/50000: episode: 506, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6465.362 -1775.1465\n",
            "wrong_move\n",
            "   797/50000: episode: 507, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5248.6904 -3910.0767\n",
            "wrong_move\n",
            "   798/50000: episode: 508, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2478.000 [2478.000, 2478.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8636.938 -2430.6243\n",
            "wrong_move\n",
            "   799/50000: episode: 509, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6684.256 -3892.2107\n",
            "wrong_move\n",
            "   800/50000: episode: 510, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7568.526 4965.3003\n",
            "wrong_move\n",
            "   802/50000: episode: 511, duration: 0.069s, episode steps:   2, steps per second:  29, episode reward: -5031.000, mean reward: -2515.500 [-5000.000, -31.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6631.151 -3657.0347\n",
            "wrong_move\n",
            "   803/50000: episode: 512, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5266.8975 -4003.6204\n",
            "wrong_move\n",
            "   804/50000: episode: 513, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 616.000 [616.000, 616.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6234.862 -2668.5461\n",
            "wrong_move\n",
            "   805/50000: episode: 514, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6304.2124 -2756.0605\n",
            "wrong_move\n",
            "   806/50000: episode: 515, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9210.22 -2104.7424\n",
            "wrong_move\n",
            "   807/50000: episode: 516, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7450.901 2916.9963\n",
            "wrong_move\n",
            "   808/50000: episode: 517, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9728.744 565.3289\n",
            "wrong_move\n",
            "   809/50000: episode: 518, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5938.3745 -338.7132\n",
            "wrong_move\n",
            "   811/50000: episode: 519, duration: 0.089s, episode steps:   2, steps per second:  22, episode reward: -5081.000, mean reward: -2540.500 [-5000.000, -81.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6212.1816 -3814.9448\n",
            "wrong_move\n",
            "   812/50000: episode: 520, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9454.427 -1873.6669\n",
            "wrong_move\n",
            "   813/50000: episode: 521, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5948.1567 1050.4845\n",
            "wrong_move\n",
            "   814/50000: episode: 522, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6588.625 -3254.0793\n",
            "wrong_move\n",
            "   816/50000: episode: 523, duration: 0.076s, episode steps:   2, steps per second:  26, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6347.1235 -4033.42\n",
            "wrong_move\n",
            "   817/50000: episode: 524, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6874.6646 -3259.9724\n",
            "wrong_move\n",
            "   818/50000: episode: 525, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5940.063 3100.3916\n",
            "wrong_move\n",
            "   819/50000: episode: 526, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7943.601 -629.82196\n",
            "wrong_move\n",
            "   822/50000: episode: 527, duration: 0.126s, episode steps:   3, steps per second:  24, episode reward: -5022.000, mean reward: -1674.000 [-5000.000, -11.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10452.02 -1832.4546\n",
            "wrong_move\n",
            "   832/50000: episode: 528, duration: 0.532s, episode steps:  10, steps per second:  19, episode reward: -5929.000, mean reward: -592.900 [-5000.000, -1.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10800.048 -1851.9674\n",
            "wrong_move\n",
            "   833/50000: episode: 529, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6817.15 -2337.5247\n",
            "wrong_move\n",
            "   834/50000: episode: 530, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5929.792 -1136.5332\n",
            "wrong_move\n",
            "   835/50000: episode: 531, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6265.3794 -3948.5142\n",
            "wrong_move\n",
            "   836/50000: episode: 532, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5990.33 -830.84344\n",
            "wrong_move\n",
            "   844/50000: episode: 533, duration: 0.390s, episode steps:   8, steps per second:  20, episode reward: -5127.000, mean reward: -640.875 [-5000.000, -1.000], mean action: 235.375 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6360.6646 -3930.2212\n",
            "wrong_move\n",
            "   845/50000: episode: 534, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5207.6987 -4028.5457\n",
            "wrong_move\n",
            "   846/50000: episode: 535, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 616.000 [616.000, 616.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6234.0215 -3035.178\n",
            "wrong_move\n",
            "   847/50000: episode: 536, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5858.6826 -2392.4912\n",
            "wrong_move\n",
            "   848/50000: episode: 537, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10626.269 -1662.9557\n",
            "wrong_move\n",
            "   855/50000: episode: 538, duration: 0.340s, episode steps:   7, steps per second:  21, episode reward: -5146.000, mean reward: -735.143 [-5000.000, -1.000], mean action: 232.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6158.0747 -2966.5107\n",
            "wrong_move\n",
            "   856/50000: episode: 539, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6357.569 -4040.1643\n",
            "wrong_move\n",
            "   857/50000: episode: 540, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6028.7573 -2100.3328\n",
            "Lose\n",
            "   863/50000: episode: 541, duration: 0.292s, episode steps:   6, steps per second:  21, episode reward: -1936.000, mean reward: -322.667 [-931.000, -1.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7874.2476 -3779.3828\n",
            "wrong_move\n",
            "   864/50000: episode: 542, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5942.5015 180.65642\n",
            "wrong_move\n",
            "   865/50000: episode: 543, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -11419.283 -276.6127\n",
            "wrong_move\n",
            "   866/50000: episode: 544, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -13018.059 5031.4956\n",
            "wrong_move\n",
            "   867/50000: episode: 545, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8792.029 -3394.0962\n",
            "wrong_move\n",
            "   868/50000: episode: 546, duration: 0.017s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2742.000 [2742.000, 2742.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5992.237 -2315.7712\n",
            "wrong_move\n",
            "   869/50000: episode: 547, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5974.5005 644.69366\n",
            "wrong_move\n",
            "   870/50000: episode: 548, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5751.9863 -234.16353\n",
            "wrong_move\n",
            "   871/50000: episode: 549, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5949.0366 -600.54816\n",
            "wrong_move\n",
            "   872/50000: episode: 550, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6422.8647 -3995.6162\n",
            "wrong_move\n",
            "   873/50000: episode: 551, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9980.033 -1819.7754\n",
            "wrong_move\n",
            "   874/50000: episode: 552, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9315.013 2252.6501\n",
            "wrong_move\n",
            "   875/50000: episode: 553, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6033.6577 -2767.6863\n",
            "wrong_move\n",
            "   881/50000: episode: 554, duration: 0.272s, episode steps:   6, steps per second:  22, episode reward: -5125.000, mean reward: -854.167 [-5000.000, -1.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5678.2505 513.81964\n",
            "wrong_move\n",
            "   889/50000: episode: 555, duration: 0.433s, episode steps:   8, steps per second:  18, episode reward: -6017.000, mean reward: -752.125 [-5000.000, -1.000], mean action: 235.375 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6170.522 -4025.8333\n",
            "wrong_move\n",
            "   890/50000: episode: 556, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9486.701 -1325.2455\n",
            "wrong_move\n",
            "   891/50000: episode: 557, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5099.6523 -3866.9048\n",
            "wrong_move\n",
            "   892/50000: episode: 558, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 843.000 [843.000, 843.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6333.9243 -3975.2842\n",
            "wrong_move\n",
            "   893/50000: episode: 559, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6050.0522 -1071.6333\n",
            "wrong_move\n",
            "   894/50000: episode: 560, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6138.7583 -4029.4656\n",
            "wrong_move\n",
            "   895/50000: episode: 561, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9695.848 1362.5145\n",
            "wrong_move\n",
            "   896/50000: episode: 562, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8785.32 -1692.2777\n",
            "wrong_move\n",
            "   897/50000: episode: 563, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7861.9556 -2811.309\n",
            "wrong_move\n",
            "   898/50000: episode: 564, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9828.809 -2352.3813\n",
            "wrong_move\n",
            "   899/50000: episode: 565, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6383.884 -3991.3992\n",
            "wrong_move\n",
            "   900/50000: episode: 566, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7905.57 -293.8971\n",
            "wrong_move\n",
            "   901/50000: episode: 567, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5483.7637 -3682.5002\n",
            "wrong_move\n",
            "   902/50000: episode: 568, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6817.918 -2078.863\n",
            "wrong_move\n",
            "   903/50000: episode: 569, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5626.191 -1856.9453\n",
            "wrong_move\n",
            "   904/50000: episode: 570, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6398.5586 -2930.6091\n",
            "wrong_move\n",
            "   905/50000: episode: 571, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8873.4375 -2324.177\n",
            "wrong_move\n",
            "   906/50000: episode: 572, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6115.766 -4108.2183\n",
            "wrong_move\n",
            "   907/50000: episode: 573, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5460.1167 -236.61275\n",
            "wrong_move\n",
            "   909/50000: episode: 574, duration: 0.102s, episode steps:   2, steps per second:  20, episode reward: -5941.000, mean reward: -2970.500 [-5000.000, -941.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5942.357 -656.2557\n",
            "wrong_move\n",
            "   915/50000: episode: 575, duration: 0.343s, episode steps:   6, steps per second:  18, episode reward: -5975.000, mean reward: -995.833 [-5000.000, -1.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7530.648 298.578\n",
            "wrong_move\n",
            "   916/50000: episode: 576, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10362.471 -2600.4517\n",
            "wrong_move\n",
            "   917/50000: episode: 577, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6367.819 -4049.563\n",
            "wrong_move\n",
            "   918/50000: episode: 578, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8857.071 -2594.8\n",
            "wrong_move\n",
            "   919/50000: episode: 579, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6122.328 -3752.3357\n",
            "wrong_move\n",
            "   920/50000: episode: 580, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6572.688 1551.4823\n",
            "wrong_move\n",
            "   922/50000: episode: 581, duration: 0.076s, episode steps:   2, steps per second:  26, episode reward: -5901.000, mean reward: -2950.500 [-5000.000, -901.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8193.217 -3384.7249\n",
            "wrong_move\n",
            "   923/50000: episode: 582, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6491.2285 -3977.9438\n",
            "wrong_move\n",
            "   924/50000: episode: 583, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9338.026 -2322.2166\n",
            "wrong_move\n",
            "   925/50000: episode: 584, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6443.0874 -3989.683\n",
            "wrong_move\n",
            "   926/50000: episode: 585, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8305.79 -2431.204\n",
            "wrong_move\n",
            "   927/50000: episode: 586, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6056.601 -4049.7336\n",
            "wrong_move\n",
            "   928/50000: episode: 587, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6107.4517 -2755.7256\n",
            "wrong_move\n",
            "   929/50000: episode: 588, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6299.018 -4007.7427\n",
            "wrong_move\n",
            "   930/50000: episode: 589, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7431.6704 -3364.6353\n",
            "wrong_move\n",
            "   931/50000: episode: 590, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6403.9604 -3084.224\n",
            "wrong_move\n",
            "   932/50000: episode: 591, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9551.512 -2307.4731\n",
            "wrong_move\n",
            "   933/50000: episode: 592, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6006.336 -4054.5315\n",
            "wrong_move\n",
            "   934/50000: episode: 593, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5551.584 -4302.034\n",
            "wrong_move\n",
            "   935/50000: episode: 594, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -2091803.1 4258994.0\n",
            "wrong_move\n",
            "   936/50000: episode: 595, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6316.7114 -3952.2202\n",
            "wrong_move\n",
            "   937/50000: episode: 596, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8722.935 -2916.2842\n",
            "wrong_move\n",
            "   938/50000: episode: 597, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6313.0 -4009.1895\n",
            "wrong_move\n",
            "   939/50000: episode: 598, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6353.864 -3988.8027\n",
            "wrong_move\n",
            "   940/50000: episode: 599, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6370.108 -3995.9297\n",
            "wrong_move\n",
            "   941/50000: episode: 600, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6334.692 -473.33868\n",
            "wrong_move\n",
            "   942/50000: episode: 601, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5601.4087 2921.1462\n",
            "wrong_move\n",
            "   943/50000: episode: 602, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 734.000 [734.000, 734.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9646.81 973.8379\n",
            "wrong_move\n",
            "   944/50000: episode: 603, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6074.3584 -3433.5896\n",
            "wrong_move\n",
            "   945/50000: episode: 604, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6383.884 -3991.3992\n",
            "wrong_move\n",
            "   946/50000: episode: 605, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9844.284 -2133.6672\n",
            "wrong_move\n",
            "   947/50000: episode: 606, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9452.273 1894.7009\n",
            "wrong_move\n",
            "   949/50000: episode: 607, duration: 0.108s, episode steps:   2, steps per second:  18, episode reward: -5911.000, mean reward: -2955.500 [-5000.000, -911.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10513.153 -1687.7637\n",
            "wrong_move\n",
            "   952/50000: episode: 608, duration: 0.164s, episode steps:   3, steps per second:  18, episode reward: -5052.000, mean reward: -1684.000 [-5000.000, -21.000], mean action: 238.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8958.456 -2662.103\n",
            "wrong_move\n",
            "   953/50000: episode: 609, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6656.6685 -2852.8855\n",
            "wrong_move\n",
            "   954/50000: episode: 610, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5972.72 -3233.1233\n",
            "wrong_move\n",
            "   955/50000: episode: 611, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9823.677 -2134.2583\n",
            "wrong_move\n",
            "   956/50000: episode: 612, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10331.178 -1852.5604\n",
            "wrong_move\n",
            "   963/50000: episode: 613, duration: 0.353s, episode steps:   7, steps per second:  20, episode reward: -5056.000, mean reward: -722.286 [-5000.000, -1.000], mean action: 232.000 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6102.1226 874.3675\n",
            "wrong_move\n",
            "   965/50000: episode: 614, duration: 0.028s, episode steps:   2, steps per second:  72, episode reward: -5921.000, mean reward: -2960.500 [-5000.000, -921.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8067.8374 3364.051\n",
            "wrong_move\n",
            "   966/50000: episode: 615, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6439.6187 -3910.716\n",
            "wrong_move\n",
            "   967/50000: episode: 616, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -5339.476 -3976.7175\n",
            "wrong_move\n",
            "   968/50000: episode: 617, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 616.000 [616.000, 616.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9254.299 -2335.2336\n",
            "wrong_move\n",
            "   969/50000: episode: 618, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6123.237 -3968.3044\n",
            "wrong_move\n",
            "   970/50000: episode: 619, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6498.728 2875.9465\n",
            "wrong_move\n",
            "   972/50000: episode: 620, duration: 0.108s, episode steps:   2, steps per second:  19, episode reward: -5951.000, mean reward: -2975.500 [-5000.000, -951.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9217.836 -185.25703\n",
            "wrong_move\n",
            "   973/50000: episode: 621, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6365.3745 -2947.589\n",
            "wrong_move\n",
            "   974/50000: episode: 622, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6353.8667 -4045.7607\n",
            "wrong_move\n",
            "   975/50000: episode: 623, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6352.4937 -3968.2068\n",
            "wrong_move\n",
            "   976/50000: episode: 624, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8579.981 -616.88153\n",
            "wrong_move\n",
            "   981/50000: episode: 625, duration: 0.253s, episode steps:   5, steps per second:  20, episode reward: -5914.000, mean reward: -1182.800 [-5000.000, -1.000], mean action: 233.800 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6287.029 -3991.1003\n",
            "wrong_move\n",
            "   982/50000: episode: 626, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9987.917 -2065.4814\n",
            "wrong_move\n",
            "   983/50000: episode: 627, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6397.4824 -2192.8298\n",
            "wrong_move\n",
            "   984/50000: episode: 628, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10637.103 -1446.3303\n",
            "wrong_move\n",
            "   985/50000: episode: 629, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2425.000 [2425.000, 2425.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10255.238 26517.201\n",
            "wrong_move\n",
            "   986/50000: episode: 630, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6384.846 -3938.029\n",
            "wrong_move\n",
            "   987/50000: episode: 631, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6388.646 -4010.9888\n",
            "wrong_move\n",
            "   988/50000: episode: 632, duration: 0.007s, episode steps:   1, steps per second: 133, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10141.091 350.88202\n",
            "wrong_move\n",
            "   989/50000: episode: 633, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10349.386 -2168.2883\n",
            "wrong_move\n",
            "   993/50000: episode: 634, duration: 0.224s, episode steps:   4, steps per second:  18, episode reward: -5093.000, mean reward: -1273.250 [-5000.000, -1.000], mean action: 227.500 [196.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6372.715 -3990.561\n",
            "wrong_move\n",
            "   994/50000: episode: 635, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9385.81 -1655.6143\n",
            "wrong_move\n",
            "   995/50000: episode: 636, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8086.7783 -2576.566\n",
            "wrong_move\n",
            "   996/50000: episode: 637, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6066.343 -825.8311\n",
            "wrong_move\n",
            "   997/50000: episode: 638, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9521.244 778.5238\n",
            "wrong_move\n",
            "   998/50000: episode: 639, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7810.5225 -2899.9138\n",
            "wrong_move\n",
            "   999/50000: episode: 640, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6188.636 -3048.9705\n",
            "wrong_move\n",
            "  1000/50000: episode: 641, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -6263.7446 -2187.5432\n",
            "wrong_move\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  1001/50000: episode: 642, duration: 1.351s, episode steps:   1, steps per second:   1, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9590.255 -2580.6077\n",
            "wrong_move\n",
            "  1002/50000: episode: 643, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: 46070352.000000, mae: 4409.521973, mean_q: 2853.369141\n",
            "Val: -17412.191 -2739.9993\n",
            "wrong_move\n",
            "  1003/50000: episode: 644, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: 21498008.000000, mae: 4474.505859, mean_q: 978.911682\n",
            "Val: -10033.435 -3565.292\n",
            "wrong_move\n",
            "  1004/50000: episode: 645, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: 22594838.000000, mae: 4534.261719, mean_q: -195.836731\n",
            "Val: -19721.79 -4209.222\n",
            "wrong_move\n",
            "  1005/50000: episode: 646, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3469.000 [3469.000, 3469.000],  loss: 34749284.000000, mae: 4571.660156, mean_q: 823.568604\n",
            "Val: -6751.4116 3166.5293\n",
            "wrong_move\n",
            "  1006/50000: episode: 647, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 37798288.000000, mae: 4604.107422, mean_q: 682.412170\n",
            "Val: -26219.291 -4193.5254\n",
            "wrong_move\n",
            "  1007/50000: episode: 648, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2724.000 [2724.000, 2724.000],  loss: 13040959.000000, mae: 4641.585938, mean_q: -538.231567\n",
            "Val: -28570.664 -4141.581\n",
            "wrong_move\n",
            "  1008/50000: episode: 649, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2724.000 [2724.000, 2724.000],  loss: 26690700.000000, mae: 4674.541016, mean_q: -375.152222\n",
            "Val: -31176.256 -4086.6323\n",
            "wrong_move\n",
            "  1009/50000: episode: 650, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3383.000 [3383.000, 3383.000],  loss: 18270928.000000, mae: 4693.169922, mean_q: -1074.015381\n",
            "Val: -31084.623 -3857.1377\n",
            "wrong_move\n",
            "  1010/50000: episode: 651, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3383.000 [3383.000, 3383.000],  loss: 14028243.000000, mae: 4694.487305, mean_q: -1156.875732\n",
            "Val: -32080.291 -3756.8071\n",
            "wrong_move\n",
            "  1011/50000: episode: 652, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3383.000 [3383.000, 3383.000],  loss: 21704560.000000, mae: 4691.022949, mean_q: -1046.720825\n",
            "Val: -34787.7 -3638.46\n",
            "wrong_move\n",
            "  1012/50000: episode: 653, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3383.000 [3383.000, 3383.000],  loss: 25189188.000000, mae: 4685.589844, mean_q: -243.029175\n",
            "Val: -32976.94 -3621.3918\n",
            "wrong_move\n",
            "  1013/50000: episode: 654, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3383.000 [3383.000, 3383.000],  loss: 15360956.000000, mae: 4677.460938, mean_q: -923.612427\n",
            "Val: -35119.098 -3483.1948\n",
            "wrong_move\n",
            "  1014/50000: episode: 655, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3383.000 [3383.000, 3383.000],  loss: 18201716.000000, mae: 4666.732422, mean_q: -559.836060\n",
            "Val: -40010.11 -3608.3372\n",
            "wrong_move\n",
            "  1015/50000: episode: 656, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3383.000 [3383.000, 3383.000],  loss: 10596120.000000, mae: 4651.675781, mean_q: -1194.054199\n",
            "Val: -42779.58 -3722.4006\n",
            "wrong_move\n",
            "  1016/50000: episode: 657, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3383.000 [3383.000, 3383.000],  loss: 16081654.000000, mae: 4640.971191, mean_q: -193.043167\n",
            "Val: -30209.676 -4005.271\n",
            "wrong_move\n",
            "  1017/50000: episode: 658, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3383.000 [3383.000, 3383.000],  loss: 11966033.000000, mae: 4635.505859, mean_q: -101.969849\n",
            "Val: -28901.383 -4140.197\n",
            "wrong_move\n",
            "  1018/50000: episode: 659, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3383.000 [3383.000, 3383.000],  loss: 17028082.000000, mae: 4628.460938, mean_q: -437.138428\n",
            "Val: -28863.107 -4155.4375\n",
            "wrong_move\n",
            "  1019/50000: episode: 660, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2289.000 [2289.000, 2289.000],  loss: 42949344.000000, mae: 4626.067871, mean_q: 203.206421\n",
            "Val: -29087.74 -4209.882\n",
            "wrong_move\n",
            "  1020/50000: episode: 661, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2289.000 [2289.000, 2289.000],  loss: 22459072.000000, mae: 4634.256836, mean_q: -281.525879\n",
            "Val: -27207.018 -3809.5002\n",
            "wrong_move\n",
            "  1021/50000: episode: 662, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2289.000 [2289.000, 2289.000],  loss: 11448298.000000, mae: 4636.091309, mean_q: -1359.889282\n",
            "Val: -28803.816 -4176.11\n",
            "wrong_move\n",
            "  1022/50000: episode: 663, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2289.000 [2289.000, 2289.000],  loss: 20844954.000000, mae: 4636.557129, mean_q: -794.403992\n",
            "Val: -29164.246 -4233.896\n",
            "wrong_move\n",
            "  1023/50000: episode: 664, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2289.000 [2289.000, 2289.000],  loss: 11686154.000000, mae: 4634.755371, mean_q: -1428.645996\n",
            "Val: -29035.332 -4240.1655\n",
            "wrong_move\n",
            "  1024/50000: episode: 665, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2289.000 [2289.000, 2289.000],  loss: 32666616.000000, mae: 4632.327148, mean_q: 200.563110\n",
            "Val: -29021.027 -4293.89\n",
            "wrong_move\n",
            "  1025/50000: episode: 666, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2289.000 [2289.000, 2289.000],  loss: 13925020.000000, mae: 4630.766602, mean_q: -279.176605\n",
            "Val: -28932.504 -4615.0957\n",
            "wrong_move\n",
            "  1026/50000: episode: 667, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2289.000 [2289.000, 2289.000],  loss: 29191572.000000, mae: 4643.591797, mean_q: 682.436096\n",
            "Val: -29637.49 -4699.867\n",
            "wrong_move\n",
            "  1027/50000: episode: 668, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 734.000 [734.000, 734.000],  loss: 13279454.000000, mae: 4659.354980, mean_q: -968.368530\n",
            "Val: -29293.74 -4832.807\n",
            "wrong_move\n",
            "  1028/50000: episode: 669, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 734.000 [734.000, 734.000],  loss: 6611232.000000, mae: 4684.474609, mean_q: -491.066437\n",
            "Val: -30510.76 -4699.044\n",
            "wrong_move\n",
            "  1029/50000: episode: 670, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 734.000 [734.000, 734.000],  loss: 12500306.000000, mae: 4700.008301, mean_q: -1012.502380\n",
            "Val: -37809.535 -3663.263\n",
            "wrong_move\n",
            "  1030/50000: episode: 671, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 734.000 [734.000, 734.000],  loss: 10819599.000000, mae: 4709.698242, mean_q: -835.102234\n",
            "Val: -30765.158 -4624.899\n",
            "wrong_move\n",
            "  1031/50000: episode: 672, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 734.000 [734.000, 734.000],  loss: 34480392.000000, mae: 4716.770508, mean_q: -106.644928\n",
            "Val: -29277.838 -4694.054\n",
            "wrong_move\n",
            "  1032/50000: episode: 673, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 734.000 [734.000, 734.000],  loss: 21565164.000000, mae: 4720.819336, mean_q: 239.398193\n",
            "Val: -28462.623 -4696.0317\n",
            "wrong_move\n",
            "  1033/50000: episode: 674, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 734.000 [734.000, 734.000],  loss: 15208562.000000, mae: 4730.361328, mean_q: -902.159668\n",
            "Val: -29922.076 -4543.655\n",
            "wrong_move\n",
            "  1034/50000: episode: 675, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 734.000 [734.000, 734.000],  loss: 20596718.000000, mae: 4739.144531, mean_q: -1051.499512\n",
            "Val: -30098.418 -2912.6855\n",
            "wrong_move\n",
            "  1035/50000: episode: 676, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 734.000 [734.000, 734.000],  loss: 9831072.000000, mae: 4741.254883, mean_q: -867.398804\n",
            "Val: -32781.402 -3142.8262\n",
            "wrong_move\n",
            "  1036/50000: episode: 677, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 734.000 [734.000, 734.000],  loss: 16664420.000000, mae: 4740.373047, mean_q: -63.751099\n",
            "Val: -31192.922 -3699.8528\n",
            "wrong_move\n",
            "  1037/50000: episode: 678, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 734.000 [734.000, 734.000],  loss: 23354180.000000, mae: 4732.685547, mean_q: -704.808716\n",
            "Val: -23790.094 -3442.1555\n",
            "wrong_move\n",
            "  1038/50000: episode: 679, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 734.000 [734.000, 734.000],  loss: 19165306.000000, mae: 4728.372559, mean_q: -1316.875000\n",
            "Val: -28274.236 -5209.4653\n",
            "wrong_move\n",
            "  1039/50000: episode: 680, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: 17491604.000000, mae: 4727.851562, mean_q: -1096.335205\n",
            "Val: -27669.248 -5219.102\n",
            "wrong_move\n",
            "  1040/50000: episode: 681, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: 12859891.000000, mae: 4731.188477, mean_q: -835.838623\n",
            "Val: -26766.621 -5287.556\n",
            "wrong_move\n",
            "  1041/50000: episode: 682, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: 9893826.000000, mae: 4735.516602, mean_q: -1991.280640\n",
            "Val: -26018.783 -5317.255\n",
            "wrong_move\n",
            "  1042/50000: episode: 683, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: 17807164.000000, mae: 4736.641602, mean_q: -1620.774414\n",
            "Val: -41538.79 -3556.972\n",
            "wrong_move\n",
            "  1043/50000: episode: 684, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 144.000 [144.000, 144.000],  loss: 17787660.000000, mae: 4722.323242, mean_q: -520.157227\n",
            "Val: -23914.703 -5287.328\n",
            "wrong_move\n",
            "  1044/50000: episode: 685, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: 7684767.000000, mae: 4708.025879, mean_q: -1036.309082\n",
            "Val: -20615.945 -4733.0464\n",
            "wrong_move\n",
            "  1046/50000: episode: 686, duration: 0.136s, episode steps:   2, steps per second:  15, episode reward: -5911.000, mean reward: -2955.500 [-5000.000, -911.000], mean action: 465.000 [196.000, 734.000],  loss: 13318510.000000, mae: 4697.420898, mean_q: -1622.151611\n",
            "Val: -25104.6 -5452.0103\n",
            "wrong_move\n",
            "  1047/50000: episode: 687, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: 11004448.000000, mae: 4692.261719, mean_q: -902.495422\n",
            "Val: -25375.957 -4130.043\n",
            "wrong_move\n",
            "  1048/50000: episode: 688, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3350.000 [3350.000, 3350.000],  loss: 5799544.000000, mae: 4686.669922, mean_q: -1452.939209\n",
            "Val: -23722.705 -3971.966\n",
            "wrong_move\n",
            "  1049/50000: episode: 689, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3350.000 [3350.000, 3350.000],  loss: 9631027.000000, mae: 4674.527344, mean_q: -1546.052490\n",
            "Val: -22867.535 -5387.635\n",
            "wrong_move\n",
            "  1050/50000: episode: 690, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: 16856926.000000, mae: 4665.633301, mean_q: -1147.980225\n",
            "Val: -22289.932 -5353.6523\n",
            "wrong_move\n",
            "  1051/50000: episode: 691, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: 9698010.000000, mae: 4661.602051, mean_q: -1313.172119\n",
            "Val: -22069.613 -3709.0952\n",
            "wrong_move\n",
            "  1052/50000: episode: 692, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2724.000 [2724.000, 2724.000],  loss: 9551934.000000, mae: 4663.059082, mean_q: -1417.794678\n",
            "Val: -21657.53 -5294.734\n",
            "wrong_move\n",
            "  1053/50000: episode: 693, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: 10237524.000000, mae: 4660.796387, mean_q: -1178.083984\n",
            "Val: -22496.041 -4516.131\n",
            "wrong_move\n",
            "  1055/50000: episode: 694, duration: 0.135s, episode steps:   2, steps per second:  15, episode reward: -5051.000, mean reward: -2525.500 [-5000.000, -51.000], mean action: 196.000 [196.000, 196.000],  loss: 6804117.000000, mae: 4661.629883, mean_q: -1342.093506\n",
            "Val: -27966.023 -3658.9417\n",
            "wrong_move\n",
            "  1056/50000: episode: 695, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 9682519.000000, mae: 4666.097656, mean_q: -1663.635376\n",
            "Val: -20422.242 -3630.8286\n",
            "wrong_move\n",
            "  1057/50000: episode: 696, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3543.000 [3543.000, 3543.000],  loss: 13885658.000000, mae: 4670.129883, mean_q: -1647.517822\n",
            "Val: -21595.574 -3756.6562\n",
            "wrong_move\n",
            "  1058/50000: episode: 697, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3482.000 [3482.000, 3482.000],  loss: 9221358.000000, mae: 4679.402344, mean_q: -926.140015\n",
            "Val: -19293.84 -4578.715\n",
            "wrong_move\n",
            "  1059/50000: episode: 698, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3543.000 [3543.000, 3543.000],  loss: 16971572.000000, mae: 4692.403320, mean_q: -698.795044\n",
            "Val: -19706.162 -4421.309\n",
            "wrong_move\n",
            "  1060/50000: episode: 699, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3543.000 [3543.000, 3543.000],  loss: 7018628.000000, mae: 4705.578125, mean_q: -1120.239136\n",
            "Val: -19730.191 -4284.0234\n",
            "wrong_move\n",
            "  1061/50000: episode: 700, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3543.000 [3543.000, 3543.000],  loss: 7179718.000000, mae: 4711.968750, mean_q: -1293.819580\n",
            "Val: -19907.414 -4378.6553\n",
            "wrong_move\n",
            "  1062/50000: episode: 701, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3543.000 [3543.000, 3543.000],  loss: 7924952.000000, mae: 4718.422852, mean_q: -1745.201660\n",
            "Val: -19174.617 -3886.3315\n",
            "wrong_move\n",
            "  1063/50000: episode: 702, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3543.000 [3543.000, 3543.000],  loss: 12795777.000000, mae: 4732.041992, mean_q: -1192.384277\n",
            "Val: -21436.78 -3180.0244\n",
            "wrong_move\n",
            "  1064/50000: episode: 703, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3543.000 [3543.000, 3543.000],  loss: 8199054.000000, mae: 4731.791016, mean_q: -1226.816162\n",
            "Val: -14297.767 -3895.5217\n",
            "wrong_move\n",
            "  1065/50000: episode: 704, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2478.000 [2478.000, 2478.000],  loss: 12111810.000000, mae: 4725.882324, mean_q: -1422.687744\n",
            "Val: -17627.676 -4078.6484\n",
            "wrong_move\n",
            "  1066/50000: episode: 705, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2478.000 [2478.000, 2478.000],  loss: 8898565.000000, mae: 4713.054199, mean_q: -1442.049316\n",
            "Val: -18291.5 -4382.5063\n",
            "wrong_move\n",
            "  1067/50000: episode: 706, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2478.000 [2478.000, 2478.000],  loss: 6795321.000000, mae: 4703.065430, mean_q: -1681.401245\n",
            "Val: -17951.834 -4333.347\n",
            "wrong_move\n",
            "  1068/50000: episode: 707, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2478.000 [2478.000, 2478.000],  loss: 13688362.000000, mae: 4694.390137, mean_q: -1767.762695\n",
            "Val: -16848.092 -4190.5757\n",
            "wrong_move\n",
            "  1069/50000: episode: 708, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2478.000 [2478.000, 2478.000],  loss: 12465110.000000, mae: 4684.730957, mean_q: -1019.180176\n",
            "Val: -16110.354 -2907.488\n",
            "wrong_move\n",
            "  1070/50000: episode: 709, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3469.000 [3469.000, 3469.000],  loss: 6925046.000000, mae: 4678.831055, mean_q: -1324.131836\n",
            "Val: -16047.411 -2895.7744\n",
            "wrong_move\n",
            "  1071/50000: episode: 710, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3469.000 [3469.000, 3469.000],  loss: 13133926.000000, mae: 4676.604492, mean_q: -699.199036\n",
            "Val: -15638.981 -2894.0957\n",
            "wrong_move\n",
            "  1072/50000: episode: 711, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 16705530.000000, mae: 4675.551270, mean_q: -1372.724976\n",
            "Val: -15501.221 -2756.3013\n",
            "wrong_move\n",
            "  1073/50000: episode: 712, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 11404660.000000, mae: 4678.122070, mean_q: -1313.380371\n",
            "Val: -16098.034 -4104.735\n",
            "wrong_move\n",
            "  1074/50000: episode: 713, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3469.000 [3469.000, 3469.000],  loss: 12796346.000000, mae: 4680.442383, mean_q: -1082.875977\n",
            "Val: -15045.635 -3755.5654\n",
            "wrong_move\n",
            "  1075/50000: episode: 714, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3469.000 [3469.000, 3469.000],  loss: 6207740.000000, mae: 4680.991211, mean_q: -1664.763306\n",
            "Val: -14699.254 -3430.8364\n",
            "wrong_move\n",
            "  1076/50000: episode: 715, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 1894066.750000, mae: 4672.773926, mean_q: -1002.627319\n",
            "Val: -14402.976 -2347.5645\n",
            "wrong_move\n",
            "  1077/50000: episode: 716, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 14040926.000000, mae: 4661.682617, mean_q: -837.032837\n",
            "Val: -15394.663 -2652.5393\n",
            "wrong_move\n",
            "  1078/50000: episode: 717, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3469.000 [3469.000, 3469.000],  loss: 9837283.000000, mae: 4649.294922, mean_q: -1441.519043\n",
            "Val: -13468.999 -2112.4846\n",
            "wrong_move\n",
            "  1079/50000: episode: 718, duration: 0.043s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 8403073.000000, mae: 4644.807617, mean_q: -917.008911\n",
            "Val: -11535.042 -2833.4963\n",
            "wrong_move\n",
            "  1080/50000: episode: 719, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3469.000 [3469.000, 3469.000],  loss: 3561054.500000, mae: 4653.587891, mean_q: -1186.968750\n",
            "Val: -13436.699 -2226.9778\n",
            "wrong_move\n",
            "  1081/50000: episode: 720, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 9861538.000000, mae: 4663.074707, mean_q: -807.953613\n",
            "Val: -14778.078 -2816.347\n",
            "wrong_move\n",
            "  1082/50000: episode: 721, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3469.000 [3469.000, 3469.000],  loss: 11829223.000000, mae: 4676.551758, mean_q: -374.297607\n",
            "Val: -12612.592 -1772.5076\n",
            "wrong_move\n",
            "  1083/50000: episode: 722, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 3594249.000000, mae: 4685.084961, mean_q: -1781.658691\n",
            "Val: -12633.55 -1939.489\n",
            "wrong_move\n",
            "  1084/50000: episode: 723, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 2560269.250000, mae: 4697.667480, mean_q: -1521.981567\n",
            "Val: -12722.626 -3372.9683\n",
            "wrong_move\n",
            "  1085/50000: episode: 724, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 3611494.750000, mae: 4704.330566, mean_q: -1441.758789\n",
            "Val: -12507.658 -2269.353\n",
            "wrong_move\n",
            "  1086/50000: episode: 725, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 7768121.500000, mae: 4701.094727, mean_q: -1896.521729\n",
            "Val: -13276.104 -3696.5354\n",
            "wrong_move\n",
            "  1087/50000: episode: 726, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3469.000 [3469.000, 3469.000],  loss: 5009227.000000, mae: 4697.344727, mean_q: -1638.496948\n",
            "Val: -16534.504 -3775.6824\n",
            "wrong_move\n",
            "  1088/50000: episode: 727, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3482.000 [3482.000, 3482.000],  loss: 10225749.000000, mae: 4700.740234, mean_q: -1247.788696\n",
            "Val: -12529.7 -2507.9421\n",
            "wrong_move\n",
            "  1089/50000: episode: 728, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 5596806.500000, mae: 4708.859375, mean_q: -1753.166992\n",
            "Val: -12406.888 -4153.104\n",
            "wrong_move\n",
            "  1090/50000: episode: 729, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3482.000 [3482.000, 3482.000],  loss: 3702573.750000, mae: 4710.817383, mean_q: -2491.400879\n",
            "Val: -11677.083 -3794.0962\n",
            "wrong_move\n",
            "  1091/50000: episode: 730, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3482.000 [3482.000, 3482.000],  loss: 12622080.000000, mae: 4716.874512, mean_q: -1567.380737\n",
            "Val: -11798.793 -2357.9663\n",
            "wrong_move\n",
            "  1092/50000: episode: 731, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 3033886.000000, mae: 4719.149902, mean_q: -2704.700195\n",
            "Val: -16562.232 -4489.8164\n",
            "wrong_move\n",
            "  1093/50000: episode: 732, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 616.000 [616.000, 616.000],  loss: 3646085.000000, mae: 4721.722168, mean_q: -2346.904297\n",
            "Val: -11227.174 -2023.419\n",
            "wrong_move\n",
            "  1094/50000: episode: 733, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 11260648.000000, mae: 4728.413574, mean_q: -1906.049316\n",
            "Val: -11618.628 -3780.1028\n",
            "wrong_move\n",
            "  1095/50000: episode: 734, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 5318615.000000, mae: 4733.404297, mean_q: -1931.712036\n",
            "Val: -11443.379 -4047.9695\n",
            "wrong_move\n",
            "  1096/50000: episode: 735, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 616.000 [616.000, 616.000],  loss: 9497905.000000, mae: 4733.326172, mean_q: -1868.449829\n",
            "Val: -10009.138 -3707.8062\n",
            "wrong_move\n",
            "  1097/50000: episode: 736, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2724.000 [2724.000, 2724.000],  loss: 3446233.750000, mae: 4735.111328, mean_q: -2423.985352\n",
            "Val: -10750.974 -2790.289\n",
            "wrong_move\n",
            "  1098/50000: episode: 737, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 6141719.000000, mae: 4731.498047, mean_q: -2229.175293\n",
            "Val: -12686.558 -3699.8135\n",
            "wrong_move\n",
            "  1099/50000: episode: 738, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2724.000 [2724.000, 2724.000],  loss: 5253824.500000, mae: 4723.815430, mean_q: -2205.125977\n",
            "Val: -9583.775 -1955.1833\n",
            "wrong_move\n",
            "  1100/50000: episode: 739, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 3620599.000000, mae: 4710.009766, mean_q: -1726.581909\n",
            "Val: -9670.639 -2651.8752\n",
            "wrong_move\n",
            "  1101/50000: episode: 740, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 3151484.500000, mae: 4698.616211, mean_q: -1669.249146\n",
            "Val: -11813.054 -3719.6023\n",
            "wrong_move\n",
            "  1102/50000: episode: 741, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 124.000 [124.000, 124.000],  loss: 6306853.500000, mae: 4686.108887, mean_q: -2068.938477\n",
            "Val: -7571.2295 -3653.2644\n",
            "wrong_move\n",
            "  1103/50000: episode: 742, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 124.000 [124.000, 124.000],  loss: 7544883.000000, mae: 4679.513672, mean_q: -1843.366577\n",
            "Val: -12265.235 -4179.015\n",
            "wrong_move\n",
            "  1104/50000: episode: 743, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1809.000 [1809.000, 1809.000],  loss: 9159667.000000, mae: 4686.382324, mean_q: -1648.357422\n",
            "Val: -9085.55 -2273.0764\n",
            "wrong_move\n",
            "  1105/50000: episode: 744, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 10713593.000000, mae: 4689.391113, mean_q: -1673.889526\n",
            "Val: -20869.314 -1791.9802\n",
            "wrong_move\n",
            "  1106/50000: episode: 745, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 3294935.000000, mae: 4694.625977, mean_q: -2264.845459\n",
            "Val: -9708.848 -3715.2246\n",
            "wrong_move\n",
            "  1107/50000: episode: 746, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1809.000 [1809.000, 1809.000],  loss: 4024994.000000, mae: 4698.269531, mean_q: -2001.745239\n",
            "Val: -11413.906 -3672.9158\n",
            "wrong_move\n",
            "  1108/50000: episode: 747, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1809.000 [1809.000, 1809.000],  loss: 6902508.000000, mae: 4695.846191, mean_q: -1385.768433\n",
            "Val: -9301.403 -3685.8293\n",
            "wrong_move\n",
            "  1109/50000: episode: 748, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 124.000 [124.000, 124.000],  loss: 6355045.000000, mae: 4689.690430, mean_q: -1491.325684\n",
            "Val: -8306.33 -3609.0574\n",
            "wrong_move\n",
            "  1110/50000: episode: 749, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 8987738.000000, mae: 4691.689941, mean_q: -1882.594360\n",
            "Val: -8799.575 -3689.6006\n",
            "wrong_move\n",
            "  1111/50000: episode: 750, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 124.000 [124.000, 124.000],  loss: 4044444.500000, mae: 4701.158691, mean_q: -1602.854248\n",
            "Val: -10981.704 -3724.2622\n",
            "wrong_move\n",
            "  1112/50000: episode: 751, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 124.000 [124.000, 124.000],  loss: 3181438.000000, mae: 4706.910645, mean_q: -1668.362549\n",
            "Val: -11749.014 -4135.9307\n",
            "wrong_move\n",
            "  1113/50000: episode: 752, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3482.000 [3482.000, 3482.000],  loss: 9181460.000000, mae: 4700.322266, mean_q: -1082.335815\n",
            "Val: -7841.777 -3635.5833\n",
            "wrong_move\n",
            "  1114/50000: episode: 753, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 5860422.000000, mae: 4699.341309, mean_q: -1848.214600\n",
            "Val: -7538.5596 -4006.8306\n",
            "wrong_move\n",
            "  1115/50000: episode: 754, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3482.000 [3482.000, 3482.000],  loss: 9711369.000000, mae: 4712.896484, mean_q: -1552.882202\n",
            "Val: -10157.985 -4462.3096\n",
            "wrong_move\n",
            "  1116/50000: episode: 755, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3482.000 [3482.000, 3482.000],  loss: 6271763.500000, mae: 4731.146484, mean_q: -2135.208984\n",
            "Val: -8209.249 -4033.5806\n",
            "wrong_move\n",
            "  1117/50000: episode: 756, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3482.000 [3482.000, 3482.000],  loss: 9151629.000000, mae: 4742.518555, mean_q: -2099.786377\n",
            "Val: -7495.678 -4084.608\n",
            "wrong_move\n",
            "  1118/50000: episode: 757, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3482.000 [3482.000, 3482.000],  loss: 5692295.000000, mae: 4735.998535, mean_q: -2272.571045\n",
            "Val: -7230.754 -4312.066\n",
            "wrong_move\n",
            "  1119/50000: episode: 758, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2.000 [2.000, 2.000],  loss: 5838988.500000, mae: 4729.711426, mean_q: -2070.494141\n",
            "Val: -10291.81 -3958.9888\n",
            "wrong_move\n",
            "  1120/50000: episode: 759, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3518.000 [3518.000, 3518.000],  loss: 3899427.000000, mae: 4734.197266, mean_q: -2059.710938\n",
            "Val: -6953.6323 -4394.47\n",
            "wrong_move\n",
            "  1121/50000: episode: 760, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2.000 [2.000, 2.000],  loss: 8458108.000000, mae: 4741.180664, mean_q: -2440.683594\n",
            "Val: -7143.3174 -4508.197\n",
            "wrong_move\n",
            "  1122/50000: episode: 761, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2.000 [2.000, 2.000],  loss: 5672352.500000, mae: 4748.811523, mean_q: -2475.528076\n",
            "Val: -9955.817 -3763.2388\n",
            "wrong_move\n",
            "  1123/50000: episode: 762, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3518.000 [3518.000, 3518.000],  loss: 1789424.750000, mae: 4751.991211, mean_q: -2469.566895\n",
            "Val: -9246.647 -3756.6887\n",
            "wrong_move\n",
            "  1124/50000: episode: 763, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3518.000 [3518.000, 3518.000],  loss: 7515406.000000, mae: 4741.313477, mean_q: -1952.578369\n",
            "Val: -8691.245 -4877.9424\n",
            "wrong_move\n",
            "  1126/50000: episode: 764, duration: 0.203s, episode steps:   2, steps per second:  10, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1178.000 [1178.000, 1178.000],  loss: 3955109.000000, mae: 4731.104492, mean_q: -2118.984131\n",
            "Val: -11033.555 -3727.0457\n",
            "wrong_move\n",
            "  1127/50000: episode: 765, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3518.000 [3518.000, 3518.000],  loss: 7289182.500000, mae: 4717.736816, mean_q: -2183.616699\n",
            "Val: -7666.535 -4634.5034\n",
            "wrong_move\n",
            "  1128/50000: episode: 766, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2219.000 [2219.000, 2219.000],  loss: 7459983.000000, mae: 4714.971191, mean_q: -2488.909180\n",
            "Val: -8497.792 -3739.3257\n",
            "wrong_move\n",
            "  1129/50000: episode: 767, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3518.000 [3518.000, 3518.000],  loss: 5798095.500000, mae: 4717.289551, mean_q: -2196.029053\n",
            "Val: -9057.627 -4444.8115\n",
            "wrong_move\n",
            "  1130/50000: episode: 768, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2030.000 [2030.000, 2030.000],  loss: 3837287.500000, mae: 4721.438965, mean_q: -2789.082520\n",
            "Val: -7986.9927 -4426.2393\n",
            "wrong_move\n",
            "  1131/50000: episode: 769, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 8050044.000000, mae: 4721.857422, mean_q: -2508.593018\n",
            "Val: -8176.291 -4281.637\n",
            "wrong_move\n",
            "  1132/50000: episode: 770, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2585.000 [2585.000, 2585.000],  loss: 4834147.500000, mae: 4719.081055, mean_q: -2340.243652\n",
            "Val: -7980.8423 -3798.8735\n",
            "wrong_move\n",
            "  1133/50000: episode: 771, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 822.000 [822.000, 822.000],  loss: 6335903.500000, mae: 4716.904785, mean_q: -2079.796875\n",
            "Val: -8020.7886 -3745.9766\n",
            "wrong_move\n",
            "  1134/50000: episode: 772, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3270.000 [3270.000, 3270.000],  loss: 5587997.000000, mae: 4705.965332, mean_q: -2143.129395\n",
            "Val: -12831.904 -3755.2344\n",
            "wrong_move\n",
            "  1135/50000: episode: 773, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 822.000 [822.000, 822.000],  loss: 6223698.000000, mae: 4693.990234, mean_q: -2160.846191\n",
            "Val: -8999.911 -4121.9805\n",
            "wrong_move\n",
            "  1136/50000: episode: 774, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4016.000 [4016.000, 4016.000],  loss: 8890575.000000, mae: 4686.682617, mean_q: -1576.116699\n",
            "Val: -7398.832 -3406.2817\n",
            "wrong_move\n",
            "  1137/50000: episode: 775, duration: 0.043s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 8078382.000000, mae: 4688.971680, mean_q: -1622.596069\n",
            "Val: -7244.0713 -3538.6582\n",
            "wrong_move\n",
            "  1138/50000: episode: 776, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 4890483.500000, mae: 4693.765137, mean_q: -2564.622070\n",
            "Val: -7204.0615 -3743.541\n",
            "wrong_move\n",
            "  1139/50000: episode: 777, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 4064363.000000, mae: 4693.712891, mean_q: -2412.404785\n",
            "Val: -8459.784 -3737.486\n",
            "wrong_move\n",
            "  1140/50000: episode: 778, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3270.000 [3270.000, 3270.000],  loss: 3296142.500000, mae: 4690.595703, mean_q: -2640.362793\n",
            "Val: -7013.7373 -4112.845\n",
            "wrong_move\n",
            "  1141/50000: episode: 779, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 2563697.500000, mae: 4683.626465, mean_q: -2612.198730\n",
            "Val: -6817.016 -4010.2627\n",
            "wrong_move\n",
            "  1142/50000: episode: 780, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 5415811.500000, mae: 4671.816406, mean_q: -1396.861328\n",
            "Val: -7357.0127 -3744.0557\n",
            "wrong_move\n",
            "  1143/50000: episode: 781, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3270.000 [3270.000, 3270.000],  loss: 2300526.500000, mae: 4662.745117, mean_q: -2505.354980\n",
            "Val: -6974.112 -4462.6064\n",
            "wrong_move\n",
            "  1144/50000: episode: 782, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 103.000 [103.000, 103.000],  loss: 566330.812500, mae: 4656.355469, mean_q: -1814.598755\n",
            "Val: -7001.8843 -4267.599\n",
            "wrong_move\n",
            "  1145/50000: episode: 783, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 7333913.000000, mae: 4653.979980, mean_q: -1361.977905\n",
            "Val: -7277.4385 -4081.3167\n",
            "wrong_move\n",
            "  1146/50000: episode: 784, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 3938226.000000, mae: 4654.170898, mean_q: -2073.531738\n",
            "Val: -7514.4214 -3646.5789\n",
            "wrong_move\n",
            "  1147/50000: episode: 785, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2947.000 [2947.000, 2947.000],  loss: 12644440.000000, mae: 4659.476562, mean_q: -1791.579346\n",
            "Val: -7461.444 -4248.1777\n",
            "wrong_move\n",
            "  1148/50000: episode: 786, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2478.000 [2478.000, 2478.000],  loss: 4314362.500000, mae: 4677.309570, mean_q: -2426.268555\n",
            "Val: -7397.301 -3800.8337\n",
            "wrong_move\n",
            "  1149/50000: episode: 787, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3270.000 [3270.000, 3270.000],  loss: 2614752.750000, mae: 4698.800781, mean_q: -2506.726074\n",
            "Val: -7611.727 -3954.8782\n",
            "wrong_move\n",
            "  1150/50000: episode: 788, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2.000 [2.000, 2.000],  loss: 4152860.750000, mae: 4702.651367, mean_q: -2856.925049\n",
            "Val: -8631.762 -4154.508\n",
            "wrong_move\n",
            "  1151/50000: episode: 789, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2478.000 [2478.000, 2478.000],  loss: 7861139.500000, mae: 4703.623047, mean_q: -2417.656494\n",
            "Val: -7718.3994 -3917.528\n",
            "wrong_move\n",
            "  1152/50000: episode: 790, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2.000 [2.000, 2.000],  loss: 6910456.000000, mae: 4697.190430, mean_q: -2224.683594\n",
            "Val: -7525.17 -3789.061\n",
            "wrong_move\n",
            "  1153/50000: episode: 791, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2.000 [2.000, 2.000],  loss: 7163802.500000, mae: 4696.032227, mean_q: -2504.653809\n",
            "Val: -7487.537 -2300.1533\n",
            "wrong_move\n",
            "  1154/50000: episode: 792, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 3342647.500000, mae: 4696.905762, mean_q: -2229.258057\n",
            "Val: -7482.189 -2961.3616\n",
            "wrong_move\n",
            "  1155/50000: episode: 793, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2.000 [2.000, 2.000],  loss: 3203030.000000, mae: 4695.557129, mean_q: -2391.361328\n",
            "Val: -8052.816 -3749.1414\n",
            "wrong_move\n",
            "  1156/50000: episode: 794, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3270.000 [3270.000, 3270.000],  loss: 6643895.000000, mae: 4693.558594, mean_q: -2493.774902\n",
            "Val: -9313.937 -3751.023\n",
            "wrong_move\n",
            "  1157/50000: episode: 795, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3518.000 [3518.000, 3518.000],  loss: 3769300.250000, mae: 4682.894531, mean_q: -2530.127441\n",
            "Val: -7933.3633 -3817.9385\n",
            "wrong_move\n",
            "  1158/50000: episode: 796, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2747.000 [2747.000, 2747.000],  loss: 7000685.000000, mae: 4672.251953, mean_q: -2037.494019\n",
            "Val: -7610.9395 -3304.4565\n",
            "wrong_move\n",
            "  1159/50000: episode: 797, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2.000 [2.000, 2.000],  loss: 5125255.000000, mae: 4663.291016, mean_q: -1505.914429\n",
            "Val: -7457.354 -3748.3713\n",
            "wrong_move\n",
            "  1160/50000: episode: 798, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2.000 [2.000, 2.000],  loss: 4265291.000000, mae: 4669.688965, mean_q: -1940.382324\n",
            "Val: -8480.507 -3798.117\n",
            "wrong_move\n",
            "  1161/50000: episode: 799, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 998.000 [998.000, 998.000],  loss: 3775850.500000, mae: 4669.014648, mean_q: -2435.554199\n",
            "Val: -8240.173 -3783.1772\n",
            "wrong_move\n",
            "  1162/50000: episode: 800, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 998.000 [998.000, 998.000],  loss: 2912025.250000, mae: 4672.206543, mean_q: -2791.801514\n",
            "Val: -8351.632 -3815.4177\n",
            "wrong_move\n",
            "  1163/50000: episode: 801, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 998.000 [998.000, 998.000],  loss: 4917581.000000, mae: 4676.588379, mean_q: -2453.268066\n",
            "Val: -7272.871 -4235.9624\n",
            "wrong_move\n",
            "  1164/50000: episode: 802, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2219.000 [2219.000, 2219.000],  loss: 3135709.750000, mae: 4686.056641, mean_q: -2650.646973\n",
            "Val: -7505.325 -4411.6074\n",
            "wrong_move\n",
            "  1165/50000: episode: 803, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2219.000 [2219.000, 2219.000],  loss: 6984766.500000, mae: 4690.825195, mean_q: -2269.614746\n",
            "Val: -8359.972 -3359.7388\n",
            "wrong_move\n",
            "  1166/50000: episode: 804, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4016.000 [4016.000, 4016.000],  loss: 4354702.000000, mae: 4698.686523, mean_q: -1722.127563\n",
            "Val: -7349.482 -4459.3257\n",
            "wrong_move\n",
            "  1167/50000: episode: 805, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2219.000 [2219.000, 2219.000],  loss: 3262711.000000, mae: 4695.870605, mean_q: -2759.364746\n",
            "Val: -7410.1514 -4327.578\n",
            "wrong_move\n",
            "  1168/50000: episode: 806, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 3383495.000000, mae: 4693.227051, mean_q: -2621.958496\n",
            "Val: -7503.8857 -4290.346\n",
            "wrong_move\n",
            "  1169/50000: episode: 807, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1893630.750000, mae: 4683.149414, mean_q: -2010.209595\n",
            "Val: -8536.9795 -2990.4563\n",
            "wrong_move\n",
            "  1170/50000: episode: 808, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4016.000 [4016.000, 4016.000],  loss: 3113112.500000, mae: 4680.582031, mean_q: -2049.402100\n",
            "Val: -7581.564 -3940.8518\n",
            "wrong_move\n",
            "  1171/50000: episode: 809, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 3275347.000000, mae: 4690.436035, mean_q: -2097.147705\n",
            "Val: -7481.844 -4292.5024\n",
            "wrong_move\n",
            "  1172/50000: episode: 810, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2076414.500000, mae: 4702.003906, mean_q: -2234.300293\n",
            "Val: -7525.8643 -4265.4326\n",
            "wrong_move\n",
            "  1173/50000: episode: 811, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 4702944.000000, mae: 4713.068359, mean_q: -2257.187988\n",
            "Val: -7301.923 -4356.1694\n",
            "wrong_move\n",
            "  1174/50000: episode: 812, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 843.000 [843.000, 843.000],  loss: 4485412.500000, mae: 4717.148926, mean_q: -2414.396484\n",
            "Val: -8422.781 -4813.5303\n",
            "wrong_move\n",
            "  1175/50000: episode: 813, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 843.000 [843.000, 843.000],  loss: 2053103.500000, mae: 4718.079102, mean_q: -2716.411865\n",
            "Val: -7450.317 -4153.624\n",
            "wrong_move\n",
            "  1176/50000: episode: 814, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 8655538.000000, mae: 4716.987793, mean_q: -1923.308960\n",
            "Val: -7558.3643 -3851.5159\n",
            "wrong_move\n",
            "  1178/50000: episode: 815, duration: 0.162s, episode steps:   2, steps per second:  12, episode reward: -5061.000, mean reward: -2530.500 [-5000.000, -61.000], mean action: 2137.500 [259.000, 4016.000],  loss: 3885743.500000, mae: 4710.808105, mean_q: -2233.625000\n",
            "Val: -7126.691 -4010.307\n",
            "wrong_move\n",
            "  1179/50000: episode: 816, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 843.000 [843.000, 843.000],  loss: 3214341.750000, mae: 4705.092285, mean_q: -1770.264893\n",
            "Val: -8452.524 -2803.503\n",
            "wrong_move\n",
            "  1180/50000: episode: 817, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4016.000 [4016.000, 4016.000],  loss: 1716129.250000, mae: 4703.538574, mean_q: -2437.030762\n",
            "Val: -7428.8394 -3001.1853\n",
            "wrong_move\n",
            "  1181/50000: episode: 818, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4016.000 [4016.000, 4016.000],  loss: 4910860.000000, mae: 4705.855469, mean_q: -2142.703369\n",
            "Val: -8437.119 -2792.873\n",
            "wrong_move\n",
            "  1182/50000: episode: 819, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4016.000 [4016.000, 4016.000],  loss: 2526199.000000, mae: 4704.470215, mean_q: -2127.293945\n",
            "Val: -8394.298 -2775.1794\n",
            "wrong_move\n",
            "  1183/50000: episode: 820, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4016.000 [4016.000, 4016.000],  loss: 3285994.000000, mae: 4704.791016, mean_q: -1771.044312\n",
            "Val: -7519.882 -3338.1199\n",
            "wrong_move\n",
            "  1184/50000: episode: 821, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2838319.000000, mae: 4708.995605, mean_q: -2431.755615\n",
            "Val: -8419.908 -3281.5024\n",
            "wrong_move\n",
            "  1185/50000: episode: 822, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4016.000 [4016.000, 4016.000],  loss: 3160868.500000, mae: 4712.250000, mean_q: -2482.887207\n",
            "Val: -8432.029 -3478.4158\n",
            "wrong_move\n",
            "  1186/50000: episode: 823, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4016.000 [4016.000, 4016.000],  loss: 1541141.250000, mae: 4716.918945, mean_q: -2874.621094\n",
            "Val: -7613.445 -3335.46\n",
            "wrong_move\n",
            "  1187/50000: episode: 824, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1908267.250000, mae: 4716.069336, mean_q: -2855.930664\n",
            "Val: -8416.012 -3870.7417\n",
            "wrong_move\n",
            "  1188/50000: episode: 825, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2100.000 [2100.000, 2100.000],  loss: 3588319.250000, mae: 4724.985840, mean_q: -2700.787109\n",
            "Val: -8521.57 -3353.7788\n",
            "wrong_move\n",
            "  1189/50000: episode: 826, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2923792.500000, mae: 4737.253906, mean_q: -2979.995361\n",
            "Val: -8309.087 -3191.5212\n",
            "wrong_move\n",
            "  1190/50000: episode: 827, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 3405371.750000, mae: 4747.951172, mean_q: -3117.877930\n",
            "Val: -8510.567 -3874.311\n",
            "wrong_move\n",
            "  1191/50000: episode: 828, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2100.000 [2100.000, 2100.000],  loss: 2489004.500000, mae: 4753.307129, mean_q: -3200.503418\n",
            "Val: -7555.918 -3812.2769\n",
            "wrong_move\n",
            "  1192/50000: episode: 829, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2627216.000000, mae: 4759.442383, mean_q: -2866.202148\n",
            "Val: -7548.8784 -3626.4846\n",
            "wrong_move\n",
            "  1193/50000: episode: 830, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2011524.250000, mae: 4745.927734, mean_q: -3294.754395\n",
            "Val: -7483.9272 -3357.661\n",
            "wrong_move\n",
            "  1194/50000: episode: 831, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1465031.250000, mae: 4721.281250, mean_q: -3035.290039\n",
            "Val: -7512.372 -2907.3623\n",
            "wrong_move\n",
            "  1195/50000: episode: 832, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2164814.000000, mae: 4698.519531, mean_q: -2802.987305\n",
            "Val: -7518.5645 -2889.7068\n",
            "wrong_move\n",
            "  1196/50000: episode: 833, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1109242.000000, mae: 4686.242188, mean_q: -2655.522461\n",
            "Val: -7488.2275 -3133.1387\n",
            "wrong_move\n",
            "  1197/50000: episode: 834, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 3178722.250000, mae: 4682.321289, mean_q: -2824.485352\n",
            "Val: -8326.607 -2548.0295\n",
            "wrong_move\n",
            "  1198/50000: episode: 835, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 4965343.000000, mae: 4684.493164, mean_q: -2208.418457\n",
            "Val: -7564.0703 -3150.3025\n",
            "wrong_move\n",
            "  1199/50000: episode: 836, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1442868.000000, mae: 4680.029785, mean_q: -2959.949707\n",
            "Val: -7569.3457 -3027.7292\n",
            "wrong_move\n",
            "  1200/50000: episode: 837, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 3933341.500000, mae: 4679.112305, mean_q: -2743.898926\n",
            "Val: -7720.624 -3289.142\n",
            "wrong_move\n",
            "  1201/50000: episode: 838, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2167748.000000, mae: 4689.990234, mean_q: -2924.931641\n",
            "Val: -7561.5303 -3322.1665\n",
            "wrong_move\n",
            "  1202/50000: episode: 839, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 4286241.500000, mae: 4693.687012, mean_q: -2808.338867\n",
            "Val: -7602.0786 -3659.04\n",
            "wrong_move\n",
            "  1203/50000: episode: 840, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 4090825.750000, mae: 4695.588867, mean_q: -2456.844482\n",
            "Val: -7830.785 -3741.647\n",
            "wrong_move\n",
            "  1204/50000: episode: 841, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 103.000 [103.000, 103.000],  loss: 4028405.500000, mae: 4707.940918, mean_q: -2517.040039\n",
            "Val: -8618.061 -3815.7573\n",
            "wrong_move\n",
            "  1205/50000: episode: 842, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 103.000 [103.000, 103.000],  loss: 4431063.000000, mae: 4717.261719, mean_q: -2889.013184\n",
            "Val: -8440.755 -3738.0198\n",
            "wrong_move\n",
            "  1206/50000: episode: 843, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 103.000 [103.000, 103.000],  loss: 1858520.750000, mae: 4718.920898, mean_q: -3377.671875\n",
            "Val: -7617.2104 -3852.4949\n",
            "wrong_move\n",
            "  1207/50000: episode: 844, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1760459.875000, mae: 4717.099609, mean_q: -3192.122314\n",
            "Val: -8389.349 -3845.2483\n",
            "wrong_move\n",
            "  1208/50000: episode: 845, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 103.000 [103.000, 103.000],  loss: 1136008.750000, mae: 4716.890625, mean_q: -3243.310303\n",
            "Val: -7340.36 -3232.6184\n",
            "wrong_move\n",
            "  1209/50000: episode: 846, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2610222.750000, mae: 4717.777832, mean_q: -2773.182617\n",
            "Val: -7744.3164 -3368.5073\n",
            "wrong_move\n",
            "  1210/50000: episode: 847, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2392909.250000, mae: 4719.021484, mean_q: -2846.299316\n",
            "Val: -7551.3057 -3219.8977\n",
            "wrong_move\n",
            "  1212/50000: episode: 848, duration: 0.200s, episode steps:   2, steps per second:  10, episode reward: -5901.000, mean reward: -2950.500 [-5000.000, -901.000], mean action: 517.000 [259.000, 775.000],  loss: 2373417.750000, mae: 4713.180664, mean_q: -3014.272461\n",
            "Val: -7566.6763 -3124.236\n",
            "wrong_move\n",
            "  1213/50000: episode: 849, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1989577.750000, mae: 4708.053223, mean_q: -3204.520264\n",
            "Val: -8424.869 -3795.8093\n",
            "wrong_move\n",
            "  1214/50000: episode: 850, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2100.000 [2100.000, 2100.000],  loss: 2232368.750000, mae: 4701.102051, mean_q: -2661.429688\n",
            "Val: -7571.5703 -3017.5808\n",
            "wrong_move\n",
            "  1215/50000: episode: 851, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2189821.000000, mae: 4698.986328, mean_q: -2796.018555\n",
            "Val: -7452.7686 -2896.4155\n",
            "wrong_move\n",
            "  1217/50000: episode: 852, duration: 0.205s, episode steps:   2, steps per second:  10, episode reward: -5911.000, mean reward: -2955.500 [-5000.000, -911.000], mean action: 517.000 [259.000, 775.000],  loss: 2521583.750000, mae: 4691.545898, mean_q: -2892.568604\n",
            "Val: -8333.043 -3806.9136\n",
            "wrong_move\n",
            "  1218/50000: episode: 853, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 804606.250000, mae: 4689.208984, mean_q: -2870.552246\n",
            "Val: -7507.504 -2884.9612\n",
            "wrong_move\n",
            "  1220/50000: episode: 854, duration: 0.193s, episode steps:   2, steps per second:  10, episode reward: -5911.000, mean reward: -2955.500 [-5000.000, -911.000], mean action: 1500.500 [259.000, 2742.000],  loss: 3680676.750000, mae: 4696.880859, mean_q: -2706.869629\n",
            "Val: -8570.594 -3966.7788\n",
            "wrong_move\n",
            "  1221/50000: episode: 855, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3070.000 [3070.000, 3070.000],  loss: 2738325.500000, mae: 4719.156250, mean_q: -2779.229980\n",
            "Val: -8446.835 -3637.002\n",
            "wrong_move\n",
            "  1222/50000: episode: 856, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3070.000 [3070.000, 3070.000],  loss: 2588161.000000, mae: 4736.869141, mean_q: -3041.065430\n",
            "Val: -7910.452 -3627.4106\n",
            "wrong_move\n",
            "  1224/50000: episode: 857, duration: 0.186s, episode steps:   2, steps per second:  11, episode reward: -5901.000, mean reward: -2950.500 [-5000.000, -901.000], mean action: 1253.000 [259.000, 2247.000],  loss: 2104081.500000, mae: 4764.333008, mean_q: -3328.676758\n",
            "Val: -8679.746 -3938.859\n",
            "wrong_move\n",
            "  1225/50000: episode: 858, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2247.000 [2247.000, 2247.000],  loss: 1710864.000000, mae: 4772.574707, mean_q: -3388.937012\n",
            "Val: -7801.4287 -4257.9688\n",
            "wrong_move\n",
            "  1226/50000: episode: 859, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1178.000 [1178.000, 1178.000],  loss: 3470320.750000, mae: 4766.935547, mean_q: -3291.120361\n",
            "Val: -7753.288 -4178.7876\n",
            "wrong_move\n",
            "  1227/50000: episode: 860, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1178.000 [1178.000, 1178.000],  loss: 385287.562500, mae: 4753.168945, mean_q: -3352.131592\n",
            "Val: -7332.876 -3890.8306\n",
            "wrong_move\n",
            "  1228/50000: episode: 861, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2191069.250000, mae: 4737.853516, mean_q: -3060.330322\n",
            "Val: -9606.393 -3587.3503\n",
            "wrong_move\n",
            "  1229/50000: episode: 862, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 2300467.500000, mae: 4717.913574, mean_q: -3076.210938\n",
            "Val: -7238.1196 -3287.037\n",
            "wrong_move\n",
            "  1230/50000: episode: 863, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2990392.000000, mae: 4701.724121, mean_q: -2574.311035\n",
            "Val: -8418.922 -3553.9355\n",
            "wrong_move\n",
            "  1231/50000: episode: 864, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1178.000 [1178.000, 1178.000],  loss: 571150.875000, mae: 4685.585938, mean_q: -2831.992188\n",
            "Val: -7517.355 -2994.2764\n",
            "wrong_move\n",
            "  1232/50000: episode: 865, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1865718.000000, mae: 4669.080078, mean_q: -2780.942383\n",
            "Val: -7430.748 -2871.187\n",
            "wrong_move\n",
            "  1233/50000: episode: 866, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 964495.062500, mae: 4659.721680, mean_q: -2890.420410\n",
            "Val: -8303.359 -3492.8293\n",
            "wrong_move\n",
            "  1234/50000: episode: 867, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1178.000 [1178.000, 1178.000],  loss: 1272957.000000, mae: 4658.606445, mean_q: -2675.707275\n",
            "Val: -7525.665 -2922.6404\n",
            "wrong_move\n",
            "  1235/50000: episode: 868, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 3267344.250000, mae: 4668.230469, mean_q: -2523.104492\n",
            "Val: -7480.802 -3093.3193\n",
            "wrong_move\n",
            "  1236/50000: episode: 869, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1341857.625000, mae: 4678.034668, mean_q: -2985.463623\n",
            "Val: -8088.671 -3452.285\n",
            "wrong_move\n",
            "  1237/50000: episode: 870, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 3301493.500000, mae: 4687.506836, mean_q: -2864.481934\n",
            "Val: -7425.6504 -3309.6116\n",
            "wrong_move\n",
            "  1238/50000: episode: 871, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2362800.000000, mae: 4695.802734, mean_q: -3015.700684\n",
            "Val: -7709.4634 -3165.0022\n",
            "wrong_move\n",
            "  1240/50000: episode: 872, duration: 0.211s, episode steps:   2, steps per second:   9, episode reward: -5931.000, mean reward: -2965.500 [-5000.000, -931.000], mean action: 718.500 [259.000, 1178.000],  loss: 2471199.000000, mae: 4705.788086, mean_q: -2867.175537\n",
            "Val: -7495.987 -3838.9944\n",
            "wrong_move\n",
            "  1241/50000: episode: 873, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 443991.437500, mae: 4712.285645, mean_q: -3293.544189\n",
            "Val: -7568.3374 -3847.6663\n",
            "wrong_move\n",
            "  1242/50000: episode: 874, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1471278.500000, mae: 4711.154297, mean_q: -3410.130371\n",
            "Val: -7604.371 -3912.271\n",
            "wrong_move\n",
            "  1243/50000: episode: 875, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 814.000 [814.000, 814.000],  loss: 2365823.750000, mae: 4706.066406, mean_q: -3033.594971\n",
            "Val: -7765.1436 -3730.156\n",
            "wrong_move\n",
            "  1244/50000: episode: 876, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2747.000 [2747.000, 2747.000],  loss: 2590920.000000, mae: 4699.114746, mean_q: -2849.327881\n",
            "Val: -6530.864 -3529.0845\n",
            "wrong_move\n",
            "  1245/50000: episode: 877, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2054952.750000, mae: 4694.836914, mean_q: -3010.043945\n",
            "Val: -6418.2554 -3588.9343\n",
            "wrong_move\n",
            "  1247/50000: episode: 878, duration: 0.202s, episode steps:   2, steps per second:  10, episode reward: -5931.000, mean reward: -2965.500 [-5000.000, -931.000], mean action: 2035.000 [259.000, 3811.000],  loss: 1323724.500000, mae: 4690.207031, mean_q: -3228.574219\n",
            "Val: -6526.165 -3318.753\n",
            "wrong_move\n",
            "  1248/50000: episode: 879, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 3393457.000000, mae: 4684.240723, mean_q: -3117.656738\n",
            "Val: -6518.934 -2271.747\n",
            "wrong_move\n",
            "  1249/50000: episode: 880, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2267237.000000, mae: 4678.895020, mean_q: -2777.593750\n",
            "Val: -6381.0264 -3112.9817\n",
            "wrong_move\n",
            "  1250/50000: episode: 881, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2447152.250000, mae: 4685.607422, mean_q: -3052.059082\n",
            "Val: -6505.5215 -3409.3335\n",
            "wrong_move\n",
            "  1251/50000: episode: 882, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1214309.250000, mae: 4702.207031, mean_q: -3499.557617\n",
            "Val: -6593.8 -3497.579\n",
            "wrong_move\n",
            "  1253/50000: episode: 883, duration: 0.226s, episode steps:   2, steps per second:   9, episode reward: -5901.000, mean reward: -2950.500 [-5000.000, -901.000], mean action: 1680.000 [259.000, 3101.000],  loss: 2164680.000000, mae: 4712.171387, mean_q: -3098.514160\n",
            "Val: -6632.9707 -3516.7827\n",
            "wrong_move\n",
            "  1254/50000: episode: 884, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2187223.500000, mae: 4715.570312, mean_q: -3192.856201\n",
            "Val: -6681.583 -3518.0896\n",
            "wrong_move\n",
            "  1255/50000: episode: 885, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2498165.750000, mae: 4711.463379, mean_q: -3011.302734\n",
            "Val: -6618.404 -3121.6284\n",
            "wrong_move\n",
            "  1256/50000: episode: 886, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 890596.062500, mae: 4710.498047, mean_q: -3199.332031\n",
            "Val: -6724.3706 -4139.5913\n",
            "wrong_move\n",
            "  1257/50000: episode: 887, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2.000 [2.000, 2.000],  loss: 1903778.625000, mae: 4708.240234, mean_q: -3121.842529\n",
            "Val: -6798.0913 -3295.1387\n",
            "wrong_move\n",
            "  1258/50000: episode: 888, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 3259021.000000, mae: 4707.876953, mean_q: -3177.221924\n",
            "Val: -6723.1797 -3151.8665\n",
            "wrong_move\n",
            "  1259/50000: episode: 889, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2255074.250000, mae: 4708.759766, mean_q: -3250.104004\n",
            "Val: -6574.5156 -3880.8093\n",
            "wrong_move\n",
            "  1260/50000: episode: 890, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3101.000 [3101.000, 3101.000],  loss: 906574.562500, mae: 4705.230957, mean_q: -3245.292969\n",
            "Val: -6892.372 -3152.459\n",
            "wrong_move\n",
            "  1261/50000: episode: 891, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1034015.062500, mae: 4698.292969, mean_q: -3295.378418\n",
            "Val: -7018.988 -3004.8467\n",
            "wrong_move\n",
            "  1262/50000: episode: 892, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 3008728.500000, mae: 4685.005859, mean_q: -3019.629883\n",
            "Val: -6827.381 -3848.9402\n",
            "wrong_move\n",
            "  1263/50000: episode: 893, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2521.000 [2521.000, 2521.000],  loss: 1416204.500000, mae: 4673.438477, mean_q: -3035.093262\n",
            "Val: -6869.3184 -3845.494\n",
            "wrong_move\n",
            "  1264/50000: episode: 894, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2521.000 [2521.000, 2521.000],  loss: 1401326.250000, mae: 4669.959473, mean_q: -3033.193115\n",
            "Val: -7274.861 -3452.2283\n",
            "wrong_move\n",
            "  1265/50000: episode: 895, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 755385.937500, mae: 4668.645020, mean_q: -3096.031250\n",
            "Val: -6908.816 -3857.324\n",
            "wrong_move\n",
            "  1266/50000: episode: 896, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2521.000 [2521.000, 2521.000],  loss: 843497.000000, mae: 4678.677734, mean_q: -3151.761719\n",
            "Val: -7205.352 -2691.3394\n",
            "wrong_move\n",
            "  1267/50000: episode: 897, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1366245.000000, mae: 4685.719238, mean_q: -3191.599121\n",
            "Val: -6931.732 -2724.7122\n",
            "wrong_move\n",
            "  1268/50000: episode: 898, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2583135.500000, mae: 4694.425781, mean_q: -2952.374023\n",
            "Val: -6903.016 -3145.466\n",
            "wrong_move\n",
            "  1269/50000: episode: 899, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 639099.000000, mae: 4701.246094, mean_q: -3034.031006\n",
            "Val: -7021.495 -3888.4211\n",
            "wrong_move\n",
            "  1270/50000: episode: 900, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1309.000 [1309.000, 1309.000],  loss: 2578062.000000, mae: 4705.638184, mean_q: -3154.192383\n",
            "Val: -7095.501 -2917.1096\n",
            "wrong_move\n",
            "  1271/50000: episode: 901, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 548348.500000, mae: 4709.230957, mean_q: -3156.450195\n",
            "Val: -7081.804 -3899.6265\n",
            "wrong_move\n",
            "  1272/50000: episode: 902, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1309.000 [1309.000, 1309.000],  loss: 1906372.250000, mae: 4712.720703, mean_q: -3050.585693\n",
            "Val: -7215.8594 -3239.1018\n",
            "wrong_move\n",
            "  1273/50000: episode: 903, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1232273.750000, mae: 4711.982422, mean_q: -3109.505371\n",
            "Val: -7299.858 -3229.6814\n",
            "wrong_move\n",
            "  1274/50000: episode: 904, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1682170.000000, mae: 4706.563477, mean_q: -3116.576904\n",
            "Val: -7425.213 -3761.4583\n",
            "wrong_move\n",
            "  1275/50000: episode: 905, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 907861.500000, mae: 4693.418945, mean_q: -3098.006104\n",
            "Val: -7376.147 -3090.8835\n",
            "wrong_move\n",
            "  1276/50000: episode: 906, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 905095.125000, mae: 4679.095703, mean_q: -2958.782471\n",
            "Val: -7110.138 -3870.1204\n",
            "wrong_move\n",
            "  1277/50000: episode: 907, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2278.000 [2278.000, 2278.000],  loss: 788696.125000, mae: 4665.869629, mean_q: -3031.512695\n",
            "Val: -7431.98 -2972.4404\n",
            "wrong_move\n",
            "  1278/50000: episode: 908, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2366056.000000, mae: 4659.418945, mean_q: -2990.136230\n",
            "Val: -7451.574 -2949.308\n",
            "wrong_move\n",
            "  1279/50000: episode: 909, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1445764.625000, mae: 4657.214844, mean_q: -3099.575439\n",
            "Val: -7238.366 -2750.3105\n",
            "wrong_move\n",
            "  1280/50000: episode: 910, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1833456.750000, mae: 4660.510742, mean_q: -2920.943848\n",
            "Val: -7228.7607 -3144.5378\n",
            "wrong_move\n",
            "  1281/50000: episode: 911, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2585.000 [2585.000, 2585.000],  loss: 1963892.000000, mae: 4665.752930, mean_q: -2678.918701\n",
            "Val: -7509.7837 -3368.8005\n",
            "wrong_move\n",
            "  1282/50000: episode: 912, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1163907.250000, mae: 4683.033691, mean_q: -2926.446289\n",
            "Val: -7553.7197 -3520.5376\n",
            "wrong_move\n",
            "  1283/50000: episode: 913, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2585.000 [2585.000, 2585.000],  loss: 2341685.500000, mae: 4711.159180, mean_q: -3051.973633\n",
            "Val: 4829367300.0 90862035000.0\n",
            "wrong_move\n",
            "  1284/50000: episode: 914, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 1778591.750000, mae: 4737.215820, mean_q: -3384.610596\n",
            "Val: -7644.4116 -3699.8633\n",
            "wrong_move\n",
            "  1285/50000: episode: 915, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2717.000 [2717.000, 2717.000],  loss: 2336659.500000, mae: 4747.456055, mean_q: -3151.140625\n",
            "Val: -7236.6353 -3930.417\n",
            "wrong_move\n",
            "  1286/50000: episode: 916, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 193.000 [193.000, 193.000],  loss: 2155181.000000, mae: 4741.765625, mean_q: -3270.615723\n",
            "Val: -7616.724 -3604.9385\n",
            "wrong_move\n",
            "  1287/50000: episode: 917, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2244718.000000, mae: 4724.748047, mean_q: -3168.064697\n",
            "Val: -7591.315 -3473.847\n",
            "wrong_move\n",
            "  1289/50000: episode: 918, duration: 0.192s, episode steps:   2, steps per second:  10, episode reward: -5951.000, mean reward: -2975.500 [-5000.000, -951.000], mean action: 226.000 [193.000, 259.000],  loss: 1855747.750000, mae: 4703.770996, mean_q: -3279.427979\n",
            "Val: -7203.255 -3912.7488\n",
            "wrong_move\n",
            "  1290/50000: episode: 919, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1303.000 [1303.000, 1303.000],  loss: 1088102.125000, mae: 4689.533203, mean_q: -3082.218262\n",
            "Val: -7414.941 -3160.8125\n",
            "wrong_move\n",
            "  1291/50000: episode: 920, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1595843.500000, mae: 4679.139648, mean_q: -3074.602539\n",
            "Val: -7351.8286 -2950.2837\n",
            "wrong_move\n",
            "  1292/50000: episode: 921, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2058018.125000, mae: 4671.625977, mean_q: -3162.264160\n",
            "Val: -7164.981 -3881.4077\n",
            "wrong_move\n",
            "  1293/50000: episode: 922, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 193.000 [193.000, 193.000],  loss: 2567001.500000, mae: 4677.483398, mean_q: -2895.399658\n",
            "Val: -7349.804 -3937.5388\n",
            "wrong_move\n",
            "  1294/50000: episode: 923, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 2402401.500000, mae: 4684.165039, mean_q: -3076.232666\n",
            "Val: -7534.832 -3566.4556\n",
            "wrong_move\n",
            "  1296/50000: episode: 924, duration: 0.208s, episode steps:   2, steps per second:  10, episode reward: -5931.000, mean reward: -2965.500 [-5000.000, -931.000], mean action: 716.500 [259.000, 1174.000],  loss: 1276988.625000, mae: 4697.886719, mean_q: -3117.885254\n",
            "Val: -7416.0674 -3375.062\n",
            "wrong_move\n",
            "  1297/50000: episode: 925, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 984071.625000, mae: 4709.926758, mean_q: -3509.510742\n",
            "Val: -7584.3105 -3773.1914\n",
            "wrong_move\n",
            "  1298/50000: episode: 926, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1769868.750000, mae: 4706.844727, mean_q: -3252.337402\n",
            "Val: -7612.062 -3594.4766\n",
            "wrong_move\n",
            "  1299/50000: episode: 927, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1808601.750000, mae: 4690.354980, mean_q: -3462.942871\n",
            "Val: -7565.806 -3477.794\n",
            "wrong_move\n",
            "  1300/50000: episode: 928, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1632.000 [1632.000, 1632.000],  loss: 1093869.000000, mae: 4671.726562, mean_q: -3208.373779\n",
            "Val: -7491.2705 -3344.7087\n",
            "wrong_move\n",
            "  1301/50000: episode: 929, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1926593.875000, mae: 4657.330078, mean_q: -2940.678223\n",
            "Val: -7559.151 -3307.2866\n",
            "wrong_move\n",
            "  1302/50000: episode: 930, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1788284.875000, mae: 4655.510742, mean_q: -3072.450684\n",
            "Val: -7469.818 -3279.5796\n",
            "wrong_move\n",
            "  1303/50000: episode: 931, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 850868.312500, mae: 4651.661133, mean_q: -3068.355713\n",
            "Val: -7167.3193 -3316.627\n",
            "wrong_move\n",
            "  1304/50000: episode: 932, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1058707.500000, mae: 4647.066406, mean_q: -3041.230469\n",
            "Val: -6078.719 -3446.6714\n",
            "wrong_move\n",
            "  1305/50000: episode: 933, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1215355.750000, mae: 4645.779297, mean_q: -2878.888428\n",
            "Val: -7053.658 -3626.6328\n",
            "wrong_move\n",
            "  1306/50000: episode: 934, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 928366.375000, mae: 4650.765137, mean_q: -3211.772461\n",
            "Val: -6095.523 -3759.3071\n",
            "wrong_move\n",
            "  1307/50000: episode: 935, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3811.000 [3811.000, 3811.000],  loss: 1480289.000000, mae: 4652.220703, mean_q: -3011.200684\n",
            "Val: -6429.7295 -3787.9255\n",
            "wrong_move\n",
            "  1308/50000: episode: 936, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 476447.718750, mae: 4657.836914, mean_q: -3181.075195\n",
            "Val: -7820.9653 -3073.972\n",
            "wrong_move\n",
            "  1309/50000: episode: 937, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2521.000 [2521.000, 2521.000],  loss: 612672.750000, mae: 4659.972168, mean_q: -3249.380859\n",
            "Val: -6688.985 -3769.7883\n",
            "wrong_move\n",
            "  1310/50000: episode: 938, duration: 0.158s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3811.000 [3811.000, 3811.000],  loss: 1310542.250000, mae: 4668.877930, mean_q: -2816.932373\n",
            "Val: -6902.2334 -3589.9524\n",
            "wrong_move\n",
            "  1311/50000: episode: 939, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2247.000 [2247.000, 2247.000],  loss: 1340093.250000, mae: 4678.783203, mean_q: -3231.477295\n",
            "Val: -7156.3604 -3209.7725\n",
            "wrong_move\n",
            "  1312/50000: episode: 940, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2247.000 [2247.000, 2247.000],  loss: 982559.125000, mae: 4676.070801, mean_q: -3135.638672\n",
            "Val: -7107.834 -3073.5747\n",
            "wrong_move\n",
            "  1313/50000: episode: 941, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2247.000 [2247.000, 2247.000],  loss: 1116573.250000, mae: 4677.359375, mean_q: -3191.167236\n",
            "Val: -7492.5903 -3940.4807\n",
            "wrong_move\n",
            "  1314/50000: episode: 942, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1850136.250000, mae: 4688.266602, mean_q: -3172.878906\n",
            "Val: -7364.6885 -3161.4119\n",
            "wrong_move\n",
            "  1316/50000: episode: 943, duration: 0.202s, episode steps:   2, steps per second:  10, episode reward: -5031.000, mean reward: -2515.500 [-5000.000, -31.000], mean action: 1632.000 [1632.000, 1632.000],  loss: 897876.125000, mae: 4704.413086, mean_q: -2860.046387\n",
            "Val: -7672.576 -3121.9268\n",
            "wrong_move\n",
            "  1317/50000: episode: 944, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1632.000 [1632.000, 1632.000],  loss: 1625854.375000, mae: 4723.929688, mean_q: -2498.267822\n",
            "Val: -7921.6636 -2881.7861\n",
            "wrong_move\n",
            "  1318/50000: episode: 945, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1632.000 [1632.000, 1632.000],  loss: 1589114.750000, mae: 4727.175781, mean_q: -2266.132812\n",
            "Val: -7618.2407 -2390.9158\n",
            "wrong_move\n",
            "  1319/50000: episode: 946, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1632.000 [1632.000, 1632.000],  loss: 556429.375000, mae: 4728.108398, mean_q: -2168.904297\n",
            "Val: -7883.6777 -2407.8784\n",
            "wrong_move\n",
            "  1321/50000: episode: 947, duration: 0.186s, episode steps:   2, steps per second:  11, episode reward: -5021.000, mean reward: -2510.500 [-5000.000, -21.000], mean action: 1632.000 [1632.000, 1632.000],  loss: 876125.625000, mae: 4718.557617, mean_q: -1896.388306\n",
            "Val: -7569.926 -1428.3619\n",
            "wrong_move\n",
            "  1323/50000: episode: 948, duration: 0.191s, episode steps:   2, steps per second:  10, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 1632.000 [1632.000, 1632.000],  loss: 1970012.125000, mae: 4697.823242, mean_q: -1652.152344\n",
            "Val: -7925.8022 -1974.9001\n",
            "wrong_move\n",
            "  1324/50000: episode: 949, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1632.000 [1632.000, 1632.000],  loss: 1056498.375000, mae: 4685.783203, mean_q: -1560.286865\n",
            "Val: -7906.374 -2339.5833\n",
            "wrong_move\n",
            "  1325/50000: episode: 950, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1632.000 [1632.000, 1632.000],  loss: 888481.250000, mae: 4679.033203, mean_q: -1962.974976\n",
            "Val: -7872.5557 -2977.9097\n",
            "wrong_move\n",
            "  1326/50000: episode: 951, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1632.000 [1632.000, 1632.000],  loss: 1097140.250000, mae: 4679.313477, mean_q: -2638.155029\n",
            "Val: -7948.038 -3612.2961\n",
            "wrong_move\n",
            "  1327/50000: episode: 952, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1632.000 [1632.000, 1632.000],  loss: 871034.875000, mae: 4671.802734, mean_q: -2995.333496\n",
            "Val: -7804.1587 -3394.5847\n",
            "wrong_move\n",
            "  1329/50000: episode: 953, duration: 0.186s, episode steps:   2, steps per second:  11, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 201.500 [144.000, 259.000],  loss: 1117309.750000, mae: 4673.555664, mean_q: -3293.153320\n",
            "Val: -7858.65 -3720.0178\n",
            "wrong_move\n",
            "  1331/50000: episode: 954, duration: 0.198s, episode steps:   2, steps per second:  10, episode reward: -5911.000, mean reward: -2955.500 [-5000.000, -911.000], mean action: 201.500 [144.000, 259.000],  loss: 630334.062500, mae: 4682.829102, mean_q: -3284.260742\n",
            "Val: -7403.2944 -3763.2136\n",
            "wrong_move\n",
            "  1332/50000: episode: 955, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 144.000 [144.000, 144.000],  loss: 870809.750000, mae: 4693.348633, mean_q: -3278.240967\n",
            "Val: -7329.559 -3703.4968\n",
            "wrong_move\n",
            "  1334/50000: episode: 956, duration: 0.204s, episode steps:   2, steps per second:  10, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 144.000 [144.000, 144.000],  loss: 976267.875000, mae: 4704.921387, mean_q: -3169.827393\n",
            "Val: -7072.0317 -3668.936\n",
            "wrong_move\n",
            "  1335/50000: episode: 957, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 144.000 [144.000, 144.000],  loss: 583347.187500, mae: 4716.629883, mean_q: -3385.879395\n",
            "Val: -7217.851 -3635.6985\n",
            "wrong_move\n",
            "  1336/50000: episode: 958, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 144.000 [144.000, 144.000],  loss: 1319485.250000, mae: 4719.499512, mean_q: -3325.960938\n",
            "Val: -7212.575 -4001.9448\n",
            "wrong_move\n",
            "  1337/50000: episode: 959, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 843.000 [843.000, 843.000],  loss: 2007636.625000, mae: 4718.450195, mean_q: -3304.428223\n",
            "Val: -7232.547 -3929.0288\n",
            "wrong_move\n",
            "  1338/50000: episode: 960, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 843.000 [843.000, 843.000],  loss: 1592826.750000, mae: 4708.861328, mean_q: -3238.455078\n",
            "Val: -7303.7837 -3893.4004\n",
            "wrong_move\n",
            "  1339/50000: episode: 961, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 843.000 [843.000, 843.000],  loss: 2035338.750000, mae: 4704.898438, mean_q: -3289.012695\n",
            "Val: -7352.823 -3868.1511\n",
            "wrong_move\n",
            "  1340/50000: episode: 962, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 843.000 [843.000, 843.000],  loss: 1181134.875000, mae: 4706.345703, mean_q: -3261.867188\n",
            "Val: -7525.6377 -3863.6921\n",
            "wrong_move\n",
            "  1341/50000: episode: 963, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 843.000 [843.000, 843.000],  loss: 1085255.125000, mae: 4702.544434, mean_q: -3424.567627\n",
            "Val: -7539.2246 -3927.3708\n",
            "wrong_move\n",
            "  1343/50000: episode: 964, duration: 0.200s, episode steps:   2, steps per second:  10, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 1159506.750000, mae: 4700.326660, mean_q: -3323.460693\n",
            "Val: -7409.745 -3882.8953\n",
            "wrong_move\n",
            "  1344/50000: episode: 965, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1178.000 [1178.000, 1178.000],  loss: 1872165.000000, mae: 4702.137695, mean_q: -3262.631836\n",
            "Val: -7379.0356 -3677.9648\n",
            "wrong_move\n",
            "  1345/50000: episode: 966, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1178.000 [1178.000, 1178.000],  loss: 867015.187500, mae: 4700.715332, mean_q: -3439.132324\n",
            "Val: -7636.801 -3927.1292\n",
            "wrong_move\n",
            "  1346/50000: episode: 967, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 847012.875000, mae: 4698.902344, mean_q: -3295.335938\n",
            "Val: -7667.7793 -3833.7617\n",
            "wrong_move\n",
            "  1347/50000: episode: 968, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1178.000 [1178.000, 1178.000],  loss: 550416.000000, mae: 4694.153320, mean_q: -3425.665527\n",
            "Val: -8020.273 -3793.3647\n",
            "wrong_move\n",
            "  1348/50000: episode: 969, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1178.000 [1178.000, 1178.000],  loss: 1082712.000000, mae: 4688.687988, mean_q: -3181.415771\n",
            "Val: -7596.9907 -3928.0896\n",
            "wrong_move\n",
            "  1350/50000: episode: 970, duration: 0.223s, episode steps:   2, steps per second:   9, episode reward: -5041.000, mean reward: -2520.500 [-5000.000, -41.000], mean action: 146.000 [146.000, 146.000],  loss: 1338936.750000, mae: 4678.728516, mean_q: -3238.211426\n",
            "Val: -7615.7163 -4045.989\n",
            "wrong_move\n",
            "  1351/50000: episode: 971, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1178.000 [1178.000, 1178.000],  loss: 1202437.125000, mae: 4666.310547, mean_q: -3221.176758\n",
            "Val: -7482.894 -3633.887\n",
            "wrong_move\n",
            "  1352/50000: episode: 972, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1090620.750000, mae: 4662.519531, mean_q: -3180.472656\n",
            "Val: -7578.9326 -4010.3435\n",
            "wrong_move\n",
            "  1353/50000: episode: 973, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: 1470207.250000, mae: 4673.057129, mean_q: -2993.668945\n",
            "Val: -7708.8066 -3931.5823\n",
            "wrong_move\n",
            "  1354/50000: episode: 974, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1303.000 [1303.000, 1303.000],  loss: 1041814.250000, mae: 4683.937988, mean_q: -3290.656494\n",
            "Val: -7727.916 -3948.0652\n",
            "wrong_move\n",
            "  1355/50000: episode: 975, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1511.000 [1511.000, 1511.000],  loss: 1089762.250000, mae: 4695.123047, mean_q: -3514.228516\n",
            "Val: -7977.2275 -3973.7505\n",
            "wrong_move\n",
            "  1356/50000: episode: 976, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1511.000 [1511.000, 1511.000],  loss: 741852.000000, mae: 4710.334961, mean_q: -3422.992188\n",
            "Val: -7312.7417 -3818.0435\n",
            "wrong_move\n",
            "  1357/50000: episode: 977, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 827187.750000, mae: 4725.597168, mean_q: -3286.295410\n",
            "Val: -7470.068 -4094.0303\n",
            "wrong_move\n",
            "  1358/50000: episode: 978, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2947.000 [2947.000, 2947.000],  loss: 762893.000000, mae: 4729.159180, mean_q: -3345.473633\n",
            "Val: -7425.051 -3361.6316\n",
            "wrong_move\n",
            "  1359/50000: episode: 979, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2947.000 [2947.000, 2947.000],  loss: 1001836.875000, mae: 4721.239258, mean_q: -3269.792969\n",
            "Val: -7730.7095 -3361.8032\n",
            "wrong_move\n",
            "  1360/50000: episode: 980, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2947.000 [2947.000, 2947.000],  loss: 617332.937500, mae: 4704.575684, mean_q: -3355.302734\n",
            "Val: -7649.789 -4050.623\n",
            "wrong_move\n",
            "  1361/50000: episode: 981, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2947.000 [2947.000, 2947.000],  loss: 1131966.000000, mae: 4688.649414, mean_q: -3162.572021\n",
            "Val: -7411.2905 -3664.5916\n",
            "wrong_move\n",
            "  1362/50000: episode: 982, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2947.000 [2947.000, 2947.000],  loss: 2162013.500000, mae: 4675.361816, mean_q: -2855.130127\n",
            "Val: -7397.399 -3591.1147\n",
            "wrong_move\n",
            "  1363/50000: episode: 983, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2947.000 [2947.000, 2947.000],  loss: 975939.500000, mae: 4672.478516, mean_q: -2943.637451\n",
            "Val: -7718.413 -3435.292\n",
            "wrong_move\n",
            "  1364/50000: episode: 984, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2947.000 [2947.000, 2947.000],  loss: 1530857.500000, mae: 4674.673828, mean_q: -3231.773193\n",
            "Val: -7352.562 -3930.2795\n",
            "wrong_move\n",
            "  1365/50000: episode: 985, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2947.000 [2947.000, 2947.000],  loss: 1090314.875000, mae: 4677.923340, mean_q: -3183.079102\n",
            "Val: -7252.33 -4149.46\n",
            "wrong_move\n",
            "  1366/50000: episode: 986, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1591.000 [1591.000, 1591.000],  loss: 1082266.875000, mae: 4685.709473, mean_q: -3256.496582\n",
            "Val: -7407.0303 -4204.071\n",
            "wrong_move\n",
            "  1367/50000: episode: 987, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2219.000 [2219.000, 2219.000],  loss: 713142.625000, mae: 4688.788086, mean_q: -3237.757080\n",
            "Val: -7420.933 -4156.0713\n",
            "wrong_move\n",
            "  1368/50000: episode: 988, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2673.000 [2673.000, 2673.000],  loss: 1715703.375000, mae: 4685.214844, mean_q: -3289.007812\n",
            "Val: -7886.125 -3061.8647\n",
            "wrong_move\n",
            "  1369/50000: episode: 989, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 585954.250000, mae: 4674.271484, mean_q: -3178.669189\n",
            "Val: -7302.922 -3869.8965\n",
            "wrong_move\n",
            "  1370/50000: episode: 990, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 629134.375000, mae: 4665.241699, mean_q: -2974.109863\n",
            "Val: -7343.519 -3960.7131\n",
            "wrong_move\n",
            "  1371/50000: episode: 991, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 698215.875000, mae: 4665.523438, mean_q: -3084.821045\n",
            "Val: -7368.01 -3816.667\n",
            "wrong_move\n",
            "  1372/50000: episode: 992, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 587991.875000, mae: 4667.951172, mean_q: -3156.902344\n",
            "Val: -7275.214 -3366.9707\n",
            "wrong_move\n",
            "  1374/50000: episode: 993, duration: 0.148s, episode steps:   2, steps per second:  13, episode reward: -5911.000, mean reward: -2955.500 [-5000.000, -911.000], mean action: 1488.000 [259.000, 2717.000],  loss: 931981.625000, mae: 4675.480469, mean_q: -3111.988770\n",
            "Val: -7369.5674 -3793.0261\n",
            "wrong_move\n",
            "  1375/50000: episode: 994, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 823267.187500, mae: 4688.069336, mean_q: -2987.947998\n",
            "Val: -7364.6816 -3166.2178\n",
            "wrong_move\n",
            "  1376/50000: episode: 995, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2717.000 [2717.000, 2717.000],  loss: 334830.906250, mae: 4700.041016, mean_q: -3060.652344\n",
            "Val: -7410.889 -4011.8135\n",
            "wrong_move\n",
            "  1377/50000: episode: 996, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 916507.250000, mae: 4710.473145, mean_q: -3205.501465\n",
            "Val: -7398.9604 -3224.6184\n",
            "wrong_move\n",
            "  1378/50000: episode: 997, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2717.000 [2717.000, 2717.000],  loss: 1442674.125000, mae: 4723.275391, mean_q: -3047.349609\n",
            "Val: -7398.49 -3189.422\n",
            "wrong_move\n",
            "  1379/50000: episode: 998, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2717.000 [2717.000, 2717.000],  loss: 538505.500000, mae: 4727.642090, mean_q: -3305.138184\n",
            "Val: -7287.1157 -3971.5483\n",
            "wrong_move\n",
            "  1380/50000: episode: 999, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2717.000 [2717.000, 2717.000],  loss: 1152481.750000, mae: 4724.277344, mean_q: -3229.546387\n",
            "Val: -7358.4795 -4236.722\n",
            "wrong_move\n",
            "  1381/50000: episode: 1000, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2219.000 [2219.000, 2219.000],  loss: 672100.125000, mae: 4716.659668, mean_q: -3399.010010\n",
            "Val: -7972.336 -3305.7869\n",
            "wrong_move\n",
            "  1382/50000: episode: 1001, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1303.000 [1303.000, 1303.000],  loss: 766556.062500, mae: 4704.423828, mean_q: -3506.032471\n",
            "Val: -7536.615 -3937.4854\n",
            "wrong_move\n",
            "  1383/50000: episode: 1002, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3101.000 [3101.000, 3101.000],  loss: 866757.625000, mae: 4692.541992, mean_q: -3395.439453\n",
            "Val: -7529.0605 -3978.0454\n",
            "wrong_move\n",
            "  1384/50000: episode: 1003, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1381.000 [1381.000, 1381.000],  loss: 695969.187500, mae: 4685.657227, mean_q: -3652.266357\n",
            "Val: -7109.0977 -3922.85\n",
            "wrong_move\n",
            "  1385/50000: episode: 1004, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1178.000 [1178.000, 1178.000],  loss: 935416.000000, mae: 4679.711914, mean_q: -3545.141846\n",
            "Val: -7363.4424 -4004.0027\n",
            "wrong_move\n",
            "  1386/50000: episode: 1005, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 983017.375000, mae: 4673.589844, mean_q: -3451.700684\n",
            "Val: -7276.465 -3843.342\n",
            "wrong_move\n",
            "  1387/50000: episode: 1006, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1133093.750000, mae: 4675.477539, mean_q: -3481.171143\n",
            "Val: -7461.7295 -2987.6033\n",
            "wrong_move\n",
            "  1388/50000: episode: 1007, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 502050.437500, mae: 4683.409180, mean_q: -3560.957275\n",
            "Val: -7192.0684 -3568.2043\n",
            "wrong_move\n",
            "  1389/50000: episode: 1008, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1054415.000000, mae: 4696.526855, mean_q: -3484.325195\n",
            "Val: -7266.9297 -3656.9617\n",
            "wrong_move\n",
            "  1391/50000: episode: 1009, duration: 0.143s, episode steps:   2, steps per second:  14, episode reward: -5911.000, mean reward: -2955.500 [-5000.000, -911.000], mean action: 1179.500 [259.000, 2100.000],  loss: 666137.812500, mae: 4708.055176, mean_q: -3554.001953\n",
            "Val: -7406.237 -3967.2915\n",
            "wrong_move\n",
            "  1392/50000: episode: 1010, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 669744.875000, mae: 4715.972656, mean_q: -3607.613037\n",
            "Val: -7542.9966 -3930.538\n",
            "wrong_move\n",
            "  1393/50000: episode: 1011, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 978795.000000, mae: 4716.388672, mean_q: -3415.607422\n",
            "Val: -7469.5386 -3921.6013\n",
            "wrong_move\n",
            "  1394/50000: episode: 1012, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 541626.000000, mae: 4713.144531, mean_q: -3637.491699\n",
            "Val: -7551.7085 -3792.6624\n",
            "wrong_move\n",
            "  1395/50000: episode: 1013, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 940391.625000, mae: 4711.349121, mean_q: -3433.484375\n",
            "Val: -7483.8633 -3830.406\n",
            "wrong_move\n",
            "  1396/50000: episode: 1014, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 943959.875000, mae: 4711.849609, mean_q: -3473.034668\n",
            "Val: -7431.4087 -3819.2292\n",
            "wrong_move\n",
            "  1397/50000: episode: 1015, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 456625.062500, mae: 4706.810547, mean_q: -3463.048828\n",
            "Val: -7301.097 -3102.2283\n",
            "wrong_move\n",
            "  1398/50000: episode: 1016, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 1139430.000000, mae: 4698.320312, mean_q: -3140.981934\n",
            "Val: -7517.7866 -3806.3027\n",
            "wrong_move\n",
            "  1399/50000: episode: 1017, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 680449.875000, mae: 4692.972656, mean_q: -3148.361816\n",
            "Val: -7483.3164 -3698.7947\n",
            "wrong_move\n",
            "  1400/50000: episode: 1018, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 353210.156250, mae: 4692.989258, mean_q: -2881.978760\n",
            "Val: -7193.445 -2277.5598\n",
            "wrong_move\n",
            "  1401/50000: episode: 1019, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 816421.937500, mae: 4698.175293, mean_q: -3048.443604\n",
            "Val: -7539.75 -3449.6765\n",
            "wrong_move\n",
            "  1402/50000: episode: 1020, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: 976864.000000, mae: 4700.064453, mean_q: -3077.071289\n",
            "Val: -7431.321 -2837.2073\n",
            "wrong_move\n",
            "  1404/50000: episode: 1021, duration: 0.159s, episode steps:   2, steps per second:  13, episode reward: -4971.000, mean reward: -2485.500 [-5000.000, 29.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 648570.437500, mae: 4703.336914, mean_q: -2920.739990\n",
            "Val: -7484.862 -3313.5686\n",
            "wrong_move\n",
            "  1405/50000: episode: 1022, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: 271640.250000, mae: 4699.520996, mean_q: -2722.797363\n",
            "Val: -7240.9844 -3343.304\n",
            "wrong_move\n",
            "  1406/50000: episode: 1023, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 538826.250000, mae: 4685.141602, mean_q: -2894.245605\n",
            "Val: -7172.9033 -3422.746\n",
            "wrong_move\n",
            "  1407/50000: episode: 1024, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: 1141603.500000, mae: 4671.595703, mean_q: -2686.491455\n",
            "Val: -7127.617 -3432.5833\n",
            "wrong_move\n",
            "  1408/50000: episode: 1025, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: 662732.750000, mae: 4653.288574, mean_q: -3228.294434\n",
            "Val: -7653.502 -3535.7317\n",
            "wrong_move\n",
            "  1409/50000: episode: 1026, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: 263307.281250, mae: 4643.544922, mean_q: -3145.494873\n",
            "Val: -7493.533 -3504.3196\n",
            "wrong_move\n",
            "  1410/50000: episode: 1027, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: 350269.656250, mae: 4643.385254, mean_q: -3088.111084\n",
            "Val: -7194.4077 -3529.0073\n",
            "wrong_move\n",
            "  1411/50000: episode: 1028, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: 420883.156250, mae: 4647.965820, mean_q: -3145.926514\n",
            "Val: -7682.1333 -3628.309\n",
            "wrong_move\n",
            "  1412/50000: episode: 1029, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: 522509.218750, mae: 4659.728516, mean_q: -3155.511230\n",
            "Val: -7109.8486 -3445.61\n",
            "wrong_move\n",
            "  1413/50000: episode: 1030, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: 667722.187500, mae: 4671.548340, mean_q: -3116.168457\n",
            "Val: -7238.0366 -3896.5984\n",
            "wrong_move\n",
            "  1414/50000: episode: 1031, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: 750539.875000, mae: 4681.801758, mean_q: -3354.891602\n",
            "Val: -7241.816 -3928.8853\n",
            "wrong_move\n",
            "  1415/50000: episode: 1032, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 716061.000000, mae: 4692.221680, mean_q: -3244.086670\n",
            "Val: -7674.2944 -3479.2288\n",
            "wrong_move\n",
            "  1416/50000: episode: 1033, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1511.000 [1511.000, 1511.000],  loss: 1142763.625000, mae: 4713.386719, mean_q: -3493.849609\n",
            "Val: -7271.5737 -4223.528\n",
            "wrong_move\n",
            "  1417/50000: episode: 1034, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 321452.687500, mae: 4729.407227, mean_q: -3376.611816\n",
            "Val: -7262.4263 -4266.594\n",
            "wrong_move\n",
            "  1418/50000: episode: 1035, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1127306.000000, mae: 4735.286133, mean_q: -3527.821289\n",
            "Val: -7282.904 -4179.454\n",
            "wrong_move\n",
            "  1420/50000: episode: 1036, duration: 0.157s, episode steps:   2, steps per second:  13, episode reward: -5031.000, mean reward: -2515.500 [-5000.000, -31.000], mean action: 885.000 [259.000, 1511.000],  loss: 514134.312500, mae: 4723.951172, mean_q: -3393.998535\n",
            "Val: -7524.88 -3333.0005\n",
            "wrong_move\n",
            "  1421/50000: episode: 1037, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1511.000 [1511.000, 1511.000],  loss: 766000.750000, mae: 4707.694336, mean_q: -3336.139160\n",
            "Val: -7250.584 -3904.218\n",
            "wrong_move\n",
            "  1422/50000: episode: 1038, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 513598.562500, mae: 4695.662598, mean_q: -3368.618164\n",
            "Val: -7148.7314 -3464.7563\n",
            "wrong_move\n",
            "  1424/50000: episode: 1039, duration: 0.145s, episode steps:   2, steps per second:  14, episode reward: -5901.000, mean reward: -2950.500 [-5000.000, -901.000], mean action: 885.000 [259.000, 1511.000],  loss: 451263.500000, mae: 4681.921387, mean_q: -3311.485107\n",
            "Val: -7368.94 -3742.6382\n",
            "wrong_move\n",
            "  1425/50000: episode: 1040, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 997727.750000, mae: 4670.852539, mean_q: -3340.731934\n",
            "Val: -7710.166 -3255.3135\n",
            "wrong_move\n",
            "  1426/50000: episode: 1041, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1511.000 [1511.000, 1511.000],  loss: 600939.187500, mae: 4666.639160, mean_q: -3348.630859\n",
            "Val: -7657.954 -3733.2244\n",
            "wrong_move\n",
            "  1427/50000: episode: 1042, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 352704.187500, mae: 4667.121094, mean_q: -3216.096436\n",
            "Val: -7759.2544 -3809.4678\n",
            "wrong_move\n",
            "  1428/50000: episode: 1043, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 1272701.375000, mae: 4672.457520, mean_q: -3227.886719\n",
            "Val: -7972.116 -3917.4766\n",
            "wrong_move\n",
            "  1429/50000: episode: 1044, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 144.000 [144.000, 144.000],  loss: 710479.125000, mae: 4681.637695, mean_q: -3431.354736\n",
            "Val: -8002.4478 -3708.2058\n",
            "wrong_move\n",
            "  1430/50000: episode: 1045, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 144.000 [144.000, 144.000],  loss: 447224.000000, mae: 4687.084961, mean_q: -3276.875488\n",
            "Val: -8104.024 -3515.6958\n",
            "wrong_move\n",
            "  1431/50000: episode: 1046, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 144.000 [144.000, 144.000],  loss: 415274.625000, mae: 4692.770508, mean_q: -3282.707031\n",
            "Val: -7748.439 -3498.642\n",
            "wrong_move\n",
            "  1432/50000: episode: 1047, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 144.000 [144.000, 144.000],  loss: 425757.750000, mae: 4695.369629, mean_q: -3131.325195\n",
            "Val: -8149.534 -3274.7363\n",
            "wrong_move\n",
            "  1433/50000: episode: 1048, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 144.000 [144.000, 144.000],  loss: 335760.750000, mae: 4688.918945, mean_q: -3115.229980\n",
            "Val: -8156.5137 -3163.6455\n",
            "wrong_move\n",
            "  1434/50000: episode: 1049, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 144.000 [144.000, 144.000],  loss: 289954.562500, mae: 4681.699219, mean_q: -3098.406738\n",
            "Val: -8178.2188 -3189.9436\n",
            "wrong_move\n",
            "  1435/50000: episode: 1050, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 144.000 [144.000, 144.000],  loss: 791212.687500, mae: 4676.911621, mean_q: -3147.076660\n",
            "Val: -8226.466 -3596.9756\n",
            "wrong_move\n",
            "  1436/50000: episode: 1051, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 144.000 [144.000, 144.000],  loss: 365723.937500, mae: 4674.116211, mean_q: -3250.338379\n",
            "Val: -8249.223 -3978.594\n",
            "wrong_move\n",
            "  1437/50000: episode: 1052, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 144.000 [144.000, 144.000],  loss: 485288.468750, mae: 4671.789551, mean_q: -3309.339355\n",
            "Val: -7702.8936 -3565.633\n",
            "wrong_move\n",
            "  1438/50000: episode: 1053, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2673.000 [2673.000, 2673.000],  loss: 810045.250000, mae: 4672.775391, mean_q: -3291.112305\n",
            "Val: -8120.0967 -3704.394\n",
            "wrong_move\n",
            "  1439/50000: episode: 1054, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1591.000 [1591.000, 1591.000],  loss: 595235.625000, mae: 4673.933594, mean_q: -3322.958496\n",
            "Val: -7963.272 -3466.5955\n",
            "wrong_move\n",
            "  1440/50000: episode: 1055, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1591.000 [1591.000, 1591.000],  loss: 409502.031250, mae: 4675.866211, mean_q: -3247.909180\n",
            "Val: -7764.686 -3434.4846\n",
            "wrong_move\n",
            "  1441/50000: episode: 1056, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1591.000 [1591.000, 1591.000],  loss: 404463.812500, mae: 4681.826660, mean_q: -3192.421875\n",
            "Val: -7785.6216 -3200.7795\n",
            "wrong_move\n",
            "  1442/50000: episode: 1057, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1591.000 [1591.000, 1591.000],  loss: 365276.187500, mae: 4687.094238, mean_q: -3065.328369\n",
            "Val: -7370.4385 -3515.1033\n",
            "wrong_move\n",
            "  1443/50000: episode: 1058, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 692350.062500, mae: 4689.229492, mean_q: -2892.446289\n",
            "Val: -7236.407 -3158.1997\n",
            "wrong_move\n",
            "  1444/50000: episode: 1059, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1591.000 [1591.000, 1591.000],  loss: 308702.687500, mae: 4688.439453, mean_q: -3018.084961\n",
            "Val: -7866.3564 -2968.4287\n",
            "wrong_move\n",
            "  1445/50000: episode: 1060, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1591.000 [1591.000, 1591.000],  loss: 265978.687500, mae: 4683.674805, mean_q: -3065.831787\n",
            "Val: -7216.0103 -3033.137\n",
            "wrong_move\n",
            "  1446/50000: episode: 1061, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1591.000 [1591.000, 1591.000],  loss: 346040.187500, mae: 4681.166992, mean_q: -2945.159668\n",
            "Val: -8196.478 -2851.0215\n",
            "wrong_move\n",
            "  1447/50000: episode: 1062, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1591.000 [1591.000, 1591.000],  loss: 598416.500000, mae: 4679.078125, mean_q: -2932.725098\n",
            "Val: -7515.158 -3213.127\n",
            "wrong_move\n",
            "  1448/50000: episode: 1063, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1591.000 [1591.000, 1591.000],  loss: 1159011.250000, mae: 4674.485352, mean_q: -2822.200195\n",
            "Val: -7246.28 -2975.9497\n",
            "wrong_move\n",
            "  1449/50000: episode: 1064, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1591.000 [1591.000, 1591.000],  loss: 455912.437500, mae: 4665.854004, mean_q: -2823.420410\n",
            "Val: -7217.178 -3469.5322\n",
            "wrong_move\n",
            "  1450/50000: episode: 1065, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1591.000 [1591.000, 1591.000],  loss: 670772.312500, mae: 4663.670898, mean_q: -3128.014893\n",
            "Val: -9070.194 -3093.6973\n",
            "wrong_move\n",
            "  1451/50000: episode: 1066, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1303.000 [1303.000, 1303.000],  loss: 1066820.500000, mae: 4667.822754, mean_q: -3259.053955\n",
            "Val: -7676.121 -3650.2966\n",
            "wrong_move\n",
            "  1452/50000: episode: 1067, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1303.000 [1303.000, 1303.000],  loss: 442667.343750, mae: 4669.281250, mean_q: -3527.748779\n",
            "Val: -7199.0566 -4245.1553\n",
            "wrong_move\n",
            "  1453/50000: episode: 1068, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 672123.625000, mae: 4673.905762, mean_q: -3647.871582\n",
            "Val: -7162.307 -4027.8916\n",
            "wrong_move\n",
            "  1455/50000: episode: 1069, duration: 0.213s, episode steps:   2, steps per second:   9, episode reward: -5951.000, mean reward: -2975.500 [-5000.000, -951.000], mean action: 2094.000 [259.000, 3929.000],  loss: 496032.625000, mae: 4683.900879, mean_q: -3687.951660\n",
            "Val: -7945.939 -4005.172\n",
            "wrong_move\n",
            "  1456/50000: episode: 1070, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 723298.500000, mae: 4698.285156, mean_q: -3723.431152\n",
            "Val: -7115.8003 -4176.377\n",
            "wrong_move\n",
            "  1457/50000: episode: 1071, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 799549.812500, mae: 4708.796875, mean_q: -3806.661621\n",
            "Val: -6400.126 -4019.243\n",
            "wrong_move\n",
            "  1458/50000: episode: 1072, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3929.000 [3929.000, 3929.000],  loss: 323885.843750, mae: 4718.205078, mean_q: -3729.884277\n",
            "Val: -7632.312 -4387.2754\n",
            "wrong_move\n",
            "  1459/50000: episode: 1073, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 373502.500000, mae: 4718.741211, mean_q: -3688.189453\n",
            "Val: -7768.903 -4309.4453\n",
            "wrong_move\n",
            "  1460/50000: episode: 1074, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 939747.625000, mae: 4714.610352, mean_q: -3625.687500\n",
            "Val: -7892.0103 -4235.9785\n",
            "wrong_move\n",
            "  1461/50000: episode: 1075, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 749089.250000, mae: 4708.532227, mean_q: -3792.167725\n",
            "Val: -7879.526 -3965.947\n",
            "wrong_move\n",
            "  1463/50000: episode: 1076, duration: 0.207s, episode steps:   2, steps per second:  10, episode reward: -5911.000, mean reward: -2955.500 [-5000.000, -911.000], mean action: 2094.000 [259.000, 3929.000],  loss: 471433.125000, mae: 4702.163574, mean_q: -3749.679199\n",
            "Val: -8062.249 -4029.9424\n",
            "wrong_move\n",
            "  1464/50000: episode: 1077, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 457307.593750, mae: 4701.444336, mean_q: -3555.514160\n",
            "Val: -6189.6646 -3853.718\n",
            "wrong_move\n",
            "  1465/50000: episode: 1078, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1381.000 [1381.000, 1381.000],  loss: 436909.437500, mae: 4702.384766, mean_q: -3598.837158\n",
            "Val: -7660.1714 -4020.624\n",
            "wrong_move\n",
            "  1466/50000: episode: 1079, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1381.000 [1381.000, 1381.000],  loss: 544413.187500, mae: 4704.678711, mean_q: -3443.706543\n",
            "Val: -6328.311 -2764.6177\n",
            "wrong_move\n",
            "  1467/50000: episode: 1080, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1381.000 [1381.000, 1381.000],  loss: 569799.000000, mae: 4711.802734, mean_q: -3562.226074\n",
            "Val: -6207.7446 -3432.5051\n",
            "wrong_move\n",
            "  1468/50000: episode: 1081, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1381.000 [1381.000, 1381.000],  loss: 460893.875000, mae: 4713.395020, mean_q: -3405.286621\n",
            "Val: -7432.511 -3792.4846\n",
            "wrong_move\n",
            "  1469/50000: episode: 1082, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1381.000 [1381.000, 1381.000],  loss: 549815.375000, mae: 4711.650879, mean_q: -3357.013916\n",
            "Val: -7258.0034 -4217.849\n",
            "wrong_move\n",
            "  1470/50000: episode: 1083, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1381.000 [1381.000, 1381.000],  loss: 719886.812500, mae: 4703.353027, mean_q: -3620.456055\n",
            "Val: -6187.361 -3928.0344\n",
            "wrong_move\n",
            "  1472/50000: episode: 1084, duration: 0.184s, episode steps:   2, steps per second:  11, episode reward: -4951.000, mean reward: -2475.500 [-5000.000, 49.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 461310.718750, mae: 4689.492676, mean_q: -3658.383301\n",
            "Val: -7100.5835 -4202.694\n",
            "wrong_move\n",
            "  1473/50000: episode: 1085, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 898518.250000, mae: 4673.729492, mean_q: -3628.288574\n",
            "Val: -7207.756 -4169.3096\n",
            "wrong_move\n",
            "  1474/50000: episode: 1086, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 343826.468750, mae: 4658.531738, mean_q: -3520.734131\n",
            "Val: -5838.238 -3823.818\n",
            "wrong_move\n",
            "  1475/50000: episode: 1087, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 490563.812500, mae: 4650.198242, mean_q: -3450.199463\n",
            "Val: -6973.9854 -3916.6343\n",
            "wrong_move\n",
            "  1476/50000: episode: 1088, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 122728.296875, mae: 4650.757812, mean_q: -3586.734863\n",
            "Val: -6178.3726 -3563.226\n",
            "wrong_move\n",
            "  1477/50000: episode: 1089, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 400843.250000, mae: 4657.808594, mean_q: -3391.711914\n",
            "Val: -6754.6206 -3758.318\n",
            "wrong_move\n",
            "  1478/50000: episode: 1090, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 763825.375000, mae: 4671.215820, mean_q: -3557.144043\n",
            "Val: -7057.1284 -3988.8289\n",
            "wrong_move\n",
            "  1479/50000: episode: 1091, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 780401.687500, mae: 4681.118164, mean_q: -3456.190430\n",
            "Val: -7106.06 -4151.018\n",
            "wrong_move\n",
            "  1480/50000: episode: 1092, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 488688.687500, mae: 4688.548828, mean_q: -3758.274658\n",
            "Val: -7220.2144 -4229.8477\n",
            "wrong_move\n",
            "  1481/50000: episode: 1093, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 577433.062500, mae: 4697.647461, mean_q: -3644.979492\n",
            "Val: -7383.561 -3536.4211\n",
            "wrong_move\n",
            "  1482/50000: episode: 1094, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 381647.937500, mae: 4702.713379, mean_q: -3721.587646\n",
            "Val: -7377.529 -3752.4153\n",
            "wrong_move\n",
            "  1483/50000: episode: 1095, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 494178.156250, mae: 4704.262695, mean_q: -3699.391602\n",
            "Val: -7438.7603 -4089.4956\n",
            "wrong_move\n",
            "  1484/50000: episode: 1096, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 355472.562500, mae: 4700.368164, mean_q: -3581.807129\n",
            "Val: -7944.5264 -4027.3518\n",
            "wrong_move\n",
            "  1485/50000: episode: 1097, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: 265673.218750, mae: 4698.083008, mean_q: -3637.259521\n",
            "Val: -7513.4883 -4248.064\n",
            "wrong_move\n",
            "  1486/50000: episode: 1098, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 767329.875000, mae: 4699.126465, mean_q: -3470.623779\n",
            "Val: -8220.797 -4011.7893\n",
            "wrong_move\n",
            "  1487/50000: episode: 1099, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 451916.843750, mae: 4696.047852, mean_q: -3500.874756\n",
            "Val: -7456.1406 -4050.1362\n",
            "wrong_move\n",
            "  1488/50000: episode: 1100, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2585.000 [2585.000, 2585.000],  loss: 835627.562500, mae: 4696.686035, mean_q: -3504.455078\n",
            "Val: -7573.8374 -4128.0166\n",
            "wrong_move\n",
            "  1489/50000: episode: 1101, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2585.000 [2585.000, 2585.000],  loss: 219388.953125, mae: 4697.427246, mean_q: -3566.965332\n",
            "Val: -8276.237 -4011.7922\n",
            "wrong_move\n",
            "  1490/50000: episode: 1102, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2521.000 [2521.000, 2521.000],  loss: 386497.812500, mae: 4699.422852, mean_q: -3660.835449\n",
            "Val: -7381.9165 -4000.6401\n",
            "wrong_move\n",
            "  1491/50000: episode: 1103, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 612970.125000, mae: 4697.377441, mean_q: -3686.525391\n",
            "Val: -6986.683 -4125.205\n",
            "wrong_move\n",
            "  1492/50000: episode: 1104, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2585.000 [2585.000, 2585.000],  loss: 661023.812500, mae: 4691.579102, mean_q: -3632.118652\n",
            "Val: -6991.5566 -4039.4458\n",
            "wrong_move\n",
            "  1493/50000: episode: 1105, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2585.000 [2585.000, 2585.000],  loss: 470518.000000, mae: 4686.034668, mean_q: -3662.934082\n",
            "Val: -6306.8384 -4045.6038\n",
            "wrong_move\n",
            "  1494/50000: episode: 1106, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 267497.718750, mae: 4678.856934, mean_q: -3643.308105\n",
            "Val: -7113.6875 -3979.7874\n",
            "wrong_move\n",
            "  1495/50000: episode: 1107, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2585.000 [2585.000, 2585.000],  loss: 507487.625000, mae: 4673.053711, mean_q: -3573.058350\n",
            "Val: -5854.875 -3996.828\n",
            "wrong_move\n",
            "  1496/50000: episode: 1108, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 292714.250000, mae: 4670.373047, mean_q: -3707.524902\n",
            "Val: -6695.7393 -4168.205\n",
            "wrong_move\n",
            "  1497/50000: episode: 1109, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 671472.687500, mae: 4667.417480, mean_q: -3524.760742\n",
            "Val: -5600.244 -3983.0159\n",
            "wrong_move\n",
            "  1498/50000: episode: 1110, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 355730.375000, mae: 4661.215820, mean_q: -3670.971680\n",
            "Val: -6846.502 -4025.1929\n",
            "wrong_move\n",
            "  1500/50000: episode: 1111, duration: 0.138s, episode steps:   2, steps per second:  15, episode reward: -5021.000, mean reward: -2510.500 [-5000.000, -21.000], mean action: 1151.000 [259.000, 2043.000],  loss: 403213.281250, mae: 4660.608398, mean_q: -3661.217773\n",
            "Val: -6957.3105 -4251.785\n",
            "wrong_move\n",
            "  1501/50000: episode: 1112, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 210309.531250, mae: 4662.710938, mean_q: -3661.906006\n",
            "Val: -5763.1987 -3718.14\n",
            "wrong_move\n",
            "  1502/50000: episode: 1113, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: 383878.062500, mae: 4668.112305, mean_q: -3584.731201\n",
            "Val: -6973.447 -4090.868\n",
            "wrong_move\n",
            "  1503/50000: episode: 1114, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3929.000 [3929.000, 3929.000],  loss: 233235.906250, mae: 4671.022461, mean_q: -3676.693604\n",
            "Val: -5966.98 -3508.444\n",
            "wrong_move\n",
            "  1505/50000: episode: 1115, duration: 0.140s, episode steps:   2, steps per second:  14, episode reward: -5051.000, mean reward: -2525.500 [-5000.000, -51.000], mean action: 2043.000 [2043.000, 2043.000],  loss: 457010.375000, mae: 4676.411133, mean_q: -3548.577148\n",
            "Val: -6173.1255 -3488.5193\n",
            "wrong_move\n",
            "  1506/50000: episode: 1116, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: 535254.875000, mae: 4674.682129, mean_q: -3583.080078\n",
            "Val: -7174.0337 -4343.606\n",
            "wrong_move\n",
            "  1507/50000: episode: 1117, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 277049.218750, mae: 4675.499512, mean_q: -3636.836670\n",
            "Val: -7161.3286 -4278.0327\n",
            "wrong_move\n",
            "  1508/50000: episode: 1118, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 263575.625000, mae: 4678.079102, mean_q: -3388.408203\n",
            "Val: -7175.7266 -4212.362\n",
            "wrong_move\n",
            "  1509/50000: episode: 1119, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 510620.187500, mae: 4682.124023, mean_q: -3591.703857\n",
            "Val: -6958.4307 -4029.569\n",
            "wrong_move\n",
            "  1511/50000: episode: 1120, duration: 0.125s, episode steps:   2, steps per second:  16, episode reward: -5041.000, mean reward: -2520.500 [-5000.000, -41.000], mean action: 1151.000 [259.000, 2043.000],  loss: 528055.562500, mae: 4689.807129, mean_q: -3553.878418\n",
            "Val: -5851.094 -3291.359\n",
            "wrong_move\n",
            "  1512/50000: episode: 1121, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: 211564.406250, mae: 4696.408203, mean_q: -3649.440186\n",
            "Val: -5911.118 -3205.4648\n",
            "wrong_move\n",
            "  1513/50000: episode: 1122, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: 498469.968750, mae: 4698.721680, mean_q: -3568.335449\n",
            "Val: -7209.5938 -4313.3496\n",
            "wrong_move\n",
            "  1514/50000: episode: 1123, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1178.000 [1178.000, 1178.000],  loss: 393599.562500, mae: 4704.928223, mean_q: -3496.665527\n",
            "Val: -7248.512 -4338.3906\n",
            "wrong_move\n",
            "  1515/50000: episode: 1124, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 442278.281250, mae: 4703.677734, mean_q: -3715.751465\n",
            "Val: -6054.395 -3959.6245\n",
            "wrong_move\n",
            "  1516/50000: episode: 1125, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: 331218.468750, mae: 4698.467285, mean_q: -3714.755859\n",
            "Val: -6089.785 -4038.0645\n",
            "wrong_move\n",
            "  1517/50000: episode: 1126, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2667.000 [2667.000, 2667.000],  loss: 166805.656250, mae: 4695.829102, mean_q: -3896.318115\n",
            "Val: -5948.7314 -3847.3687\n",
            "wrong_move\n",
            "  1519/50000: episode: 1127, duration: 0.128s, episode steps:   2, steps per second:  16, episode reward: -5911.000, mean reward: -2955.500 [-5000.000, -911.000], mean action: 1463.000 [259.000, 2667.000],  loss: 400943.000000, mae: 4692.749023, mean_q: -3795.108643\n",
            "Val: -7109.134 -4113.416\n",
            "wrong_move\n",
            "  1520/50000: episode: 1128, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 585373.375000, mae: 4694.313477, mean_q: -3792.252197\n",
            "Val: -7053.1846 -4029.6934\n",
            "wrong_move\n",
            "  1521/50000: episode: 1129, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 176371.546875, mae: 4694.898438, mean_q: -3806.814453\n",
            "Val: -6878.316 -4288.3438\n",
            "wrong_move\n",
            "  1522/50000: episode: 1130, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 127793.726562, mae: 4700.004883, mean_q: -3857.667969\n",
            "Val: -7013.266 -4260.3047\n",
            "wrong_move\n",
            "  1523/50000: episode: 1131, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 613104.250000, mae: 4710.341797, mean_q: -3904.640137\n",
            "Val: -7042.0005 -4170.6655\n",
            "wrong_move\n",
            "  1524/50000: episode: 1132, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 581499.437500, mae: 4714.212891, mean_q: -3816.628906\n",
            "Val: -7137.745 -4097.4326\n",
            "wrong_move\n",
            "  1525/50000: episode: 1133, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 249025.062500, mae: 4711.614746, mean_q: -3850.652344\n",
            "Val: -6862.529 -4043.248\n",
            "wrong_move\n",
            "  1526/50000: episode: 1134, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 323.000 [323.000, 323.000],  loss: 406536.000000, mae: 4701.680176, mean_q: -3707.602051\n",
            "Val: -7048.929 -4218.0547\n",
            "wrong_move\n",
            "  1527/50000: episode: 1135, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 288608.187500, mae: 4688.580078, mean_q: -3803.645996\n",
            "Val: -6952.881 -4299.0195\n",
            "wrong_move\n",
            "  1528/50000: episode: 1136, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 234926.593750, mae: 4675.195312, mean_q: -3755.340332\n",
            "Val: -6959.3506 -4213.2334\n",
            "wrong_move\n",
            "  1529/50000: episode: 1137, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 160179.828125, mae: 4663.492188, mean_q: -3721.884766\n",
            "Val: -6776.03 -4167.5737\n",
            "wrong_move\n",
            "  1530/50000: episode: 1138, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 362034.156250, mae: 4659.643555, mean_q: -3744.309570\n",
            "Val: -6871.416 -4159.0664\n",
            "wrong_move\n",
            "  1531/50000: episode: 1139, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 534472.562500, mae: 4659.489258, mean_q: -3668.877441\n",
            "Val: -6784.78 -4200.3535\n",
            "wrong_move\n",
            "  1532/50000: episode: 1140, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 229951.671875, mae: 4660.858398, mean_q: -3730.749023\n",
            "Val: -6668.3384 -4002.4924\n",
            "wrong_move\n",
            "  1533/50000: episode: 1141, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 360864.312500, mae: 4660.541504, mean_q: -3683.784912\n",
            "Val: -6795.4995 -3830.5908\n",
            "wrong_move\n",
            "  1534/50000: episode: 1142, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 601838.250000, mae: 4673.764648, mean_q: -3455.845215\n",
            "Val: -7171.3125 -3789.51\n",
            "wrong_move\n",
            "  1535/50000: episode: 1143, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 456543.156250, mae: 4688.542480, mean_q: -3504.230957\n",
            "Val: -6807.016 -3913.279\n",
            "wrong_move\n",
            "  1536/50000: episode: 1144, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 430370.812500, mae: 4696.652344, mean_q: -3437.933105\n",
            "Val: -20706.438 5871.396\n",
            "wrong_move\n",
            "  1537/50000: episode: 1145, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 247924.625000, mae: 4694.879883, mean_q: -3565.235352\n",
            "Val: -10419.867 -2302.4756\n",
            "wrong_move\n",
            "  1538/50000: episode: 1146, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 540465.062500, mae: 4693.274414, mean_q: -3615.579102\n",
            "Val: -13956.734 860.239\n",
            "wrong_move\n",
            "  1539/50000: episode: 1147, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 353440.968750, mae: 4684.955566, mean_q: -3477.542480\n",
            "Val: -5953.289 -3882.1562\n",
            "wrong_move\n",
            "  1540/50000: episode: 1148, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 775.000 [775.000, 775.000],  loss: 436661.593750, mae: 4683.890625, mean_q: -3541.830811\n",
            "Val: -5761.162 -3512.3784\n",
            "wrong_move\n",
            "  1541/50000: episode: 1149, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2667.000 [2667.000, 2667.000],  loss: 647138.437500, mae: 4683.762207, mean_q: -3441.885742\n",
            "Val: -11468.697 -744.1225\n",
            "wrong_move\n",
            "  1542/50000: episode: 1150, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 533179.500000, mae: 4682.291992, mean_q: -3335.296143\n",
            "Val: -5621.5767 -3470.1191\n",
            "wrong_move\n",
            "  1543/50000: episode: 1151, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2667.000 [2667.000, 2667.000],  loss: 460231.156250, mae: 4675.708008, mean_q: -3246.568604\n",
            "Val: -10418.428 -1485.2626\n",
            "wrong_move\n",
            "  1544/50000: episode: 1152, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 463242.937500, mae: 4668.470703, mean_q: -3302.838379\n",
            "Val: -10323.574 -1711.8407\n",
            "wrong_move\n",
            "  1545/50000: episode: 1153, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 449866.562500, mae: 4665.055664, mean_q: -3219.948730\n",
            "Val: -6306.3745 -3462.4524\n",
            "wrong_move\n",
            "  1546/50000: episode: 1154, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2.000 [2.000, 2.000],  loss: 397758.406250, mae: 4671.731445, mean_q: -3275.714355\n",
            "Val: -6645.624 -3377.7205\n",
            "wrong_move\n",
            "  1547/50000: episode: 1155, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1381.000 [1381.000, 1381.000],  loss: 276784.781250, mae: 4684.966797, mean_q: -3436.729004\n",
            "Val: -6365.3647 -3795.9219\n",
            "wrong_move\n",
            "  1548/50000: episode: 1156, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2289.000 [2289.000, 2289.000],  loss: 240310.000000, mae: 4703.291016, mean_q: -3460.225098\n",
            "Val: -9773.639 -3548.2153\n",
            "wrong_move\n",
            "  1549/50000: episode: 1157, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 207831.343750, mae: 4719.229492, mean_q: -3469.460205\n",
            "Val: -9121.622 -4164.2666\n",
            "wrong_move\n",
            "  1550/50000: episode: 1158, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 252320.593750, mae: 4734.561523, mean_q: -3601.556152\n",
            "Val: -9409.08 -3969.0105\n",
            "wrong_move\n",
            "  1552/50000: episode: 1159, duration: 0.193s, episode steps:   2, steps per second:  10, episode reward: -4981.000, mean reward: -2490.500 [-5000.000, 19.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 347543.750000, mae: 4738.133789, mean_q: -3694.426758\n",
            "Val: -9755.222 -3911.5376\n",
            "wrong_move\n",
            "  1553/50000: episode: 1160, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 682.000 [682.000, 682.000],  loss: 364850.781250, mae: 4735.904785, mean_q: -3713.157227\n",
            "Val: -8964.301 -3803.3499\n",
            "wrong_move\n",
            "  1554/50000: episode: 1161, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 350988.625000, mae: 4727.918457, mean_q: -3746.373047\n",
            "Val: -8841.316 -3707.4207\n",
            "wrong_move\n",
            "  1556/50000: episode: 1162, duration: 0.189s, episode steps:   2, steps per second:  11, episode reward: -4981.000, mean reward: -2490.500 [-5000.000, 19.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 482842.812500, mae: 4718.731445, mean_q: -3656.698242\n",
            "Val: -6598.0083 -3851.3018\n",
            "wrong_move\n",
            "  1557/50000: episode: 1163, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 682.000 [682.000, 682.000],  loss: 406128.812500, mae: 4715.159180, mean_q: -3495.282471\n",
            "Val: -8477.453 -4058.2114\n",
            "wrong_move\n",
            "  1558/50000: episode: 1164, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 466287.875000, mae: 4712.404297, mean_q: -3532.151367\n",
            "Val: -8782.027 -4101.12\n",
            "wrong_move\n",
            "  1559/50000: episode: 1165, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 355231.875000, mae: 4715.929688, mean_q: -3629.137451\n",
            "Val: -7753.032 -4268.635\n",
            "wrong_move\n",
            "  1560/50000: episode: 1166, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 592904.500000, mae: 4720.026855, mean_q: -3681.717529\n",
            "Val: -8551.033 -4041.662\n",
            "wrong_move\n",
            "  1561/50000: episode: 1167, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 204680.203125, mae: 4726.165527, mean_q: -3669.124512\n",
            "Val: -8350.632 -4411.0513\n",
            "wrong_move\n",
            "  1562/50000: episode: 1168, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 403755.937500, mae: 4732.967773, mean_q: -3702.763672\n",
            "Val: -7996.9585 -4828.273\n",
            "wrong_move\n",
            "  1563/50000: episode: 1169, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2585.000 [2585.000, 2585.000],  loss: 293202.031250, mae: 4735.268066, mean_q: -3590.012207\n",
            "Val: -8480.015 -4700.5923\n",
            "wrong_move\n",
            "  1564/50000: episode: 1170, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 2040250.875000, mae: 5305.630371, mean_q: 76.654297\n",
            "Val: -8453.311 -4689.758\n",
            "wrong_move\n",
            "  1565/50000: episode: 1171, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 391138.937500, mae: 4710.302246, mean_q: -3542.324707\n",
            "Val: -8169.3145 -4406.0264\n",
            "wrong_move\n",
            "  1566/50000: episode: 1172, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 133911328.000000, mae: 4695.454102, mean_q: -877.758240\n",
            "Val: -6276.46 -3718.8145\n",
            "wrong_move\n",
            "  1567/50000: episode: 1173, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: 414999.062500, mae: 4710.961914, mean_q: -3542.349854\n",
            "Val: -7854.5396 -4729.0435\n",
            "wrong_move\n",
            "  1568/50000: episode: 1174, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 522359.593750, mae: 4720.730469, mean_q: -3589.686768\n",
            "Val: -6743.44 -3870.833\n",
            "wrong_move\n",
            "  1569/50000: episode: 1175, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1019.000 [1019.000, 1019.000],  loss: 461896.312500, mae: 4730.315430, mean_q: -3722.588135\n",
            "Val: -8402.715 -4678.767\n",
            "wrong_move\n",
            "  1570/50000: episode: 1176, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 584856.312500, mae: 4736.587891, mean_q: -3792.117188\n",
            "Val: -8604.132 -4565.984\n",
            "wrong_move\n",
            "  1571/50000: episode: 1177, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 428627.656250, mae: 4732.477539, mean_q: -3767.211914\n",
            "Val: -8630.243 -4537.12\n",
            "wrong_move\n",
            "  1572/50000: episode: 1178, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 431059.281250, mae: 4717.428711, mean_q: -3618.198242\n",
            "Val: -7203.4077 -3817.966\n",
            "wrong_move\n",
            "  1573/50000: episode: 1179, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3605.000 [3605.000, 3605.000],  loss: 369067.500000, mae: 4705.859863, mean_q: -3705.907959\n",
            "Val: -7255.802 -3911.4976\n",
            "wrong_move\n",
            "  1574/50000: episode: 1180, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1019.000 [1019.000, 1019.000],  loss: 371397.125000, mae: 4702.257812, mean_q: -3493.154297\n",
            "Val: -8744.556 -4840.6655\n",
            "wrong_move\n",
            "  1575/50000: episode: 1181, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 380280.000000, mae: 4703.800781, mean_q: -3475.117432\n",
            "Val: -8394.904 -4526.313\n",
            "wrong_move\n",
            "  1576/50000: episode: 1182, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 636347.312500, mae: 4712.063477, mean_q: -3283.201172\n",
            "Val: -7619.997 -3404.2136\n",
            "wrong_move\n",
            "  1577/50000: episode: 1183, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 775.000 [775.000, 775.000],  loss: 194340.515625, mae: 4732.452637, mean_q: -3533.623535\n",
            "Val: -8318.545 -4761.1597\n",
            "wrong_move\n",
            "  1578/50000: episode: 1184, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 210922.500000, mae: 4748.432617, mean_q: -3597.738281\n",
            "Val: -7142.616 -3611.4155\n",
            "wrong_move\n",
            "  1579/50000: episode: 1185, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 775.000 [775.000, 775.000],  loss: 195765.484375, mae: 4761.141113, mean_q: -3636.197021\n",
            "Val: -8901.704 -5134.8584\n",
            "wrong_move\n",
            "  1580/50000: episode: 1186, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 283012.187500, mae: 4764.458496, mean_q: -3678.199463\n",
            "Val: -8878.804 -5108.0845\n",
            "wrong_move\n",
            "  1581/50000: episode: 1187, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 179415.625000, mae: 4760.435547, mean_q: -3688.039795\n",
            "Val: -8350.955 -4769.9634\n",
            "wrong_move\n",
            "  1582/50000: episode: 1188, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 734.000 [734.000, 734.000],  loss: 349935.750000, mae: 4752.663086, mean_q: -3613.645508\n",
            "Val: -7760.496 -3317.1821\n",
            "wrong_move\n",
            "  1583/50000: episode: 1189, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 775.000 [775.000, 775.000],  loss: 490808.406250, mae: 4739.378418, mean_q: -3619.142822\n",
            "Val: -7985.652 -5064.7456\n",
            "wrong_move\n",
            "  1584/50000: episode: 1190, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3383.000 [3383.000, 3383.000],  loss: 298192.250000, mae: 4729.949219, mean_q: -3851.650879\n",
            "Val: -7622.082 -4953.5366\n",
            "wrong_move\n",
            "  1585/50000: episode: 1191, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3383.000 [3383.000, 3383.000],  loss: 180703.046875, mae: 4717.870117, mean_q: -3891.219727\n",
            "Val: -7651.1313 -3759.7595\n",
            "wrong_move\n",
            "  1586/50000: episode: 1192, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3605.000 [3605.000, 3605.000],  loss: 228657.531250, mae: 4706.592285, mean_q: -3701.703125\n",
            "Val: -7636.845 -4630.222\n",
            "wrong_move\n",
            "  1587/50000: episode: 1193, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 297202.125000, mae: 4700.134277, mean_q: -3651.529785\n",
            "Val: -7574.608 -4678.635\n",
            "wrong_move\n",
            "  1588/50000: episode: 1194, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 268315.187500, mae: 4698.484375, mean_q: -3590.422363\n",
            "Val: -7930.0913 -4520.9805\n",
            "wrong_move\n",
            "  1590/50000: episode: 1195, duration: 0.224s, episode steps:   2, steps per second:   9, episode reward: -4981.000, mean reward: -2490.500 [-5000.000, 19.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 369816.218750, mae: 4699.333496, mean_q: -3537.327148\n",
            "Val: -8024.127 -3770.0955\n",
            "wrong_move\n",
            "  1591/50000: episode: 1196, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3605.000 [3605.000, 3605.000],  loss: 521675.406250, mae: 4704.279297, mean_q: -3629.864014\n",
            "Val: -8337.841 -4488.3184\n",
            "wrong_move\n",
            "  1592/50000: episode: 1197, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1591.000 [1591.000, 1591.000],  loss: 397410.218750, mae: 4710.022949, mean_q: -3580.312256\n",
            "Val: -8345.92 -4540.6143\n",
            "wrong_move\n",
            "  1593/50000: episode: 1198, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1591.000 [1591.000, 1591.000],  loss: 287349.500000, mae: 4716.165039, mean_q: -3718.541504\n",
            "Val: -7857.2544 -3758.9612\n",
            "wrong_move\n",
            "  1594/50000: episode: 1199, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3605.000 [3605.000, 3605.000],  loss: 341213.656250, mae: 4721.099609, mean_q: -3634.821289\n",
            "Val: -7843.2095 -3744.6975\n",
            "wrong_move\n",
            "  1595/50000: episode: 1200, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3605.000 [3605.000, 3605.000],  loss: 180460.437500, mae: 4717.891113, mean_q: -3667.627441\n",
            "Val: -7366.8354 -3854.6082\n",
            "wrong_move\n",
            "  1596/50000: episode: 1201, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 682.000 [682.000, 682.000],  loss: 274694.250000, mae: 4716.211914, mean_q: -3687.866699\n",
            "Val: -8766.495 -3486.1094\n",
            "wrong_move\n",
            "  1597/50000: episode: 1202, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 682.000 [682.000, 682.000],  loss: 120855.984375, mae: 4717.583984, mean_q: -3684.616455\n",
            "Val: -8728.686 -4778.6504\n",
            "wrong_move\n",
            "  1598/50000: episode: 1203, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3929.000 [3929.000, 3929.000],  loss: 195209.156250, mae: 4719.949219, mean_q: -3638.008301\n",
            "Val: -7730.4907 -3777.0493\n",
            "wrong_move\n",
            "  1599/50000: episode: 1204, duration: 0.159s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3124.000 [3124.000, 3124.000],  loss: 107898.828125, mae: 4724.939453, mean_q: -3937.452148\n",
            "Val: -7946.8223 -3791.5015\n",
            "wrong_move\n",
            "  1600/50000: episode: 1205, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 585.000 [585.000, 585.000],  loss: 228771.375000, mae: 4729.802734, mean_q: -3970.752930\n",
            "Val: -8974.322 -4747.6943\n",
            "wrong_move\n",
            "  1601/50000: episode: 1206, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 310595.218750, mae: 4731.031738, mean_q: -3862.002441\n",
            "Val: -7872.6567 -3793.6726\n",
            "wrong_move\n",
            "  1602/50000: episode: 1207, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3124.000 [3124.000, 3124.000],  loss: 537225.437500, mae: 4724.482422, mean_q: -3177.415039\n",
            "Val: -9064.087 -4273.5205\n",
            "wrong_move\n",
            "  1603/50000: episode: 1208, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 127888.937500, mae: 4710.535156, mean_q: -3631.614746\n",
            "Val: -8979.966 -4137.2656\n",
            "wrong_move\n",
            "  1604/50000: episode: 1209, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 275672.437500, mae: 4700.667969, mean_q: -3625.191162\n",
            "Val: -8064.6953 -3739.049\n",
            "wrong_move\n",
            "  1605/50000: episode: 1210, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 410724.843750, mae: 4695.255859, mean_q: -3417.221191\n",
            "Val: -9041.005 -3565.998\n",
            "wrong_move\n",
            "  1606/50000: episode: 1211, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 146576.500000, mae: 4693.857422, mean_q: -3550.472168\n",
            "Val: -8264.679 -3753.938\n",
            "wrong_move\n",
            "  1607/50000: episode: 1212, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 581904.250000, mae: 4692.209961, mean_q: -3532.293457\n",
            "Val: -8374.63 -4531.413\n",
            "wrong_move\n",
            "  1608/50000: episode: 1213, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 224478.796875, mae: 4693.849609, mean_q: -3627.535400\n",
            "Val: -8253.286 -4325.0977\n",
            "wrong_move\n",
            "  1609/50000: episode: 1214, duration: 0.141s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2717.000 [2717.000, 2717.000],  loss: 168699.218750, mae: 4702.204102, mean_q: -3681.104492\n",
            "Val: -8801.906 -4543.094\n",
            "wrong_move\n",
            "  1610/50000: episode: 1215, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2717.000 [2717.000, 2717.000],  loss: 212091.890625, mae: 4713.258789, mean_q: -3637.628418\n",
            "Val: -7160.187 -3624.5278\n",
            "wrong_move\n",
            "  1611/50000: episode: 1216, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1019.000 [1019.000, 1019.000],  loss: 88368.187500, mae: 4720.763672, mean_q: -3830.294189\n",
            "Val: -7274.1655 -3751.051\n",
            "wrong_move\n",
            "  1612/50000: episode: 1217, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1019.000 [1019.000, 1019.000],  loss: 311230.375000, mae: 4722.458496, mean_q: -3672.145264\n",
            "Val: -9082.416 -4799.6885\n",
            "wrong_move\n",
            "  1613/50000: episode: 1218, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: 327216.625000, mae: 4721.714844, mean_q: -3723.871582\n",
            "Val: -7245.0986 -3794.8525\n",
            "wrong_move\n",
            "  1614/50000: episode: 1219, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 706.000 [706.000, 706.000],  loss: 279319.250000, mae: 4716.657227, mean_q: -3749.489746\n",
            "Val: -9193.755 -4669.3916\n",
            "wrong_move\n",
            "  1615/50000: episode: 1220, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 103824.218750, mae: 4714.472656, mean_q: -3727.156982\n",
            "Val: -9210.115 -4634.789\n",
            "wrong_move\n",
            "  1616/50000: episode: 1221, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 409807.000000, mae: 4711.976074, mean_q: -3758.061768\n",
            "Val: -9219.77 -4594.6304\n",
            "wrong_move\n",
            "  1617/50000: episode: 1222, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 495100.718750, mae: 4712.365723, mean_q: -3771.752441\n",
            "Val: -9301.253 -4486.3\n",
            "wrong_move\n",
            "  1618/50000: episode: 1223, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 434826.375000, mae: 4712.985352, mean_q: -3718.791504\n",
            "Val: -8096.726 -3609.0122\n",
            "wrong_move\n",
            "  1619/50000: episode: 1224, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2747.000 [2747.000, 2747.000],  loss: 162263.531250, mae: 4711.486328, mean_q: -3650.701904\n",
            "Val: -9333.468 -4236.039\n",
            "wrong_move\n",
            "  1620/50000: episode: 1225, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 349526.156250, mae: 4710.819824, mean_q: -3636.604980\n",
            "Val: -8871.529 -3986.3208\n",
            "wrong_move\n",
            "  1621/50000: episode: 1226, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 145452.468750, mae: 4704.410645, mean_q: -3687.684570\n",
            "Val: -9166.835 -4030.8848\n",
            "wrong_move\n",
            "  1622/50000: episode: 1227, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 160607.921875, mae: 4700.053711, mean_q: -3627.963135\n",
            "Val: -9255.109 -4208.5054\n",
            "wrong_move\n",
            "  1623/50000: episode: 1228, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 217463.812500, mae: 4700.034668, mean_q: -3668.071045\n",
            "Val: -9292.786 -4645.691\n",
            "wrong_move\n",
            "  1624/50000: episode: 1229, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 197895.046875, mae: 4706.119141, mean_q: -3783.051758\n",
            "Val: -8402.217 -3830.0103\n",
            "wrong_move\n",
            "  1625/50000: episode: 1230, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: 159659.484375, mae: 4719.686523, mean_q: -3759.210938\n",
            "Val: -9317.439 -4744.373\n",
            "wrong_move\n",
            "  1626/50000: episode: 1231, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3383.000 [3383.000, 3383.000],  loss: 203363.765625, mae: 4738.201172, mean_q: -3854.277344\n",
            "Val: -8146.6226 -3821.6836\n",
            "wrong_move\n",
            "  1627/50000: episode: 1232, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 706.000 [706.000, 706.000],  loss: 168504.562500, mae: 4754.560059, mean_q: -3992.653564\n",
            "Val: -8189.2876 -3839.2769\n",
            "wrong_move\n",
            "  1628/50000: episode: 1233, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 706.000 [706.000, 706.000],  loss: 249825.531250, mae: 4757.965820, mean_q: -3922.410400\n",
            "Val: -9326.379 -4660.804\n",
            "wrong_move\n",
            "  1629/50000: episode: 1234, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2219.000 [2219.000, 2219.000],  loss: 251241.265625, mae: 4749.171875, mean_q: -3910.830078\n",
            "Val: -15044596000.0 1018480700000.0\n",
            "wrong_move\n",
            "  1630/50000: episode: 1235, duration: 0.155s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 185999.359375, mae: 4737.900391, mean_q: -3843.646973\n",
            "Val: -8500.375 -4424.2007\n",
            "wrong_move\n",
            "  1631/50000: episode: 1236, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2219.000 [2219.000, 2219.000],  loss: 547979.937500, mae: 4729.002930, mean_q: -3547.777832\n",
            "Val: -7989.9507 -3815.1948\n",
            "wrong_move\n",
            "  1633/50000: episode: 1237, duration: 0.207s, episode steps:   2, steps per second:  10, episode reward: -5041.000, mean reward: -2520.500 [-5000.000, -41.000], mean action: 1462.500 [706.000, 2219.000],  loss: 141603.000000, mae: 4714.792480, mean_q: -3731.103516\n",
            "Val: -8061.444 -3790.8955\n",
            "wrong_move\n",
            "  1634/50000: episode: 1238, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 706.000 [706.000, 706.000],  loss: 155300.718750, mae: 4708.394531, mean_q: -3746.866699\n",
            "Val: -8018.153 -4296.9346\n",
            "wrong_move\n",
            "  1635/50000: episode: 1239, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2219.000 [2219.000, 2219.000],  loss: 162231.500000, mae: 4711.181641, mean_q: -3777.539551\n",
            "Val: -8221.21 -4389.661\n",
            "wrong_move\n",
            "  1636/50000: episode: 1240, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2219.000 [2219.000, 2219.000],  loss: 142530.093750, mae: 4717.389648, mean_q: -3847.077637\n",
            "Val: -8290.943 -4408.0503\n",
            "wrong_move\n",
            "  1637/50000: episode: 1241, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2219.000 [2219.000, 2219.000],  loss: 210214.500000, mae: 4723.941406, mean_q: -3793.204590\n",
            "Val: -7924.439 -4248.1475\n",
            "wrong_move\n",
            "  1638/50000: episode: 1242, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2219.000 [2219.000, 2219.000],  loss: 222703.515625, mae: 4717.744629, mean_q: -3780.533203\n",
            "Val: -8165.7856 -4490.7993\n",
            "wrong_move\n",
            "  1639/50000: episode: 1243, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2219.000 [2219.000, 2219.000],  loss: 80164.523438, mae: 4710.783203, mean_q: -3912.801025\n",
            "Val: -8295.397 -4589.6064\n",
            "wrong_move\n",
            "  1640/50000: episode: 1244, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 122299.750000, mae: 4706.524414, mean_q: -3749.178223\n",
            "Val: -8193.506 -4590.105\n",
            "wrong_move\n",
            "  1641/50000: episode: 1245, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2947.000 [2947.000, 2947.000],  loss: 230175.281250, mae: 4700.677734, mean_q: -3762.086670\n",
            "Val: -8340.933 -4465.5225\n",
            "wrong_move\n",
            "  1643/50000: episode: 1246, duration: 0.225s, episode steps:   2, steps per second:   9, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 1733.500 [259.000, 3208.000],  loss: 81005944.000000, mae: 4701.004883, mean_q: -2130.332275\n",
            "Val: -8573.16 -3784.463\n",
            "wrong_move\n",
            "  1644/50000: episode: 1247, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2737.000 [2737.000, 2737.000],  loss: 334663.687500, mae: 4730.940430, mean_q: -3807.774902\n",
            "Val: -8746.987 -3796.877\n",
            "wrong_move\n",
            "  1645/50000: episode: 1248, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2737.000 [2737.000, 2737.000],  loss: 266943.312500, mae: 4746.833984, mean_q: -3957.958252\n",
            "Val: -8189.5083 -4749.034\n",
            "wrong_move\n",
            "  1646/50000: episode: 1249, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2947.000 [2947.000, 2947.000],  loss: 257423.171875, mae: 4749.054688, mean_q: -3882.794678\n",
            "Val: -8183.195 -4749.3105\n",
            "wrong_move\n",
            "  1647/50000: episode: 1250, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2278.000 [2278.000, 2278.000],  loss: 567928.625000, mae: 4742.004883, mean_q: -3945.922363\n",
            "Val: -7551.585 -3780.0798\n",
            "wrong_move\n",
            "  1648/50000: episode: 1251, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2737.000 [2737.000, 2737.000],  loss: 303698.062500, mae: 4724.171875, mean_q: -3864.331299\n",
            "Val: -6802.116 -3988.983\n",
            "wrong_move\n",
            "  1649/50000: episode: 1252, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2673.000 [2673.000, 2673.000],  loss: 208249.531250, mae: 4708.257324, mean_q: -3760.329102\n",
            "Val: -7198.9097 -4698.562\n",
            "wrong_move\n",
            "  1650/50000: episode: 1253, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2278.000 [2278.000, 2278.000],  loss: 413484.687500, mae: 4695.195312, mean_q: -3817.892090\n",
            "Val: -6804.7 -4504.029\n",
            "wrong_move\n",
            "  1651/50000: episode: 1254, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2278.000 [2278.000, 2278.000],  loss: 161960.843750, mae: 4688.215332, mean_q: -3701.518799\n",
            "Val: -7234.935 -4618.6455\n",
            "wrong_move\n",
            "  1652/50000: episode: 1255, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2278.000 [2278.000, 2278.000],  loss: 346025.937500, mae: 4682.724609, mean_q: -2683.733887\n",
            "Val: -7457.3457 -4549.754\n",
            "wrong_move\n",
            "  1653/50000: episode: 1256, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2278.000 [2278.000, 2278.000],  loss: 165879.156250, mae: 4678.680664, mean_q: -3576.091309\n",
            "Val: -7385.3896 -4548.259\n",
            "wrong_move\n",
            "  1654/50000: episode: 1257, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2278.000 [2278.000, 2278.000],  loss: 243964.375000, mae: 4681.196289, mean_q: -3585.787109\n",
            "Val: -7406.225 -4499.2144\n",
            "wrong_move\n",
            "  1655/50000: episode: 1258, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2278.000 [2278.000, 2278.000],  loss: 366525.062500, mae: 4697.283691, mean_q: -3549.688965\n",
            "Val: -6245.704 -3475.5688\n",
            "wrong_move\n",
            "  1656/50000: episode: 1259, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3605.000 [3605.000, 3605.000],  loss: 547367.125000, mae: 4720.222168, mean_q: -3753.551270\n",
            "Val: -6319.9062 -3459.6543\n",
            "wrong_move\n",
            "  1657/50000: episode: 1260, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3605.000 [3605.000, 3605.000],  loss: 228158.109375, mae: 4730.719727, mean_q: -3721.889404\n",
            "Val: -7575.9727 -4468.995\n",
            "wrong_move\n",
            "  1658/50000: episode: 1261, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2278.000 [2278.000, 2278.000],  loss: 385373.937500, mae: 4724.419922, mean_q: -3694.312012\n",
            "Val: -7519.159 -4413.9077\n",
            "wrong_move\n",
            "  1659/50000: episode: 1262, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2278.000 [2278.000, 2278.000],  loss: 270487.000000, mae: 4705.958496, mean_q: -3697.096191\n",
            "Val: -7527.2427 -4356.1094\n",
            "wrong_move\n",
            "  1660/50000: episode: 1263, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2278.000 [2278.000, 2278.000],  loss: 598988.125000, mae: 4688.034180, mean_q: -3633.976318\n",
            "Val: -7091.4526 -4212.666\n",
            "wrong_move\n",
            "  1661/50000: episode: 1264, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2278.000 [2278.000, 2278.000],  loss: 221712.984375, mae: 4670.742676, mean_q: -3615.978027\n",
            "Val: -5989.303 -2519.1309\n",
            "wrong_move\n",
            "  1662/50000: episode: 1265, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3605.000 [3605.000, 3605.000],  loss: 242647.750000, mae: 4660.685059, mean_q: -3438.541260\n",
            "Val: -6451.738 -3420.1963\n",
            "wrong_move\n",
            "  1663/50000: episode: 1266, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2737.000 [2737.000, 2737.000],  loss: 208375.609375, mae: 4662.717773, mean_q: -3489.386230\n",
            "Val: -7521.3 -4042.5942\n",
            "wrong_move\n",
            "  1664/50000: episode: 1267, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 322274.375000, mae: 4671.349609, mean_q: -3471.199707\n",
            "Val: -7452.054 -3922.3132\n",
            "wrong_move\n",
            "  1665/50000: episode: 1268, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 424967.343750, mae: 4687.258301, mean_q: -3491.149902\n",
            "Val: -7579.417 -4066.37\n",
            "wrong_move\n",
            "  1666/50000: episode: 1269, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 79374.703125, mae: 4698.862793, mean_q: -3484.337402\n",
            "Val: -7455.585 -4204.934\n",
            "wrong_move\n",
            "  1667/50000: episode: 1270, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 329717.562500, mae: 4705.501953, mean_q: -3582.612305\n",
            "Val: -7683.751 -4437.343\n",
            "wrong_move\n",
            "  1668/50000: episode: 1271, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 519139.500000, mae: 4709.845703, mean_q: -3567.953125\n",
            "Val: -7315.745 -4412.3203\n",
            "wrong_move\n",
            "  1669/50000: episode: 1272, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 179348.125000, mae: 4712.118652, mean_q: -3636.955811\n",
            "Val: -7962.9546 -4628.8945\n",
            "wrong_move\n",
            "  1670/50000: episode: 1273, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 298631.968750, mae: 4715.100586, mean_q: -3605.667725\n",
            "Val: -7107.689 -2942.5422\n",
            "wrong_move\n",
            "  1671/50000: episode: 1274, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2737.000 [2737.000, 2737.000],  loss: 297094.937500, mae: 4715.134766, mean_q: -3630.832764\n",
            "Val: -7879.2144 -4662.8896\n",
            "wrong_move\n",
            "  1672/50000: episode: 1275, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1381.000 [1381.000, 1381.000],  loss: 266418.312500, mae: 4712.737305, mean_q: -3600.674316\n",
            "Val: -6047.728 -3304.3079\n",
            "wrong_move\n",
            "  1673/50000: episode: 1276, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2737.000 [2737.000, 2737.000],  loss: 141977.968750, mae: 4710.463867, mean_q: -3619.633789\n",
            "Val: -7266.519 -4593.2207\n",
            "wrong_move\n",
            "  1674/50000: episode: 1277, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 181913.125000, mae: 4707.874023, mean_q: -3562.100586\n",
            "Val: -5671.417 -2786.7144\n",
            "wrong_move\n",
            "  1675/50000: episode: 1278, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 706.000 [706.000, 706.000],  loss: 174453.812500, mae: 4710.196289, mean_q: -3615.843750\n",
            "Val: -6351.3086 -3407.4832\n",
            "wrong_move\n",
            "  1676/50000: episode: 1279, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2737.000 [2737.000, 2737.000],  loss: 185401.531250, mae: 4713.567383, mean_q: -3705.833008\n",
            "Val: -6362.86 -3325.7512\n",
            "wrong_move\n",
            "  1677/50000: episode: 1280, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 706.000 [706.000, 706.000],  loss: 92219.406250, mae: 4716.483398, mean_q: -3717.807617\n",
            "Val: -6993.199 -4459.441\n",
            "wrong_move\n",
            "  1678/50000: episode: 1281, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1174.000 [1174.000, 1174.000],  loss: 182663.890625, mae: 4718.627930, mean_q: -3661.159912\n",
            "Val: -6561.8403 -3355.744\n",
            "wrong_move\n",
            "  1679/50000: episode: 1282, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 814.000 [814.000, 814.000],  loss: 200773.156250, mae: 4719.799805, mean_q: -3710.758301\n",
            "Val: -7698.8774 -4793.6685\n",
            "wrong_move\n",
            "  1680/50000: episode: 1283, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2289.000 [2289.000, 2289.000],  loss: 230231.468750, mae: 4718.580566, mean_q: -3763.931152\n",
            "Val: -7548.236 -4679.119\n",
            "wrong_move\n",
            "  1681/50000: episode: 1284, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3929.000 [3929.000, 3929.000],  loss: 85626.468750, mae: 4711.059082, mean_q: -3740.469727\n",
            "Val: -7933.6343 -4666.408\n",
            "wrong_move\n",
            "  1682/50000: episode: 1285, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3929.000 [3929.000, 3929.000],  loss: 444038.562500, mae: 4706.336914, mean_q: -3634.579102\n",
            "Val: -7009.0503 -3345.5933\n",
            "wrong_move\n",
            "  1683/50000: episode: 1286, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 95725.671875, mae: 4699.416016, mean_q: -3687.605469\n",
            "Val: -8357.467 -4617.7114\n",
            "wrong_move\n",
            "  1684/50000: episode: 1287, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3929.000 [3929.000, 3929.000],  loss: 264522.687500, mae: 4697.301758, mean_q: -3677.170410\n",
            "Val: -7742.059 -3248.2146\n",
            "wrong_move\n",
            "  1685/50000: episode: 1288, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 164707.171875, mae: 4696.403320, mean_q: -3638.419434\n",
            "Val: -8675.966 -4546.552\n",
            "wrong_move\n",
            "  1686/50000: episode: 1289, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 130066.242188, mae: 4697.546387, mean_q: -3680.744873\n",
            "Val: -8504.395 -4529.146\n",
            "wrong_move\n",
            "  1687/50000: episode: 1290, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 401022.781250, mae: 4700.600586, mean_q: -3622.162842\n",
            "Val: -8010.251 -4602.441\n",
            "wrong_move\n",
            "  1688/50000: episode: 1291, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 454305.562500, mae: 4704.067871, mean_q: -3596.272705\n",
            "Val: -6824.9116 -3553.3552\n",
            "wrong_move\n",
            "  1689/50000: episode: 1292, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 814.000 [814.000, 814.000],  loss: 236613.640625, mae: 4702.156738, mean_q: -3647.813965\n",
            "Val: -7564.05 -4665.8013\n",
            "wrong_move\n",
            "  1690/50000: episode: 1293, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2585.000 [2585.000, 2585.000],  loss: 217011.625000, mae: 4701.916992, mean_q: -3674.146973\n",
            "Val: -7506.031 -4582.2905\n",
            "wrong_move\n",
            "  1691/50000: episode: 1294, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2585.000 [2585.000, 2585.000],  loss: 240336.625000, mae: 4703.744141, mean_q: -3592.212402\n",
            "Val: -6989.266 -4082.4475\n",
            "wrong_move\n",
            "  1692/50000: episode: 1295, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 706.000 [706.000, 706.000],  loss: 98556.531250, mae: 4699.108398, mean_q: -3488.467285\n",
            "Val: -6980.375 -4130.8706\n",
            "wrong_move\n",
            "  1693/50000: episode: 1296, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 706.000 [706.000, 706.000],  loss: 199032.750000, mae: 4693.773438, mean_q: -3483.637207\n",
            "Val: -6887.8594 -4462.537\n",
            "wrong_move\n",
            "  1694/50000: episode: 1297, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2585.000 [2585.000, 2585.000],  loss: 342548.718750, mae: 4694.602539, mean_q: -3707.379150\n",
            "Val: -7418.9517 -4425.213\n",
            "wrong_move\n",
            "  1695/50000: episode: 1298, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2585.000 [2585.000, 2585.000],  loss: 138611.703125, mae: 4694.104492, mean_q: -3640.982422\n",
            "Val: -6919.418 -3343.6533\n",
            "wrong_move\n",
            "  1696/50000: episode: 1299, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 814.000 [814.000, 814.000],  loss: 197418.781250, mae: 4700.416992, mean_q: -3708.158203\n",
            "Val: -7406.823 -4590.21\n",
            "wrong_move\n",
            "  1697/50000: episode: 1300, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2585.000 [2585.000, 2585.000],  loss: 173718.125000, mae: 4709.720215, mean_q: -3737.166016\n",
            "Val: -7178.6206 -4732.2964\n",
            "wrong_move\n",
            "  1698/50000: episode: 1301, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2585.000 [2585.000, 2585.000],  loss: 116059.273438, mae: 4726.250000, mean_q: -3784.831055\n",
            "Val: -7219.4062 -4774.4014\n",
            "wrong_move\n",
            "  1699/50000: episode: 1302, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2585.000 [2585.000, 2585.000],  loss: 172070.093750, mae: 4736.565430, mean_q: -3830.952393\n",
            "Val: -7234.631 -3465.6821\n",
            "wrong_move\n",
            "  1700/50000: episode: 1303, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2737.000 [2737.000, 2737.000],  loss: 213831.625000, mae: 4740.115234, mean_q: -3860.177979\n",
            "Val: -7315.986 -3555.8892\n",
            "wrong_move\n",
            "  1701/50000: episode: 1304, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2737.000 [2737.000, 2737.000],  loss: 71689.835938, mae: 4742.292969, mean_q: -3993.915039\n",
            "Val: -7384.725 -3708.4895\n",
            "wrong_move\n",
            "  1703/50000: episode: 1305, duration: 0.151s, episode steps:   2, steps per second:  13, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2737.000 [2737.000, 2737.000],  loss: 188160.578125, mae: 4739.114746, mean_q: -3838.676758\n",
            "Val: -7321.504 -3793.3203\n",
            "wrong_move\n",
            "  1704/50000: episode: 1306, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 790.000 [790.000, 790.000],  loss: 189347.343750, mae: 4726.078125, mean_q: -3914.454346\n",
            "Val: -7207.996 -4749.4893\n",
            "wrong_move\n",
            "  1705/50000: episode: 1307, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1632.000 [1632.000, 1632.000],  loss: 210335.875000, mae: 4710.340820, mean_q: -3824.832520\n",
            "Val: -7070.295 -4647.12\n",
            "wrong_move\n",
            "  1706/50000: episode: 1308, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 473721.593750, mae: 4696.463867, mean_q: -3356.199219\n",
            "Val: -6320.019 -3094.335\n",
            "wrong_move\n",
            "  1707/50000: episode: 1309, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: 129296.187500, mae: 4678.996094, mean_q: -3592.721680\n",
            "Val: -7070.9517 -4606.6177\n",
            "wrong_move\n",
            "  1708/50000: episode: 1310, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 120604.093750, mae: 4668.954102, mean_q: -3505.636230\n",
            "Val: -5829.297 -3251.5637\n",
            "wrong_move\n",
            "  1709/50000: episode: 1311, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: 213410.890625, mae: 4666.690430, mean_q: -3157.956543\n",
            "Val: -6943.131 -4248.887\n",
            "wrong_move\n",
            "  1710/50000: episode: 1312, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2278.000 [2278.000, 2278.000],  loss: 274359.968750, mae: 4674.870605, mean_q: -3374.061523\n",
            "Val: -7917.277 -2461.869\n",
            "wrong_move\n",
            "  1711/50000: episode: 1313, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: 284314.437500, mae: 4687.869629, mean_q: -3371.736816\n",
            "Val: -6889.805 -4336.7676\n",
            "wrong_move\n",
            "  1712/50000: episode: 1314, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2278.000 [2278.000, 2278.000],  loss: 265695.687500, mae: 4695.757812, mean_q: -3357.958496\n",
            "Val: -6988.6733 -4434.398\n",
            "wrong_move\n",
            "  1713/50000: episode: 1315, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 154695.812500, mae: 4701.585938, mean_q: -3342.182617\n",
            "Val: -6975.857 -4330.884\n",
            "wrong_move\n",
            "  1714/50000: episode: 1316, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 706.000 [706.000, 706.000],  loss: 140510.796875, mae: 4702.777832, mean_q: -3251.732178\n",
            "Val: -6493.7324 -2338.1133\n",
            "wrong_move\n",
            "  1715/50000: episode: 1317, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: 107794.289062, mae: 4692.982422, mean_q: -3175.909668\n",
            "Val: -6979.3105 -4138.652\n",
            "wrong_move\n",
            "  1716/50000: episode: 1318, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2673.000 [2673.000, 2673.000],  loss: 164328.875000, mae: 4680.714844, mean_q: -3170.216309\n",
            "Val: -5820.722 -2268.0032\n",
            "wrong_move\n",
            "  1717/50000: episode: 1319, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: 108876.164062, mae: 4673.996094, mean_q: -3126.960449\n",
            "Val: -7012.287 -3944.9185\n",
            "wrong_move\n",
            "  1718/50000: episode: 1320, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2673.000 [2673.000, 2673.000],  loss: 184647.500000, mae: 4669.271484, mean_q: -3081.990234\n",
            "Val: -6373.349 -2349.1628\n",
            "wrong_move\n",
            "  1719/50000: episode: 1321, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: 147472.500000, mae: 4674.473145, mean_q: -3192.681641\n",
            "Val: -7187.7344 -4216.4775\n",
            "wrong_move\n",
            "  1720/50000: episode: 1322, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 175747.625000, mae: 4685.948242, mean_q: -3183.820312\n",
            "Val: -6857.38 -2740.1418\n",
            "wrong_move\n",
            "  1721/50000: episode: 1323, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: 800735.437500, mae: 4697.829102, mean_q: -1874.034180\n",
            "Val: -7315.8784 -4364.4224\n",
            "wrong_move\n",
            "  1722/50000: episode: 1324, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 138998.296875, mae: 4696.487793, mean_q: -3258.338867\n",
            "Val: -7206.8115 -4200.4736\n",
            "wrong_move\n",
            "  1723/50000: episode: 1325, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: 221557.406250, mae: 4696.049805, mean_q: -3299.708008\n",
            "Val: -7251.623 -4380.9585\n",
            "wrong_move\n",
            "  1724/50000: episode: 1326, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 96166.710938, mae: 4694.519531, mean_q: -3576.958008\n",
            "Val: -7416.379 -4446.024\n",
            "wrong_move\n",
            "  1725/50000: episode: 1327, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 118887.718750, mae: 4692.736328, mean_q: -3723.483154\n",
            "Val: -7697.5303 -3778.9631\n",
            "wrong_move\n",
            "  1726/50000: episode: 1328, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2148.000 [2148.000, 2148.000],  loss: 145158.437500, mae: 4695.407227, mean_q: -3843.269775\n",
            "Val: -7858.135 -3774.734\n",
            "wrong_move\n",
            "  1727/50000: episode: 1329, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2148.000 [2148.000, 2148.000],  loss: 143337.625000, mae: 4702.119629, mean_q: -3894.998535\n",
            "Val: -7894.3457 -4002.1628\n",
            "wrong_move\n",
            "  1728/50000: episode: 1330, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 218949.218750, mae: 4712.272949, mean_q: -3708.456299\n",
            "Val: -8077.1294 -3793.7532\n",
            "wrong_move\n",
            "  1729/50000: episode: 1331, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2148.000 [2148.000, 2148.000],  loss: 381774.781250, mae: 4721.668945, mean_q: -3559.397461\n",
            "Val: -8005.5415 -3570.9805\n",
            "wrong_move\n",
            "  1730/50000: episode: 1332, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 313000.781250, mae: 4731.057617, mean_q: -3445.484375\n",
            "Val: -8131.9575 -3689.171\n",
            "wrong_move\n",
            "  1731/50000: episode: 1333, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 240430.343750, mae: 4729.480957, mean_q: -3574.476807\n",
            "Val: -8493.6045 -3097.7878\n",
            "wrong_move\n",
            "  1732/50000: episode: 1334, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3124.000 [3124.000, 3124.000],  loss: 148460.750000, mae: 4720.512695, mean_q: -3464.082031\n",
            "Val: -8941.05 -3474.522\n",
            "wrong_move\n",
            "  1733/50000: episode: 1335, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 790.000 [790.000, 790.000],  loss: 194906.812500, mae: 4707.033691, mean_q: -3499.526123\n",
            "Val: -8222.519 -3778.8809\n",
            "wrong_move\n",
            "  1734/50000: episode: 1336, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 294420.031250, mae: 4691.780273, mean_q: -3560.542969\n",
            "Val: -8395.072 -3754.2695\n",
            "wrong_move\n",
            "  1735/50000: episode: 1337, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2148.000 [2148.000, 2148.000],  loss: 190581.187500, mae: 4687.682617, mean_q: -3689.188965\n",
            "Val: -8214.516 -4365.935\n",
            "wrong_move\n",
            "  1736/50000: episode: 1338, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 158520.281250, mae: 4691.762695, mean_q: -3931.596680\n",
            "Val: -8291.113 -4668.661\n",
            "wrong_move\n",
            "  1737/50000: episode: 1339, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3164.000 [3164.000, 3164.000],  loss: 173734.062500, mae: 4697.222656, mean_q: -3859.160645\n",
            "Val: -8147.577 -4564.38\n",
            "wrong_move\n",
            "  1738/50000: episode: 1340, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 127252.187500, mae: 4698.069336, mean_q: -3979.194336\n",
            "Val: -8160.613 -4522.2046\n",
            "wrong_move\n",
            "  1739/50000: episode: 1341, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 145996.843750, mae: 4695.626953, mean_q: -3867.859375\n",
            "Val: -8299.614 -4617.2227\n",
            "wrong_move\n",
            "  1740/50000: episode: 1342, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 435569.875000, mae: 4696.918945, mean_q: -3936.492188\n",
            "Val: -8230.208 -3924.326\n",
            "wrong_move\n",
            "  1741/50000: episode: 1343, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3396.000 [3396.000, 3396.000],  loss: 67880.984375, mae: 4702.190918, mean_q: -3851.676758\n",
            "Val: -8305.449 -4585.821\n",
            "wrong_move\n",
            "  1742/50000: episode: 1344, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2278.000 [2278.000, 2278.000],  loss: 127175.289062, mae: 4707.530273, mean_q: -3912.603516\n",
            "Val: -8368.3545 -4643.796\n",
            "wrong_move\n",
            "  1743/50000: episode: 1345, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 323.000 [323.000, 323.000],  loss: 145150.062500, mae: 4710.220215, mean_q: -3852.861328\n",
            "Val: -8634.462 -3972.109\n",
            "wrong_move\n",
            "  1744/50000: episode: 1346, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 1696914.625000, mae: 4711.778320, mean_q: -3153.789062\n",
            "Val: -8317.694 -4572.7646\n",
            "wrong_move\n",
            "  1745/50000: episode: 1347, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 323.000 [323.000, 323.000],  loss: 151774.750000, mae: 4678.835938, mean_q: -3718.443359\n",
            "Val: -8520.185 -3628.1829\n",
            "wrong_move\n",
            "  1746/50000: episode: 1348, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 335604.343750, mae: 4657.563477, mean_q: -3554.134766\n",
            "Val: -8294.046 -4206.4624\n",
            "wrong_move\n",
            "  1747/50000: episode: 1349, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 173583.187500, mae: 4646.955078, mean_q: -3515.891113\n",
            "Val: -8728.07 -3535.1323\n",
            "wrong_move\n",
            "  1748/50000: episode: 1350, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 191174.171875, mae: 4648.462891, mean_q: -3506.578125\n",
            "Val: -8353.679 -4292.329\n",
            "wrong_move\n",
            "  1749/50000: episode: 1351, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 742.000 [742.000, 742.000],  loss: 158682.093750, mae: 4664.422852, mean_q: -3681.749512\n",
            "Val: -8471.95 -4332.0884\n",
            "wrong_move\n",
            "  1750/50000: episode: 1352, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 122182.968750, mae: 4679.650879, mean_q: -3736.144531\n",
            "Val: -8615.612 -3775.9858\n",
            "wrong_move\n",
            "  1751/50000: episode: 1353, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 132989.171875, mae: 4693.962891, mean_q: -3755.097656\n",
            "Val: -9188.71 -4296.0146\n",
            "wrong_move\n",
            "  1752/50000: episode: 1354, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 221915.640625, mae: 4701.746582, mean_q: -3832.148438\n",
            "Val: -8847.267 -3804.3193\n",
            "wrong_move\n",
            "  1753/50000: episode: 1355, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 402842.187500, mae: 4702.057617, mean_q: -3892.715820\n",
            "Val: -9534.066 -4516.2554\n",
            "wrong_move\n",
            "  1754/50000: episode: 1356, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 323.000 [323.000, 323.000],  loss: 176379.250000, mae: 4699.780273, mean_q: -3937.669922\n",
            "Val: -9279.469 -3793.7087\n",
            "wrong_move\n",
            "  1755/50000: episode: 1357, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 141268.312500, mae: 4694.023438, mean_q: -3870.418457\n",
            "Val: -9594.838 -4380.7407\n",
            "wrong_move\n",
            "  1756/50000: episode: 1358, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 323.000 [323.000, 323.000],  loss: 130344.515625, mae: 4688.142578, mean_q: -3901.948486\n",
            "Val: -9122.67 -4012.978\n",
            "wrong_move\n",
            "  1757/50000: episode: 1359, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 323.000 [323.000, 323.000],  loss: 81784.570312, mae: 4687.425781, mean_q: -3888.011719\n",
            "Val: -10067.147 -4527.2817\n",
            "wrong_move\n",
            "  1758/50000: episode: 1360, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 323.000 [323.000, 323.000],  loss: 150978.484375, mae: 4690.943359, mean_q: -3924.935059\n",
            "Val: -10125.47 -4525.8223\n",
            "wrong_move\n",
            "  1759/50000: episode: 1361, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 323.000 [323.000, 323.000],  loss: 66174.906250, mae: 4697.904297, mean_q: -3874.240234\n",
            "Val: -9647.952 -3796.6719\n",
            "wrong_move\n",
            "  1760/50000: episode: 1362, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2026.000 [2026.000, 2026.000],  loss: 114586.125000, mae: 4705.682617, mean_q: -3988.512207\n",
            "Val: -10276.053 -4631.1025\n",
            "wrong_move\n",
            "  1761/50000: episode: 1363, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 706.000 [706.000, 706.000],  loss: 153145.921875, mae: 4715.828613, mean_q: -3994.110352\n",
            "Val: -10180.959 -4496.284\n",
            "wrong_move\n",
            "  1762/50000: episode: 1364, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 706.000 [706.000, 706.000],  loss: 134518.171875, mae: 4718.489258, mean_q: -3919.520508\n",
            "Val: -9801.921 -3824.5771\n",
            "wrong_move\n",
            "  1763/50000: episode: 1365, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 68.000 [68.000, 68.000],  loss: 141606.531250, mae: 4716.296875, mean_q: -3925.136719\n",
            "Val: -10312.075 -4440.5967\n",
            "wrong_move\n",
            "  1764/50000: episode: 1366, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 99482.234375, mae: 4711.095703, mean_q: -3912.099365\n",
            "Val: -10334.699 -4300.7163\n",
            "wrong_move\n",
            "  1765/50000: episode: 1367, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 189336.906250, mae: 4707.762695, mean_q: -3816.476562\n",
            "Val: -10291.1045 -4030.0476\n",
            "wrong_move\n",
            "  1766/50000: episode: 1368, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 214863.375000, mae: 4708.596680, mean_q: -3768.193848\n",
            "Val: -10354.937 -3909.471\n",
            "wrong_move\n",
            "  1767/50000: episode: 1369, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 142175.062500, mae: 4703.665039, mean_q: -3703.489258\n",
            "Val: -9713.939 -3784.9827\n",
            "wrong_move\n",
            "  1768/50000: episode: 1370, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2372.000 [2372.000, 2372.000],  loss: 174965.140625, mae: 4697.204102, mean_q: -3587.953125\n",
            "Val: -10354.926 -3966.9856\n",
            "wrong_move\n",
            "  1769/50000: episode: 1371, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 123422.484375, mae: 4690.269531, mean_q: -3727.537598\n",
            "Val: -9624.377 -3766.1016\n",
            "wrong_move\n",
            "  1770/50000: episode: 1372, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2372.000 [2372.000, 2372.000],  loss: 137542.250000, mae: 4682.440430, mean_q: -3677.877930\n",
            "Val: -9742.0 -3770.4255\n",
            "wrong_move\n",
            "  1771/50000: episode: 1373, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2372.000 [2372.000, 2372.000],  loss: 98998.312500, mae: 4678.403320, mean_q: -3638.371094\n",
            "Val: -10302.6045 -4064.529\n",
            "wrong_move\n",
            "  1772/50000: episode: 1374, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 91319.617188, mae: 4680.319824, mean_q: -3622.479492\n",
            "Val: -9746.919 -3785.995\n",
            "wrong_move\n",
            "  1773/50000: episode: 1375, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 254.000 [254.000, 254.000],  loss: 111177.335938, mae: 4687.887207, mean_q: -3635.541504\n",
            "Val: -10277.53 -4066.905\n",
            "wrong_move\n",
            "  1774/50000: episode: 1376, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 1002928.625000, mae: 4700.279297, mean_q: -3688.167969\n",
            "Val: -9637.051 -4069.3054\n",
            "wrong_move\n",
            "  1775/50000: episode: 1377, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 246300.437500, mae: 4708.625000, mean_q: -3665.043701\n",
            "Val: -9301.773 -4157.419\n",
            "wrong_move\n",
            "  1776/50000: episode: 1378, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 191735.312500, mae: 4712.899414, mean_q: -3782.368164\n",
            "Val: -9072.371 -4160.2344\n",
            "wrong_move\n",
            "  1777/50000: episode: 1379, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 109046.437500, mae: 4711.840820, mean_q: -3932.951172\n",
            "Val: -9108.3 -3802.5938\n",
            "wrong_move\n",
            "  1778/50000: episode: 1380, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1569.000 [1569.000, 1569.000],  loss: 249616.218750, mae: 4696.491211, mean_q: -3866.695557\n",
            "Val: -8467.282 -3780.6455\n",
            "wrong_move\n",
            "  1779/50000: episode: 1381, duration: 0.133s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 254.000 [254.000, 254.000],  loss: 90168.984375, mae: 4672.520508, mean_q: -3811.567383\n",
            "Val: -8630.768 -3761.3152\n",
            "wrong_move\n",
            "  1780/50000: episode: 1382, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 489.000 [489.000, 489.000],  loss: 96183.515625, mae: 4652.814453, mean_q: -3795.383301\n",
            "Val: -9167.176 -4056.8699\n",
            "wrong_move\n",
            "  1781/50000: episode: 1383, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: 149217.125000, mae: 4642.859863, mean_q: -3818.841553\n",
            "Val: -8602.639 -3918.6257\n",
            "wrong_move\n",
            "  1782/50000: episode: 1384, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: 209573.296875, mae: 4637.787109, mean_q: -3678.862305\n",
            "Val: -8996.771 -3238.0857\n",
            "wrong_move\n",
            "  1783/50000: episode: 1385, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 742.000 [742.000, 742.000],  loss: 458874.062500, mae: 4643.094238, mean_q: -3557.419922\n",
            "Val: -8600.878 -3203.8794\n",
            "wrong_move\n",
            "  1784/50000: episode: 1386, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 742.000 [742.000, 742.000],  loss: 211134.906250, mae: 4656.572266, mean_q: -3676.439941\n",
            "Val: -8957.228 -4329.9062\n",
            "wrong_move\n",
            "  1785/50000: episode: 1387, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: 93107.906250, mae: 4668.205078, mean_q: -3572.970947\n",
            "Val: -8281.614 -2897.0386\n",
            "wrong_move\n",
            "  1786/50000: episode: 1388, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 742.000 [742.000, 742.000],  loss: 149532.468750, mae: 4678.152832, mean_q: -3495.456543\n",
            "Val: -8593.937 -2526.7715\n",
            "wrong_move\n",
            "  1787/50000: episode: 1389, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 742.000 [742.000, 742.000],  loss: 19472620.000000, mae: 4683.586914, mean_q: -2462.293701\n",
            "Val: -9331.383 -4613.255\n",
            "wrong_move\n",
            "  1788/50000: episode: 1390, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 814.000 [814.000, 814.000],  loss: 331292.937500, mae: 4750.484863, mean_q: -3382.273193\n",
            "Val: -8514.037 -2756.409\n",
            "wrong_move\n",
            "  1789/50000: episode: 1391, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 742.000 [742.000, 742.000],  loss: 323944.250000, mae: 4778.001953, mean_q: -3519.548828\n",
            "Val: -9272.479 -4572.7935\n",
            "wrong_move\n",
            "  1790/50000: episode: 1392, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 814.000 [814.000, 814.000],  loss: 333069.562500, mae: 4784.173828, mean_q: -3490.098145\n",
            "Val: -8360.354 -2708.496\n",
            "wrong_move\n",
            "  1791/50000: episode: 1393, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 742.000 [742.000, 742.000],  loss: 433729.125000, mae: 4765.101562, mean_q: -3446.925293\n",
            "Val: -8585.209 -2953.107\n",
            "wrong_move\n",
            "  1792/50000: episode: 1394, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 742.000 [742.000, 742.000],  loss: 311739.500000, mae: 4714.048828, mean_q: -3406.167725\n",
            "Val: -9226.255 -4234.0356\n",
            "wrong_move\n",
            "  1793/50000: episode: 1395, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1174.000 [1174.000, 1174.000],  loss: 121364.195312, mae: 4659.545410, mean_q: -3457.795654\n",
            "Val: -8665.313 -2982.0261\n",
            "wrong_move\n",
            "  1794/50000: episode: 1396, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1174.000 [1174.000, 1174.000],  loss: 293085.437500, mae: 4617.077148, mean_q: -3295.849609\n",
            "Val: -9170.987 -4088.3438\n",
            "wrong_move\n",
            "  1795/50000: episode: 1397, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1174.000 [1174.000, 1174.000],  loss: 223261.375000, mae: 4590.282227, mean_q: -3196.141357\n",
            "Val: -8289.664 -2884.2456\n",
            "wrong_move\n",
            "  1796/50000: episode: 1398, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1174.000 [1174.000, 1174.000],  loss: 157222.828125, mae: 4580.476562, mean_q: -3273.119873\n",
            "Val: -8594.376 -2752.2896\n",
            "wrong_move\n",
            "  1797/50000: episode: 1399, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 254.000 [254.000, 254.000],  loss: 108740.976562, mae: 4583.282227, mean_q: -3229.702637\n",
            "Val: -9004.28 -2261.1414\n",
            "wrong_move\n",
            "  1798/50000: episode: 1400, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 254.000 [254.000, 254.000],  loss: 274737.500000, mae: 4589.417969, mean_q: -3182.044922\n",
            "Val: -9289.354 -3910.244\n",
            "wrong_move\n",
            "  1799/50000: episode: 1401, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 254.000 [254.000, 254.000],  loss: 109779.796875, mae: 4601.991699, mean_q: -3101.208008\n",
            "Val: -9234.444 -3805.0317\n",
            "wrong_move\n",
            "  1800/50000: episode: 1402, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 254.000 [254.000, 254.000],  loss: 92847.265625, mae: 4612.905762, mean_q: -3010.874023\n",
            "Val: -9288.44 -3773.2573\n",
            "wrong_move\n",
            "  1801/50000: episode: 1403, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 254.000 [254.000, 254.000],  loss: 207059.968750, mae: 4624.388672, mean_q: -2933.177246\n",
            "Val: -9210.214 -3668.6301\n",
            "wrong_move\n",
            "  1802/50000: episode: 1404, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 254.000 [254.000, 254.000],  loss: 193542.500000, mae: 4632.534180, mean_q: -2903.585449\n",
            "Val: -8790.1875 -2111.8362\n",
            "wrong_move\n",
            "  1803/50000: episode: 1405, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 254.000 [254.000, 254.000],  loss: 219601.968750, mae: 4638.226074, mean_q: -3015.664062\n",
            "Val: -8337.266 -2859.893\n",
            "wrong_move\n",
            "  1804/50000: episode: 1406, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 254.000 [254.000, 254.000],  loss: 491155.031250, mae: 4642.924316, mean_q: -3346.570068\n",
            "Val: -9266.072 -2424.3474\n",
            "wrong_move\n",
            "  1805/50000: episode: 1407, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 254.000 [254.000, 254.000],  loss: 387099.906250, mae: 4639.140625, mean_q: -3604.513672\n",
            "Val: -7988.2563 -4602.9365\n",
            "wrong_move\n",
            "  1806/50000: episode: 1408, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1640.000 [1640.000, 1640.000],  loss: 86018.781250, mae: 4631.220703, mean_q: -3605.076660\n",
            "Val: -9352.058 -2563.4448\n",
            "wrong_move\n",
            "  1807/50000: episode: 1409, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 254.000 [254.000, 254.000],  loss: 158750.218750, mae: 4627.246582, mean_q: -3687.942871\n",
            "Val: -7979.322 -4611.085\n",
            "wrong_move\n",
            "  1808/50000: episode: 1410, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1640.000 [1640.000, 1640.000],  loss: 100318.578125, mae: 4627.423828, mean_q: -3686.542725\n",
            "Val: -7900.7114 -4387.0737\n",
            "wrong_move\n",
            "  1809/50000: episode: 1411, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 243229.031250, mae: 4627.915039, mean_q: -3725.360596\n",
            "Val: -8575.945 -3778.2476\n",
            "wrong_move\n",
            "  1810/50000: episode: 1412, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 167277.687500, mae: 4634.130859, mean_q: -3892.983643\n",
            "Val: -8703.167 -3796.9456\n",
            "wrong_move\n",
            "  1811/50000: episode: 1413, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 227314.781250, mae: 4646.399414, mean_q: -3855.823730\n",
            "Val: -8052.4688 -4612.1157\n",
            "wrong_move\n",
            "  1812/50000: episode: 1414, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 88481.656250, mae: 4660.191895, mean_q: -3829.858643\n",
            "Val: -9023.131 -3818.0505\n",
            "wrong_move\n",
            "  1813/50000: episode: 1415, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 489.000 [489.000, 489.000],  loss: 220820.750000, mae: 4674.663086, mean_q: -3914.812988\n",
            "Val: -8069.5503 -4636.6064\n",
            "wrong_move\n",
            "  1814/50000: episode: 1416, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2724.000 [2724.000, 2724.000],  loss: 284210.843750, mae: 4685.678711, mean_q: -3953.766602\n",
            "Val: -8573.596 -3512.8335\n",
            "wrong_move\n",
            "  1815/50000: episode: 1417, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3164.000 [3164.000, 3164.000],  loss: 92088.625000, mae: 4680.903320, mean_q: -3994.951660\n",
            "Val: -7965.461 -3669.6543\n",
            "wrong_move\n",
            "  1816/50000: episode: 1418, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1640.000 [1640.000, 1640.000],  loss: 146213.656250, mae: 4668.985352, mean_q: -3618.906250\n",
            "Val: -7857.2427 -2938.4448\n",
            "wrong_move\n",
            "  1817/50000: episode: 1419, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1640.000 [1640.000, 1640.000],  loss: 141622.406250, mae: 4653.425781, mean_q: -3109.218506\n",
            "Val: -8415.149 -2604.0422\n",
            "wrong_move\n",
            "  1818/50000: episode: 1420, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1640.000 [1640.000, 1640.000],  loss: 86195.984375, mae: 4639.947266, mean_q: -2659.235352\n",
            "Val: -8014.5103 -2136.2788\n",
            "wrong_move\n",
            "  1819/50000: episode: 1421, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1640.000 [1640.000, 1640.000],  loss: 151804.062500, mae: 4623.470703, mean_q: -2212.924561\n",
            "Val: -7925.5205 -1673.349\n",
            "wrong_move\n",
            "  1820/50000: episode: 1422, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1640.000 [1640.000, 1640.000],  loss: 185644.109375, mae: 4610.120117, mean_q: -1970.999512\n",
            "Val: -7968.505 -1452.3711\n",
            "wrong_move\n",
            "  1821/50000: episode: 1423, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1640.000 [1640.000, 1640.000],  loss: 144047.906250, mae: 4600.762207, mean_q: -1726.952393\n",
            "Val: -7998.5728 -1247.8112\n",
            "wrong_move\n",
            "  1822/50000: episode: 1424, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1640.000 [1640.000, 1640.000],  loss: 132897.281250, mae: 4598.929688, mean_q: -1532.046509\n",
            "Val: -8787.97 -1854.2125\n",
            "wrong_move\n",
            "  1823/50000: episode: 1425, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1640.000 [1640.000, 1640.000],  loss: 737189.437500, mae: 4599.486328, mean_q: -1374.307861\n",
            "Val: -8008.736 -1837.4178\n",
            "wrong_move\n",
            "  1824/50000: episode: 1426, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1640.000 [1640.000, 1640.000],  loss: 123959.984375, mae: 4603.754395, mean_q: -2107.740234\n",
            "Val: -7987.705 -2427.239\n",
            "wrong_move\n",
            "  1825/50000: episode: 1427, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1640.000 [1640.000, 1640.000],  loss: 278512.468750, mae: 4613.670898, mean_q: -2703.060059\n",
            "Val: -8031.6055 -3290.6384\n",
            "wrong_move\n",
            "  1826/50000: episode: 1428, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1640.000 [1640.000, 1640.000],  loss: 232297.750000, mae: 4625.340820, mean_q: -3309.011475\n",
            "Val: -8025.857 -3925.176\n",
            "wrong_move\n",
            "  1827/50000: episode: 1429, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1174.000 [1174.000, 1174.000],  loss: 139552.062500, mae: 4638.230469, mean_q: -3694.593262\n",
            "Val: -8049.893 -3913.6086\n",
            "wrong_move\n",
            "  1828/50000: episode: 1430, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1174.000 [1174.000, 1174.000],  loss: 99413.242188, mae: 4644.742676, mean_q: -3648.213867\n",
            "Val: -7941.2427 -3823.265\n",
            "wrong_move\n",
            "  1829/50000: episode: 1431, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1174.000 [1174.000, 1174.000],  loss: 163528.437500, mae: 4647.742676, mean_q: -3670.330322\n",
            "Val: -8203.856 -3810.326\n",
            "wrong_move\n",
            "  1830/50000: episode: 1432, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 735.000 [735.000, 735.000],  loss: 95118.539062, mae: 4651.307617, mean_q: -3673.850830\n",
            "Val: -8117.5264 -3888.889\n",
            "wrong_move\n",
            "  1831/50000: episode: 1433, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1174.000 [1174.000, 1174.000],  loss: 91540.484375, mae: 4651.962402, mean_q: -3675.015869\n",
            "Val: -7786.799 -3884.4382\n",
            "wrong_move\n",
            "  1832/50000: episode: 1434, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1174.000 [1174.000, 1174.000],  loss: 74247.062500, mae: 4648.235352, mean_q: -3701.242188\n",
            "Val: -7413.618 -3717.6436\n",
            "wrong_move\n",
            "  1833/50000: episode: 1435, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1174.000 [1174.000, 1174.000],  loss: 374571.250000, mae: 4640.946289, mean_q: -3648.450195\n",
            "Val: -7775.0176 -3851.5623\n",
            "wrong_move\n",
            "  1834/50000: episode: 1436, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1174.000 [1174.000, 1174.000],  loss: 307808.312500, mae: 4630.044922, mean_q: -3689.924805\n",
            "Val: -7660.6655 -3842.765\n",
            "wrong_move\n",
            "  1835/50000: episode: 1437, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1174.000 [1174.000, 1174.000],  loss: 149351.062500, mae: 4620.666992, mean_q: -3561.655518\n",
            "Val: -7701.955 -3834.0532\n",
            "wrong_move\n",
            "  1836/50000: episode: 1438, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1174.000 [1174.000, 1174.000],  loss: 231422.312500, mae: 4612.623047, mean_q: -3613.805664\n",
            "Val: -8188.2305 -3759.206\n",
            "wrong_move\n",
            "  1837/50000: episode: 1439, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2930.000 [2930.000, 2930.000],  loss: 162256.625000, mae: 4604.181641, mean_q: -3635.619629\n",
            "Val: -8325.701 -3771.5713\n",
            "wrong_move\n",
            "  1838/50000: episode: 1440, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2501.000 [2501.000, 2501.000],  loss: 174583.125000, mae: 4599.540039, mean_q: -3535.479248\n",
            "Val: -7902.451 -4175.1523\n",
            "wrong_move\n",
            "  1839/50000: episode: 1441, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1174.000 [1174.000, 1174.000],  loss: 175371.718750, mae: 4595.855469, mean_q: -3586.378906\n",
            "Val: -8521.271 -3740.6658\n",
            "wrong_move\n",
            "  1840/50000: episode: 1442, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 68.000 [68.000, 68.000],  loss: 100998.453125, mae: 4592.236328, mean_q: -3657.149658\n",
            "Val: -38829556000.0 1174064600000.0\n",
            "wrong_move\n",
            "  1841/50000: episode: 1443, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 113617.968750, mae: 4592.071289, mean_q: -3644.957764\n",
            "Val: -8135.83 -4366.934\n",
            "wrong_move\n",
            "  1842/50000: episode: 1444, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 125773.937500, mae: 4599.204102, mean_q: -3536.337891\n",
            "Val: -8115.919 -4438.3335\n",
            "wrong_move\n",
            "  1843/50000: episode: 1445, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 96670.453125, mae: 4615.747070, mean_q: -3781.248535\n",
            "Val: -9514.297 -3353.3042\n",
            "wrong_move\n",
            "  1844/50000: episode: 1446, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 68.000 [68.000, 68.000],  loss: 103402.000000, mae: 4637.985840, mean_q: -3694.438477\n",
            "Val: -8076.8535 -4448.2554\n",
            "wrong_move\n",
            "  1845/50000: episode: 1447, duration: 0.159s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 132283.312500, mae: 4656.375977, mean_q: -3776.010010\n",
            "Val: -8236.65 -4263.897\n",
            "wrong_move\n",
            "  1846/50000: episode: 1448, duration: 0.165s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 304374.531250, mae: 4660.156250, mean_q: -3644.990479\n",
            "Val: -8200.421 -4137.5303\n",
            "wrong_move\n",
            "  1847/50000: episode: 1449, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 333666.093750, mae: 4649.743652, mean_q: -3669.768799\n",
            "Val: -9272.706 -3461.4656\n",
            "wrong_move\n",
            "  1848/50000: episode: 1450, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 68.000 [68.000, 68.000],  loss: 286255.500000, mae: 4635.267090, mean_q: -3592.683594\n",
            "Val: -8450.725 -3618.0916\n",
            "wrong_move\n",
            "  1849/50000: episode: 1451, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 68.000 [68.000, 68.000],  loss: 186390.375000, mae: 4619.386719, mean_q: -3468.196045\n",
            "Val: -8335.559 -3808.6663\n",
            "wrong_move\n",
            "  1850/50000: episode: 1452, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 198088.609375, mae: 4609.072754, mean_q: -3394.462891\n",
            "Val: -7720.9365 -3741.0325\n",
            "wrong_move\n",
            "  1851/50000: episode: 1453, duration: 0.142s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 176156.062500, mae: 4599.785645, mean_q: -3366.172119\n",
            "Val: -7813.7217 -3687.7139\n",
            "wrong_move\n",
            "  1852/50000: episode: 1454, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 101045.203125, mae: 4598.112305, mean_q: -3358.102783\n",
            "Val: -7675.1055 -3511.5305\n",
            "wrong_move\n",
            "  1853/50000: episode: 1455, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 331306.187500, mae: 4599.047852, mean_q: -3340.543457\n",
            "Val: -7921.6367 -3940.15\n",
            "wrong_move\n",
            "  1854/50000: episode: 1456, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 119734.617188, mae: 4603.444336, mean_q: -3568.217285\n",
            "Val: -7997.432 -4319.3223\n",
            "wrong_move\n",
            "  1855/50000: episode: 1457, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 822.000 [822.000, 822.000],  loss: 96299.250000, mae: 4609.817383, mean_q: -3754.445068\n",
            "Val: -7902.8184 -3541.855\n",
            "wrong_move\n",
            "  1856/50000: episode: 1458, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 822.000 [822.000, 822.000],  loss: 94835.617188, mae: 4614.885254, mean_q: -3849.434570\n",
            "Val: -8400.275 -3121.6199\n",
            "wrong_move\n",
            "  1857/50000: episode: 1459, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 822.000 [822.000, 822.000],  loss: 280194.250000, mae: 4615.528809, mean_q: -3735.216309\n",
            "Val: -7902.15 -4312.012\n",
            "wrong_move\n",
            "  1858/50000: episode: 1460, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: 209626.000000, mae: 4613.021484, mean_q: -3800.846191\n",
            "Val: -8162.764 -4403.976\n",
            "wrong_move\n",
            "  1859/50000: episode: 1461, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1619.000 [1619.000, 1619.000],  loss: 292424.000000, mae: 4608.772949, mean_q: -3881.938965\n",
            "Val: -7844.7993 -4302.089\n",
            "wrong_move\n",
            "  1860/50000: episode: 1462, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 194.000 [194.000, 194.000],  loss: 129931.429688, mae: 4602.711426, mean_q: -3868.831055\n",
            "Val: -8097.499 -4421.545\n",
            "wrong_move\n",
            "  1861/50000: episode: 1463, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 194.000 [194.000, 194.000],  loss: 116519.531250, mae: 4593.568848, mean_q: -3786.219238\n",
            "Val: -8080.5923 -4424.655\n",
            "wrong_move\n",
            "  1862/50000: episode: 1464, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 194.000 [194.000, 194.000],  loss: 145007.828125, mae: 4585.315430, mean_q: -3825.790771\n",
            "Val: -8278.326 -3723.3835\n",
            "wrong_move\n",
            "  1863/50000: episode: 1465, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1632.000 [1632.000, 1632.000],  loss: 143534.703125, mae: 4579.185547, mean_q: -3741.476318\n",
            "Val: -8095.0166 -4403.642\n",
            "wrong_move\n",
            "  1864/50000: episode: 1466, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 68778.390625, mae: 4581.529785, mean_q: -3748.962402\n",
            "Val: -8429.226 -3737.917\n",
            "wrong_move\n",
            "  1865/50000: episode: 1467, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1632.000 [1632.000, 1632.000],  loss: 60771.132812, mae: 4588.108398, mean_q: -3683.735352\n",
            "Val: -8115.486 -4289.134\n",
            "wrong_move\n",
            "  1866/50000: episode: 1468, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 63500.105469, mae: 4595.109863, mean_q: -3682.945312\n",
            "Val: -8008.393 -4168.232\n",
            "wrong_move\n",
            "  1867/50000: episode: 1469, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 303753.437500, mae: 4602.171875, mean_q: -3724.666504\n",
            "Val: -8550.627 -3371.0652\n",
            "wrong_move\n",
            "  1869/50000: episode: 1470, duration: 0.170s, episode steps:   2, steps per second:  12, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1569.000 [1569.000, 1569.000],  loss: 292452.406250, mae: 4600.946289, mean_q: -3644.185059\n",
            "Val: -8110.1147 -4240.957\n",
            "wrong_move\n",
            "  1870/50000: episode: 1471, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 83079.273438, mae: 4595.922852, mean_q: -3645.397705\n",
            "Val: -8335.757 -4260.875\n",
            "wrong_move\n",
            "  1871/50000: episode: 1472, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 303519.000000, mae: 4593.248047, mean_q: -3606.955566\n",
            "Val: -8290.064 -4140.254\n",
            "wrong_move\n",
            "  1872/50000: episode: 1473, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 216846.500000, mae: 4589.999023, mean_q: -3664.098633\n",
            "Val: -8503.233 -4217.9146\n",
            "wrong_move\n",
            "  1873/50000: episode: 1474, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 83670.828125, mae: 4588.800293, mean_q: -3572.137207\n",
            "Val: -8446.442 -4144.372\n",
            "wrong_move\n",
            "  1874/50000: episode: 1475, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 238306.515625, mae: 4587.340332, mean_q: -3576.119141\n",
            "Val: -8404.995 -3596.7334\n",
            "wrong_move\n",
            "  1875/50000: episode: 1476, duration: 0.147s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 790.000 [790.000, 790.000],  loss: 94345.234375, mae: 4587.883789, mean_q: -3566.189697\n",
            "Val: -8574.099 -4108.704\n",
            "wrong_move\n",
            "  1876/50000: episode: 1477, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 75787.375000, mae: 4588.797363, mean_q: -3359.250488\n",
            "Val: -8782.787 -3892.4585\n",
            "wrong_move\n",
            "  1877/50000: episode: 1478, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 194.000 [194.000, 194.000],  loss: 140618.796875, mae: 4593.280273, mean_q: -3214.563477\n",
            "Val: -8886.519 -3577.8494\n",
            "wrong_move\n",
            "  1878/50000: episode: 1479, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 194.000 [194.000, 194.000],  loss: 137595.750000, mae: 4602.465332, mean_q: -2997.428711\n",
            "Val: -8387.115 -1731.2545\n",
            "wrong_move\n",
            "  1880/50000: episode: 1480, duration: 0.252s, episode steps:   2, steps per second:   8, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 194.000 [194.000, 194.000],  loss: 127158.203125, mae: 4616.018555, mean_q: -2644.010498\n",
            "Val: -8692.315 -2700.8884\n",
            "wrong_move\n",
            "  1881/50000: episode: 1481, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 194.000 [194.000, 194.000],  loss: 120584.875000, mae: 4622.299805, mean_q: -2352.004395\n",
            "Val: -8418.17 -1031.3705\n",
            "wrong_move\n",
            "  1882/50000: episode: 1482, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 194.000 [194.000, 194.000],  loss: 79837.265625, mae: 4620.845703, mean_q: -2156.012207\n",
            "Val: -8220.839 -947.079\n",
            "wrong_move\n",
            "  1883/50000: episode: 1483, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 194.000 [194.000, 194.000],  loss: 176576.468750, mae: 4615.852539, mean_q: -1938.131958\n",
            "Val: -8025.728 -1031.6499\n",
            "wrong_move\n",
            "  1884/50000: episode: 1484, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 194.000 [194.000, 194.000],  loss: 159645.578125, mae: 4605.594238, mean_q: -2203.930176\n",
            "Val: -8984.0 -3285.1318\n",
            "wrong_move\n",
            "  1885/50000: episode: 1485, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 194.000 [194.000, 194.000],  loss: 123562.437500, mae: 4598.643555, mean_q: -2775.177246\n",
            "Val: -9009.052 -3841.8093\n",
            "wrong_move\n",
            "  1886/50000: episode: 1486, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 194.000 [194.000, 194.000],  loss: 123250.414062, mae: 4597.873047, mean_q: -3214.748047\n",
            "Val: -8947.488 -4214.629\n",
            "wrong_move\n",
            "  1887/50000: episode: 1487, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3164.000 [3164.000, 3164.000],  loss: 143072.500000, mae: 4606.282227, mean_q: -3360.072754\n",
            "Val: -8920.307 -4253.5415\n",
            "wrong_move\n",
            "  1888/50000: episode: 1488, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3164.000 [3164.000, 3164.000],  loss: 135209.937500, mae: 4624.004883, mean_q: -3591.361572\n",
            "Val: -8858.064 -4164.733\n",
            "wrong_move\n",
            "  1890/50000: episode: 1489, duration: 0.287s, episode steps:   2, steps per second:   7, episode reward: -4961.000, mean reward: -2480.500 [-5000.000, 39.000], mean action: 3164.000 [3164.000, 3164.000],  loss: 147363.218750, mae: 4637.478516, mean_q: -3594.440430\n",
            "Val: -8417.832 -3251.079\n",
            "wrong_move\n",
            "  1891/50000: episode: 1490, duration: 0.153s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 59736.148438, mae: 4640.487305, mean_q: -3693.105713\n",
            "Val: -9075.229 -4249.384\n",
            "wrong_move\n",
            "  1892/50000: episode: 1491, duration: 0.154s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3164.000 [3164.000, 3164.000],  loss: 123637.515625, mae: 4639.222656, mean_q: -3702.154297\n",
            "Val: -9126.3125 -4219.614\n",
            "wrong_move\n",
            "  1893/50000: episode: 1492, duration: 0.164s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3164.000 [3164.000, 3164.000],  loss: 229805.703125, mae: 4632.379883, mean_q: -3650.558594\n",
            "Val: -8297.875 -3802.2004\n",
            "wrong_move\n",
            "  1894/50000: episode: 1493, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 72650.265625, mae: 4629.772461, mean_q: -3732.906250\n",
            "Val: -9056.101 -4134.1807\n",
            "wrong_move\n",
            "  1895/50000: episode: 1494, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 91033.390625, mae: 4633.082031, mean_q: -3719.004883\n",
            "Val: -9142.781 -4152.683\n",
            "wrong_move\n",
            "  1896/50000: episode: 1495, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 74960.281250, mae: 4638.106445, mean_q: -3652.128174\n",
            "Val: -9168.232 -4209.4355\n",
            "wrong_move\n",
            "  1897/50000: episode: 1496, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 216772.187500, mae: 4640.766602, mean_q: -3641.897949\n",
            "Val: -8113.076 -3801.5366\n",
            "wrong_move\n",
            "  1898/50000: episode: 1497, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 108249.875000, mae: 4637.983398, mean_q: -3794.041260\n",
            "Val: -9072.67 -4467.6694\n",
            "wrong_move\n",
            "  1899/50000: episode: 1498, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 138241.218750, mae: 4636.646484, mean_q: -3804.727539\n",
            "Val: -9127.535 -4523.132\n",
            "wrong_move\n",
            "  1900/50000: episode: 1499, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 102639.218750, mae: 4630.018066, mean_q: -3893.721191\n",
            "Val: -9061.506 -4526.4595\n",
            "wrong_move\n",
            "  1901/50000: episode: 1500, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3350.000 [3350.000, 3350.000],  loss: 129905.640625, mae: 4620.296387, mean_q: -3953.569336\n",
            "Val: -8088.5503 -3824.12\n",
            "wrong_move\n",
            "  1902/50000: episode: 1501, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2501.000 [2501.000, 2501.000],  loss: 286879.500000, mae: 4606.744629, mean_q: -3849.759033\n",
            "Val: -8957.95 -4445.7974\n",
            "wrong_move\n",
            "  1903/50000: episode: 1502, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 86849.312500, mae: 4593.005859, mean_q: -3854.921631\n",
            "Val: -9038.816 -4452.395\n",
            "wrong_move\n",
            "  1904/50000: episode: 1503, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3350.000 [3350.000, 3350.000],  loss: 178412.500000, mae: 4581.848145, mean_q: -3906.729492\n",
            "Val: -9399.165 -4629.065\n",
            "wrong_move\n",
            "  1905/50000: episode: 1504, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1640.000 [1640.000, 1640.000],  loss: 112130.609375, mae: 4579.502930, mean_q: -3827.244385\n",
            "Val: -8961.141 -4396.5474\n",
            "wrong_move\n",
            "  1906/50000: episode: 1505, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3763.000 [3763.000, 3763.000],  loss: 85392.695312, mae: 4580.260742, mean_q: -3778.939941\n",
            "Val: -8602.073 -4191.057\n",
            "wrong_move\n",
            "  1907/50000: episode: 1506, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 163206.546875, mae: 4585.426758, mean_q: -3763.460205\n",
            "Val: -8038.6714 -3810.2441\n",
            "wrong_move\n",
            "  1908/50000: episode: 1507, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2456.000 [2456.000, 2456.000],  loss: 363104.375000, mae: 4590.892090, mean_q: -3805.517578\n",
            "Val: -9223.254 -4404.8193\n",
            "wrong_move\n",
            "  1909/50000: episode: 1508, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 67816.421875, mae: 4592.761719, mean_q: -3820.620605\n",
            "Val: -8209.72 -3775.3394\n",
            "wrong_move\n",
            "  1910/50000: episode: 1509, duration: 0.159s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 127803.875000, mae: 4594.313477, mean_q: -3790.492676\n",
            "Val: -9059.194 -4252.313\n",
            "wrong_move\n",
            "  1911/50000: episode: 1510, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 115793.562500, mae: 4595.304688, mean_q: -3818.280762\n",
            "Val: -8304.484 -3815.8499\n",
            "wrong_move\n",
            "  1912/50000: episode: 1511, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 476.000 [476.000, 476.000],  loss: 112615.289062, mae: 4597.065430, mean_q: -3732.779297\n",
            "Val: -8418.877 -3815.527\n",
            "wrong_move\n",
            "  1913/50000: episode: 1512, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2456.000 [2456.000, 2456.000],  loss: 140770.062500, mae: 4598.417969, mean_q: -3785.984375\n",
            "Val: -9087.168 -4403.161\n",
            "wrong_move\n",
            "  1914/50000: episode: 1513, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 194.000 [194.000, 194.000],  loss: 224411.890625, mae: 4596.926758, mean_q: -3819.891602\n",
            "Val: -9052.544 -4257.117\n",
            "wrong_move\n",
            "  1915/50000: episode: 1514, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 177082.234375, mae: 4592.174805, mean_q: -3756.744141\n",
            "Val: -8416.233 -3807.4338\n",
            "wrong_move\n",
            "  1916/50000: episode: 1515, duration: 0.158s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2456.000 [2456.000, 2456.000],  loss: 142402.156250, mae: 4593.034668, mean_q: -3813.176758\n",
            "Val: -9116.491 -4406.4814\n",
            "wrong_move\n",
            "  1917/50000: episode: 1516, duration: 0.157s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 194.000 [194.000, 194.000],  loss: 93727.632812, mae: 4593.069336, mean_q: -3768.146729\n",
            "Val: -8988.453 -3818.315\n",
            "wrong_move\n",
            "  1918/50000: episode: 1517, duration: 0.153s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2456.000 [2456.000, 2456.000],  loss: 93915.875000, mae: 4598.891113, mean_q: -3819.681152\n",
            "Val: -9122.806 -4378.4497\n",
            "wrong_move\n",
            "  1919/50000: episode: 1518, duration: 0.145s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2673.000 [2673.000, 2673.000],  loss: 104415.414062, mae: 4605.097168, mean_q: -3779.176758\n",
            "Val: -9156.1045 -4312.144\n",
            "wrong_move\n",
            "  1920/50000: episode: 1519, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2673.000 [2673.000, 2673.000],  loss: 104398.828125, mae: 4612.364258, mean_q: -3778.352051\n",
            "Val: -9000.629 -3744.0762\n",
            "wrong_move\n",
            "  1921/50000: episode: 1520, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1569.000 [1569.000, 1569.000],  loss: 85436.085938, mae: 4619.038574, mean_q: -3767.481445\n",
            "Val: -8664.266 -4160.216\n",
            "wrong_move\n",
            "  1922/50000: episode: 1521, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2673.000 [2673.000, 2673.000],  loss: 99234.609375, mae: 4621.761719, mean_q: -3675.749023\n",
            "Val: -8974.86 -3691.2673\n",
            "wrong_move\n",
            "  1923/50000: episode: 1522, duration: 0.141s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1569.000 [1569.000, 1569.000],  loss: 171160.125000, mae: 4619.910156, mean_q: -3762.355469\n",
            "Val: -9054.474 -4230.1274\n",
            "wrong_move\n",
            "  1924/50000: episode: 1523, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2673.000 [2673.000, 2673.000],  loss: 110725.203125, mae: 4610.344727, mean_q: -3791.195801\n",
            "Val: -8971.537 -4155.4478\n",
            "wrong_move\n",
            "  1925/50000: episode: 1524, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2673.000 [2673.000, 2673.000],  loss: 85653.703125, mae: 4599.813477, mean_q: -3706.768555\n",
            "Val: -9089.589 -3903.5286\n",
            "wrong_move\n",
            "  1926/50000: episode: 1525, duration: 0.145s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3350.000 [3350.000, 3350.000],  loss: 70918.742188, mae: 4593.134766, mean_q: -3636.619141\n",
            "Val: -9180.124 -3572.4465\n",
            "wrong_move\n",
            "  1927/50000: episode: 1526, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 26.000 [26.000, 26.000],  loss: 131939.093750, mae: 4590.719727, mean_q: -3450.791992\n",
            "Val: -9118.496 -3414.3325\n",
            "wrong_move\n",
            "  1928/50000: episode: 1527, duration: 0.170s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3350.000 [3350.000, 3350.000],  loss: 109201.226562, mae: 4596.012207, mean_q: -3407.051514\n",
            "Val: -9103.542 -3760.21\n",
            "wrong_move\n",
            "  1929/50000: episode: 1528, duration: 0.152s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3350.000 [3350.000, 3350.000],  loss: 64096.210938, mae: 4602.965820, mean_q: -3312.454102\n",
            "Val: -9024.522 -3101.015\n",
            "wrong_move\n",
            "  1930/50000: episode: 1529, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3350.000 [3350.000, 3350.000],  loss: 433018.156250, mae: 4605.029785, mean_q: -3221.700684\n",
            "Val: -9090.61 -3475.5579\n",
            "wrong_move\n",
            "  1931/50000: episode: 1530, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3350.000 [3350.000, 3350.000],  loss: 108836.812500, mae: 4602.289062, mean_q: -3148.444092\n",
            "Val: -9108.553 -3089.7808\n",
            "wrong_move\n",
            "  1932/50000: episode: 1531, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3350.000 [3350.000, 3350.000],  loss: 113571.546875, mae: 4601.467285, mean_q: -3199.551514\n",
            "Val: -9144.913 -3829.8699\n",
            "wrong_move\n",
            "  1933/50000: episode: 1532, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4037.000 [4037.000, 4037.000],  loss: 66826.257812, mae: 4605.093750, mean_q: -3563.295410\n",
            "Val: -9194.248 -3727.2266\n",
            "wrong_move\n",
            "  1934/50000: episode: 1533, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3350.000 [3350.000, 3350.000],  loss: 347925.375000, mae: 4611.179688, mean_q: -3713.956543\n",
            "Val: -9120.046 -4098.0776\n",
            "wrong_move\n",
            "  1935/50000: episode: 1534, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3350.000 [3350.000, 3350.000],  loss: 98655.578125, mae: 4614.237305, mean_q: -3729.665527\n",
            "Val: -9193.212 -3887.9424\n",
            "wrong_move\n",
            "  1936/50000: episode: 1535, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 98911.765625, mae: 4619.703125, mean_q: -3540.532715\n",
            "Val: -9076.213 -3462.9895\n",
            "wrong_move\n",
            "  1937/50000: episode: 1536, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 71391.476562, mae: 4622.112305, mean_q: -3303.477783\n",
            "Val: -8341.063 -2931.2625\n",
            "wrong_move\n",
            "  1938/50000: episode: 1537, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 110150.578125, mae: 4622.072266, mean_q: -3063.942383\n",
            "Val: -9286.313 -3087.6792\n",
            "wrong_move\n",
            "  1939/50000: episode: 1538, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 115953.195312, mae: 4617.272949, mean_q: -2843.487305\n",
            "Val: -8643.93 -2732.2842\n",
            "wrong_move\n",
            "  1940/50000: episode: 1539, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 89445.546875, mae: 4610.109375, mean_q: -2691.826660\n",
            "Val: -9160.202 -2645.6248\n",
            "wrong_move\n",
            "  1941/50000: episode: 1540, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 265314.093750, mae: 4602.829102, mean_q: -2547.312988\n",
            "Val: -9140.118 -2471.976\n",
            "wrong_move\n",
            "  1942/50000: episode: 1541, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 81245.000000, mae: 4591.235352, mean_q: -2388.081543\n",
            "Val: -9153.924 -2410.3635\n",
            "wrong_move\n",
            "  1943/50000: episode: 1542, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 255232.718750, mae: 4581.070312, mean_q: -2322.160156\n",
            "Val: -9085.698 -2879.2107\n",
            "wrong_move\n",
            "  1944/50000: episode: 1543, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 138773.406250, mae: 4575.578125, mean_q: -2756.570068\n",
            "Val: -8681.199 -3418.965\n",
            "wrong_move\n",
            "  1945/50000: episode: 1544, duration: 0.163s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 69895.742188, mae: 4571.974609, mean_q: -3311.964844\n",
            "Val: -8188.75 -3511.5618\n",
            "wrong_move\n",
            "  1946/50000: episode: 1545, duration: 0.171s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 74100.414062, mae: 4575.074707, mean_q: -3708.939209\n",
            "Val: -8604.023 -3781.239\n",
            "wrong_move\n",
            "  1947/50000: episode: 1546, duration: 0.175s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: 143210.140625, mae: 4575.254883, mean_q: -3876.629395\n",
            "Val: -9172.306 -4575.9585\n",
            "wrong_move\n",
            "  1948/50000: episode: 1547, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3763.000 [3763.000, 3763.000],  loss: 89565.554688, mae: 4571.410156, mean_q: -3766.326660\n",
            "Val: -9119.846 -4564.665\n",
            "wrong_move\n",
            "  1949/50000: episode: 1548, duration: 0.148s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3763.000 [3763.000, 3763.000],  loss: 149352.375000, mae: 4570.983398, mean_q: -3864.599365\n",
            "Val: -8798.397 -4378.2334\n",
            "wrong_move\n",
            "  1951/50000: episode: 1549, duration: 0.322s, episode steps:   2, steps per second:   6, episode reward: -5051.000, mean reward: -2525.500 [-5000.000, -51.000], mean action: 512.500 [259.000, 766.000],  loss: 220435.625000, mae: 4577.490234, mean_q: -3895.645264\n",
            "Val: -9146.752 -4600.4463\n",
            "wrong_move\n",
            "  1952/50000: episode: 1550, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3929.000 [3929.000, 3929.000],  loss: 69035.671875, mae: 4577.724121, mean_q: -3876.651611\n",
            "Val: -9129.816 -4554.034\n",
            "wrong_move\n",
            "  1953/50000: episode: 1551, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3929.000 [3929.000, 3929.000],  loss: 345847.781250, mae: 4578.649902, mean_q: -3842.113525\n",
            "Val: -8741.313 -4021.054\n",
            "wrong_move\n",
            "  1954/50000: episode: 1552, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 128646.039062, mae: 4571.933594, mean_q: -3659.214844\n",
            "Val: -9073.88 -3860.0354\n",
            "wrong_move\n",
            "  1955/50000: episode: 1553, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 115185.914062, mae: 4560.007812, mean_q: -3404.881836\n",
            "Val: -9283.693 -3612.0383\n",
            "wrong_move\n",
            "  1956/50000: episode: 1554, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 99955.234375, mae: 4553.930664, mean_q: -3354.987305\n",
            "Val: -9075.629 -3344.9297\n",
            "wrong_move\n",
            "  1957/50000: episode: 1555, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 163245.687500, mae: 4555.103027, mean_q: -3340.668945\n",
            "Val: -9327.043 -3518.7805\n",
            "wrong_move\n",
            "  1958/50000: episode: 1556, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 75155.390625, mae: 4558.398926, mean_q: -3381.572266\n",
            "Val: -9772.949 -3666.3855\n",
            "wrong_move\n",
            "  1959/50000: episode: 1557, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 176173.359375, mae: 4565.644531, mean_q: -3526.782471\n",
            "Val: -9736.241 -3652.1387\n",
            "wrong_move\n",
            "  1960/50000: episode: 1558, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 26.000 [26.000, 26.000],  loss: 75995.453125, mae: 4573.615723, mean_q: -3687.834717\n",
            "Val: -10215.257 -4407.4697\n",
            "wrong_move\n",
            "  1961/50000: episode: 1559, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3350.000 [3350.000, 3350.000],  loss: 97258.281250, mae: 4581.923340, mean_q: -3736.308594\n",
            "Val: -10562.933 -4321.9062\n",
            "wrong_move\n",
            "  1962/50000: episode: 1560, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3350.000 [3350.000, 3350.000],  loss: 269311.781250, mae: 4579.895996, mean_q: -3667.572266\n",
            "Val: -9666.6455 -3774.8875\n",
            "wrong_move\n",
            "  1963/50000: episode: 1561, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1632.000 [1632.000, 1632.000],  loss: 102482.890625, mae: 4573.125000, mean_q: -3677.074219\n",
            "Val: -10610.776 -4410.0195\n",
            "wrong_move\n",
            "  1964/50000: episode: 1562, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 98109.312500, mae: 4565.514160, mean_q: -3724.472168\n",
            "Val: -10816.003 -4492.865\n",
            "wrong_move\n",
            "  1965/50000: episode: 1563, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3350.000 [3350.000, 3350.000],  loss: 143038.671875, mae: 4563.208984, mean_q: -3703.968262\n",
            "Val: -10660.157 -4450.6147\n",
            "wrong_move\n",
            "  1966/50000: episode: 1564, duration: 0.154s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 26.000 [26.000, 26.000],  loss: 74378.875000, mae: 4559.176758, mean_q: -3782.779053\n",
            "Val: -9526.447 -3548.0823\n",
            "wrong_move\n",
            "  1967/50000: episode: 1565, duration: 0.163s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 26.000 [26.000, 26.000],  loss: 139055.062500, mae: 4557.256348, mean_q: -3823.096680\n",
            "Val: -10886.443 -4506.0977\n",
            "wrong_move\n",
            "  1968/50000: episode: 1566, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 26.000 [26.000, 26.000],  loss: 94778.156250, mae: 4558.180664, mean_q: -3772.996582\n",
            "Val: -10814.569 -4479.7246\n",
            "wrong_move\n",
            "  1969/50000: episode: 1567, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 26.000 [26.000, 26.000],  loss: 683412.687500, mae: 4561.377441, mean_q: -3803.390137\n",
            "Val: -10207.148 -4531.74\n",
            "wrong_move\n",
            "  1970/50000: episode: 1568, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 26.000 [26.000, 26.000],  loss: 97016.171875, mae: 4556.521973, mean_q: -3722.297852\n",
            "Val: -9604.504 -4543.8545\n",
            "wrong_move\n",
            "  1971/50000: episode: 1569, duration: 0.203s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3233.000 [3233.000, 3233.000],  loss: 423763.218750, mae: 4554.009766, mean_q: -3877.543213\n",
            "Val: -9139.893 -4544.909\n",
            "wrong_move\n",
            "  1972/50000: episode: 1570, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 822.000 [822.000, 822.000],  loss: 73654.101562, mae: 4548.966309, mean_q: -3894.880615\n",
            "Val: -8153.1016 -3768.2273\n",
            "wrong_move\n",
            "  1973/50000: episode: 1571, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2919.000 [2919.000, 2919.000],  loss: 66720.296875, mae: 4541.435547, mean_q: -3881.723145\n",
            "Val: -8220.703 -4386.4863\n",
            "wrong_move\n",
            "  1974/50000: episode: 1572, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 194.000 [194.000, 194.000],  loss: 100624.140625, mae: 4532.323730, mean_q: -3872.620361\n",
            "Val: -7973.2188 -4301.154\n",
            "wrong_move\n",
            "  1975/50000: episode: 1573, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 194.000 [194.000, 194.000],  loss: 44736.304688, mae: 4520.169922, mean_q: -3804.783936\n",
            "Val: -8086.0156 -3741.351\n",
            "wrong_move\n",
            "  1976/50000: episode: 1574, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3377.000 [3377.000, 3377.000],  loss: 81761.648438, mae: 4515.889160, mean_q: -3729.079102\n",
            "Val: -8118.8594 -3740.2427\n",
            "wrong_move\n",
            "  1977/50000: episode: 1575, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1476.000 [1476.000, 1476.000],  loss: 123245.734375, mae: 4513.291016, mean_q: -3694.008789\n",
            "Val: -8015.5205 -4198.2295\n",
            "wrong_move\n",
            "  1978/50000: episode: 1576, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 81218.859375, mae: 4514.312012, mean_q: -3700.650391\n",
            "Val: -8005.2793 -4041.2036\n",
            "wrong_move\n",
            "  1979/50000: episode: 1577, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 103495.140625, mae: 4515.814453, mean_q: -3650.228027\n",
            "Val: -8117.9062 -3626.2527\n",
            "wrong_move\n",
            "  1980/50000: episode: 1578, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 50609.492188, mae: 4520.181641, mean_q: -3688.198486\n",
            "Val: -8148.0776 -3754.5735\n",
            "wrong_move\n",
            "  1981/50000: episode: 1579, duration: 0.182s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 766.000 [766.000, 766.000],  loss: 140285.500000, mae: 4527.133301, mean_q: -3651.449707\n",
            "Val: -7980.07 -4228.914\n",
            "wrong_move\n",
            "  1982/50000: episode: 1580, duration: 0.159s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1569.000 [1569.000, 1569.000],  loss: 80237.531250, mae: 4532.032227, mean_q: -3650.857422\n",
            "Val: -8078.2114 -4212.949\n",
            "wrong_move\n",
            "  1983/50000: episode: 1581, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1569.000 [1569.000, 1569.000],  loss: 97925.960938, mae: 4541.483398, mean_q: -3700.928711\n",
            "Val: -8099.6978 -4267.768\n",
            "wrong_move\n",
            "  1984/50000: episode: 1582, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1569.000 [1569.000, 1569.000],  loss: 165120.625000, mae: 4551.797852, mean_q: -3681.610840\n",
            "Val: -8029.412 -4345.135\n",
            "wrong_move\n",
            "  1985/50000: episode: 1583, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1569.000 [1569.000, 1569.000],  loss: 98471.937500, mae: 4554.832031, mean_q: -3593.978516\n",
            "Val: -8041.1553 -4411.4033\n",
            "wrong_move\n",
            "  1987/50000: episode: 1584, duration: 0.164s, episode steps:   2, steps per second:  12, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2663.500 [1569.000, 3758.000],  loss: 93652.695312, mae: 4549.951660, mean_q: -3699.320312\n",
            "Val: -7862.0957 -4348.8296\n",
            "wrong_move\n",
            "  1988/50000: episode: 1585, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 79448.132812, mae: 4538.601074, mean_q: -3615.789307\n",
            "Val: -7924.109 -4177.8857\n",
            "wrong_move\n",
            "  1989/50000: episode: 1586, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 165278.562500, mae: 4529.156250, mean_q: -3638.815430\n",
            "Val: -8063.5723 -4274.898\n",
            "wrong_move\n",
            "  1990/50000: episode: 1587, duration: 0.153s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 96473.867188, mae: 4520.845703, mean_q: -3520.025635\n",
            "Val: -8263.78 -4332.3794\n",
            "wrong_move\n",
            "  1991/50000: episode: 1588, duration: 0.143s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 88222.835938, mae: 4516.974121, mean_q: -3397.198975\n",
            "Val: -8264.411 -4246.6714\n",
            "wrong_move\n",
            "  1992/50000: episode: 1589, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 166542.406250, mae: 4517.127930, mean_q: -3499.655029\n",
            "Val: -8402.275 -3037.1042\n",
            "wrong_move\n",
            "  1994/50000: episode: 1590, duration: 0.245s, episode steps:   2, steps per second:   8, episode reward: -4951.000, mean reward: -2475.500 [-5000.000, 49.000], mean action: 3763.000 [3763.000, 3763.000],  loss: 88724.562500, mae: 4516.393555, mean_q: -3485.790527\n",
            "Val: -8288.14 -4503.567\n",
            "wrong_move\n",
            "  1995/50000: episode: 1591, duration: 0.146s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3233.000 [3233.000, 3233.000],  loss: 455139.781250, mae: 4523.527344, mean_q: -3501.513428\n",
            "Val: -9200.447 -3188.7136\n",
            "wrong_move\n",
            "  1996/50000: episode: 1592, duration: 0.159s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3763.000 [3763.000, 3763.000],  loss: 65788.164062, mae: 4527.735352, mean_q: -3668.058350\n",
            "Val: -8047.349 -4280.046\n",
            "wrong_move\n",
            "  1997/50000: episode: 1593, duration: 0.187s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2398.000 [2398.000, 2398.000],  loss: 61741.171875, mae: 4530.129883, mean_q: -3754.542480\n",
            "Val: -8842.968 -3565.812\n",
            "wrong_move\n",
            "  1998/50000: episode: 1594, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1619.000 [1619.000, 1619.000],  loss: 80621.585938, mae: 4527.245117, mean_q: -3730.675781\n",
            "Val: -8418.372 -4448.7627\n",
            "wrong_move\n",
            "  1999/50000: episode: 1595, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1619.000 [1619.000, 1619.000],  loss: 85012.500000, mae: 4523.757812, mean_q: -3736.587158\n",
            "Val: -8485.126 -4511.925\n",
            "wrong_move\n",
            "  2000/50000: episode: 1596, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3233.000 [3233.000, 3233.000],  loss: 87239.265625, mae: 4519.301758, mean_q: -3757.527344\n",
            "Val: -8481.94 -4503.031\n",
            "wrong_move\n",
            "  2001/50000: episode: 1597, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3233.000 [3233.000, 3233.000],  loss: 169696.328125, mae: 4519.378418, mean_q: -3765.717285\n",
            "Val: -8600.947 -3744.5964\n",
            "wrong_move\n",
            "  2002/50000: episode: 1598, duration: 0.154s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3377.000 [3377.000, 3377.000],  loss: 151867.140625, mae: 4521.206055, mean_q: -3794.410400\n",
            "Val: -8502.171 -4501.4375\n",
            "wrong_move\n",
            "  2003/50000: episode: 1599, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 35379.664062, mae: 4522.614258, mean_q: -3784.096436\n",
            "Val: -8525.591 -4514.283\n",
            "wrong_move\n",
            "  2004/50000: episode: 1600, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 65742.421875, mae: 4528.676758, mean_q: -3832.107422\n",
            "Val: -8374.918 -4488.8955\n",
            "wrong_move\n",
            "  2005/50000: episode: 1601, duration: 0.148s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3513.000 [3513.000, 3513.000],  loss: 259008.062500, mae: 4533.447266, mean_q: -3754.569336\n",
            "Val: -8515.351 -4504.174\n",
            "wrong_move\n",
            "  2006/50000: episode: 1602, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 79953.632812, mae: 4532.770996, mean_q: -3697.216553\n",
            "Val: -8500.639 -4494.775\n",
            "wrong_move\n",
            "  2007/50000: episode: 1603, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 153235.593750, mae: 4533.784180, mean_q: -3796.254395\n",
            "Val: -8348.288 -4365.4507\n",
            "wrong_move\n",
            "  2008/50000: episode: 1604, duration: 0.160s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 128080.257812, mae: 4536.714844, mean_q: -3837.330078\n",
            "Val: -8325.496 -4313.3643\n",
            "wrong_move\n",
            "  2009/50000: episode: 1605, duration: 0.143s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 346599.156250, mae: 4545.717773, mean_q: -3764.066650\n",
            "Val: -8297.113 -4360.531\n",
            "wrong_move\n",
            "  2010/50000: episode: 1606, duration: 0.187s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3513.000 [3513.000, 3513.000],  loss: 71262.687500, mae: 4550.740723, mean_q: -3840.214844\n",
            "Val: -8496.954 -4494.5786\n",
            "wrong_move\n",
            "  2012/50000: episode: 1607, duration: 0.284s, episode steps:   2, steps per second:   7, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 1943.000 [644.000, 3242.000],  loss: 51648.433594, mae: 4557.813477, mean_q: -3809.496094\n",
            "Val: -8533.367 -4459.8438\n",
            "wrong_move\n",
            "  2013/50000: episode: 1608, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 119341.921875, mae: 4558.750000, mean_q: -3807.832031\n",
            "Val: -8720.954 -3634.2275\n",
            "wrong_move\n",
            "  2014/50000: episode: 1609, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3164.000 [3164.000, 3164.000],  loss: 88607.109375, mae: 4552.549316, mean_q: -3785.864746\n",
            "Val: -8589.331 -3643.6318\n",
            "wrong_move\n",
            "  2015/50000: episode: 1610, duration: 0.177s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3164.000 [3164.000, 3164.000],  loss: 110713.906250, mae: 4538.481445, mean_q: -3742.751221\n",
            "Val: -8557.912 -3663.8594\n",
            "wrong_move\n",
            "  2016/50000: episode: 1611, duration: 0.156s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3164.000 [3164.000, 3164.000],  loss: 66720.656250, mae: 4519.729492, mean_q: -3716.891357\n",
            "Val: -8648.075 -3260.0132\n",
            "wrong_move\n",
            "  2017/50000: episode: 1612, duration: 0.176s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 103796.531250, mae: 4505.425781, mean_q: -3534.533203\n",
            "Val: -8764.568 -2756.7202\n",
            "wrong_move\n",
            "  2018/50000: episode: 1613, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 141647.906250, mae: 4500.110840, mean_q: -3420.142578\n",
            "Val: -8272.07 -4430.2354\n",
            "wrong_move\n",
            "  2019/50000: episode: 1614, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 119918.101562, mae: 4498.093750, mean_q: -3423.534668\n",
            "Val: -8283.242 -4443.8145\n",
            "wrong_move\n",
            "  2020/50000: episode: 1615, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1697.000 [1697.000, 1697.000],  loss: 82408.687500, mae: 4500.485352, mean_q: -3114.493652\n",
            "Val: -8209.857 -4413.1714\n",
            "wrong_move\n",
            "  2021/50000: episode: 1616, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1569.000 [1569.000, 1569.000],  loss: 114867.585938, mae: 4503.729004, mean_q: -3206.316895\n",
            "Val: -8345.025 -4404.8374\n",
            "wrong_move\n",
            "  2022/50000: episode: 1617, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 74801.921875, mae: 4508.380859, mean_q: -3101.200684\n",
            "Val: -8726.515 -1857.1295\n",
            "wrong_move\n",
            "  2023/50000: episode: 1618, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 77840.125000, mae: 4510.962402, mean_q: -2950.256836\n",
            "Val: -8784.897 -1960.9539\n",
            "wrong_move\n",
            "  2024/50000: episode: 1619, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 119913.703125, mae: 4519.537109, mean_q: -2955.366211\n",
            "Val: -8462.946 -4378.5757\n",
            "wrong_move\n",
            "  2025/50000: episode: 1620, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2425.000 [2425.000, 2425.000],  loss: 145607.937500, mae: 4531.147461, mean_q: -3152.936523\n",
            "Val: -8218.209 -4155.957\n",
            "wrong_move\n",
            "  2026/50000: episode: 1621, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2425.000 [2425.000, 2425.000],  loss: 75543.460938, mae: 4535.658203, mean_q: -3326.935303\n",
            "Val: -8605.242 -4259.957\n",
            "wrong_move\n",
            "  2027/50000: episode: 1622, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2425.000 [2425.000, 2425.000],  loss: 54282.507812, mae: 4531.239258, mean_q: -3170.538086\n",
            "Val: -8847.266 -2351.668\n",
            "wrong_move\n",
            "  2028/50000: episode: 1623, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 73327.445312, mae: 4527.930176, mean_q: -3216.118164\n",
            "Val: -8850.475 -2380.9702\n",
            "wrong_move\n",
            "  2029/50000: episode: 1624, duration: 0.149s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 308636.843750, mae: 4525.981934, mean_q: -3194.764648\n",
            "Val: -8828.985 -2524.9487\n",
            "wrong_move\n",
            "  2030/50000: episode: 1625, duration: 0.146s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 161915.062500, mae: 4519.233887, mean_q: -3260.247559\n",
            "Val: -8862.685 -4396.989\n",
            "wrong_move\n",
            "  2031/50000: episode: 1626, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2919.000 [2919.000, 2919.000],  loss: 67439.773438, mae: 4513.984863, mean_q: -3402.585938\n",
            "Val: -8769.78 -4417.4443\n",
            "wrong_move\n",
            "  2032/50000: episode: 1627, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2919.000 [2919.000, 2919.000],  loss: 105338.937500, mae: 4514.512695, mean_q: -3511.217529\n",
            "Val: -8587.361 -3102.8674\n",
            "wrong_move\n",
            "  2033/50000: episode: 1628, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 114853.351562, mae: 4518.913086, mean_q: -3434.448730\n",
            "Val: -9679.434 -1453.6218\n",
            "wrong_move\n",
            "  2034/50000: episode: 1629, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 76607.062500, mae: 4523.455078, mean_q: -3349.917236\n",
            "Val: -8988.501 -4381.778\n",
            "wrong_move\n",
            "  2035/50000: episode: 1630, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2919.000 [2919.000, 2919.000],  loss: 102321.578125, mae: 4525.824219, mean_q: -3221.892578\n",
            "Val: -9010.185 -4368.2627\n",
            "wrong_move\n",
            "  2036/50000: episode: 1631, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2919.000 [2919.000, 2919.000],  loss: 132995.250000, mae: 4523.663574, mean_q: -3548.011963\n",
            "Val: -9173.561 -4355.396\n",
            "wrong_move\n",
            "  2037/50000: episode: 1632, duration: 0.186s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2919.000 [2919.000, 2919.000],  loss: 372830.937500, mae: 4513.896973, mean_q: -3307.650146\n",
            "Val: -8920.433 -3521.8093\n",
            "wrong_move\n",
            "  2038/50000: episode: 1633, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 192683.843750, mae: 4499.914062, mean_q: -3662.108398\n",
            "Val: -9141.179 -4227.7935\n",
            "wrong_move\n",
            "  2039/50000: episode: 1634, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 26.000 [26.000, 26.000],  loss: 180837.343750, mae: 4480.713867, mean_q: -3664.044189\n",
            "Val: -8562.345 -3654.381\n",
            "wrong_move\n",
            "  2040/50000: episode: 1635, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3233.000 [3233.000, 3233.000],  loss: 139999.312500, mae: 4463.491211, mean_q: -3656.647217\n",
            "Val: -9061.212 -4313.5513\n",
            "wrong_move\n",
            "  2041/50000: episode: 1636, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1619.000 [1619.000, 1619.000],  loss: 109298.187500, mae: 4451.687500, mean_q: -3615.570801\n",
            "Val: -9040.03 -4248.2896\n",
            "wrong_move\n",
            "  2042/50000: episode: 1637, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1619.000 [1619.000, 1619.000],  loss: 108139.796875, mae: 4450.247559, mean_q: -3495.552490\n",
            "Val: -8645.908 -3411.6648\n",
            "wrong_move\n",
            "  2043/50000: episode: 1638, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3233.000 [3233.000, 3233.000],  loss: 186370.312500, mae: 4459.265625, mean_q: -3343.892090\n",
            "Val: -9151.508 -4054.4424\n",
            "wrong_move\n",
            "  2044/50000: episode: 1639, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1619.000 [1619.000, 1619.000],  loss: 122658.265625, mae: 4465.731934, mean_q: -3478.040039\n",
            "Val: -9049.663 -4002.0354\n",
            "wrong_move\n",
            "  2045/50000: episode: 1640, duration: 0.157s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1619.000 [1619.000, 1619.000],  loss: 251873.984375, mae: 4472.871582, mean_q: -3512.883789\n",
            "Val: -9027.234 -3983.4067\n",
            "wrong_move\n",
            "  2046/50000: episode: 1641, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1619.000 [1619.000, 1619.000],  loss: 85779.062500, mae: 4477.975098, mean_q: -3387.373047\n",
            "Val: -8997.629 -3930.8591\n",
            "wrong_move\n",
            "  2047/50000: episode: 1642, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1619.000 [1619.000, 1619.000],  loss: 127846.890625, mae: 4483.302734, mean_q: -3388.743896\n",
            "Val: -8534.032 -3900.0063\n",
            "wrong_move\n",
            "  2048/50000: episode: 1643, duration: 0.141s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1619.000 [1619.000, 1619.000],  loss: 115965.429688, mae: 4485.774902, mean_q: -3394.087646\n",
            "Val: -8005.3306 -3885.8118\n",
            "wrong_move\n",
            "  2049/50000: episode: 1644, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1619.000 [1619.000, 1619.000],  loss: 79372.179688, mae: 4486.931641, mean_q: -3354.841797\n",
            "Val: -7911.4966 -3875.2197\n",
            "wrong_move\n",
            "  2050/50000: episode: 1645, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1619.000 [1619.000, 1619.000],  loss: 107140.132812, mae: 4490.457520, mean_q: -3376.547852\n",
            "Val: -8750.75 -3279.3835\n",
            "wrong_move\n",
            "  2051/50000: episode: 1646, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 766.000 [766.000, 766.000],  loss: 120523.648438, mae: 4497.280273, mean_q: -3389.350098\n",
            "Val: -8934.956 -2775.3542\n",
            "wrong_move\n",
            "  2052/50000: episode: 1647, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 766.000 [766.000, 766.000],  loss: 212767.671875, mae: 4496.825684, mean_q: -3269.178711\n",
            "Val: -8963.151 -2712.5146\n",
            "wrong_move\n",
            "  2053/50000: episode: 1648, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 766.000 [766.000, 766.000],  loss: 341202.468750, mae: 4495.755371, mean_q: -3380.423340\n",
            "Val: -9198.93 -2613.0286\n",
            "wrong_move\n",
            "  2054/50000: episode: 1649, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 766.000 [766.000, 766.000],  loss: 121094.296875, mae: 4503.696289, mean_q: -3431.689453\n",
            "Val: -7860.756 -4255.145\n",
            "wrong_move\n",
            "  2055/50000: episode: 1650, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2425.000 [2425.000, 2425.000],  loss: 111819.382812, mae: 4512.708008, mean_q: -3326.081787\n",
            "Val: -9687.456 -3288.5405\n",
            "wrong_move\n",
            "  2056/50000: episode: 1651, duration: 0.171s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 306905.906250, mae: 4520.728516, mean_q: -3386.626221\n",
            "Val: -9142.472 -2619.0334\n",
            "wrong_move\n",
            "  2057/50000: episode: 1652, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 766.000 [766.000, 766.000],  loss: 187227.531250, mae: 4521.725586, mean_q: -3347.844238\n",
            "Val: -7933.0376 -4395.313\n",
            "wrong_move\n",
            "  2058/50000: episode: 1653, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 86871.273438, mae: 4522.181641, mean_q: -3517.043945\n",
            "Val: -7943.1016 -4191.7026\n",
            "wrong_move\n",
            "  2059/50000: episode: 1654, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 97445.710938, mae: 4522.280273, mean_q: -3547.714600\n",
            "Val: -9051.088 -3309.4338\n",
            "wrong_move\n",
            "  2060/50000: episode: 1655, duration: 0.155s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3513.000 [3513.000, 3513.000],  loss: 67627.875000, mae: 4516.565918, mean_q: -3410.557129\n",
            "Val: -7754.3223 -3677.2024\n",
            "wrong_move\n",
            "  2061/50000: episode: 1656, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 68689.914062, mae: 4506.274902, mean_q: -3423.716309\n",
            "Val: -7940.093 -4194.679\n",
            "wrong_move\n",
            "  2062/50000: episode: 1657, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 170070.625000, mae: 4499.208984, mean_q: -3515.983154\n",
            "Val: -7669.344 -3398.6155\n",
            "wrong_move\n",
            "  2063/50000: episode: 1658, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3242.000 [3242.000, 3242.000],  loss: 236293.156250, mae: 4490.477539, mean_q: -3109.251709\n",
            "Val: -7895.9688 -2940.6987\n",
            "wrong_move\n",
            "  2064/50000: episode: 1659, duration: 0.150s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3242.000 [3242.000, 3242.000],  loss: 97395.273438, mae: 4481.192871, mean_q: -2724.934814\n",
            "Val: -7886.2183 -2407.2146\n",
            "wrong_move\n",
            "  2065/50000: episode: 1660, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3242.000 [3242.000, 3242.000],  loss: 73863.671875, mae: 4470.971680, mean_q: -2321.868652\n",
            "Val: -8329.948 -2688.9153\n",
            "wrong_move\n",
            "  2066/50000: episode: 1661, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3513.000 [3513.000, 3513.000],  loss: 74517.125000, mae: 4460.711426, mean_q: -2439.071289\n",
            "Val: -7862.017 -2870.4863\n",
            "wrong_move\n",
            "  2067/50000: episode: 1662, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3242.000 [3242.000, 3242.000],  loss: 124902.812500, mae: 4457.469727, mean_q: -2711.206055\n",
            "Val: -7864.0444 -3417.7698\n",
            "wrong_move\n",
            "  2068/50000: episode: 1663, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3242.000 [3242.000, 3242.000],  loss: 112583.437500, mae: 4455.524414, mean_q: -3074.979492\n",
            "Val: -8122.854 -2926.5073\n",
            "wrong_move\n",
            "  2069/50000: episode: 1664, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3513.000 [3513.000, 3513.000],  loss: 74821.789062, mae: 4458.479004, mean_q: -3264.920898\n",
            "Val: -8124.215 -2801.502\n",
            "wrong_move\n",
            "  2070/50000: episode: 1665, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3513.000 [3513.000, 3513.000],  loss: 96760.890625, mae: 4467.723145, mean_q: -3111.688965\n",
            "Val: -7858.8853 -3421.0664\n",
            "wrong_move\n",
            "  2071/50000: episode: 1666, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3377.000 [3377.000, 3377.000],  loss: 204164.203125, mae: 4483.244141, mean_q: -3083.259521\n",
            "Val: -7865.927 -3251.7302\n",
            "wrong_move\n",
            "  2072/50000: episode: 1667, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3377.000 [3377.000, 3377.000],  loss: 98155.789062, mae: 4499.134277, mean_q: -3198.106934\n",
            "Val: -7835.3726 -3240.0085\n",
            "wrong_move\n",
            "  2073/50000: episode: 1668, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3377.000 [3377.000, 3377.000],  loss: 169810.140625, mae: 4501.706543, mean_q: -3130.078857\n",
            "Val: -8563.915 -2978.2622\n",
            "wrong_move\n",
            "  2074/50000: episode: 1669, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2398.000 [2398.000, 2398.000],  loss: 165086.031250, mae: 4498.612305, mean_q: -3220.781738\n",
            "Val: -8217.236 -3083.1042\n",
            "wrong_move\n",
            "  2075/50000: episode: 1670, duration: 0.185s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2398.000 [2398.000, 2398.000],  loss: 41402.046875, mae: 4486.172852, mean_q: -3267.468994\n",
            "Val: -7865.9307 -3791.431\n",
            "wrong_move\n",
            "  2076/50000: episode: 1671, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2398.000 [2398.000, 2398.000],  loss: 84166.484375, mae: 4476.990723, mean_q: -3260.131836\n",
            "Val: -7833.0303 -3940.5417\n",
            "wrong_move\n",
            "  2077/50000: episode: 1672, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1697.000 [1697.000, 1697.000],  loss: 82463.562500, mae: 4477.299805, mean_q: -3490.049072\n",
            "Val: -8119.6704 -3700.914\n",
            "wrong_move\n",
            "  2078/50000: episode: 1673, duration: 0.183s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2398.000 [2398.000, 2398.000],  loss: 114182.125000, mae: 4480.650391, mean_q: -3609.273682\n",
            "Val: -8360.2 -3556.4421\n",
            "wrong_move\n",
            "  2079/50000: episode: 1674, duration: 0.155s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4037.000 [4037.000, 4037.000],  loss: 81895.054688, mae: 4480.885742, mean_q: -3524.434814\n",
            "Val: -7954.629 -3843.2754\n",
            "wrong_move\n",
            "  2080/50000: episode: 1675, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1697.000 [1697.000, 1697.000],  loss: 129161.695312, mae: 4475.587891, mean_q: -3384.492432\n",
            "Val: -8138.4146 -3595.8345\n",
            "wrong_move\n",
            "  2081/50000: episode: 1676, duration: 0.177s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2026.000 [2026.000, 2026.000],  loss: 79506.898438, mae: 4471.326172, mean_q: -3477.284668\n",
            "Val: -8129.1836 -3553.6125\n",
            "wrong_move\n",
            "  2082/50000: episode: 1677, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2026.000 [2026.000, 2026.000],  loss: 143591.109375, mae: 4468.577148, mean_q: -3317.517578\n",
            "Val: -8019.8975 -3263.4172\n",
            "wrong_move\n",
            "  2083/50000: episode: 1678, duration: 0.208s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3763.000 [3763.000, 3763.000],  loss: 170819.359375, mae: 4469.540527, mean_q: -3369.125977\n",
            "Val: -8219.335 -3601.169\n",
            "wrong_move\n",
            "  2084/50000: episode: 1679, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4037.000 [4037.000, 4037.000],  loss: 141698.468750, mae: 4471.419922, mean_q: -3313.310547\n",
            "Val: -7923.3765 -3529.4324\n",
            "wrong_move\n",
            "  2085/50000: episode: 1680, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3763.000 [3763.000, 3763.000],  loss: 42745.558594, mae: 4476.471680, mean_q: -3374.545898\n",
            "Val: -8202.208 -3742.3442\n",
            "wrong_move\n",
            "  2086/50000: episode: 1681, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3763.000 [3763.000, 3763.000],  loss: 126689.164062, mae: 4482.713867, mean_q: -3364.630371\n",
            "Val: -8155.5405 -3648.081\n",
            "wrong_move\n",
            "  2087/50000: episode: 1682, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3763.000 [3763.000, 3763.000],  loss: 137297.031250, mae: 4482.767578, mean_q: -3442.204102\n",
            "Val: -8087.115 -3740.2922\n",
            "wrong_move\n",
            "  2088/50000: episode: 1683, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1697.000 [1697.000, 1697.000],  loss: 46034.925781, mae: 4481.096680, mean_q: -3384.465820\n",
            "Val: -8094.649 -3631.3623\n",
            "wrong_move\n",
            "  2089/50000: episode: 1684, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1697.000 [1697.000, 1697.000],  loss: 68840.601562, mae: 4481.847656, mean_q: -3378.374512\n",
            "Val: -7876.102 -3593.6199\n",
            "wrong_move\n",
            "  2090/50000: episode: 1685, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3763.000 [3763.000, 3763.000],  loss: 141529.031250, mae: 4483.208008, mean_q: -3530.972656\n",
            "Val: -8671.741 -3311.318\n",
            "wrong_move\n",
            "  2091/50000: episode: 1686, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4037.000 [4037.000, 4037.000],  loss: 105245.421875, mae: 4488.855957, mean_q: -3592.093262\n",
            "Val: -7766.535 -4090.2012\n",
            "wrong_move\n",
            "  2092/50000: episode: 1687, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3763.000 [3763.000, 3763.000],  loss: 74399.578125, mae: 4491.018555, mean_q: -3655.652344\n",
            "Val: -7855.799 -4158.563\n",
            "wrong_move\n",
            "  2093/50000: episode: 1688, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3763.000 [3763.000, 3763.000],  loss: 116272.617188, mae: 4489.042480, mean_q: -3648.782227\n",
            "Val: -7818.3843 -4192.93\n",
            "wrong_move\n",
            "  2094/50000: episode: 1689, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3377.000 [3377.000, 3377.000],  loss: 158477.390625, mae: 4484.233398, mean_q: -3619.588135\n",
            "Val: -7639.528 -3997.6714\n",
            "wrong_move\n",
            "  2095/50000: episode: 1690, duration: 0.168s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3377.000 [3377.000, 3377.000],  loss: 120372.343750, mae: 4478.246094, mean_q: -3688.142578\n",
            "Val: -7850.8403 -4030.6248\n",
            "wrong_move\n",
            "  2096/50000: episode: 1691, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3377.000 [3377.000, 3377.000],  loss: 48925.617188, mae: 4475.151367, mean_q: -3616.890625\n",
            "Val: -8171.5884 -3663.3318\n",
            "wrong_move\n",
            "  2097/50000: episode: 1692, duration: 0.161s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4037.000 [4037.000, 4037.000],  loss: 274965.593750, mae: 4476.743652, mean_q: -3609.133301\n",
            "Val: -7810.1084 -3910.7966\n",
            "wrong_move\n",
            "  2098/50000: episode: 1693, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3377.000 [3377.000, 3377.000],  loss: 107162.140625, mae: 4477.364258, mean_q: -3609.960938\n",
            "Val: -7833.5254 -3884.9941\n",
            "wrong_move\n",
            "  2099/50000: episode: 1694, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3377.000 [3377.000, 3377.000],  loss: 141601.953125, mae: 4470.594727, mean_q: -3553.858887\n",
            "Val: -8232.818 -3611.0505\n",
            "wrong_move\n",
            "  2100/50000: episode: 1695, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4037.000 [4037.000, 4037.000],  loss: 82838.445312, mae: 4467.267578, mean_q: -3522.892578\n",
            "Val: -8022.634 -3675.2556\n",
            "wrong_move\n",
            "  2101/50000: episode: 1696, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1569.000 [1569.000, 1569.000],  loss: 154166.234375, mae: 4467.985352, mean_q: -3594.324707\n",
            "Val: -7820.106 -3996.0254\n",
            "wrong_move\n",
            "  2102/50000: episode: 1697, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2398.000 [2398.000, 2398.000],  loss: 93072.437500, mae: 4470.365234, mean_q: -3528.440918\n",
            "Val: -7812.811 -3923.7856\n",
            "wrong_move\n",
            "  2103/50000: episode: 1698, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2398.000 [2398.000, 2398.000],  loss: 140122.984375, mae: 4470.575195, mean_q: -3529.009277\n",
            "Val: -8228.994 -3367.6592\n",
            "wrong_move\n",
            "  2104/50000: episode: 1699, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1569.000 [1569.000, 1569.000],  loss: 92680.937500, mae: 4472.750000, mean_q: -3622.223633\n",
            "Val: -10236.508 -3210.3162\n",
            "wrong_move\n",
            "  2105/50000: episode: 1700, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 323.000 [323.000, 323.000],  loss: 105580.687500, mae: 4480.857910, mean_q: -3551.500977\n",
            "Val: -7848.4497 -3802.4265\n",
            "wrong_move\n",
            "  2106/50000: episode: 1701, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2398.000 [2398.000, 2398.000],  loss: 88469.562500, mae: 4488.145508, mean_q: -3621.156250\n",
            "Val: -8205.255 -3716.4246\n",
            "wrong_move\n",
            "  2107/50000: episode: 1702, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1569.000 [1569.000, 1569.000],  loss: 131935.250000, mae: 4494.923828, mean_q: -3575.301025\n",
            "Val: -7852.57 -3798.35\n",
            "wrong_move\n",
            "  2108/50000: episode: 1703, duration: 0.133s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2398.000 [2398.000, 2398.000],  loss: 97259.781250, mae: 4497.541504, mean_q: -3556.653320\n",
            "Val: -7927.153 -3820.0989\n",
            "wrong_move\n",
            "  2109/50000: episode: 1704, duration: 0.162s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2398.000 [2398.000, 2398.000],  loss: 103525.046875, mae: 4498.896973, mean_q: -3624.655762\n",
            "Val: -7857.159 -3717.2593\n",
            "wrong_move\n",
            "  2110/50000: episode: 1705, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2398.000 [2398.000, 2398.000],  loss: 93492.375000, mae: 4496.809570, mean_q: -3621.369141\n",
            "Val: -7491.8013 -3921.3704\n",
            "wrong_move\n",
            "  2111/50000: episode: 1706, duration: 0.171s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2398.000 [2398.000, 2398.000],  loss: 77578.578125, mae: 4498.585938, mean_q: -3605.348145\n",
            "Val: -6875.7744 -4340.8447\n",
            "wrong_move\n",
            "  2112/50000: episode: 1707, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2398.000 [2398.000, 2398.000],  loss: 67430.351562, mae: 4500.334961, mean_q: -3850.238770\n",
            "Val: -7286.35 -3764.5393\n",
            "wrong_move\n",
            "  2113/50000: episode: 1708, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4044.000 [4044.000, 4044.000],  loss: 142980.609375, mae: 4501.482422, mean_q: -3905.471680\n",
            "Val: -6582.4116 -4340.3237\n",
            "wrong_move\n",
            "  2114/50000: episode: 1709, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 247.000 [247.000, 247.000],  loss: 104066.125000, mae: 4502.564453, mean_q: -3825.925781\n",
            "Val: -6395.9634 -3595.0378\n",
            "wrong_move\n",
            "  2115/50000: episode: 1710, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 50.000 [50.000, 50.000],  loss: 121922.000000, mae: 4506.382324, mean_q: -3896.562500\n",
            "Val: -6343.4204 -4365.367\n",
            "wrong_move\n",
            "  2116/50000: episode: 1711, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3.000 [3.000, 3.000],  loss: 74463.117188, mae: 4505.981445, mean_q: -3880.838623\n",
            "Val: -6305.9106 -4214.92\n",
            "wrong_move\n",
            "  2117/50000: episode: 1712, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 247.000 [247.000, 247.000],  loss: 135616.875000, mae: 4504.237305, mean_q: -3785.297852\n",
            "Val: -7424.63 -3472.0625\n",
            "wrong_move\n",
            "  2118/50000: episode: 1713, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 476.000 [476.000, 476.000],  loss: 88869.593750, mae: 4496.078125, mean_q: -3639.532471\n",
            "Val: -7167.2393 -3248.6497\n",
            "wrong_move\n",
            "  2119/50000: episode: 1714, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 476.000 [476.000, 476.000],  loss: 61906.992188, mae: 4488.205566, mean_q: -3337.447754\n",
            "Val: -7458.6816 -2820.0598\n",
            "wrong_move\n",
            "  2120/50000: episode: 1715, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 476.000 [476.000, 476.000],  loss: 97202.851562, mae: 4488.327148, mean_q: -3210.940674\n",
            "Val: -6518.9365 -3480.121\n",
            "wrong_move\n",
            "  2121/50000: episode: 1716, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 476.000 [476.000, 476.000],  loss: 134699.718750, mae: 4484.023438, mean_q: -3128.431152\n",
            "Val: -6562.305 -3227.8508\n",
            "wrong_move\n",
            "  2122/50000: episode: 1717, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 476.000 [476.000, 476.000],  loss: 74087.625000, mae: 4478.410156, mean_q: -2967.303711\n",
            "Val: -6581.131 -3096.5698\n",
            "wrong_move\n",
            "  2123/50000: episode: 1718, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 476.000 [476.000, 476.000],  loss: 40952.296875, mae: 4469.195312, mean_q: -2810.498535\n",
            "Val: -6634.747 -3010.6038\n",
            "wrong_move\n",
            "  2124/50000: episode: 1719, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 476.000 [476.000, 476.000],  loss: 37990.859375, mae: 4462.178711, mean_q: -2725.762451\n",
            "Val: -6667.1196 -2910.2927\n",
            "wrong_move\n",
            "  2125/50000: episode: 1720, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 476.000 [476.000, 476.000],  loss: 275667.281250, mae: 4457.926758, mean_q: -2593.761230\n",
            "Val: -6717.633 -3249.9622\n",
            "wrong_move\n",
            "  2126/50000: episode: 1721, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 476.000 [476.000, 476.000],  loss: 83444.390625, mae: 4459.358398, mean_q: -2844.105469\n",
            "Val: -6783.422 -3865.358\n",
            "wrong_move\n",
            "  2127/50000: episode: 1722, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 476.000 [476.000, 476.000],  loss: 110020.109375, mae: 4463.108398, mean_q: -3185.095215\n",
            "Val: -6774.7026 -4006.361\n",
            "wrong_move\n",
            "  2128/50000: episode: 1723, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1619.000 [1619.000, 1619.000],  loss: 113411.406250, mae: 4463.713867, mean_q: -3542.854492\n",
            "Val: -6817.119 -4119.717\n",
            "wrong_move\n",
            "  2129/50000: episode: 1724, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: 64451.718750, mae: 4468.825195, mean_q: -3628.535645\n",
            "Val: -6854.7764 -4148.6187\n",
            "wrong_move\n",
            "  2130/50000: episode: 1725, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: 33198.070312, mae: 4480.141602, mean_q: -3692.676514\n",
            "Val: -8561.625 -3519.7776\n",
            "wrong_move\n",
            "  2131/50000: episode: 1726, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1476.000 [1476.000, 1476.000],  loss: 107506.656250, mae: 4491.444336, mean_q: -3761.184082\n",
            "Val: -6847.6514 -3798.9111\n",
            "wrong_move\n",
            "  2132/50000: episode: 1727, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3601.000 [3601.000, 3601.000],  loss: 108549.593750, mae: 4505.444824, mean_q: -3750.645020\n",
            "Val: -6907.865 -4083.0723\n",
            "wrong_move\n",
            "  2133/50000: episode: 1728, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: 83708.734375, mae: 4522.534668, mean_q: -3722.291504\n",
            "Val: -6912.269 -4019.3948\n",
            "wrong_move\n",
            "  2134/50000: episode: 1729, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: 68617.703125, mae: 4531.593750, mean_q: -3796.872559\n",
            "Val: -8383.647 -3635.0627\n",
            "wrong_move\n",
            "  2135/50000: episode: 1730, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1476.000 [1476.000, 1476.000],  loss: 182325.968750, mae: 4536.068848, mean_q: -3779.933594\n",
            "Val: -6869.537 -4052.904\n",
            "wrong_move\n",
            "  2136/50000: episode: 1731, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: 67716.625000, mae: 4530.828613, mean_q: -3782.819092\n",
            "Val: -7644.232 -3803.4927\n",
            "wrong_move\n",
            "  2137/50000: episode: 1732, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2511.000 [2511.000, 2511.000],  loss: 81177.687500, mae: 4524.614746, mean_q: -3693.341797\n",
            "Val: -7608.893 -3937.9175\n",
            "wrong_move\n",
            "  2138/50000: episode: 1733, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: 86711.367188, mae: 4522.084961, mean_q: -3736.651855\n",
            "Val: -7788.7 -3942.3728\n",
            "wrong_move\n",
            "  2139/50000: episode: 1734, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: 213469.171875, mae: 4521.464844, mean_q: -3731.418213\n",
            "Val: -8346.162 -3809.74\n",
            "wrong_move\n",
            "  2140/50000: episode: 1735, duration: 0.205s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1813.000 [1813.000, 1813.000],  loss: 120549.406250, mae: 4516.323730, mean_q: -3562.810059\n",
            "Val: -8157.86 -3811.1855\n",
            "wrong_move\n",
            "  2141/50000: episode: 1736, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: 155431.812500, mae: 4505.443359, mean_q: -3457.761230\n",
            "Val: -8270.87 -3934.621\n",
            "wrong_move\n",
            "  2142/50000: episode: 1737, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: 144904.187500, mae: 4498.945312, mean_q: -3501.411865\n",
            "Val: -7949.9883 -3585.9817\n",
            "wrong_move\n",
            "  2143/50000: episode: 1738, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: 86203.726562, mae: 4491.134766, mean_q: -3403.128662\n",
            "Val: -8705.864 -3309.1946\n",
            "wrong_move\n",
            "  2144/50000: episode: 1739, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: 114095.398438, mae: 4484.327148, mean_q: -3372.409180\n",
            "Val: -8588.918 -3738.2412\n",
            "wrong_move\n",
            "  2145/50000: episode: 1740, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: 388079.562500, mae: 4478.058105, mean_q: -3387.293457\n",
            "Val: -8556.636 -3582.7249\n",
            "wrong_move\n",
            "  2146/50000: episode: 1741, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: 121522.437500, mae: 4469.470703, mean_q: -3307.270508\n",
            "Val: -8769.041 -3633.8176\n",
            "wrong_move\n",
            "  2147/50000: episode: 1742, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: 176933.968750, mae: 4465.053223, mean_q: -3271.898926\n",
            "Val: -7784.6807 -3101.537\n",
            "wrong_move\n",
            "  2148/50000: episode: 1743, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: 63340.195312, mae: 4466.683594, mean_q: -3254.245361\n",
            "Val: -8868.855 -3588.0908\n",
            "wrong_move\n",
            "  2149/50000: episode: 1744, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: 107721.335938, mae: 4473.552734, mean_q: -3294.768555\n",
            "Val: -8506.412 -3387.4236\n",
            "wrong_move\n",
            "  2150/50000: episode: 1745, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: 126778.328125, mae: 4480.160156, mean_q: -3223.468262\n",
            "Val: -8586.693 -3521.3396\n",
            "wrong_move\n",
            "  2151/50000: episode: 1746, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: 109744.242188, mae: 4484.355469, mean_q: -3275.931152\n",
            "Val: -8929.964 -3661.7034\n",
            "wrong_move\n",
            "  2152/50000: episode: 1747, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: 116921.890625, mae: 4477.209961, mean_q: -3330.966309\n",
            "Val: -8973.101 -3691.5464\n",
            "wrong_move\n",
            "  2153/50000: episode: 1748, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: 81154.375000, mae: 4468.832520, mean_q: -3314.178467\n",
            "Val: -8513.978 -4058.0762\n",
            "wrong_move\n",
            "  2154/50000: episode: 1749, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: 63317.093750, mae: 4458.498047, mean_q: -3499.075684\n",
            "Val: -8476.32 -4319.8237\n",
            "wrong_move\n",
            "  2155/50000: episode: 1750, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1865.000 [1865.000, 1865.000],  loss: 237809.328125, mae: 4450.435547, mean_q: -3571.541748\n",
            "Val: -9409.304 -2834.3657\n",
            "wrong_move\n",
            "  2156/50000: episode: 1751, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 323.000 [323.000, 323.000],  loss: 96275.968750, mae: 4448.245117, mean_q: -3710.642334\n",
            "Val: -8128.605 -3671.0288\n",
            "wrong_move\n",
            "  2157/50000: episode: 1752, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4044.000 [4044.000, 4044.000],  loss: 95373.257812, mae: 4446.482422, mean_q: -3549.877930\n",
            "Val: -8519.806 -4324.6934\n",
            "wrong_move\n",
            "  2158/50000: episode: 1753, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1865.000 [1865.000, 1865.000],  loss: 68336.468750, mae: 4453.199219, mean_q: -3569.751465\n",
            "Val: -8522.285 -4298.5854\n",
            "wrong_move\n",
            "  2159/50000: episode: 1754, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1697.000 [1697.000, 1697.000],  loss: 51777.847656, mae: 4464.581055, mean_q: -3503.373779\n",
            "Val: -8099.7183 -3251.5098\n",
            "wrong_move\n",
            "  2160/50000: episode: 1755, duration: 0.147s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3.000 [3.000, 3.000],  loss: 81344.234375, mae: 4473.172363, mean_q: -3443.128418\n",
            "Val: -8046.011 -3947.305\n",
            "wrong_move\n",
            "  2161/50000: episode: 1756, duration: 0.190s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3242.000 [3242.000, 3242.000],  loss: 69803.085938, mae: 4471.966797, mean_q: -3306.801025\n",
            "Val: -8173.604 -2821.2817\n",
            "wrong_move\n",
            "  2162/50000: episode: 1757, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3.000 [3.000, 3.000],  loss: 31108.089844, mae: 4469.694824, mean_q: -3404.056885\n",
            "Val: -8354.026 -4119.1626\n",
            "wrong_move\n",
            "  2163/50000: episode: 1758, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1697.000 [1697.000, 1697.000],  loss: 64629.039062, mae: 4463.399414, mean_q: -3258.698730\n",
            "Val: -8310.177 -2127.5713\n",
            "wrong_move\n",
            "  2164/50000: episode: 1759, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3.000 [3.000, 3.000],  loss: 110748.429688, mae: 4456.052734, mean_q: -3206.794434\n",
            "Val: -8039.385 -2556.249\n",
            "wrong_move\n",
            "  2165/50000: episode: 1760, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3.000 [3.000, 3.000],  loss: 89707.328125, mae: 4446.631836, mean_q: -3086.184570\n",
            "Val: -8543.355 -3923.2566\n",
            "wrong_move\n",
            "  2166/50000: episode: 1761, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1697.000 [1697.000, 1697.000],  loss: 66734.062500, mae: 4440.109375, mean_q: -3018.529297\n",
            "Val: -8544.658 -3927.4778\n",
            "wrong_move\n",
            "  2167/50000: episode: 1762, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1697.000 [1697.000, 1697.000],  loss: 162678.765625, mae: 4443.894531, mean_q: -2938.818848\n",
            "Val: -8188.1187 -2283.5415\n",
            "wrong_move\n",
            "  2168/50000: episode: 1763, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3.000 [3.000, 3.000],  loss: 66409.953125, mae: 4454.021973, mean_q: -3103.505859\n",
            "Val: -8374.531 -1421.8348\n",
            "wrong_move\n",
            "  2169/50000: episode: 1764, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3.000 [3.000, 3.000],  loss: 126457.906250, mae: 4466.694336, mean_q: -3065.745117\n",
            "Val: -8593.581 -4288.732\n",
            "wrong_move\n",
            "  2170/50000: episode: 1765, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2919.000 [2919.000, 2919.000],  loss: 70073.539062, mae: 4484.210938, mean_q: -3055.697021\n",
            "Val: -8341.9375 -4199.0586\n",
            "wrong_move\n",
            "  2171/50000: episode: 1766, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 50197.167969, mae: 4498.779785, mean_q: -3099.685303\n",
            "Val: -8702.272 -4339.1396\n",
            "wrong_move\n",
            "  2172/50000: episode: 1767, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2919.000 [2919.000, 2919.000],  loss: 123772.875000, mae: 4500.104980, mean_q: -3101.895020\n",
            "Val: -8281.947 -2444.163\n",
            "wrong_move\n",
            "  2173/50000: episode: 1768, duration: 0.175s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3.000 [3.000, 3.000],  loss: 47365.507812, mae: 4491.601562, mean_q: -3312.515625\n",
            "Val: -8775.186 -4333.2207\n",
            "wrong_move\n",
            "  2174/50000: episode: 1769, duration: 0.152s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 790.000 [790.000, 790.000],  loss: 115856.421875, mae: 4481.073730, mean_q: -3539.971680\n",
            "Val: -8820.103 -4324.785\n",
            "wrong_move\n",
            "  2175/50000: episode: 1770, duration: 0.149s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 790.000 [790.000, 790.000],  loss: 85968.406250, mae: 4476.267578, mean_q: -3652.423584\n",
            "Val: -8232.661 -3779.5005\n",
            "wrong_move\n",
            "  2176/50000: episode: 1771, duration: 0.146s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: 34870.421875, mae: 4473.333984, mean_q: -3704.870605\n",
            "Val: -8917.549 -4297.525\n",
            "wrong_move\n",
            "  2177/50000: episode: 1772, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 790.000 [790.000, 790.000],  loss: 66410.312500, mae: 4470.963867, mean_q: -3789.240234\n",
            "Val: -8215.497 -3772.6672\n",
            "wrong_move\n",
            "  2178/50000: episode: 1773, duration: 0.196s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2511.000 [2511.000, 2511.000],  loss: 93431.484375, mae: 4469.057129, mean_q: -3745.527344\n",
            "Val: -8906.571 -4357.786\n",
            "wrong_move\n",
            "  2179/50000: episode: 1774, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 790.000 [790.000, 790.000],  loss: 107893.437500, mae: 4473.005859, mean_q: -3842.982422\n",
            "Val: -8975.861 -4292.8916\n",
            "wrong_move\n",
            "  2180/50000: episode: 1775, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 790.000 [790.000, 790.000],  loss: 93783.984375, mae: 4475.453125, mean_q: -3776.583496\n",
            "Val: -8961.967 -4329.981\n",
            "wrong_move\n",
            "  2181/50000: episode: 1776, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 790.000 [790.000, 790.000],  loss: 166482.671875, mae: 4481.872070, mean_q: -3690.058105\n",
            "Val: -8173.1357 -3704.8984\n",
            "wrong_move\n",
            "  2182/50000: episode: 1777, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2030.000 [2030.000, 2030.000],  loss: 69487.531250, mae: 4482.858887, mean_q: -3830.200684\n",
            "Val: -8674.589 -4361.269\n",
            "wrong_move\n",
            "  2183/50000: episode: 1778, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 156671.250000, mae: 4482.840820, mean_q: -3810.691162\n",
            "Val: -8990.188 -4263.986\n",
            "wrong_move\n",
            "  2184/50000: episode: 1779, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 790.000 [790.000, 790.000],  loss: 196628.125000, mae: 4480.589844, mean_q: -3790.519531\n",
            "Val: -9005.845 -4245.8657\n",
            "wrong_move\n",
            "  2185/50000: episode: 1780, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 790.000 [790.000, 790.000],  loss: 49815.304688, mae: 4473.276367, mean_q: -3748.186035\n",
            "Val: -8981.13 -4221.4478\n",
            "wrong_move\n",
            "  2186/50000: episode: 1781, duration: 0.160s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 790.000 [790.000, 790.000],  loss: 348981.718750, mae: 4464.117676, mean_q: -3677.111816\n",
            "Val: -8743.822 -4302.904\n",
            "wrong_move\n",
            "  2187/50000: episode: 1782, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2030.000 [2030.000, 2030.000],  loss: 86565.710938, mae: 4451.416504, mean_q: -3701.140137\n",
            "Val: -7952.654 -3252.0088\n",
            "wrong_move\n",
            "  2188/50000: episode: 1783, duration: 0.162s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3.000 [3.000, 3.000],  loss: 131124.640625, mae: 4446.152832, mean_q: -3663.930664\n",
            "Val: -8230.02 -3353.227\n",
            "wrong_move\n",
            "  2189/50000: episode: 1784, duration: 0.146s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3601.000 [3601.000, 3601.000],  loss: 69823.195312, mae: 4441.671875, mean_q: -3578.418457\n",
            "Val: -9150.319 -4310.3413\n",
            "wrong_move\n",
            "  2190/50000: episode: 1785, duration: 0.163s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3601.000 [3601.000, 3601.000],  loss: 91638.937500, mae: 4441.570312, mean_q: -3477.973145\n",
            "Val: -8520.674 -2921.221\n",
            "wrong_move\n",
            "  2191/50000: episode: 1786, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3601.000 [3601.000, 3601.000],  loss: 123040.851562, mae: 4446.129395, mean_q: -3316.798828\n",
            "Val: -8970.172 -2722.0278\n",
            "wrong_move\n",
            "  2192/50000: episode: 1787, duration: 0.221s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3601.000 [3601.000, 3601.000],  loss: 82094.554688, mae: 4456.010254, mean_q: -3176.450928\n",
            "Val: -8974.608 -3286.7083\n",
            "wrong_move\n",
            "  2193/50000: episode: 1788, duration: 0.181s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 93422.031250, mae: 4461.342285, mean_q: -2998.932617\n",
            "Val: -8761.353 -2629.5647\n",
            "wrong_move\n",
            "  2194/50000: episode: 1789, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 61467.570312, mae: 4461.805176, mean_q: -2547.037598\n",
            "Val: -8887.027 -2173.3547\n",
            "wrong_move\n",
            "  2195/50000: episode: 1790, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 54261.812500, mae: 4458.128906, mean_q: -2180.386963\n",
            "Val: -8543.655 -2135.0618\n",
            "wrong_move\n",
            "  2196/50000: episode: 1791, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 142831.984375, mae: 4450.688965, mean_q: -1859.546753\n",
            "Val: -8527.672 -1275.6827\n",
            "wrong_move\n",
            "  2197/50000: episode: 1792, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 444196.562500, mae: 4449.933594, mean_q: -1663.281250\n",
            "Val: -8249.345 -1196.8198\n",
            "wrong_move\n",
            "  2198/50000: episode: 1793, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 86455.304688, mae: 4448.696777, mean_q: -1495.688354\n",
            "Val: -8055.3755 -869.6082\n",
            "wrong_move\n",
            "  2199/50000: episode: 1794, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 246218.109375, mae: 4447.736816, mean_q: -1347.895996\n",
            "Val: -8467.61 -1000.52167\n",
            "wrong_move\n",
            "  2200/50000: episode: 1795, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 107076.882812, mae: 4444.789551, mean_q: -1204.678467\n",
            "Val: -7949.192 -545.64154\n",
            "wrong_move\n",
            "  2201/50000: episode: 1796, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 52081.359375, mae: 4441.217773, mean_q: -1087.898560\n",
            "Val: -7870.6313 -614.2871\n",
            "wrong_move\n",
            "  2202/50000: episode: 1797, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 373465.125000, mae: 4436.518555, mean_q: -989.749390\n",
            "Val: -7000.377 -655.71106\n",
            "wrong_move\n",
            "  2203/50000: episode: 1798, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 470794.906250, mae: 4426.890625, mean_q: -910.902161\n",
            "Val: -6811.9214 -1453.0044\n",
            "wrong_move\n",
            "  2204/50000: episode: 1799, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 84415.718750, mae: 4424.236328, mean_q: -1674.488037\n",
            "Val: -6467.9434 -1921.1212\n",
            "wrong_move\n",
            "  2205/50000: episode: 1800, duration: 0.154s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 229882.796875, mae: 4417.427734, mean_q: -2277.509766\n",
            "Val: -6170.843 -2812.3044\n",
            "wrong_move\n",
            "  2206/50000: episode: 1801, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 165185.250000, mae: 4416.774414, mean_q: -3086.570557\n",
            "Val: -5987.1655 -3874.7405\n",
            "wrong_move\n",
            "  2207/50000: episode: 1802, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 170906.484375, mae: 4423.395508, mean_q: -3600.699219\n",
            "Val: -6448.968 -3965.9216\n",
            "wrong_move\n",
            "  2208/50000: episode: 1803, duration: 0.161s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2398.000 [2398.000, 2398.000],  loss: 126121.031250, mae: 4432.334961, mean_q: -3544.907715\n",
            "Val: -6009.1294 -3561.9795\n",
            "wrong_move\n",
            "  2209/50000: episode: 1804, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 88717.414062, mae: 4444.214844, mean_q: -3391.419922\n",
            "Val: -6588.5767 -3259.8083\n",
            "wrong_move\n",
            "  2210/50000: episode: 1805, duration: 0.148s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 59704.820312, mae: 4456.147461, mean_q: -3321.465332\n",
            "Val: -6857.136 -3349.3057\n",
            "wrong_move\n",
            "  2211/50000: episode: 1806, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 94084.796875, mae: 4468.642578, mean_q: -3180.330078\n",
            "Val: -8242.348 -2363.6233\n",
            "wrong_move\n",
            "  2212/50000: episode: 1807, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 313809.875000, mae: 4472.182617, mean_q: -2993.995605\n",
            "Val: -7043.0493 -3053.7686\n",
            "wrong_move\n",
            "  2213/50000: episode: 1808, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 212561.000000, mae: 4461.327637, mean_q: -2883.944092\n",
            "Val: -6751.399 -2267.0142\n",
            "wrong_move\n",
            "  2214/50000: episode: 1809, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 256671.421875, mae: 4440.799316, mean_q: -2754.420898\n",
            "Val: -6398.3037 -2827.2327\n",
            "wrong_move\n",
            "  2215/50000: episode: 1810, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 113866.476562, mae: 4421.667969, mean_q: -2654.346924\n",
            "Val: -6182.7617 -2819.837\n",
            "wrong_move\n",
            "  2216/50000: episode: 1811, duration: 0.142s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 79468.593750, mae: 4415.020020, mean_q: -2575.831299\n",
            "Val: -6180.2856 -2720.6626\n",
            "wrong_move\n",
            "  2217/50000: episode: 1812, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 84436.968750, mae: 4410.143555, mean_q: -2512.484863\n",
            "Val: -6198.9653 -2629.7886\n",
            "wrong_move\n",
            "  2218/50000: episode: 1813, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 122943.476562, mae: 4412.378906, mean_q: -2466.798340\n",
            "Val: -6130.355 -2156.1106\n",
            "wrong_move\n",
            "  2219/50000: episode: 1814, duration: 0.157s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 524501.500000, mae: 4412.351562, mean_q: -2428.053223\n",
            "Val: -6347.691 -2577.1604\n",
            "wrong_move\n",
            "  2220/50000: episode: 1815, duration: 0.161s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 104351.171875, mae: 4409.749023, mean_q: -2383.333984\n",
            "Val: -6783.4224 -1985.1428\n",
            "wrong_move\n",
            "  2221/50000: episode: 1816, duration: 0.168s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 111789.218750, mae: 4408.780273, mean_q: -2364.239258\n",
            "Val: -6138.333 -2183.4165\n",
            "wrong_move\n",
            "  2222/50000: episode: 1817, duration: 0.150s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 186969.906250, mae: 4415.208984, mean_q: -2353.949219\n",
            "Val: -6440.375 -2853.9832\n",
            "wrong_move\n",
            "  2223/50000: episode: 1818, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 79517.296875, mae: 4424.423828, mean_q: -2611.598633\n",
            "Val: -6472.471 -3289.0627\n",
            "wrong_move\n",
            "  2224/50000: episode: 1819, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 98744.984375, mae: 4437.975586, mean_q: -2861.239502\n",
            "Val: -6436.6836 -3443.3022\n",
            "wrong_move\n",
            "  2225/50000: episode: 1820, duration: 0.155s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: 141960.093750, mae: 4448.117188, mean_q: -3118.364990\n",
            "Val: -6743.5537 -3568.9468\n",
            "wrong_move\n",
            "  2226/50000: episode: 1821, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: 41563.882812, mae: 4454.047852, mean_q: -3253.380371\n",
            "Val: -7004.22 -3776.6816\n",
            "wrong_move\n",
            "  2227/50000: episode: 1822, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: 59447.140625, mae: 4461.143555, mean_q: -3351.769287\n",
            "Val: -7200.9663 -3734.2058\n",
            "wrong_move\n",
            "  2228/50000: episode: 1823, duration: 0.145s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: 137557.671875, mae: 4463.962891, mean_q: -3404.957520\n",
            "Val: -7615.572 -3649.2896\n",
            "wrong_move\n",
            "  2229/50000: episode: 1824, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: 123457.421875, mae: 4463.979980, mean_q: -3564.030273\n",
            "Val: -7439.158 -3668.726\n",
            "wrong_move\n",
            "  2230/50000: episode: 1825, duration: 0.205s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 52151.625000, mae: 4458.960938, mean_q: -3510.585205\n",
            "Val: -7959.8677 -3412.4812\n",
            "wrong_move\n",
            "  2231/50000: episode: 1826, duration: 0.175s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: 79818.476562, mae: 4456.590332, mean_q: -3587.573242\n",
            "Val: -7845.487 -4340.946\n",
            "wrong_move\n",
            "  2232/50000: episode: 1827, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: 59680.300781, mae: 4452.002441, mean_q: -3613.378906\n",
            "Val: -8109.598 -4313.5356\n",
            "wrong_move\n",
            "  2233/50000: episode: 1828, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: 181136.484375, mae: 4445.519043, mean_q: -3609.035645\n",
            "Val: -8197.475 -3393.292\n",
            "wrong_move\n",
            "  2234/50000: episode: 1829, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: 157721.968750, mae: 4430.256348, mean_q: -3590.675293\n",
            "Val: -7755.0635 -4262.9727\n",
            "wrong_move\n",
            "  2235/50000: episode: 1830, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3763.000 [3763.000, 3763.000],  loss: 193826.812500, mae: 4415.735352, mean_q: -3512.001953\n",
            "Val: -7736.2256 -4128.012\n",
            "wrong_move\n",
            "  2236/50000: episode: 1831, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3242.000 [3242.000, 3242.000],  loss: 107251.992188, mae: 4410.569336, mean_q: -3552.492188\n",
            "Val: -7707.873 -4090.837\n",
            "wrong_move\n",
            "  2238/50000: episode: 1832, duration: 0.267s, episode steps:   2, steps per second:   8, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 3763.000 [3763.000, 3763.000],  loss: 87086.609375, mae: 4415.892578, mean_q: -3480.808350\n",
            "Val: -7805.2397 -4016.622\n",
            "wrong_move\n",
            "  2240/50000: episode: 1833, duration: 0.258s, episode steps:   2, steps per second:   8, episode reward: -4951.000, mean reward: -2475.500 [-5000.000, 49.000], mean action: 3763.000 [3763.000, 3763.000],  loss: 104992.507812, mae: 4419.569336, mean_q: -3341.670166\n",
            "Val: -8441.409 -2491.564\n",
            "wrong_move\n",
            "  2241/50000: episode: 1834, duration: 0.158s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 97647.554688, mae: 4422.130859, mean_q: -2947.133057\n",
            "Val: -7823.818 -2854.4\n",
            "wrong_move\n",
            "  2242/50000: episode: 1835, duration: 0.184s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: 113033.609375, mae: 4424.762695, mean_q: -2501.405029\n",
            "Val: -7894.723 -2283.5896\n",
            "wrong_move\n",
            "  2243/50000: episode: 1836, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: 135724.375000, mae: 4428.725586, mean_q: -2093.934082\n",
            "Val: -7967.5547 -1969.0248\n",
            "wrong_move\n",
            "  2244/50000: episode: 1837, duration: 0.143s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: 171370.031250, mae: 4429.816406, mean_q: -1713.492798\n",
            "Val: -7899.472 -1493.141\n",
            "wrong_move\n",
            "  2245/50000: episode: 1838, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: 126893.859375, mae: 4422.207031, mean_q: -1410.368164\n",
            "Val: -7832.4673 -1098.5363\n",
            "wrong_move\n",
            "  2246/50000: episode: 1839, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: 168247.234375, mae: 4418.963379, mean_q: -1138.137817\n",
            "Val: -7923.3613 -1061.4994\n",
            "wrong_move\n",
            "  2247/50000: episode: 1840, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: 109485.859375, mae: 4418.357910, mean_q: -927.161255\n",
            "Val: -8508.728 -754.01715\n",
            "wrong_move\n",
            "  2248/50000: episode: 1841, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: 236400.203125, mae: 4420.891602, mean_q: -754.215027\n",
            "Val: -7898.091 -635.8854\n",
            "wrong_move\n",
            "  2249/50000: episode: 1842, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: 113588.718750, mae: 4418.863770, mean_q: -605.119507\n",
            "Val: -7773.154 -391.4517\n",
            "wrong_move\n",
            "  2250/50000: episode: 1843, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: 297201.125000, mae: 4413.848633, mean_q: -480.910889\n",
            "Val: -8559.293 -366.0054\n",
            "wrong_move\n",
            "  2251/50000: episode: 1844, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: 96486.421875, mae: 4407.752441, mean_q: -378.802795\n",
            "Val: -7877.9785 -422.06\n",
            "wrong_move\n",
            "  2252/50000: episode: 1845, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: 145396.000000, mae: 4404.156738, mean_q: -297.185852\n",
            "Val: -7989.3467 -227.25116\n",
            "wrong_move\n",
            "  2253/50000: episode: 1846, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: 149559.250000, mae: 4400.522461, mean_q: -230.569275\n",
            "Val: -7875.6846 -660.5989\n",
            "wrong_move\n",
            "  2254/50000: episode: 1847, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: 586417.375000, mae: 4408.386719, mean_q: -642.937195\n",
            "Val: -7886.6455 -1922.9154\n",
            "wrong_move\n",
            "  2255/50000: episode: 1848, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: 121426.328125, mae: 4427.961914, mean_q: -1844.029907\n",
            "Val: -7940.0 -3083.2383\n",
            "wrong_move\n",
            "  2256/50000: episode: 1849, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: 372329.312500, mae: 4451.087891, mean_q: -2942.780273\n",
            "Val: -7669.014 -3951.01\n",
            "wrong_move\n",
            "  2257/50000: episode: 1850, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 379.000 [379.000, 379.000],  loss: 159528.687500, mae: 4460.191406, mean_q: -3547.094238\n",
            "Val: -6692.16 -4295.334\n",
            "wrong_move\n",
            "  2258/50000: episode: 1851, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2355.000 [2355.000, 2355.000],  loss: 256753.312500, mae: 4463.221191, mean_q: -3695.129883\n",
            "Val: -6465.2666 -4285.4595\n",
            "wrong_move\n",
            "  2259/50000: episode: 1852, duration: 0.197s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2355.000 [2355.000, 2355.000],  loss: 100277.851562, mae: 4453.043945, mean_q: -3772.968018\n",
            "Val: -6464.1333 -4260.4775\n",
            "wrong_move\n",
            "  2260/50000: episode: 1853, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2462.000 [2462.000, 2462.000],  loss: 166695.203125, mae: 4442.535156, mean_q: -3733.294434\n",
            "Val: -6638.49 -3763.7559\n",
            "wrong_move\n",
            "  2261/50000: episode: 1854, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1164.000 [1164.000, 1164.000],  loss: 173270.312500, mae: 4429.747070, mean_q: -3763.857422\n",
            "Val: -6400.6475 -4185.3574\n",
            "wrong_move\n",
            "  2262/50000: episode: 1855, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 77.000 [77.000, 77.000],  loss: 104263.437500, mae: 4424.297852, mean_q: -3692.902588\n",
            "Val: -6744.4263 -4269.439\n",
            "wrong_move\n",
            "  2263/50000: episode: 1856, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2462.000 [2462.000, 2462.000],  loss: 279823.531250, mae: 4429.732910, mean_q: -3698.130859\n",
            "Val: -7052.453 -4198.8325\n",
            "wrong_move\n",
            "  2264/50000: episode: 1857, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 77.000 [77.000, 77.000],  loss: 137781.546875, mae: 4438.341797, mean_q: -3710.957031\n",
            "Val: -7619.6753 -4237.664\n",
            "wrong_move\n",
            "  2266/50000: episode: 1858, duration: 0.185s, episode steps:   2, steps per second:  11, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2462.000 [2462.000, 2462.000],  loss: 140183.750000, mae: 4449.950684, mean_q: -3771.779785\n",
            "Val: -7943.8735 -4273.4365\n",
            "wrong_move\n",
            "  2267/50000: episode: 1859, duration: 0.142s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1502.000 [1502.000, 1502.000],  loss: 113964.000000, mae: 4462.145508, mean_q: -3804.291992\n",
            "Val: -7614.208 -3096.5037\n",
            "wrong_move\n",
            "  2268/50000: episode: 1860, duration: 0.169s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 247.000 [247.000, 247.000],  loss: 119679.156250, mae: 4466.985352, mean_q: -3866.727051\n",
            "Val: -8361.247 -4296.8125\n",
            "wrong_move\n",
            "  2269/50000: episode: 1861, duration: 0.166s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1502.000 [1502.000, 1502.000],  loss: 150273.968750, mae: 4456.120117, mean_q: -3830.747070\n",
            "Val: -8177.0664 -3795.0051\n",
            "wrong_move\n",
            "  2270/50000: episode: 1862, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3514.000 [3514.000, 3514.000],  loss: 138190.375000, mae: 4438.542969, mean_q: -3814.852051\n",
            "Val: -8219.406 -4090.2388\n",
            "wrong_move\n",
            "  2271/50000: episode: 1863, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 835.000 [835.000, 835.000],  loss: 127891.437500, mae: 4418.577148, mean_q: -3677.508789\n",
            "Val: -8638.523 -4176.069\n",
            "wrong_move\n",
            "  2272/50000: episode: 1864, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1502.000 [1502.000, 1502.000],  loss: 100725.953125, mae: 4400.874512, mean_q: -3664.087891\n",
            "Val: -8060.8555 -3733.6445\n",
            "wrong_move\n",
            "  2273/50000: episode: 1865, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1164.000 [1164.000, 1164.000],  loss: 185187.093750, mae: 4390.390625, mean_q: -3508.687012\n",
            "Val: -8869.395 -4119.169\n",
            "wrong_move\n",
            "  2274/50000: episode: 1866, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2462.000 [2462.000, 2462.000],  loss: 99372.875000, mae: 4387.765137, mean_q: -3672.964355\n",
            "Val: -8750.362 -4151.198\n",
            "wrong_move\n",
            "  2275/50000: episode: 1867, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1865.000 [1865.000, 1865.000],  loss: 112039.726562, mae: 4397.500977, mean_q: -3547.209473\n",
            "Val: -8971.362 -4098.5566\n",
            "wrong_move\n",
            "  2276/50000: episode: 1868, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1865.000 [1865.000, 1865.000],  loss: 106154.632812, mae: 4413.002930, mean_q: -3519.774902\n",
            "Val: -8851.493 -3069.966\n",
            "wrong_move\n",
            "  2277/50000: episode: 1869, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2355.000 [2355.000, 2355.000],  loss: 95302.609375, mae: 4423.183594, mean_q: -3380.921631\n",
            "Val: -8815.086 -2936.8523\n",
            "wrong_move\n",
            "  2278/50000: episode: 1870, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2355.000 [2355.000, 2355.000],  loss: 122164.203125, mae: 4430.792969, mean_q: -3363.857178\n",
            "Val: -8791.765 -2871.5247\n",
            "wrong_move\n",
            "  2279/50000: episode: 1871, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2355.000 [2355.000, 2355.000],  loss: 170872.859375, mae: 4429.354492, mean_q: -3234.706299\n",
            "Val: -9061.948 -3898.728\n",
            "wrong_move\n",
            "  2280/50000: episode: 1872, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2355.000 [2355.000, 2355.000],  loss: 90223.578125, mae: 4422.128418, mean_q: -3236.024658\n",
            "Val: -8946.612 -2645.5083\n",
            "wrong_move\n",
            "  2281/50000: episode: 1873, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2355.000 [2355.000, 2355.000],  loss: 127606.445312, mae: 4413.270508, mean_q: -3203.623535\n",
            "Val: -9039.063 -2570.6099\n",
            "wrong_move\n",
            "  2282/50000: episode: 1874, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2355.000 [2355.000, 2355.000],  loss: 127005.046875, mae: 4411.880859, mean_q: -3065.420410\n",
            "Val: -8772.2295 -3430.9868\n",
            "wrong_move\n",
            "  2283/50000: episode: 1875, duration: 0.142s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2462.000 [2462.000, 2462.000],  loss: 119082.593750, mae: 4412.112305, mean_q: -2953.728516\n",
            "Val: -8800.706 -2589.0\n",
            "wrong_move\n",
            "  2284/50000: episode: 1876, duration: 0.193s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1846.000 [1846.000, 1846.000],  loss: 96008.734375, mae: 4416.866211, mean_q: -2755.947266\n",
            "Val: -8769.514 -2600.4534\n",
            "wrong_move\n",
            "  2285/50000: episode: 1877, duration: 0.168s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2355.000 [2355.000, 2355.000],  loss: 70499.093750, mae: 4425.254395, mean_q: -2654.305664\n",
            "Val: -9147.91 -2717.4917\n",
            "wrong_move\n",
            "  2286/50000: episode: 1878, duration: 0.146s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2462.000 [2462.000, 2462.000],  loss: 121609.000000, mae: 4439.831055, mean_q: -2513.287842\n",
            "Val: -8584.474 -2706.7864\n",
            "wrong_move\n",
            "  2287/50000: episode: 1879, duration: 0.160s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2462.000 [2462.000, 2462.000],  loss: 228191.593750, mae: 4453.833008, mean_q: -2460.049561\n",
            "Val: -9180.763 -2942.4548\n",
            "wrong_move\n",
            "  2288/50000: episode: 1880, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2462.000 [2462.000, 2462.000],  loss: 114355.929688, mae: 4467.919922, mean_q: -2796.590820\n",
            "Val: -9158.52 -3249.1248\n",
            "wrong_move\n",
            "  2289/50000: episode: 1881, duration: 0.164s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2462.000 [2462.000, 2462.000],  loss: 118509.281250, mae: 4475.390137, mean_q: -3100.500977\n",
            "Val: -9500.393 -3171.6377\n",
            "wrong_move\n",
            "  2290/50000: episode: 1882, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2462.000 [2462.000, 2462.000],  loss: 75252.335938, mae: 4472.363770, mean_q: -3322.731445\n",
            "Val: -8363.894 -3585.662\n",
            "wrong_move\n",
            "  2291/50000: episode: 1883, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2462.000 [2462.000, 2462.000],  loss: 122636.015625, mae: 4462.652344, mean_q: -3395.395508\n",
            "Val: -9149.047 -4071.84\n",
            "wrong_move\n",
            "  2292/50000: episode: 1884, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2462.000 [2462.000, 2462.000],  loss: 72440.453125, mae: 4447.252930, mean_q: -3672.566895\n",
            "Val: -8819.565 -4195.4844\n",
            "wrong_move\n",
            "  2293/50000: episode: 1885, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1569.000 [1569.000, 1569.000],  loss: 74800.671875, mae: 4433.927246, mean_q: -3724.590820\n",
            "Val: -8893.278 -4298.53\n",
            "wrong_move\n",
            "  2294/50000: episode: 1886, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3523.000 [3523.000, 3523.000],  loss: 91342.937500, mae: 4424.193359, mean_q: -3731.145996\n",
            "Val: -8563.923 -4164.8447\n",
            "wrong_move\n",
            "  2296/50000: episode: 1887, duration: 0.183s, episode steps:   2, steps per second:  11, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 880.500 [259.000, 1502.000],  loss: 153997.296875, mae: 4416.781738, mean_q: -3625.980469\n",
            "Val: -8843.682 -3616.0847\n",
            "wrong_move\n",
            "  2297/50000: episode: 1888, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1502.000 [1502.000, 1502.000],  loss: 92334.937500, mae: 4417.422852, mean_q: -3452.579102\n",
            "Val: -8949.258 -3383.2021\n",
            "wrong_move\n",
            "  2298/50000: episode: 1889, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1502.000 [1502.000, 1502.000],  loss: 74543.257812, mae: 4431.153809, mean_q: -3249.783691\n",
            "Val: -9127.122 -3256.7244\n",
            "wrong_move\n",
            "  2299/50000: episode: 1890, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1502.000 [1502.000, 1502.000],  loss: 89146.328125, mae: 4447.134766, mean_q: -3311.460693\n",
            "Val: -8746.163 -3317.5942\n",
            "wrong_move\n",
            "  2300/50000: episode: 1891, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1502.000 [1502.000, 1502.000],  loss: 98451.562500, mae: 4451.048828, mean_q: -3155.355469\n",
            "Val: -9125.248 -3226.5063\n",
            "wrong_move\n",
            "  2301/50000: episode: 1892, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1502.000 [1502.000, 1502.000],  loss: 144154.703125, mae: 4446.683105, mean_q: -3192.603027\n",
            "Val: -8523.996 -3560.647\n",
            "wrong_move\n",
            "  2302/50000: episode: 1893, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1502.000 [1502.000, 1502.000],  loss: 60145.156250, mae: 4437.847168, mean_q: -3430.317383\n",
            "Val: -9105.009 -3741.844\n",
            "wrong_move\n",
            "  2303/50000: episode: 1894, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1502.000 [1502.000, 1502.000],  loss: 51069.289062, mae: 4428.795898, mean_q: -3597.033936\n",
            "Val: -8471.407 -3697.7524\n",
            "wrong_move\n",
            "  2304/50000: episode: 1895, duration: 0.161s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 735.000 [735.000, 735.000],  loss: 71056.195312, mae: 4423.173828, mean_q: -3643.574219\n",
            "Val: -8322.243 -4123.4844\n",
            "wrong_move\n",
            "  2305/50000: episode: 1896, duration: 0.187s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3601.000 [3601.000, 3601.000],  loss: 71042.726562, mae: 4413.555176, mean_q: -3712.212891\n",
            "Val: -8823.814 -4251.367\n",
            "wrong_move\n",
            "  2306/50000: episode: 1897, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 71502.984375, mae: 4408.390625, mean_q: -3622.968994\n",
            "Val: -8565.077 -3654.188\n",
            "wrong_move\n",
            "  2307/50000: episode: 1898, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 735.000 [735.000, 735.000],  loss: 62381.656250, mae: 4412.493164, mean_q: -3612.723633\n",
            "Val: -8300.293 -3746.051\n",
            "wrong_move\n",
            "  2308/50000: episode: 1899, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 735.000 [735.000, 735.000],  loss: 63243.984375, mae: 4419.429199, mean_q: -3577.836914\n",
            "Val: -8902.325 -3973.079\n",
            "wrong_move\n",
            "  2309/50000: episode: 1900, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 77.000 [77.000, 77.000],  loss: 103383.406250, mae: 4422.083984, mean_q: -3603.385254\n",
            "Val: -8480.36 -3698.4585\n",
            "wrong_move\n",
            "  2310/50000: episode: 1901, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 735.000 [735.000, 735.000],  loss: 148675.406250, mae: 4428.679688, mean_q: -3622.918457\n",
            "Val: -9131.247 -3670.5767\n",
            "wrong_move\n",
            "  2311/50000: episode: 1902, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 77.000 [77.000, 77.000],  loss: 87465.742188, mae: 4434.521484, mean_q: -3522.662109\n",
            "Val: -8732.811 -3688.843\n",
            "wrong_move\n",
            "  2312/50000: episode: 1903, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 77.000 [77.000, 77.000],  loss: 88064.828125, mae: 4433.828613, mean_q: -3540.211426\n",
            "Val: -8864.377 -3707.6816\n",
            "wrong_move\n",
            "  2313/50000: episode: 1904, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 77.000 [77.000, 77.000],  loss: 157986.593750, mae: 4428.493164, mean_q: -3463.177734\n",
            "Val: -9171.557 -3650.6614\n",
            "wrong_move\n",
            "  2314/50000: episode: 1905, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 77.000 [77.000, 77.000],  loss: 129565.453125, mae: 4419.566406, mean_q: -3585.725342\n",
            "Val: -9151.019 -4011.249\n",
            "wrong_move\n",
            "  2315/50000: episode: 1906, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 193.000 [193.000, 193.000],  loss: 102820.062500, mae: 4419.313477, mean_q: -3731.897705\n",
            "Val: -9136.641 -4236.7563\n",
            "wrong_move\n",
            "  2316/50000: episode: 1907, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2461.000 [2461.000, 2461.000],  loss: 120749.515625, mae: 4425.883789, mean_q: -3782.081055\n",
            "Val: -8789.272 -3820.6345\n",
            "wrong_move\n",
            "  2317/50000: episode: 1908, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3840.000 [3840.000, 3840.000],  loss: 58679.929688, mae: 4434.849609, mean_q: -3803.720947\n",
            "Val: -8580.0 -3827.7856\n",
            "wrong_move\n",
            "  2318/50000: episode: 1909, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: 199976.156250, mae: 4449.450684, mean_q: -3833.032715\n",
            "Val: -8932.319 -4323.4253\n",
            "wrong_move\n",
            "  2319/50000: episode: 1910, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3343.000 [3343.000, 3343.000],  loss: 58088.015625, mae: 4458.862305, mean_q: -3777.463379\n",
            "Val: -8521.624 -3840.2673\n",
            "wrong_move\n",
            "  2320/50000: episode: 1911, duration: 0.159s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: 92546.343750, mae: 4464.225098, mean_q: -3833.586182\n",
            "Val: -9120.164 -4297.7417\n",
            "wrong_move\n",
            "  2321/50000: episode: 1912, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2461.000 [2461.000, 2461.000],  loss: 113543.804688, mae: 4465.787109, mean_q: -3853.855957\n",
            "Val: -9180.272 -4258.9194\n",
            "wrong_move\n",
            "  2322/50000: episode: 1913, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2461.000 [2461.000, 2461.000],  loss: 89223.156250, mae: 4464.585938, mean_q: -3816.675293\n",
            "Val: -9702.803 -3339.1218\n",
            "wrong_move\n",
            "  2323/50000: episode: 1914, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: 62663.218750, mae: 4454.770020, mean_q: -3889.426514\n",
            "Val: -9261.138 -3253.0942\n",
            "wrong_move\n",
            "  2324/50000: episode: 1915, duration: 0.152s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: 81609.109375, mae: 4444.858398, mean_q: -3841.241943\n",
            "Val: -9103.943 -4223.684\n",
            "wrong_move\n",
            "  2325/50000: episode: 1916, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2461.000 [2461.000, 2461.000],  loss: 71242.265625, mae: 4433.941406, mean_q: -3793.458008\n",
            "Val: -8710.311 -4304.376\n",
            "wrong_move\n",
            "  2326/50000: episode: 1917, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3544.000 [3544.000, 3544.000],  loss: 93848.671875, mae: 4425.779297, mean_q: -3775.733398\n",
            "Val: -9089.917 -4151.427\n",
            "wrong_move\n",
            "  2327/50000: episode: 1918, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: 90720.398438, mae: 4418.205078, mean_q: -3687.686768\n",
            "Val: -8271.451 -3786.7017\n",
            "wrong_move\n",
            "done, took 166.911 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 50000 steps ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val: -10024.964 -2745.3667\n",
            "wrong_move\n",
            "     1/50000: episode: 1, duration: 0.650s, episode steps:   1, steps per second:   2, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8727.393 -4056.0562\n",
            "wrong_move\n",
            "     2/50000: episode: 2, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8712.96 -3936.6865\n",
            "wrong_move\n",
            "     3/50000: episode: 3, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8266.046 -3782.3062\n",
            "wrong_move\n",
            "     4/50000: episode: 4, duration: 0.024s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8747.434 -4105.471\n",
            "wrong_move\n",
            "     5/50000: episode: 5, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8648.22 -4042.5989\n",
            "wrong_move\n",
            "     6/50000: episode: 6, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "     7/50000: episode: 7, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8585.023 -3794.817\n",
            "wrong_move\n",
            "     8/50000: episode: 8, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8489.645 -3787.7424\n",
            "wrong_move\n",
            "     9/50000: episode: 9, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9071.863 -4000.6953\n",
            "wrong_move\n",
            "    10/50000: episode: 10, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7970.6367 -3797.0815\n",
            "wrong_move\n",
            "    11/50000: episode: 11, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1175.000 [1175.000, 1175.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8894.037 -4063.5132\n",
            "wrong_move\n",
            "    12/50000: episode: 12, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8520.249 -3790.7405\n",
            "wrong_move\n",
            "    13/50000: episode: 13, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8953.669 -3987.9644\n",
            "wrong_move\n",
            "    14/50000: episode: 14, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8071.3184 -3773.4895\n",
            "wrong_move\n",
            "    15/50000: episode: 15, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9014.462 -4011.8745\n",
            "wrong_move\n",
            "    16/50000: episode: 16, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8505.534 -3789.7195\n",
            "wrong_move\n",
            "    17/50000: episode: 17, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8393.371 -3787.1377\n",
            "wrong_move\n",
            "    18/50000: episode: 18, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9079.627 -4001.8713\n",
            "wrong_move\n",
            "    19/50000: episode: 19, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8798.216 -4105.405\n",
            "wrong_move\n",
            "    20/50000: episode: 20, duration: 0.043s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8795.526 -4053.562\n",
            "wrong_move\n",
            "    21/50000: episode: 21, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8710.726 -4074.222\n",
            "wrong_move\n",
            "    22/50000: episode: 22, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9120.44 -3947.2002\n",
            "wrong_move\n",
            "    23/50000: episode: 23, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9171.27 -3934.867\n",
            "wrong_move\n",
            "    24/50000: episode: 24, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8442.245 -3991.3826\n",
            "wrong_move\n",
            "    25/50000: episode: 25, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9054.644 -3999.4578\n",
            "wrong_move\n",
            "    26/50000: episode: 26, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9045.171 -4020.9932\n",
            "wrong_move\n",
            "    27/50000: episode: 27, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8459.732 -3789.7964\n",
            "wrong_move\n",
            "    28/50000: episode: 28, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8597.125 -3794.6008\n",
            "wrong_move\n",
            "    29/50000: episode: 29, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9128.846 -3932.998\n",
            "wrong_move\n",
            "    30/50000: episode: 30, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8106.1772 -3774.8994\n",
            "wrong_move\n",
            "    31/50000: episode: 31, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9363.105 -3683.2153\n",
            "wrong_move\n",
            "    32/50000: episode: 32, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8950.434 -4024.6714\n",
            "wrong_move\n",
            "    33/50000: episode: 33, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.675 -4024.3408\n",
            "wrong_move\n",
            "    34/50000: episode: 34, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8508.849 -3789.734\n",
            "wrong_move\n",
            "    35/50000: episode: 35, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8932.524 -3934.6086\n",
            "wrong_move\n",
            "    36/50000: episode: 36, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8657.365 -4057.3264\n",
            "wrong_move\n",
            "    37/50000: episode: 37, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1125.000 [1125.000, 1125.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8680.895 -3795.3645\n",
            "wrong_move\n",
            "    38/50000: episode: 38, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9052.886 -4026.1338\n",
            "wrong_move\n",
            "    39/50000: episode: 39, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8200.249 -4033.1855\n",
            "wrong_move\n",
            "    40/50000: episode: 40, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 985.000 [985.000, 985.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9169.626 -3932.5374\n",
            "wrong_move\n",
            "    41/50000: episode: 41, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8317.506 -3781.2957\n",
            "wrong_move\n",
            "    42/50000: episode: 42, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8987.831 -3948.384\n",
            "wrong_move\n",
            "    43/50000: episode: 43, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9068.638 -4016.1433\n",
            "wrong_move\n",
            "    44/50000: episode: 44, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9069.102 -4008.2405\n",
            "wrong_move\n",
            "    45/50000: episode: 45, duration: 0.027s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8502.843 -3790.222\n",
            "wrong_move\n",
            "    46/50000: episode: 46, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8997.397 -3642.7625\n",
            "wrong_move\n",
            "    47/50000: episode: 47, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8170.9395 -3778.6135\n",
            "wrong_move\n",
            "    48/50000: episode: 48, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9086.492 -4001.907\n",
            "wrong_move\n",
            "    49/50000: episode: 49, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8780.589 -4105.2847\n",
            "wrong_move\n",
            "    50/50000: episode: 50, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8933.799 -3999.7808\n",
            "wrong_move\n",
            "    51/50000: episode: 51, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.721 -4009.2183\n",
            "wrong_move\n",
            "    52/50000: episode: 52, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8835.901 -4082.745\n",
            "wrong_move\n",
            "    53/50000: episode: 53, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8550.1875 -3791.883\n",
            "wrong_move\n",
            "    54/50000: episode: 54, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9046.122 -4026.0203\n",
            "wrong_move\n",
            "    55/50000: episode: 55, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8858.925 -4043.6616\n",
            "wrong_move\n",
            "    56/50000: episode: 56, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9071.956 -4011.3752\n",
            "wrong_move\n",
            "    57/50000: episode: 57, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8535.153 -3792.039\n",
            "wrong_move\n",
            "    58/50000: episode: 58, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9170.9375 -3905.2908\n",
            "wrong_move\n",
            "    59/50000: episode: 59, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8879.7 -4054.3066\n",
            "wrong_move\n",
            "    60/50000: episode: 60, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9054.503 -4018.057\n",
            "wrong_move\n",
            "    61/50000: episode: 61, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9091.373 -4002.0154\n",
            "wrong_move\n",
            "    62/50000: episode: 62, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8785.8955 -4050.5635\n",
            "wrong_move\n",
            "    63/50000: episode: 63, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8449.175 -3793.6821\n",
            "wrong_move\n",
            "    64/50000: episode: 64, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8422.145 -4083.8489\n",
            "wrong_move\n",
            "    65/50000: episode: 65, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9102.268 -3999.2964\n",
            "wrong_move\n",
            "    66/50000: episode: 66, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "    67/50000: episode: 67, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8984.578 -3998.669\n",
            "wrong_move\n",
            "    68/50000: episode: 68, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8828.82 -4103.5913\n",
            "wrong_move\n",
            "    69/50000: episode: 69, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9627.501 -3408.8342\n",
            "wrong_move\n",
            "    70/50000: episode: 70, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8418.298 -4140.5054\n",
            "wrong_move\n",
            "    71/50000: episode: 71, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9055.672 -4024.3662\n",
            "wrong_move\n",
            "    72/50000: episode: 72, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8672.7705 -3981.1812\n",
            "wrong_move\n",
            "    73/50000: episode: 73, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9063.531 -4018.0342\n",
            "wrong_move\n",
            "    74/50000: episode: 74, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8878.66 -4062.5034\n",
            "wrong_move\n",
            "    75/50000: episode: 75, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8580.806 -4112.746\n",
            "wrong_move\n",
            "    76/50000: episode: 76, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8336.485 -3944.669\n",
            "wrong_move\n",
            "    77/50000: episode: 77, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8394.323 -3785.009\n",
            "wrong_move\n",
            "    78/50000: episode: 78, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9137.668 -3956.5261\n",
            "wrong_move\n",
            "    79/50000: episode: 79, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8636.304 -4070.9258\n",
            "wrong_move\n",
            "    80/50000: episode: 80, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8589.082 -3962.721\n",
            "wrong_move\n",
            "    81/50000: episode: 81, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9157.155 -3947.7034\n",
            "wrong_move\n",
            "    82/50000: episode: 82, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9067.572 -4016.5461\n",
            "wrong_move\n",
            "    83/50000: episode: 83, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8389.403 -3786.0388\n",
            "wrong_move\n",
            "    84/50000: episode: 84, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9060.415 -4011.2095\n",
            "wrong_move\n",
            "    85/50000: episode: 85, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8947.622 -4036.716\n",
            "wrong_move\n",
            "    86/50000: episode: 86, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9076.219 -4010.0217\n",
            "wrong_move\n",
            "    87/50000: episode: 87, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9054.251 -4016.1255\n",
            "wrong_move\n",
            "    88/50000: episode: 88, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8372.388 -4133.795\n",
            "wrong_move\n",
            "    89/50000: episode: 89, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8649.053 -4063.1492\n",
            "wrong_move\n",
            "    90/50000: episode: 90, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8533.214 -4128.259\n",
            "wrong_move\n",
            "    91/50000: episode: 91, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8907.528 -4050.778\n",
            "wrong_move\n",
            "    92/50000: episode: 92, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8547.211 -3790.906\n",
            "wrong_move\n",
            "    93/50000: episode: 93, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8469.778 -3789.9556\n",
            "wrong_move\n",
            "    94/50000: episode: 94, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8307.481 -3783.7922\n",
            "wrong_move\n",
            "    95/50000: episode: 95, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8792.242 -3275.7188\n",
            "wrong_move\n",
            "    96/50000: episode: 96, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "    97/50000: episode: 97, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8668.726 -4149.298\n",
            "wrong_move\n",
            "    98/50000: episode: 98, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8389.423 -3786.0989\n",
            "wrong_move\n",
            "    99/50000: episode: 99, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9070.163 -3997.0112\n",
            "wrong_move\n",
            "   100/50000: episode: 100, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8426.487 -3788.418\n",
            "wrong_move\n",
            "   101/50000: episode: 101, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8743.426 -3799.9336\n",
            "wrong_move\n",
            "   102/50000: episode: 102, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8536.444 -4139.014\n",
            "wrong_move\n",
            "   103/50000: episode: 103, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8476.298 -3789.1443\n",
            "wrong_move\n",
            "   104/50000: episode: 104, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9077.854 -4014.9531\n",
            "wrong_move\n",
            "   105/50000: episode: 105, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8761.438 -4064.1406\n",
            "wrong_move\n",
            "   106/50000: episode: 106, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9060.593 -4007.1099\n",
            "wrong_move\n",
            "   107/50000: episode: 107, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8425.26 -3788.5867\n",
            "wrong_move\n",
            "   108/50000: episode: 108, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9201.386 -2867.6255\n",
            "wrong_move\n",
            "   109/50000: episode: 109, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9060.06 -4021.9363\n",
            "wrong_move\n",
            "   110/50000: episode: 110, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9056.87 -4025.2253\n",
            "wrong_move\n",
            "   111/50000: episode: 111, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8545.526 -4072.3672\n",
            "wrong_move\n",
            "   112/50000: episode: 112, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8773.465 -4106.6978\n",
            "wrong_move\n",
            "   113/50000: episode: 113, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8790.022 -4076.2393\n",
            "wrong_move\n",
            "   114/50000: episode: 114, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9136.455 -3938.5215\n",
            "wrong_move\n",
            "   115/50000: episode: 115, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8039.032 -3767.5496\n",
            "wrong_move\n",
            "   116/50000: episode: 116, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9088.447 -4003.3528\n",
            "wrong_move\n",
            "   117/50000: episode: 117, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8427.96 -3788.0303\n",
            "wrong_move\n",
            "   118/50000: episode: 118, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8658.583 -3796.077\n",
            "wrong_move\n",
            "   119/50000: episode: 119, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9042.797 -4022.171\n",
            "wrong_move\n",
            "   120/50000: episode: 120, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9083.081 -4016.9336\n",
            "wrong_move\n",
            "   121/50000: episode: 121, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8922.73 -4074.2637\n",
            "wrong_move\n",
            "   122/50000: episode: 122, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8689.109 -4152.4097\n",
            "wrong_move\n",
            "   123/50000: episode: 123, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9062.068 -4024.424\n",
            "wrong_move\n",
            "   124/50000: episode: 124, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8396.974 -4182.386\n",
            "wrong_move\n",
            "   125/50000: episode: 125, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9144.792 -3935.9666\n",
            "wrong_move\n",
            "   126/50000: episode: 126, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9063.889 -4017.345\n",
            "wrong_move\n",
            "   127/50000: episode: 127, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9057.639 -4021.3875\n",
            "wrong_move\n",
            "   128/50000: episode: 128, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8567.767 -4148.0454\n",
            "wrong_move\n",
            "   129/50000: episode: 129, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9082.972 -4009.9172\n",
            "wrong_move\n",
            "   130/50000: episode: 130, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8557.681 -3791.8347\n",
            "wrong_move\n",
            "   131/50000: episode: 131, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8449.775 -4137.597\n",
            "wrong_move\n",
            "   132/50000: episode: 132, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8778.316 -4071.2021\n",
            "wrong_move\n",
            "   133/50000: episode: 133, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9082.142 -4008.8494\n",
            "wrong_move\n",
            "   134/50000: episode: 134, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8940.758 -4038.9312\n",
            "wrong_move\n",
            "   135/50000: episode: 135, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9000.333 -3895.4226\n",
            "wrong_move\n",
            "   136/50000: episode: 136, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9074.643 -4012.9688\n",
            "wrong_move\n",
            "   137/50000: episode: 137, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9097.399 -3970.9626\n",
            "wrong_move\n",
            "   138/50000: episode: 138, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9141.176 -3960.6816\n",
            "wrong_move\n",
            "   139/50000: episode: 139, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9077.536 -3804.23\n",
            "wrong_move\n",
            "   140/50000: episode: 140, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8398.174 -3785.4956\n",
            "wrong_move\n",
            "   141/50000: episode: 141, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8471.382 -3786.9968\n",
            "wrong_move\n",
            "   142/50000: episode: 142, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9067.716 -4020.6392\n",
            "wrong_move\n",
            "   143/50000: episode: 143, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   144/50000: episode: 144, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8271.724 -3763.9373\n",
            "wrong_move\n",
            "   145/50000: episode: 145, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 985.000 [985.000, 985.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   146/50000: episode: 146, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9024.351 -3680.638\n",
            "wrong_move\n",
            "   147/50000: episode: 147, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8869.572 -4003.2485\n",
            "wrong_move\n",
            "   148/50000: episode: 148, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8781.34 -3435.216\n",
            "wrong_move\n",
            "   149/50000: episode: 149, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9059.122 -4017.0327\n",
            "wrong_move\n",
            "   150/50000: episode: 150, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8699.665 -4134.9204\n",
            "wrong_move\n",
            "   151/50000: episode: 151, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8982.807 -4009.9255\n",
            "wrong_move\n",
            "   152/50000: episode: 152, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9107.199 -3971.3438\n",
            "wrong_move\n",
            "   153/50000: episode: 153, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8557.005 -4185.331\n",
            "wrong_move\n",
            "   154/50000: episode: 154, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9057.019 -4016.0989\n",
            "wrong_move\n",
            "   155/50000: episode: 155, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   156/50000: episode: 156, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9083.181 -4001.1208\n",
            "wrong_move\n",
            "   157/50000: episode: 157, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8836.782 -4022.622\n",
            "wrong_move\n",
            "   158/50000: episode: 158, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9054.379 -4021.7705\n",
            "wrong_move\n",
            "   159/50000: episode: 159, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8363.122 -4122.985\n",
            "wrong_move\n",
            "   160/50000: episode: 160, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8791.142 -4100.04\n",
            "wrong_move\n",
            "   161/50000: episode: 161, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8841.456 -4085.7126\n",
            "wrong_move\n",
            "   162/50000: episode: 162, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9092.152 -4004.7996\n",
            "wrong_move\n",
            "   163/50000: episode: 163, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7959.464 -4097.21\n",
            "wrong_move\n",
            "   164/50000: episode: 164, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2239.000 [2239.000, 2239.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9071.988 -4011.007\n",
            "wrong_move\n",
            "   165/50000: episode: 165, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8438.711 -3786.0679\n",
            "wrong_move\n",
            "   166/50000: episode: 166, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8515.694 -3791.8638\n",
            "wrong_move\n",
            "   167/50000: episode: 167, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8726.759 -4087.1077\n",
            "wrong_move\n",
            "   168/50000: episode: 168, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8360.052 -3783.7532\n",
            "wrong_move\n",
            "   169/50000: episode: 169, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7859.3096 -3867.1619\n",
            "wrong_move\n",
            "   170/50000: episode: 170, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2239.000 [2239.000, 2239.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8390.518 -3786.5808\n",
            "wrong_move\n",
            "   171/50000: episode: 171, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8541.602 -3791.7166\n",
            "wrong_move\n",
            "   172/50000: episode: 172, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 524.000 [524.000, 524.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   173/50000: episode: 173, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8395.364 -4116.857\n",
            "wrong_move\n",
            "   174/50000: episode: 174, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9173.716 -3912.9226\n",
            "wrong_move\n",
            "   175/50000: episode: 175, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8450.978 -3788.884\n",
            "wrong_move\n",
            "   176/50000: episode: 176, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8061.988 -3780.1682\n",
            "wrong_move\n",
            "   177/50000: episode: 177, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2609.000 [2609.000, 2609.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8859.507 -4057.288\n",
            "wrong_move\n",
            "   178/50000: episode: 178, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8908.819 -3771.9888\n",
            "wrong_move\n",
            "   179/50000: episode: 179, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8347.905 -3785.5217\n",
            "wrong_move\n",
            "   180/50000: episode: 180, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8393.869 -3967.1255\n",
            "wrong_move\n",
            "   181/50000: episode: 181, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8546.639 -3796.6619\n",
            "wrong_move\n",
            "   182/50000: episode: 182, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3271.000 [3271.000, 3271.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8058.4395 -4101.899\n",
            "wrong_move\n",
            "   183/50000: episode: 183, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8726.323 -4124.4893\n",
            "wrong_move\n",
            "   184/50000: episode: 184, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9083.734 -4006.363\n",
            "wrong_move\n",
            "   185/50000: episode: 185, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8598.783 -3800.8364\n",
            "wrong_move\n",
            "   186/50000: episode: 186, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8786.564 -4112.9062\n",
            "wrong_move\n",
            "   187/50000: episode: 187, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9108.589 -3953.9885\n",
            "wrong_move\n",
            "   188/50000: episode: 188, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8430.413 -3788.818\n",
            "wrong_move\n",
            "   189/50000: episode: 189, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8619.966 -4093.3804\n",
            "wrong_move\n",
            "   190/50000: episode: 190, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8453.373 -3787.7478\n",
            "wrong_move\n",
            "   191/50000: episode: 191, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8201.327 -3782.7063\n",
            "wrong_move\n",
            "   192/50000: episode: 192, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8401.515 -4255.6978\n",
            "wrong_move\n",
            "   193/50000: episode: 193, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9082.073 -4002.0747\n",
            "wrong_move\n",
            "   194/50000: episode: 194, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9068.204 -4019.711\n",
            "wrong_move\n",
            "   195/50000: episode: 195, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8788.529 -4112.524\n",
            "wrong_move\n",
            "   196/50000: episode: 196, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8854.024 -4095.168\n",
            "wrong_move\n",
            "   197/50000: episode: 197, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8852.036 -4021.955\n",
            "wrong_move\n",
            "   198/50000: episode: 198, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9072.424 -4008.7068\n",
            "wrong_move\n",
            "   199/50000: episode: 199, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9091.511 -4006.0635\n",
            "wrong_move\n",
            "   200/50000: episode: 200, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8524.876 -3950.639\n",
            "wrong_move\n",
            "   201/50000: episode: 201, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9082.821 -4009.4421\n",
            "wrong_move\n",
            "   202/50000: episode: 202, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9078.3125 -3996.4004\n",
            "wrong_move\n",
            "   203/50000: episode: 203, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8068.5474 -3773.0476\n",
            "wrong_move\n",
            "   204/50000: episode: 204, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8490.846 -3790.6086\n",
            "wrong_move\n",
            "   205/50000: episode: 205, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8688.382 -4080.6448\n",
            "wrong_move\n",
            "   206/50000: episode: 206, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2672.000 [2672.000, 2672.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8556.403 -4055.2595\n",
            "wrong_move\n",
            "   207/50000: episode: 207, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9126.846 -3956.8076\n",
            "wrong_move\n",
            "   208/50000: episode: 208, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8891.898 -4099.683\n",
            "wrong_move\n",
            "   209/50000: episode: 209, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8674.553 -4084.4316\n",
            "wrong_move\n",
            "   210/50000: episode: 210, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8592.284 -4045.3428\n",
            "wrong_move\n",
            "   211/50000: episode: 211, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9047.311 -4018.1428\n",
            "wrong_move\n",
            "   212/50000: episode: 212, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8516.44 -3791.759\n",
            "wrong_move\n",
            "   213/50000: episode: 213, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9078.768 -3999.6487\n",
            "wrong_move\n",
            "   214/50000: episode: 214, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8744.327 -4119.944\n",
            "wrong_move\n",
            "   215/50000: episode: 215, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9089.527 -3993.6921\n",
            "wrong_move\n",
            "   216/50000: episode: 216, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8516.171 -3792.0042\n",
            "wrong_move\n",
            "   217/50000: episode: 217, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8448.666 -3787.0762\n",
            "wrong_move\n",
            "   218/50000: episode: 218, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8450.694 -3786.368\n",
            "wrong_move\n",
            "   219/50000: episode: 219, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8787.601 -4013.0835\n",
            "wrong_move\n",
            "   220/50000: episode: 220, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9071.699 -4010.1157\n",
            "wrong_move\n",
            "   221/50000: episode: 221, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8511.242 -3793.9102\n",
            "wrong_move\n",
            "   222/50000: episode: 222, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8323.616 -3782.1106\n",
            "wrong_move\n",
            "   223/50000: episode: 223, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9060.987 -4009.106\n",
            "wrong_move\n",
            "   224/50000: episode: 224, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8687.438 -3799.1533\n",
            "wrong_move\n",
            "   225/50000: episode: 225, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8449.961 -3789.717\n",
            "wrong_move\n",
            "   226/50000: episode: 226, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8969.822 -3952.9727\n",
            "wrong_move\n",
            "   227/50000: episode: 227, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8695.103 -4079.6328\n",
            "wrong_move\n",
            "   228/50000: episode: 228, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2672.000 [2672.000, 2672.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7959.441 -3763.3423\n",
            "wrong_move\n",
            "   229/50000: episode: 229, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9067.029 -3733.9734\n",
            "wrong_move\n",
            "   230/50000: episode: 230, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   231/50000: episode: 231, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9114.697 -3972.616\n",
            "wrong_move\n",
            "   232/50000: episode: 232, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9106.137 -3967.3608\n",
            "wrong_move\n",
            "   233/50000: episode: 233, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9082.069 -4001.909\n",
            "wrong_move\n",
            "   234/50000: episode: 234, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8622.63 -3796.4875\n",
            "wrong_move\n",
            "   235/50000: episode: 235, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9023.544 -4037.2888\n",
            "wrong_move\n",
            "   236/50000: episode: 236, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8922.059 -4026.4568\n",
            "wrong_move\n",
            "   237/50000: episode: 237, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9069.778 -3681.771\n",
            "wrong_move\n",
            "   238/50000: episode: 238, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9071.727 -4015.7424\n",
            "wrong_move\n",
            "   239/50000: episode: 239, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9069.632 -4006.4868\n",
            "wrong_move\n",
            "   240/50000: episode: 240, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9096.37 -3989.5088\n",
            "wrong_move\n",
            "   241/50000: episode: 241, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8634.964 -3795.2834\n",
            "wrong_move\n",
            "   242/50000: episode: 242, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9070.207 -4023.1045\n",
            "wrong_move\n",
            "   243/50000: episode: 243, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9063.521 -4018.4282\n",
            "wrong_move\n",
            "   244/50000: episode: 244, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8564.504 -4095.982\n",
            "wrong_move\n",
            "   245/50000: episode: 245, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8321.327 -3784.3567\n",
            "wrong_move\n",
            "   246/50000: episode: 246, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8763.1455 -3120.002\n",
            "wrong_move\n",
            "   247/50000: episode: 247, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9066.049 -4015.7712\n",
            "wrong_move\n",
            "   248/50000: episode: 248, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8691.147 -4151.9023\n",
            "wrong_move\n",
            "   249/50000: episode: 249, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8855.731 -4021.4214\n",
            "wrong_move\n",
            "   250/50000: episode: 250, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9091.829 -4000.098\n",
            "wrong_move\n",
            "   251/50000: episode: 251, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9068.423 -4017.1875\n",
            "wrong_move\n",
            "   252/50000: episode: 252, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9007.177 -4030.669\n",
            "wrong_move\n",
            "   253/50000: episode: 253, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8711.195 -3798.1167\n",
            "wrong_move\n",
            "   254/50000: episode: 254, duration: 0.044s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2454.000 [2454.000, 2454.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8322.543 -3784.723\n",
            "wrong_move\n",
            "   255/50000: episode: 255, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8200.468 -3778.5852\n",
            "wrong_move\n",
            "   256/50000: episode: 256, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1401.000 [1401.000, 1401.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9027.044 -4025.652\n",
            "wrong_move\n",
            "   257/50000: episode: 257, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8430.838 -3786.3416\n",
            "wrong_move\n",
            "   258/50000: episode: 258, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7799.8228 -3751.3323\n",
            "wrong_move\n",
            "   259/50000: episode: 259, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8697.095 -4077.0928\n",
            "wrong_move\n"
          ]
        }
      ],
      "source": [
        "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
        "# even the metrics!\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "for i in range (10):\n",
        "  policy = EpsGreedyQPolicy(0.01)\n",
        "  dqn = DQNAgent(model=model, nb_actions=NB_ACTIONS, memory=memory,\n",
        "                target_model_update=1e-2, policy=policy)\n",
        "  dqn.compile(Adam(lr=1e-1), metrics=['mae'])\n",
        "\n",
        "  # Okay, now it's time to learn something! We visualize the training here for show, but this\n",
        "  # slows down training quite a lot. You can always safely abort the training prematurely using\n",
        "  # Ctrl + C.\n",
        "  his = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
        "  \n",
        "  model.save('chess_model_sf.h5')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "rl_dqn_vs_sf_train.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
