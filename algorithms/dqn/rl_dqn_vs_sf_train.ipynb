{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEsaesKwwFnf",
        "outputId": "c020eac5-a8d3-42da-a1c8-a1ef30ecce3b"
      },
      "outputs": [],
      "source": [
        "# ! pip install keras-rl2\n",
        "# ! pip install chess\n",
        "# ! pip install python-chess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ocqViokxFxM",
        "outputId": "a32fdb04-daf7-474b-ab41-be5bee6850bd"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INImGNcRyKhX",
        "outputId": "afe25e4d-72d3-4bc4-88df-4bb6be1b84d1"
      },
      "outputs": [],
      "source": [
        "# ls drive/MyDrive/Data/Chess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten,\\\n",
        "     Input,BatchNormalization, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "# import gym_chess\n",
        "\n",
        "import chess\n",
        "from sys import platform\n",
        "import os\n",
        "import chess.engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 5327364616367050006\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "os.system('chmod +x stockfish_14.1_linux_x64')\n",
        "engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish_14.1_linux_x64\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # %%\n",
        "# if platform == \"linux\" or platform == \"linux2\":\n",
        "#     os.system('chmod +x ../stockfish/stockfish_14.1_linux_x64')\n",
        "#     engine = chess.engine.SimpleEngine.popen_uci(r\"../stockfish/stockfish_14.1_linux_x64\")\n",
        "# elif platform == \"win32\":\n",
        "#     engine = chess.engine.SimpleEngine.popen_uci(r\"../stockfish/stockfish_14.1_win_32bit.exe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_move(env):\n",
        "    result = engine.play(env.env, chess.engine.Limit(time=0.05))\n",
        "    return result.move"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "STATE_SHAPE = (65, )\n",
        "NB_ACTIONS = 4096"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChessEnv:\n",
        "    '''\n",
        "    state - obser: ndarray - (65,): [:65] is flatten from int_board; [65] is color of bot; 1 is white and -1 is black\n",
        "    step: int. step_range = (0, 4096) , is encoded from square A to square B (64 x 64 val)\n",
        "    reward: int\n",
        "    '''\n",
        "\n",
        "    mapped = {\n",
        "            'P': 10,     # White Pawn\n",
        "            'p': -10,    # Black Pawn\n",
        "            'N': 20,     # White Knight\n",
        "            'n': -20,    # Black Knight\n",
        "            'B': 30,     # White Bishop\n",
        "            'b': -30,    # Black Bishop\n",
        "            'R': 40,     # White Rook\n",
        "            'r': -40,    # Black Rook\n",
        "            'Q': 50,     # White Queen\n",
        "            'q': -50,    # Black Queen\n",
        "            'K': 900,     # White King\n",
        "            'k': -900     # Black King\n",
        "    }\n",
        "    # state_shape = (8, 8)\n",
        "    # nb_actions = 4096\n",
        "    model = None\n",
        "    \n",
        "    def __init__(self, model: Sequential, neg_r_each_step = -1) -> None:\n",
        "        self.env = chess.Board()\n",
        "        self.state = self.reset()\n",
        "        # [-1] = 1 -> white, -1 -> black\n",
        "        self.bot_color = self.env.turn * 2 - 1\n",
        "        self.neg_r_each_step = neg_r_each_step\n",
        "        self.model = model\n",
        "\n",
        "    def is_draw(self):\n",
        "        if self.env.is_stalemate():\n",
        "            print(\"statlemate\")\n",
        "            return True\n",
        "        if self.env.is_fivefold_repetition():\n",
        "            print(\"fivefold repetition\")\n",
        "            return True\n",
        "        if self.env.is_seventyfive_moves():\n",
        "            print(\"75 moves\")\n",
        "            return True\n",
        "        if self.env.is_insufficient_material():\n",
        "            print(\"Insufficient Material\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def is_checkmate(self):\n",
        "        # If There is checkmate then it will be TRUE else FALSE.It will be a boolean value.\n",
        "        return self.env.is_checkmate()\n",
        "\n",
        "    def convert_board_to_int(self):\n",
        "        epd_string = self.env.epd()\n",
        "        list_int = np.empty((0, ))\n",
        "        for i in epd_string:\n",
        "            if i == \" \":\n",
        "                list_int = list_int.reshape((8, 8))\n",
        "                return list_int\n",
        "            elif i != \"/\":\n",
        "                if i in self.mapped:\n",
        "                    list_int = np.append(list_int, self.mapped[i])\n",
        "                else:\n",
        "                    for counter in range(0, int(i)):\n",
        "                        list_int = np.append(list_int, 0)\n",
        "        list_int = list_int.reshape((8, 8))\n",
        "        return list_int\n",
        "\n",
        "    def get_state(self) -> np.ndarray:\n",
        "        return np.append(self.convert_board_to_int().reshape(64,), self.env.turn * 2 - 1)\n",
        "\n",
        "    def legal_moves(self):\n",
        "        return list(self.env.legal_moves)\n",
        "\n",
        "    def encodeMove(self, move_uci:str):\n",
        "        if len(move_uci) != 4:\n",
        "            raise ValueError()\n",
        "        a, b = chess.parse_square(move_uci[:2]), chess.parse_square(move_uci[2:])\n",
        "        return a * 64 + b\n",
        "\n",
        "    def decodeMove(self, move_int:int):\n",
        "        a, b = move_int//64, move_int%64\n",
        "        # a, b = chess.square_name(a), chess.square_name(b)\n",
        "\n",
        "        move = self.env.find_move(from_square= a,to_square= b)\n",
        "        return move\n",
        "\n",
        "    def render(self):\n",
        "        print(self.env.unicode())\n",
        "\n",
        "    def reset(self):\n",
        "        # random state\n",
        "        redo = True\n",
        "        num_sample_steps = 0\n",
        "        while redo:\n",
        "            redo = False\n",
        "            self.env = chess.Board()\n",
        "            num_sample_steps = np.random.randint(0, 50)\n",
        "            for i in range (num_sample_steps):\n",
        "                lg_move = self.legal_moves()\n",
        "                if len(lg_move) != 0:\n",
        "                    move = np.random.choice(self.legal_moves())\n",
        "                    self.env.push(move)\n",
        "                else:\n",
        "                    redo = True\n",
        "                    break\n",
        "        return self.get_state()\n",
        "\n",
        "    def step(self, action: int):\n",
        "        reward = 0\n",
        "        done = True\n",
        "\n",
        "        try:\n",
        "            # move in legal move\n",
        "            move = self.decodeMove(action)\n",
        "\n",
        "            # neg reward each step\n",
        "            reward = self.neg_r_each_step\n",
        "\n",
        "            # location to_square\n",
        "            to_r, to_c = move.to_square//8, move.to_square%8\n",
        "            reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
        "\n",
        "            # action\n",
        "            self.env.push(move)\n",
        "            self.state = self.get_state()\n",
        "\n",
        "            # check end game\n",
        "            if self.is_checkmate():\n",
        "                reward += self.mapped['K']\n",
        "                done = True\n",
        "                print('Win')\n",
        "            elif self.is_draw():\n",
        "                reward += 300\n",
        "                done = True\n",
        "\n",
        "            # opponent's turn   \n",
        "            else:\n",
        "                done = False\n",
        "\n",
        "                move = find_move(self)\n",
        "\n",
        "                # location to_square\n",
        "                to_r, to_c = move.to_square//8, move.to_square%8\n",
        "                reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
        "\n",
        "                # action\n",
        "                self.env.push(move)\n",
        "                self.state = self.get_state()\n",
        "\n",
        "                # check end game\n",
        "                if self.is_checkmate():\n",
        "                    reward -= self.mapped['K']\n",
        "                    done = True\n",
        "                    print(\"Lose\")\n",
        "                elif self.is_draw():\n",
        "                    reward += 300\n",
        "                    done = True\n",
        "\n",
        "        except:\n",
        "            # wrong move\n",
        "            reward = -5000\n",
        "            done = True\n",
        "            print('wrong_move')\n",
        "\n",
        "        return self.state, reward, done, {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "WARNING:tensorflow:From /home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/layers/normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 65)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               8448      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4096)              528384    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 4096)              0         \n",
            "=================================================================\n",
            "Total params: 554,368\n",
            "Trainable params: 553,856\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# model\n",
        "model = Sequential()\n",
        "model.add(Input((1, ) + STATE_SHAPE))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(NB_ACTIONS))\n",
        "model.add(Activation('linear'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = ChessEnv(model, neg_r_each_step=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_weights('chess_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  warnings.warn(\n",
            "2021-12-14 13:28:06.652713: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 1999965000 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 50000 steps ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wrong_move\n",
            "     1/50000: episode: 1, duration: 0.255s, episode steps:   1, steps per second:   4, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1027.000 [1027.000, 1027.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "     2/50000: episode: 2, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1439.000 [1439.000, 1439.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "     3/50000: episode: 3, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "     4/50000: episode: 4, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3155.000 [3155.000, 3155.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "     5/50000: episode: 5, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 420.000 [420.000, 420.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "     6/50000: episode: 6, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1032.000 [1032.000, 1032.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "     7/50000: episode: 7, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3368.000 [3368.000, 3368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "     8/50000: episode: 8, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 190.000 [190.000, 190.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "     9/50000: episode: 9, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 388.000 [388.000, 388.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    10/50000: episode: 10, duration: 0.009s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    11/50000: episode: 11, duration: 0.010s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    12/50000: episode: 12, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 276.000 [276.000, 276.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    13/50000: episode: 13, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1203.000 [1203.000, 1203.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    14/50000: episode: 14, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    15/50000: episode: 15, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    16/50000: episode: 16, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2618.000 [2618.000, 2618.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    17/50000: episode: 17, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    18/50000: episode: 18, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1837.000 [1837.000, 1837.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    19/50000: episode: 19, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    20/50000: episode: 20, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    21/50000: episode: 21, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    22/50000: episode: 22, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    23/50000: episode: 23, duration: 0.007s, episode steps:   1, steps per second: 146, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    24/50000: episode: 24, duration: 0.007s, episode steps:   1, steps per second: 153, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 423.000 [423.000, 423.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    25/50000: episode: 25, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    26/50000: episode: 26, duration: 0.004s, episode steps:   1, steps per second: 225, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1181.000 [1181.000, 1181.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    27/50000: episode: 27, duration: 0.006s, episode steps:   1, steps per second: 163, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    28/50000: episode: 28, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    29/50000: episode: 29, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    30/50000: episode: 30, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2391.000 [2391.000, 2391.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    31/50000: episode: 31, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    32/50000: episode: 32, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3887.000 [3887.000, 3887.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    33/50000: episode: 33, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 666.000 [666.000, 666.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    34/50000: episode: 34, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    35/50000: episode: 35, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 113.000 [113.000, 113.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    36/50000: episode: 36, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    37/50000: episode: 37, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    38/50000: episode: 38, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    39/50000: episode: 39, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4042.000 [4042.000, 4042.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    40/50000: episode: 40, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    41/50000: episode: 41, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1706.000 [1706.000, 1706.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    42/50000: episode: 42, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2688.000 [2688.000, 2688.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    43/50000: episode: 43, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    44/50000: episode: 44, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1523.000 [1523.000, 1523.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    45/50000: episode: 45, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3507.000 [3507.000, 3507.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    46/50000: episode: 46, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    47/50000: episode: 47, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    48/50000: episode: 48, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1174.000 [1174.000, 1174.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    49/50000: episode: 49, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    50/50000: episode: 50, duration: 0.006s, episode steps:   1, steps per second: 174, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    51/50000: episode: 51, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3844.000 [3844.000, 3844.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    52/50000: episode: 52, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    53/50000: episode: 53, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    54/50000: episode: 54, duration: 0.006s, episode steps:   1, steps per second: 161, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3571.000 [3571.000, 3571.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    55/50000: episode: 55, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    56/50000: episode: 56, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    57/50000: episode: 57, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    58/50000: episode: 58, duration: 0.005s, episode steps:   1, steps per second: 211, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2197.000 [2197.000, 2197.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    59/50000: episode: 59, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    60/50000: episode: 60, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2457.000 [2457.000, 2457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    61/50000: episode: 61, duration: 0.005s, episode steps:   1, steps per second: 200, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    62/50000: episode: 62, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    63/50000: episode: 63, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3373.000 [3373.000, 3373.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    64/50000: episode: 64, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    65/50000: episode: 65, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3114.000 [3114.000, 3114.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    66/50000: episode: 66, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3883.000 [3883.000, 3883.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    67/50000: episode: 67, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 928.000 [928.000, 928.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    68/50000: episode: 68, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1838.000 [1838.000, 1838.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    69/50000: episode: 69, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    70/50000: episode: 70, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1945.000 [1945.000, 1945.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    71/50000: episode: 71, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    72/50000: episode: 72, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3368.000 [3368.000, 3368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    73/50000: episode: 73, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    74/50000: episode: 74, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    75/50000: episode: 75, duration: 0.006s, episode steps:   1, steps per second: 172, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    76/50000: episode: 76, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 814.000 [814.000, 814.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    77/50000: episode: 77, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    78/50000: episode: 78, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    79/50000: episode: 79, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2160.000 [2160.000, 2160.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    80/50000: episode: 80, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3862.000 [3862.000, 3862.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    81/50000: episode: 81, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 420.000 [420.000, 420.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    82/50000: episode: 82, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 348.000 [348.000, 348.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    83/50000: episode: 83, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    84/50000: episode: 84, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3320.000 [3320.000, 3320.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    85/50000: episode: 85, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    86/50000: episode: 86, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    87/50000: episode: 87, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    88/50000: episode: 88, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3387.000 [3387.000, 3387.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    89/50000: episode: 89, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    90/50000: episode: 90, duration: 0.007s, episode steps:   1, steps per second: 147, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    91/50000: episode: 91, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    92/50000: episode: 92, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    93/50000: episode: 93, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 241.000 [241.000, 241.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    94/50000: episode: 94, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    95/50000: episode: 95, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    96/50000: episode: 96, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 987.000 [987.000, 987.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    97/50000: episode: 97, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    98/50000: episode: 98, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    99/50000: episode: 99, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   100/50000: episode: 100, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   101/50000: episode: 101, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   102/50000: episode: 102, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2269.000 [2269.000, 2269.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   103/50000: episode: 103, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   104/50000: episode: 104, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   105/50000: episode: 105, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 75.000 [75.000, 75.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   106/50000: episode: 106, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   107/50000: episode: 107, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   108/50000: episode: 108, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3073.000 [3073.000, 3073.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   109/50000: episode: 109, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   110/50000: episode: 110, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   111/50000: episode: 111, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2167.000 [2167.000, 2167.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   112/50000: episode: 112, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3507.000 [3507.000, 3507.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   113/50000: episode: 113, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   114/50000: episode: 114, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   115/50000: episode: 115, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1286.000 [1286.000, 1286.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   116/50000: episode: 116, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1226.000 [1226.000, 1226.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   117/50000: episode: 117, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1281.000 [1281.000, 1281.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   118/50000: episode: 118, duration: 0.005s, episode steps:   1, steps per second: 183, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   119/50000: episode: 119, duration: 0.005s, episode steps:   1, steps per second: 213, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 377.000 [377.000, 377.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   120/50000: episode: 120, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3373.000 [3373.000, 3373.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   121/50000: episode: 121, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   122/50000: episode: 122, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3715.000 [3715.000, 3715.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   123/50000: episode: 123, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   124/50000: episode: 124, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   125/50000: episode: 125, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   126/50000: episode: 126, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   127/50000: episode: 127, duration: 0.006s, episode steps:   1, steps per second: 174, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   128/50000: episode: 128, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   129/50000: episode: 129, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2790.000 [2790.000, 2790.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   130/50000: episode: 130, duration: 0.008s, episode steps:   1, steps per second: 133, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   131/50000: episode: 131, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3337.000 [3337.000, 3337.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   132/50000: episode: 132, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2245.000 [2245.000, 2245.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   133/50000: episode: 133, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   134/50000: episode: 134, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1695.000 [1695.000, 1695.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   135/50000: episode: 135, duration: 0.004s, episode steps:   1, steps per second: 244, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   136/50000: episode: 136, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   137/50000: episode: 137, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 492.000 [492.000, 492.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   138/50000: episode: 138, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   139/50000: episode: 139, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1138.000 [1138.000, 1138.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   140/50000: episode: 140, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   141/50000: episode: 141, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2442.000 [2442.000, 2442.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   142/50000: episode: 142, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   143/50000: episode: 143, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   144/50000: episode: 144, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   145/50000: episode: 145, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 503.000 [503.000, 503.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   146/50000: episode: 146, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1137.000 [1137.000, 1137.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   147/50000: episode: 147, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 481.000 [481.000, 481.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   148/50000: episode: 148, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   149/50000: episode: 149, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   150/50000: episode: 150, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   151/50000: episode: 151, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   152/50000: episode: 152, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1706.000 [1706.000, 1706.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   153/50000: episode: 153, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   154/50000: episode: 154, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2445.000 [2445.000, 2445.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   155/50000: episode: 155, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3617.000 [3617.000, 3617.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   156/50000: episode: 156, duration: 0.007s, episode steps:   1, steps per second: 140, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   157/50000: episode: 157, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3054.000 [3054.000, 3054.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   158/50000: episode: 158, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3523.000 [3523.000, 3523.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   159/50000: episode: 159, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1481.000 [1481.000, 1481.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   160/50000: episode: 160, duration: 0.005s, episode steps:   1, steps per second: 193, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2061.000 [2061.000, 2061.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   161/50000: episode: 161, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2881.000 [2881.000, 2881.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   162/50000: episode: 162, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 420.000 [420.000, 420.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   163/50000: episode: 163, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2309.000 [2309.000, 2309.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   164/50000: episode: 164, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1841.000 [1841.000, 1841.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   165/50000: episode: 165, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   166/50000: episode: 166, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1980.000 [1980.000, 1980.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   167/50000: episode: 167, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1488.000 [1488.000, 1488.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   168/50000: episode: 168, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3517.000 [3517.000, 3517.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   169/50000: episode: 169, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   170/50000: episode: 170, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   171/50000: episode: 171, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   172/50000: episode: 172, duration: 0.008s, episode steps:   1, steps per second: 132, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2214.000 [2214.000, 2214.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   173/50000: episode: 173, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   174/50000: episode: 174, duration: 0.007s, episode steps:   1, steps per second: 140, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   175/50000: episode: 175, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 299.000 [299.000, 299.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   176/50000: episode: 176, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   177/50000: episode: 177, duration: 0.006s, episode steps:   1, steps per second: 178, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1040.000 [1040.000, 1040.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   178/50000: episode: 178, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1820.000 [1820.000, 1820.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   179/50000: episode: 179, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3681.000 [3681.000, 3681.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   180/50000: episode: 180, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   181/50000: episode: 181, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   182/50000: episode: 182, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   183/50000: episode: 183, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   184/50000: episode: 184, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   185/50000: episode: 185, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 814.000 [814.000, 814.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   186/50000: episode: 186, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   187/50000: episode: 187, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   188/50000: episode: 188, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   189/50000: episode: 189, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   190/50000: episode: 190, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   191/50000: episode: 191, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   192/50000: episode: 192, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   193/50000: episode: 193, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1907.000 [1907.000, 1907.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   194/50000: episode: 194, duration: 0.005s, episode steps:   1, steps per second: 185, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1299.000 [1299.000, 1299.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   195/50000: episode: 195, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2824.000 [2824.000, 2824.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   196/50000: episode: 196, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2099.000 [2099.000, 2099.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   197/50000: episode: 197, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3844.000 [3844.000, 3844.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   198/50000: episode: 198, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   199/50000: episode: 199, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   200/50000: episode: 200, duration: 0.004s, episode steps:   1, steps per second: 234, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   201/50000: episode: 201, duration: 0.007s, episode steps:   1, steps per second: 138, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   202/50000: episode: 202, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 600.000 [600.000, 600.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   203/50000: episode: 203, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   204/50000: episode: 204, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   205/50000: episode: 205, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   206/50000: episode: 206, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   207/50000: episode: 207, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2523.000 [2523.000, 2523.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   208/50000: episode: 208, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3107.000 [3107.000, 3107.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   209/50000: episode: 209, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   210/50000: episode: 210, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   211/50000: episode: 211, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1307.000 [1307.000, 1307.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   212/50000: episode: 212, duration: 0.005s, episode steps:   1, steps per second: 216, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2243.000 [2243.000, 2243.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   213/50000: episode: 213, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   214/50000: episode: 214, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   215/50000: episode: 215, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   216/50000: episode: 216, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   217/50000: episode: 217, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   218/50000: episode: 218, duration: 0.006s, episode steps:   1, steps per second: 176, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   219/50000: episode: 219, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2560.000 [2560.000, 2560.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   220/50000: episode: 220, duration: 0.005s, episode steps:   1, steps per second: 187, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   221/50000: episode: 221, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   222/50000: episode: 222, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1201.000 [1201.000, 1201.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   223/50000: episode: 223, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   224/50000: episode: 224, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   225/50000: episode: 225, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   226/50000: episode: 226, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 439.000 [439.000, 439.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   227/50000: episode: 227, duration: 0.004s, episode steps:   1, steps per second: 262, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   228/50000: episode: 228, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   229/50000: episode: 229, duration: 0.004s, episode steps:   1, steps per second: 234, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 607.000 [607.000, 607.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   230/50000: episode: 230, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3659.000 [3659.000, 3659.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   231/50000: episode: 231, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   232/50000: episode: 232, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   233/50000: episode: 233, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2732.000 [2732.000, 2732.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   234/50000: episode: 234, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2449.000 [2449.000, 2449.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   235/50000: episode: 235, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 474.000 [474.000, 474.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   236/50000: episode: 236, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1025.000 [1025.000, 1025.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   237/50000: episode: 237, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2266.000 [2266.000, 2266.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   238/50000: episode: 238, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   239/50000: episode: 239, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   240/50000: episode: 240, duration: 0.007s, episode steps:   1, steps per second: 134, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1890.000 [1890.000, 1890.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   241/50000: episode: 241, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   242/50000: episode: 242, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   243/50000: episode: 243, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 148.000 [148.000, 148.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   244/50000: episode: 244, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   245/50000: episode: 245, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2679.000 [2679.000, 2679.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   246/50000: episode: 246, duration: 0.005s, episode steps:   1, steps per second: 193, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   247/50000: episode: 247, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 177.000 [177.000, 177.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   248/50000: episode: 248, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   249/50000: episode: 249, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   250/50000: episode: 250, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   251/50000: episode: 251, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   252/50000: episode: 252, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   253/50000: episode: 253, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   254/50000: episode: 254, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3671.000 [3671.000, 3671.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   255/50000: episode: 255, duration: 0.006s, episode steps:   1, steps per second: 168, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 252.000 [252.000, 252.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   256/50000: episode: 256, duration: 0.004s, episode steps:   1, steps per second: 235, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2746.000 [2746.000, 2746.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   257/50000: episode: 257, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 193.000 [193.000, 193.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   258/50000: episode: 258, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1423.000 [1423.000, 1423.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   259/50000: episode: 259, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 468.000 [468.000, 468.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   260/50000: episode: 260, duration: 0.005s, episode steps:   1, steps per second: 182, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   261/50000: episode: 261, duration: 0.006s, episode steps:   1, steps per second: 158, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 544.000 [544.000, 544.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   262/50000: episode: 262, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   263/50000: episode: 263, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   264/50000: episode: 264, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3763.000 [3763.000, 3763.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   265/50000: episode: 265, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   266/50000: episode: 266, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   267/50000: episode: 267, duration: 0.010s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   268/50000: episode: 268, duration: 0.006s, episode steps:   1, steps per second: 173, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1578.000 [1578.000, 1578.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   269/50000: episode: 269, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2199.000 [2199.000, 2199.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   270/50000: episode: 270, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   271/50000: episode: 271, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1973.000 [1973.000, 1973.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   272/50000: episode: 272, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   273/50000: episode: 273, duration: 0.006s, episode steps:   1, steps per second: 160, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1875.000 [1875.000, 1875.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   274/50000: episode: 274, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   275/50000: episode: 275, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   276/50000: episode: 276, duration: 0.006s, episode steps:   1, steps per second: 173, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   277/50000: episode: 277, duration: 0.004s, episode steps:   1, steps per second: 263, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   278/50000: episode: 278, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   279/50000: episode: 279, duration: 0.005s, episode steps:   1, steps per second: 220, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   280/50000: episode: 280, duration: 0.004s, episode steps:   1, steps per second: 278, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 543.000 [543.000, 543.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   281/50000: episode: 281, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   282/50000: episode: 282, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1105.000 [1105.000, 1105.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   283/50000: episode: 283, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3373.000 [3373.000, 3373.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   284/50000: episode: 284, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   285/50000: episode: 285, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2480.000 [2480.000, 2480.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   286/50000: episode: 286, duration: 0.004s, episode steps:   1, steps per second: 248, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   287/50000: episode: 287, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   288/50000: episode: 288, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   289/50000: episode: 289, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3051.000 [3051.000, 3051.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   290/50000: episode: 290, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   291/50000: episode: 291, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   292/50000: episode: 292, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2441.000 [2441.000, 2441.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   293/50000: episode: 293, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   294/50000: episode: 294, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   295/50000: episode: 295, duration: 0.006s, episode steps:   1, steps per second: 163, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   296/50000: episode: 296, duration: 0.004s, episode steps:   1, steps per second: 228, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3711.000 [3711.000, 3711.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   297/50000: episode: 297, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   298/50000: episode: 298, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3368.000 [3368.000, 3368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   299/50000: episode: 299, duration: 0.005s, episode steps:   1, steps per second: 204, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3967.000 [3967.000, 3967.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   300/50000: episode: 300, duration: 0.004s, episode steps:   1, steps per second: 222, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   301/50000: episode: 301, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   302/50000: episode: 302, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3034.000 [3034.000, 3034.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   303/50000: episode: 303, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3373.000 [3373.000, 3373.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   304/50000: episode: 304, duration: 0.004s, episode steps:   1, steps per second: 275, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   305/50000: episode: 305, duration: 0.006s, episode steps:   1, steps per second: 176, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   306/50000: episode: 306, duration: 0.004s, episode steps:   1, steps per second: 239, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   307/50000: episode: 307, duration: 0.007s, episode steps:   1, steps per second: 146, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3286.000 [3286.000, 3286.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   308/50000: episode: 308, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   309/50000: episode: 309, duration: 0.004s, episode steps:   1, steps per second: 222, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   310/50000: episode: 310, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   311/50000: episode: 311, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3825.000 [3825.000, 3825.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   312/50000: episode: 312, duration: 0.006s, episode steps:   1, steps per second: 156, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   313/50000: episode: 313, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2827.000 [2827.000, 2827.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   314/50000: episode: 314, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   315/50000: episode: 315, duration: 0.003s, episode steps:   1, steps per second: 369, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3837.000 [3837.000, 3837.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   316/50000: episode: 316, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2059.000 [2059.000, 2059.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   317/50000: episode: 317, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   318/50000: episode: 318, duration: 0.005s, episode steps:   1, steps per second: 190, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   319/50000: episode: 319, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3748.000 [3748.000, 3748.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   320/50000: episode: 320, duration: 0.003s, episode steps:   1, steps per second: 397, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3061.000 [3061.000, 3061.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   321/50000: episode: 321, duration: 0.004s, episode steps:   1, steps per second: 240, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   322/50000: episode: 322, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3455.000 [3455.000, 3455.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   323/50000: episode: 323, duration: 0.006s, episode steps:   1, steps per second: 170, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   324/50000: episode: 324, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   325/50000: episode: 325, duration: 0.004s, episode steps:   1, steps per second: 271, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   326/50000: episode: 326, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1263.000 [1263.000, 1263.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   327/50000: episode: 327, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2158.000 [2158.000, 2158.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   328/50000: episode: 328, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 935.000 [935.000, 935.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   329/50000: episode: 329, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1102.000 [1102.000, 1102.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   330/50000: episode: 330, duration: 0.003s, episode steps:   1, steps per second: 389, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   331/50000: episode: 331, duration: 0.007s, episode steps:   1, steps per second: 150, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   332/50000: episode: 332, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2827.000 [2827.000, 2827.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   333/50000: episode: 333, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   334/50000: episode: 334, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   335/50000: episode: 335, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   336/50000: episode: 336, duration: 0.004s, episode steps:   1, steps per second: 234, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   337/50000: episode: 337, duration: 0.006s, episode steps:   1, steps per second: 174, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   338/50000: episode: 338, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   339/50000: episode: 339, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   340/50000: episode: 340, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2409.000 [2409.000, 2409.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   341/50000: episode: 341, duration: 0.005s, episode steps:   1, steps per second: 194, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   342/50000: episode: 342, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3501.000 [3501.000, 3501.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   343/50000: episode: 343, duration: 0.003s, episode steps:   1, steps per second: 306, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   344/50000: episode: 344, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2089.000 [2089.000, 2089.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   345/50000: episode: 345, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   346/50000: episode: 346, duration: 0.005s, episode steps:   1, steps per second: 215, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 6.000 [6.000, 6.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   347/50000: episode: 347, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   348/50000: episode: 348, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   349/50000: episode: 349, duration: 0.003s, episode steps:   1, steps per second: 307, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 892.000 [892.000, 892.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   350/50000: episode: 350, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   351/50000: episode: 351, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   352/50000: episode: 352, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   353/50000: episode: 353, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   354/50000: episode: 354, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   355/50000: episode: 355, duration: 0.003s, episode steps:   1, steps per second: 366, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   356/50000: episode: 356, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   357/50000: episode: 357, duration: 0.006s, episode steps:   1, steps per second: 162, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3623.000 [3623.000, 3623.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   358/50000: episode: 358, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4056.000 [4056.000, 4056.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   359/50000: episode: 359, duration: 0.004s, episode steps:   1, steps per second: 284, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   360/50000: episode: 360, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   361/50000: episode: 361, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   362/50000: episode: 362, duration: 0.005s, episode steps:   1, steps per second: 217, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   363/50000: episode: 363, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 977.000 [977.000, 977.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   364/50000: episode: 364, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   365/50000: episode: 365, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3659.000 [3659.000, 3659.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   366/50000: episode: 366, duration: 0.004s, episode steps:   1, steps per second: 246, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   367/50000: episode: 367, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 876.000 [876.000, 876.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   368/50000: episode: 368, duration: 0.005s, episode steps:   1, steps per second: 205, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   369/50000: episode: 369, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4056.000 [4056.000, 4056.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   370/50000: episode: 370, duration: 0.003s, episode steps:   1, steps per second: 340, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   371/50000: episode: 371, duration: 0.005s, episode steps:   1, steps per second: 186, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3086.000 [3086.000, 3086.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   372/50000: episode: 372, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   373/50000: episode: 373, duration: 0.006s, episode steps:   1, steps per second: 169, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   374/50000: episode: 374, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   375/50000: episode: 375, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2228.000 [2228.000, 2228.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   376/50000: episode: 376, duration: 0.006s, episode steps:   1, steps per second: 159, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   377/50000: episode: 377, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   378/50000: episode: 378, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   379/50000: episode: 379, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   380/50000: episode: 380, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   381/50000: episode: 381, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   382/50000: episode: 382, duration: 0.005s, episode steps:   1, steps per second: 205, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2360.000 [2360.000, 2360.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   383/50000: episode: 383, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3179.000 [3179.000, 3179.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   384/50000: episode: 384, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   385/50000: episode: 385, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 814.000 [814.000, 814.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   386/50000: episode: 386, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 172.000 [172.000, 172.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   387/50000: episode: 387, duration: 0.005s, episode steps:   1, steps per second: 216, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4072.000 [4072.000, 4072.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   388/50000: episode: 388, duration: 0.006s, episode steps:   1, steps per second: 167, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   389/50000: episode: 389, duration: 0.006s, episode steps:   1, steps per second: 182, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3767.000 [3767.000, 3767.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   390/50000: episode: 390, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   391/50000: episode: 391, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   392/50000: episode: 392, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   393/50000: episode: 393, duration: 0.005s, episode steps:   1, steps per second: 182, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1266.000 [1266.000, 1266.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   394/50000: episode: 394, duration: 0.003s, episode steps:   1, steps per second: 296, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   395/50000: episode: 395, duration: 0.004s, episode steps:   1, steps per second: 224, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   396/50000: episode: 396, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2391.000 [2391.000, 2391.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   397/50000: episode: 397, duration: 0.006s, episode steps:   1, steps per second: 177, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   398/50000: episode: 398, duration: 0.003s, episode steps:   1, steps per second: 364, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   399/50000: episode: 399, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 876.000 [876.000, 876.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   400/50000: episode: 400, duration: 0.007s, episode steps:   1, steps per second: 140, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2086.000 [2086.000, 2086.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   401/50000: episode: 401, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   402/50000: episode: 402, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1032.000 [1032.000, 1032.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   403/50000: episode: 403, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3320.000 [3320.000, 3320.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   404/50000: episode: 404, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3373.000 [3373.000, 3373.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   405/50000: episode: 405, duration: 0.005s, episode steps:   1, steps per second: 205, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   406/50000: episode: 406, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1706.000 [1706.000, 1706.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   407/50000: episode: 407, duration: 0.005s, episode steps:   1, steps per second: 194, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   408/50000: episode: 408, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3368.000 [3368.000, 3368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   409/50000: episode: 409, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   410/50000: episode: 410, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   411/50000: episode: 411, duration: 0.006s, episode steps:   1, steps per second: 174, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   412/50000: episode: 412, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2481.000 [2481.000, 2481.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   413/50000: episode: 413, duration: 0.003s, episode steps:   1, steps per second: 324, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   414/50000: episode: 414, duration: 0.004s, episode steps:   1, steps per second: 273, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2384.000 [2384.000, 2384.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   415/50000: episode: 415, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 262.000 [262.000, 262.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   416/50000: episode: 416, duration: 0.006s, episode steps:   1, steps per second: 155, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   418/50000: episode: 417, duration: 1.019s, episode steps:   2, steps per second:   2, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1745.000 [1307.000, 2183.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   419/50000: episode: 418, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 760.000 [760.000, 760.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   420/50000: episode: 419, duration: 0.007s, episode steps:   1, steps per second: 153, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2025.000 [2025.000, 2025.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   421/50000: episode: 420, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1882.000 [1882.000, 1882.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   422/50000: episode: 421, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1032.000 [1032.000, 1032.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   423/50000: episode: 422, duration: 0.004s, episode steps:   1, steps per second: 260, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   424/50000: episode: 423, duration: 0.003s, episode steps:   1, steps per second: 313, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   425/50000: episode: 424, duration: 0.003s, episode steps:   1, steps per second: 290, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   426/50000: episode: 425, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3834.000 [3834.000, 3834.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   427/50000: episode: 426, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   428/50000: episode: 427, duration: 0.004s, episode steps:   1, steps per second: 232, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   429/50000: episode: 428, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4066.000 [4066.000, 4066.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   430/50000: episode: 429, duration: 0.006s, episode steps:   1, steps per second: 165, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   431/50000: episode: 430, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   432/50000: episode: 431, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1288.000 [1288.000, 1288.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   433/50000: episode: 432, duration: 0.006s, episode steps:   1, steps per second: 154, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   434/50000: episode: 433, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   435/50000: episode: 434, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   436/50000: episode: 435, duration: 0.005s, episode steps:   1, steps per second: 194, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 382.000 [382.000, 382.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   437/50000: episode: 436, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   438/50000: episode: 437, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   439/50000: episode: 438, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   440/50000: episode: 439, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   441/50000: episode: 440, duration: 0.003s, episode steps:   1, steps per second: 316, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   442/50000: episode: 441, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   443/50000: episode: 442, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 420.000 [420.000, 420.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   444/50000: episode: 443, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   445/50000: episode: 444, duration: 0.007s, episode steps:   1, steps per second: 151, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 420.000 [420.000, 420.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   446/50000: episode: 445, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   447/50000: episode: 446, duration: 0.006s, episode steps:   1, steps per second: 174, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   448/50000: episode: 447, duration: 0.006s, episode steps:   1, steps per second: 160, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   449/50000: episode: 448, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   450/50000: episode: 449, duration: 0.016s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   451/50000: episode: 450, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2279.000 [2279.000, 2279.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   452/50000: episode: 451, duration: 0.006s, episode steps:   1, steps per second: 181, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   453/50000: episode: 452, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1671.000 [1671.000, 1671.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   454/50000: episode: 453, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   455/50000: episode: 454, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   456/50000: episode: 455, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   457/50000: episode: 456, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   458/50000: episode: 457, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   459/50000: episode: 458, duration: 0.004s, episode steps:   1, steps per second: 275, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   460/50000: episode: 459, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3368.000 [3368.000, 3368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   461/50000: episode: 460, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   462/50000: episode: 461, duration: 0.007s, episode steps:   1, steps per second: 151, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1187.000 [1187.000, 1187.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   463/50000: episode: 462, duration: 0.005s, episode steps:   1, steps per second: 217, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   464/50000: episode: 463, duration: 0.004s, episode steps:   1, steps per second: 247, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   465/50000: episode: 464, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 810.000 [810.000, 810.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   466/50000: episode: 465, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   467/50000: episode: 466, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   468/50000: episode: 467, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   469/50000: episode: 468, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 591.000 [591.000, 591.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   470/50000: episode: 469, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 299.000 [299.000, 299.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   471/50000: episode: 470, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   472/50000: episode: 471, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3882.000 [3882.000, 3882.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   473/50000: episode: 472, duration: 0.006s, episode steps:   1, steps per second: 180, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2186.000 [2186.000, 2186.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   474/50000: episode: 473, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   475/50000: episode: 474, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   476/50000: episode: 475, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 814.000 [814.000, 814.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   477/50000: episode: 476, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1330.000 [1330.000, 1330.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   478/50000: episode: 477, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   479/50000: episode: 478, duration: 0.005s, episode steps:   1, steps per second: 215, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   480/50000: episode: 479, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2768.000 [2768.000, 2768.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   481/50000: episode: 480, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 325.000 [325.000, 325.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   482/50000: episode: 481, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3523.000 [3523.000, 3523.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   483/50000: episode: 482, duration: 0.005s, episode steps:   1, steps per second: 217, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2155.000 [2155.000, 2155.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   484/50000: episode: 483, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 834.000 [834.000, 834.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   485/50000: episode: 484, duration: 0.007s, episode steps:   1, steps per second: 140, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   486/50000: episode: 485, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3659.000 [3659.000, 3659.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   487/50000: episode: 486, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   488/50000: episode: 487, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4044.000 [4044.000, 4044.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   489/50000: episode: 488, duration: 0.006s, episode steps:   1, steps per second: 165, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 246.000 [246.000, 246.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   490/50000: episode: 489, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 629.000 [629.000, 629.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   491/50000: episode: 490, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   492/50000: episode: 491, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1488.000 [1488.000, 1488.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   493/50000: episode: 492, duration: 0.008s, episode steps:   1, steps per second: 132, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   495/50000: episode: 493, duration: 1.008s, episode steps:   2, steps per second:   2, episode reward: -4981.000, mean reward: -2490.500 [-5000.000, 19.000], mean action: 3216.500 [2939.000, 3494.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   496/50000: episode: 494, duration: 0.002s, episode steps:   1, steps per second: 419, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   497/50000: episode: 495, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1164.000 [1164.000, 1164.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   498/50000: episode: 496, duration: 0.005s, episode steps:   1, steps per second: 205, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   499/50000: episode: 497, duration: 0.005s, episode steps:   1, steps per second: 188, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 292.000 [292.000, 292.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   500/50000: episode: 498, duration: 0.003s, episode steps:   1, steps per second: 369, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2270.000 [2270.000, 2270.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   501/50000: episode: 499, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   502/50000: episode: 500, duration: 0.003s, episode steps:   1, steps per second: 323, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   503/50000: episode: 501, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   504/50000: episode: 502, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3659.000 [3659.000, 3659.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   505/50000: episode: 503, duration: 0.006s, episode steps:   1, steps per second: 177, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   506/50000: episode: 504, duration: 0.006s, episode steps:   1, steps per second: 155, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2737.000 [2737.000, 2737.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   507/50000: episode: 505, duration: 0.005s, episode steps:   1, steps per second: 183, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   508/50000: episode: 506, duration: 0.007s, episode steps:   1, steps per second: 147, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   509/50000: episode: 507, duration: 0.004s, episode steps:   1, steps per second: 258, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 531.000 [531.000, 531.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   510/50000: episode: 508, duration: 0.003s, episode steps:   1, steps per second: 344, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   511/50000: episode: 509, duration: 0.002s, episode steps:   1, steps per second: 406, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3952.000 [3952.000, 3952.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   512/50000: episode: 510, duration: 0.004s, episode steps:   1, steps per second: 227, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   513/50000: episode: 511, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   514/50000: episode: 512, duration: 0.004s, episode steps:   1, steps per second: 231, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   515/50000: episode: 513, duration: 0.006s, episode steps:   1, steps per second: 159, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   516/50000: episode: 514, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   517/50000: episode: 515, duration: 0.003s, episode steps:   1, steps per second: 321, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1748.000 [1748.000, 1748.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   518/50000: episode: 516, duration: 0.005s, episode steps:   1, steps per second: 210, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   519/50000: episode: 517, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   520/50000: episode: 518, duration: 0.003s, episode steps:   1, steps per second: 308, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   521/50000: episode: 519, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   522/50000: episode: 520, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   523/50000: episode: 521, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 752.000 [752.000, 752.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   525/50000: episode: 522, duration: 1.016s, episode steps:   2, steps per second:   2, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 1956.000 [1956.000, 1956.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   526/50000: episode: 523, duration: 0.006s, episode steps:   1, steps per second: 178, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 815.000 [815.000, 815.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   527/50000: episode: 524, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   528/50000: episode: 525, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2891.000 [2891.000, 2891.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   529/50000: episode: 526, duration: 0.006s, episode steps:   1, steps per second: 155, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   530/50000: episode: 527, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 784.000 [784.000, 784.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   531/50000: episode: 528, duration: 0.006s, episode steps:   1, steps per second: 161, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   532/50000: episode: 529, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   533/50000: episode: 530, duration: 0.006s, episode steps:   1, steps per second: 156, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   534/50000: episode: 531, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   535/50000: episode: 532, duration: 0.007s, episode steps:   1, steps per second: 145, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   536/50000: episode: 533, duration: 0.002s, episode steps:   1, steps per second: 444, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   537/50000: episode: 534, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   538/50000: episode: 535, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   539/50000: episode: 536, duration: 0.004s, episode steps:   1, steps per second: 283, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   540/50000: episode: 537, duration: 0.004s, episode steps:   1, steps per second: 226, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2961.000 [2961.000, 2961.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   541/50000: episode: 538, duration: 0.007s, episode steps:   1, steps per second: 145, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   542/50000: episode: 539, duration: 0.004s, episode steps:   1, steps per second: 250, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   543/50000: episode: 540, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2996.000 [2996.000, 2996.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   544/50000: episode: 541, duration: 0.007s, episode steps:   1, steps per second: 151, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   545/50000: episode: 542, duration: 0.003s, episode steps:   1, steps per second: 299, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   546/50000: episode: 543, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3894.000 [3894.000, 3894.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   547/50000: episode: 544, duration: 0.003s, episode steps:   1, steps per second: 368, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3998.000 [3998.000, 3998.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   548/50000: episode: 545, duration: 0.007s, episode steps:   1, steps per second: 146, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2099.000 [2099.000, 2099.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   549/50000: episode: 546, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   550/50000: episode: 547, duration: 0.006s, episode steps:   1, steps per second: 180, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3464.000 [3464.000, 3464.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   551/50000: episode: 548, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   552/50000: episode: 549, duration: 0.005s, episode steps:   1, steps per second: 199, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   553/50000: episode: 550, duration: 0.003s, episode steps:   1, steps per second: 289, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   554/50000: episode: 551, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   555/50000: episode: 552, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 876.000 [876.000, 876.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   556/50000: episode: 553, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   557/50000: episode: 554, duration: 0.006s, episode steps:   1, steps per second: 176, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   558/50000: episode: 555, duration: 0.005s, episode steps:   1, steps per second: 201, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   559/50000: episode: 556, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3275.000 [3275.000, 3275.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   560/50000: episode: 557, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   561/50000: episode: 558, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   562/50000: episode: 559, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 782.000 [782.000, 782.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   563/50000: episode: 560, duration: 0.006s, episode steps:   1, steps per second: 180, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   564/50000: episode: 561, duration: 0.005s, episode steps:   1, steps per second: 186, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4031.000 [4031.000, 4031.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   565/50000: episode: 562, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1820.000 [1820.000, 1820.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   566/50000: episode: 563, duration: 0.004s, episode steps:   1, steps per second: 276, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   567/50000: episode: 564, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1032.000 [1032.000, 1032.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   568/50000: episode: 565, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   569/50000: episode: 566, duration: 0.006s, episode steps:   1, steps per second: 180, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   570/50000: episode: 567, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3368.000 [3368.000, 3368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   571/50000: episode: 568, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3263.000 [3263.000, 3263.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   572/50000: episode: 569, duration: 0.003s, episode steps:   1, steps per second: 304, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3104.000 [3104.000, 3104.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   573/50000: episode: 570, duration: 0.004s, episode steps:   1, steps per second: 256, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   574/50000: episode: 571, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   575/50000: episode: 572, duration: 0.005s, episode steps:   1, steps per second: 217, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   576/50000: episode: 573, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   577/50000: episode: 574, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   578/50000: episode: 575, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   579/50000: episode: 576, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   580/50000: episode: 577, duration: 0.004s, episode steps:   1, steps per second: 250, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   581/50000: episode: 578, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   582/50000: episode: 579, duration: 0.004s, episode steps:   1, steps per second: 279, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2634.000 [2634.000, 2634.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   583/50000: episode: 580, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 777.000 [777.000, 777.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   584/50000: episode: 581, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2497.000 [2497.000, 2497.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   585/50000: episode: 582, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   586/50000: episode: 583, duration: 0.002s, episode steps:   1, steps per second: 473, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   587/50000: episode: 584, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2451.000 [2451.000, 2451.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   588/50000: episode: 585, duration: 0.006s, episode steps:   1, steps per second: 173, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   589/50000: episode: 586, duration: 0.005s, episode steps:   1, steps per second: 212, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   590/50000: episode: 587, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   591/50000: episode: 588, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3850.000 [3850.000, 3850.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   592/50000: episode: 589, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   593/50000: episode: 590, duration: 0.006s, episode steps:   1, steps per second: 167, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   594/50000: episode: 591, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2558.000 [2558.000, 2558.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   595/50000: episode: 592, duration: 0.005s, episode steps:   1, steps per second: 183, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3731.000 [3731.000, 3731.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   596/50000: episode: 593, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 963.000 [963.000, 963.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   597/50000: episode: 594, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2099.000 [2099.000, 2099.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   598/50000: episode: 595, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1945.000 [1945.000, 1945.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   599/50000: episode: 596, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1142.000 [1142.000, 1142.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   600/50000: episode: 597, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3368.000 [3368.000, 3368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   601/50000: episode: 598, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 313.000 [313.000, 313.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   602/50000: episode: 599, duration: 0.006s, episode steps:   1, steps per second: 160, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3707.000 [3707.000, 3707.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   603/50000: episode: 600, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1287.000 [1287.000, 1287.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   604/50000: episode: 601, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2863.000 [2863.000, 2863.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   605/50000: episode: 602, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   606/50000: episode: 603, duration: 0.007s, episode steps:   1, steps per second: 147, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1290.000 [1290.000, 1290.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   607/50000: episode: 604, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   608/50000: episode: 605, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 814.000 [814.000, 814.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   609/50000: episode: 606, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   610/50000: episode: 607, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2890.000 [2890.000, 2890.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   611/50000: episode: 608, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   612/50000: episode: 609, duration: 0.006s, episode steps:   1, steps per second: 155, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   613/50000: episode: 610, duration: 0.006s, episode steps:   1, steps per second: 172, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2584.000 [2584.000, 2584.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   614/50000: episode: 611, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   615/50000: episode: 612, duration: 0.008s, episode steps:   1, steps per second: 133, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3844.000 [3844.000, 3844.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   616/50000: episode: 613, duration: 0.004s, episode steps:   1, steps per second: 243, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   617/50000: episode: 614, duration: 0.005s, episode steps:   1, steps per second: 204, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   618/50000: episode: 615, duration: 0.005s, episode steps:   1, steps per second: 182, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   619/50000: episode: 616, duration: 0.004s, episode steps:   1, steps per second: 231, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1451.000 [1451.000, 1451.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   620/50000: episode: 617, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   621/50000: episode: 618, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   622/50000: episode: 619, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   623/50000: episode: 620, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3091.000 [3091.000, 3091.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   624/50000: episode: 621, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3744.000 [3744.000, 3744.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   625/50000: episode: 622, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1155.000 [1155.000, 1155.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   626/50000: episode: 623, duration: 0.003s, episode steps:   1, steps per second: 301, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   627/50000: episode: 624, duration: 0.005s, episode steps:   1, steps per second: 201, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   628/50000: episode: 625, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   629/50000: episode: 626, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3408.000 [3408.000, 3408.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   630/50000: episode: 627, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   631/50000: episode: 628, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1419.000 [1419.000, 1419.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   632/50000: episode: 629, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4035.000 [4035.000, 4035.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   633/50000: episode: 630, duration: 0.004s, episode steps:   1, steps per second: 230, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3031.000 [3031.000, 3031.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   634/50000: episode: 631, duration: 0.005s, episode steps:   1, steps per second: 189, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   635/50000: episode: 632, duration: 0.007s, episode steps:   1, steps per second: 147, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   636/50000: episode: 633, duration: 0.004s, episode steps:   1, steps per second: 242, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1359.000 [1359.000, 1359.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   637/50000: episode: 634, duration: 0.004s, episode steps:   1, steps per second: 249, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   638/50000: episode: 635, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   639/50000: episode: 636, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   640/50000: episode: 637, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3302.000 [3302.000, 3302.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   641/50000: episode: 638, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1139.000 [1139.000, 1139.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   642/50000: episode: 639, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   643/50000: episode: 640, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   644/50000: episode: 641, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 288.000 [288.000, 288.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   645/50000: episode: 642, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3987.000 [3987.000, 3987.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   646/50000: episode: 643, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   647/50000: episode: 644, duration: 0.004s, episode steps:   1, steps per second: 248, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   648/50000: episode: 645, duration: 0.004s, episode steps:   1, steps per second: 247, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1239.000 [1239.000, 1239.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   649/50000: episode: 646, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 941.000 [941.000, 941.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   650/50000: episode: 647, duration: 0.004s, episode steps:   1, steps per second: 243, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1922.000 [1922.000, 1922.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   651/50000: episode: 648, duration: 0.004s, episode steps:   1, steps per second: 234, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   652/50000: episode: 649, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3554.000 [3554.000, 3554.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   653/50000: episode: 650, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 904.000 [904.000, 904.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   654/50000: episode: 651, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 119.000 [119.000, 119.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   655/50000: episode: 652, duration: 0.004s, episode steps:   1, steps per second: 275, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   656/50000: episode: 653, duration: 0.006s, episode steps:   1, steps per second: 170, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 836.000 [836.000, 836.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   657/50000: episode: 654, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   658/50000: episode: 655, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3844.000 [3844.000, 3844.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   659/50000: episode: 656, duration: 0.004s, episode steps:   1, steps per second: 243, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   660/50000: episode: 657, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   661/50000: episode: 658, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   662/50000: episode: 659, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   663/50000: episode: 660, duration: 0.004s, episode steps:   1, steps per second: 242, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   664/50000: episode: 661, duration: 0.007s, episode steps:   1, steps per second: 150, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   665/50000: episode: 662, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   666/50000: episode: 663, duration: 0.007s, episode steps:   1, steps per second: 150, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   667/50000: episode: 664, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 707.000 [707.000, 707.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   668/50000: episode: 665, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   669/50000: episode: 666, duration: 0.004s, episode steps:   1, steps per second: 245, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   670/50000: episode: 667, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 325.000 [325.000, 325.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   671/50000: episode: 668, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2032.000 [2032.000, 2032.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   672/50000: episode: 669, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   673/50000: episode: 670, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   674/50000: episode: 671, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1697.000 [1697.000, 1697.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   675/50000: episode: 672, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3765.000 [3765.000, 3765.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   676/50000: episode: 673, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   677/50000: episode: 674, duration: 0.004s, episode steps:   1, steps per second: 284, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   678/50000: episode: 675, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1331.000 [1331.000, 1331.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   679/50000: episode: 676, duration: 0.004s, episode steps:   1, steps per second: 245, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   680/50000: episode: 677, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   681/50000: episode: 678, duration: 0.004s, episode steps:   1, steps per second: 255, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1773.000 [1773.000, 1773.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   682/50000: episode: 679, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   683/50000: episode: 680, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   684/50000: episode: 681, duration: 0.004s, episode steps:   1, steps per second: 257, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2935.000 [2935.000, 2935.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   685/50000: episode: 682, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   686/50000: episode: 683, duration: 0.007s, episode steps:   1, steps per second: 153, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1174.000 [1174.000, 1174.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   687/50000: episode: 684, duration: 0.003s, episode steps:   1, steps per second: 387, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   688/50000: episode: 685, duration: 0.006s, episode steps:   1, steps per second: 172, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2637.000 [2637.000, 2637.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   689/50000: episode: 686, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   690/50000: episode: 687, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   691/50000: episode: 688, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 203.000 [203.000, 203.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   692/50000: episode: 689, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3852.000 [3852.000, 3852.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   693/50000: episode: 690, duration: 0.005s, episode steps:   1, steps per second: 217, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   694/50000: episode: 691, duration: 0.005s, episode steps:   1, steps per second: 201, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   695/50000: episode: 692, duration: 0.006s, episode steps:   1, steps per second: 182, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   696/50000: episode: 693, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   697/50000: episode: 694, duration: 0.004s, episode steps:   1, steps per second: 276, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3608.000 [3608.000, 3608.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   698/50000: episode: 695, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3353.000 [3353.000, 3353.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   699/50000: episode: 696, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   700/50000: episode: 697, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3822.000 [3822.000, 3822.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   701/50000: episode: 698, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   702/50000: episode: 699, duration: 0.005s, episode steps:   1, steps per second: 219, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3848.000 [3848.000, 3848.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   703/50000: episode: 700, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2099.000 [2099.000, 2099.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   704/50000: episode: 701, duration: 0.008s, episode steps:   1, steps per second: 132, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   705/50000: episode: 702, duration: 0.007s, episode steps:   1, steps per second: 153, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   706/50000: episode: 703, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   707/50000: episode: 704, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3320.000 [3320.000, 3320.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   708/50000: episode: 705, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   709/50000: episode: 706, duration: 0.006s, episode steps:   1, steps per second: 169, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   710/50000: episode: 707, duration: 0.008s, episode steps:   1, steps per second: 132, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 639.000 [639.000, 639.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   711/50000: episode: 708, duration: 0.006s, episode steps:   1, steps per second: 171, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   712/50000: episode: 709, duration: 0.004s, episode steps:   1, steps per second: 239, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   713/50000: episode: 710, duration: 0.007s, episode steps:   1, steps per second: 145, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   714/50000: episode: 711, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2064.000 [2064.000, 2064.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   715/50000: episode: 712, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3368.000 [3368.000, 3368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   716/50000: episode: 713, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1820.000 [1820.000, 1820.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   717/50000: episode: 714, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2099.000 [2099.000, 2099.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   718/50000: episode: 715, duration: 0.007s, episode steps:   1, steps per second: 134, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3507.000 [3507.000, 3507.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   719/50000: episode: 716, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 331.000 [331.000, 331.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   720/50000: episode: 717, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   721/50000: episode: 718, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   722/50000: episode: 719, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   723/50000: episode: 720, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2661.000 [2661.000, 2661.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   724/50000: episode: 721, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1945.000 [1945.000, 1945.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   725/50000: episode: 722, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   726/50000: episode: 723, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   727/50000: episode: 724, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   728/50000: episode: 725, duration: 0.005s, episode steps:   1, steps per second: 210, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   729/50000: episode: 726, duration: 0.007s, episode steps:   1, steps per second: 138, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   730/50000: episode: 727, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   731/50000: episode: 728, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3368.000 [3368.000, 3368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   732/50000: episode: 729, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   733/50000: episode: 730, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 325.000 [325.000, 325.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   734/50000: episode: 731, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   735/50000: episode: 732, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1879.000 [1879.000, 1879.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   736/50000: episode: 733, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   737/50000: episode: 734, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   738/50000: episode: 735, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   739/50000: episode: 736, duration: 0.004s, episode steps:   1, steps per second: 233, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   740/50000: episode: 737, duration: 0.005s, episode steps:   1, steps per second: 209, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   741/50000: episode: 738, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1170.000 [1170.000, 1170.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   742/50000: episode: 739, duration: 0.006s, episode steps:   1, steps per second: 175, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2053.000 [2053.000, 2053.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   743/50000: episode: 740, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1562.000 [1562.000, 1562.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   744/50000: episode: 741, duration: 0.005s, episode steps:   1, steps per second: 196, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   745/50000: episode: 742, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   746/50000: episode: 743, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2099.000 [2099.000, 2099.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   747/50000: episode: 744, duration: 0.005s, episode steps:   1, steps per second: 211, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   748/50000: episode: 745, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 325.000 [325.000, 325.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   749/50000: episode: 746, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   750/50000: episode: 747, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1945.000 [1945.000, 1945.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   751/50000: episode: 748, duration: 0.003s, episode steps:   1, steps per second: 294, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3482.000 [3482.000, 3482.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   752/50000: episode: 749, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1488.000 [1488.000, 1488.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   753/50000: episode: 750, duration: 0.005s, episode steps:   1, steps per second: 215, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   754/50000: episode: 751, duration: 0.006s, episode steps:   1, steps per second: 162, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1945.000 [1945.000, 1945.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   755/50000: episode: 752, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2114.000 [2114.000, 2114.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   756/50000: episode: 753, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   757/50000: episode: 754, duration: 0.007s, episode steps:   1, steps per second: 154, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   758/50000: episode: 755, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   759/50000: episode: 756, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   760/50000: episode: 757, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 652.000 [652.000, 652.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   761/50000: episode: 758, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   762/50000: episode: 759, duration: 0.005s, episode steps:   1, steps per second: 215, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   763/50000: episode: 760, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   764/50000: episode: 761, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   765/50000: episode: 762, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3779.000 [3779.000, 3779.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   766/50000: episode: 763, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 882.000 [882.000, 882.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   767/50000: episode: 764, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   768/50000: episode: 765, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   769/50000: episode: 766, duration: 0.007s, episode steps:   1, steps per second: 153, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   770/50000: episode: 767, duration: 0.003s, episode steps:   1, steps per second: 348, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   771/50000: episode: 768, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   772/50000: episode: 769, duration: 0.005s, episode steps:   1, steps per second: 192, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   773/50000: episode: 770, duration: 0.006s, episode steps:   1, steps per second: 178, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2652.000 [2652.000, 2652.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   774/50000: episode: 771, duration: 0.003s, episode steps:   1, steps per second: 297, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   775/50000: episode: 772, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3168.000 [3168.000, 3168.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   776/50000: episode: 773, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3559.000 [3559.000, 3559.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   777/50000: episode: 774, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3807.000 [3807.000, 3807.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   778/50000: episode: 775, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 191.000 [191.000, 191.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   779/50000: episode: 776, duration: 0.007s, episode steps:   1, steps per second: 145, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   780/50000: episode: 777, duration: 0.007s, episode steps:   1, steps per second: 146, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   781/50000: episode: 778, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 420.000 [420.000, 420.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   782/50000: episode: 779, duration: 0.008s, episode steps:   1, steps per second: 133, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   783/50000: episode: 780, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   784/50000: episode: 781, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   785/50000: episode: 782, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   786/50000: episode: 783, duration: 0.004s, episode steps:   1, steps per second: 261, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2500.000 [2500.000, 2500.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   787/50000: episode: 784, duration: 0.007s, episode steps:   1, steps per second: 140, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   788/50000: episode: 785, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   789/50000: episode: 786, duration: 0.008s, episode steps:   1, steps per second: 132, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1196.000 [1196.000, 1196.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   790/50000: episode: 787, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   791/50000: episode: 788, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   792/50000: episode: 789, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   793/50000: episode: 790, duration: 0.003s, episode steps:   1, steps per second: 340, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 701.000 [701.000, 701.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   794/50000: episode: 791, duration: 0.004s, episode steps:   1, steps per second: 272, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   795/50000: episode: 792, duration: 0.005s, episode steps:   1, steps per second: 192, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2449.000 [2449.000, 2449.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   796/50000: episode: 793, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3897.000 [3897.000, 3897.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   797/50000: episode: 794, duration: 0.004s, episode steps:   1, steps per second: 227, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2186.000 [2186.000, 2186.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   799/50000: episode: 795, duration: 1.017s, episode steps:   2, steps per second:   2, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2446.500 [1634.000, 3259.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   800/50000: episode: 796, duration: 0.005s, episode steps:   1, steps per second: 191, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   801/50000: episode: 797, duration: 0.003s, episode steps:   1, steps per second: 323, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   802/50000: episode: 798, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   803/50000: episode: 799, duration: 0.004s, episode steps:   1, steps per second: 231, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   804/50000: episode: 800, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1731.000 [1731.000, 1731.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   805/50000: episode: 801, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   806/50000: episode: 802, duration: 0.004s, episode steps:   1, steps per second: 284, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   807/50000: episode: 803, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1032.000 [1032.000, 1032.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   808/50000: episode: 804, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2930.000 [2930.000, 2930.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   809/50000: episode: 805, duration: 0.004s, episode steps:   1, steps per second: 260, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   810/50000: episode: 806, duration: 0.003s, episode steps:   1, steps per second: 377, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   811/50000: episode: 807, duration: 0.005s, episode steps:   1, steps per second: 197, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   812/50000: episode: 808, duration: 0.003s, episode steps:   1, steps per second: 307, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   813/50000: episode: 809, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   814/50000: episode: 810, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   815/50000: episode: 811, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   816/50000: episode: 812, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   817/50000: episode: 813, duration: 0.006s, episode steps:   1, steps per second: 155, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 538.000 [538.000, 538.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   818/50000: episode: 814, duration: 0.004s, episode steps:   1, steps per second: 285, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   819/50000: episode: 815, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2912.000 [2912.000, 2912.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   820/50000: episode: 816, duration: 0.004s, episode steps:   1, steps per second: 232, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   821/50000: episode: 817, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   822/50000: episode: 818, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3507.000 [3507.000, 3507.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   823/50000: episode: 819, duration: 0.004s, episode steps:   1, steps per second: 237, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   824/50000: episode: 820, duration: 0.006s, episode steps:   1, steps per second: 172, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   825/50000: episode: 821, duration: 0.006s, episode steps:   1, steps per second: 177, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   826/50000: episode: 822, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 396.000 [396.000, 396.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   827/50000: episode: 823, duration: 0.003s, episode steps:   1, steps per second: 392, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   828/50000: episode: 824, duration: 0.003s, episode steps:   1, steps per second: 302, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   829/50000: episode: 825, duration: 0.006s, episode steps:   1, steps per second: 154, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   830/50000: episode: 826, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3659.000 [3659.000, 3659.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   831/50000: episode: 827, duration: 0.003s, episode steps:   1, steps per second: 315, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   832/50000: episode: 828, duration: 0.005s, episode steps:   1, steps per second: 189, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 462.000 [462.000, 462.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   833/50000: episode: 829, duration: 0.004s, episode steps:   1, steps per second: 257, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   834/50000: episode: 830, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   835/50000: episode: 831, duration: 0.004s, episode steps:   1, steps per second: 266, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   836/50000: episode: 832, duration: 0.006s, episode steps:   1, steps per second: 169, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 307.000 [307.000, 307.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   837/50000: episode: 833, duration: 0.005s, episode steps:   1, steps per second: 193, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   838/50000: episode: 834, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3251.000 [3251.000, 3251.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   839/50000: episode: 835, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3622.000 [3622.000, 3622.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   840/50000: episode: 836, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   841/50000: episode: 837, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   842/50000: episode: 838, duration: 0.005s, episode steps:   1, steps per second: 206, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   843/50000: episode: 839, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3369.000 [3369.000, 3369.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   844/50000: episode: 840, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 420.000 [420.000, 420.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   845/50000: episode: 841, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   846/50000: episode: 842, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2603.000 [2603.000, 2603.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   847/50000: episode: 843, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4056.000 [4056.000, 4056.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   848/50000: episode: 844, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   849/50000: episode: 845, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   850/50000: episode: 846, duration: 0.004s, episode steps:   1, steps per second: 252, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1476.000 [1476.000, 1476.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   851/50000: episode: 847, duration: 0.005s, episode steps:   1, steps per second: 195, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2241.000 [2241.000, 2241.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   852/50000: episode: 848, duration: 0.004s, episode steps:   1, steps per second: 279, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3231.000 [3231.000, 3231.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   853/50000: episode: 849, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2827.000 [2827.000, 2827.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   854/50000: episode: 850, duration: 0.006s, episode steps:   1, steps per second: 159, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 725.000 [725.000, 725.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   855/50000: episode: 851, duration: 0.007s, episode steps:   1, steps per second: 140, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   856/50000: episode: 852, duration: 0.003s, episode steps:   1, steps per second: 312, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   857/50000: episode: 853, duration: 0.003s, episode steps:   1, steps per second: 311, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   858/50000: episode: 854, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1820.000 [1820.000, 1820.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   859/50000: episode: 855, duration: 0.006s, episode steps:   1, steps per second: 168, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   860/50000: episode: 856, duration: 0.005s, episode steps:   1, steps per second: 191, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   861/50000: episode: 857, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 760.000 [760.000, 760.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   862/50000: episode: 858, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1455.000 [1455.000, 1455.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   863/50000: episode: 859, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   864/50000: episode: 860, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2647.000 [2647.000, 2647.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   865/50000: episode: 861, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   866/50000: episode: 862, duration: 0.005s, episode steps:   1, steps per second: 201, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   867/50000: episode: 863, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   868/50000: episode: 864, duration: 0.004s, episode steps:   1, steps per second: 232, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   869/50000: episode: 865, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   870/50000: episode: 866, duration: 0.003s, episode steps:   1, steps per second: 394, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   871/50000: episode: 867, duration: 0.004s, episode steps:   1, steps per second: 254, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   872/50000: episode: 868, duration: 0.004s, episode steps:   1, steps per second: 257, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   873/50000: episode: 869, duration: 0.004s, episode steps:   1, steps per second: 264, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   874/50000: episode: 870, duration: 0.006s, episode steps:   1, steps per second: 168, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   875/50000: episode: 871, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   876/50000: episode: 872, duration: 0.006s, episode steps:   1, steps per second: 154, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   877/50000: episode: 873, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3373.000 [3373.000, 3373.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   878/50000: episode: 874, duration: 0.005s, episode steps:   1, steps per second: 198, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   879/50000: episode: 875, duration: 0.003s, episode steps:   1, steps per second: 315, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   880/50000: episode: 876, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1389.000 [1389.000, 1389.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   881/50000: episode: 877, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1488.000 [1488.000, 1488.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   882/50000: episode: 878, duration: 0.003s, episode steps:   1, steps per second: 305, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   883/50000: episode: 879, duration: 0.006s, episode steps:   1, steps per second: 162, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   884/50000: episode: 880, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   885/50000: episode: 881, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   886/50000: episode: 882, duration: 0.006s, episode steps:   1, steps per second: 174, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   887/50000: episode: 883, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   888/50000: episode: 884, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   889/50000: episode: 885, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   890/50000: episode: 886, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1170.000 [1170.000, 1170.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   891/50000: episode: 887, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1103.000 [1103.000, 1103.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   892/50000: episode: 888, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3066.000 [3066.000, 3066.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   893/50000: episode: 889, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   894/50000: episode: 890, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   895/50000: episode: 891, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3844.000 [3844.000, 3844.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   896/50000: episode: 892, duration: 0.006s, episode steps:   1, steps per second: 171, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   897/50000: episode: 893, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1493.000 [1493.000, 1493.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   898/50000: episode: 894, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   899/50000: episode: 895, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   900/50000: episode: 896, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   901/50000: episode: 897, duration: 0.003s, episode steps:   1, steps per second: 303, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   902/50000: episode: 898, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   903/50000: episode: 899, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 434.000 [434.000, 434.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   904/50000: episode: 900, duration: 0.007s, episode steps:   1, steps per second: 154, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   905/50000: episode: 901, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1693.000 [1693.000, 1693.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   906/50000: episode: 902, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3474.000 [3474.000, 3474.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   907/50000: episode: 903, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   908/50000: episode: 904, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   909/50000: episode: 905, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1488.000 [1488.000, 1488.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   910/50000: episode: 906, duration: 0.004s, episode steps:   1, steps per second: 260, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   911/50000: episode: 907, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   912/50000: episode: 908, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 135.000 [135.000, 135.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   913/50000: episode: 909, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 791.000 [791.000, 791.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   914/50000: episode: 910, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 547.000 [547.000, 547.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   915/50000: episode: 911, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   916/50000: episode: 912, duration: 0.003s, episode steps:   1, steps per second: 307, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   917/50000: episode: 913, duration: 0.004s, episode steps:   1, steps per second: 277, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4067.000 [4067.000, 4067.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   918/50000: episode: 914, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   919/50000: episode: 915, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   920/50000: episode: 916, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   921/50000: episode: 917, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   922/50000: episode: 918, duration: 0.003s, episode steps:   1, steps per second: 306, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   923/50000: episode: 919, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   924/50000: episode: 920, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   925/50000: episode: 921, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1419.000 [1419.000, 1419.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   926/50000: episode: 922, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   927/50000: episode: 923, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   928/50000: episode: 924, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3842.000 [3842.000, 3842.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   929/50000: episode: 925, duration: 0.006s, episode steps:   1, steps per second: 156, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3882.000 [3882.000, 3882.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   930/50000: episode: 926, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1071.000 [1071.000, 1071.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   931/50000: episode: 927, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3340.000 [3340.000, 3340.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   932/50000: episode: 928, duration: 0.005s, episode steps:   1, steps per second: 202, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   933/50000: episode: 929, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   934/50000: episode: 930, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1488.000 [1488.000, 1488.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   935/50000: episode: 931, duration: 0.005s, episode steps:   1, steps per second: 221, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   936/50000: episode: 932, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 649.000 [649.000, 649.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   937/50000: episode: 933, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   938/50000: episode: 934, duration: 0.003s, episode steps:   1, steps per second: 326, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2479.000 [2479.000, 2479.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   939/50000: episode: 935, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   940/50000: episode: 936, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3520.000 [3520.000, 3520.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   941/50000: episode: 937, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3656.000 [3656.000, 3656.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   942/50000: episode: 938, duration: 0.004s, episode steps:   1, steps per second: 233, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 420.000 [420.000, 420.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   943/50000: episode: 939, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1820.000 [1820.000, 1820.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   944/50000: episode: 940, duration: 0.004s, episode steps:   1, steps per second: 284, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 420.000 [420.000, 420.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   945/50000: episode: 941, duration: 0.006s, episode steps:   1, steps per second: 163, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   946/50000: episode: 942, duration: 0.006s, episode steps:   1, steps per second: 170, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   947/50000: episode: 943, duration: 0.004s, episode steps:   1, steps per second: 235, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 448.000 [448.000, 448.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   948/50000: episode: 944, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 540.000 [540.000, 540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   949/50000: episode: 945, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   950/50000: episode: 946, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   951/50000: episode: 947, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3320.000 [3320.000, 3320.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   952/50000: episode: 948, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   953/50000: episode: 949, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2170.000 [2170.000, 2170.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   954/50000: episode: 950, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 169.000 [169.000, 169.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   955/50000: episode: 951, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   956/50000: episode: 952, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   957/50000: episode: 953, duration: 0.009s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   958/50000: episode: 954, duration: 0.006s, episode steps:   1, steps per second: 175, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   959/50000: episode: 955, duration: 0.004s, episode steps:   1, steps per second: 283, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   960/50000: episode: 956, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   961/50000: episode: 957, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   962/50000: episode: 958, duration: 0.004s, episode steps:   1, steps per second: 232, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1761.000 [1761.000, 1761.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   963/50000: episode: 959, duration: 0.005s, episode steps:   1, steps per second: 188, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   964/50000: episode: 960, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1521.000 [1521.000, 1521.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   965/50000: episode: 961, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   966/50000: episode: 962, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   967/50000: episode: 963, duration: 0.003s, episode steps:   1, steps per second: 317, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   968/50000: episode: 964, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3603.000 [3603.000, 3603.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   969/50000: episode: 965, duration: 0.007s, episode steps:   1, steps per second: 145, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   970/50000: episode: 966, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   971/50000: episode: 967, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 478.000 [478.000, 478.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   972/50000: episode: 968, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2296.000 [2296.000, 2296.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   973/50000: episode: 969, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1419.000 [1419.000, 1419.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   974/50000: episode: 970, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   975/50000: episode: 971, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2662.000 [2662.000, 2662.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   976/50000: episode: 972, duration: 0.006s, episode steps:   1, steps per second: 170, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2766.000 [2766.000, 2766.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   977/50000: episode: 973, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   978/50000: episode: 974, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2063.000 [2063.000, 2063.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   979/50000: episode: 975, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3373.000 [3373.000, 3373.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   980/50000: episode: 976, duration: 0.006s, episode steps:   1, steps per second: 173, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1385.000 [1385.000, 1385.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   981/50000: episode: 977, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   982/50000: episode: 978, duration: 0.006s, episode steps:   1, steps per second: 170, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   983/50000: episode: 979, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2854.000 [2854.000, 2854.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   984/50000: episode: 980, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   985/50000: episode: 981, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   986/50000: episode: 982, duration: 0.004s, episode steps:   1, steps per second: 226, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1910.000 [1910.000, 1910.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   987/50000: episode: 983, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   988/50000: episode: 984, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 808.000 [808.000, 808.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   989/50000: episode: 985, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 325.000 [325.000, 325.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   990/50000: episode: 986, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   991/50000: episode: 987, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2517.000 [2517.000, 2517.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   992/50000: episode: 988, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   993/50000: episode: 989, duration: 0.006s, episode steps:   1, steps per second: 178, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2433.000 [2433.000, 2433.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   994/50000: episode: 990, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1488.000 [1488.000, 1488.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   995/50000: episode: 991, duration: 0.003s, episode steps:   1, steps per second: 298, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   996/50000: episode: 992, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4015.000 [4015.000, 4015.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   997/50000: episode: 993, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   998/50000: episode: 994, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   999/50000: episode: 995, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "  1000/50000: episode: 996, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3792.000 [3792.000, 3792.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  1001/50000: episode: 997, duration: 1.055s, episode steps:   1, steps per second:   1, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3572.000 [3572.000, 3572.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "  1002/50000: episode: 998, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 892.000 [892.000, 892.000],  loss: 12500346.000000, mae: 1.406158, mean_q: 0.834001\n",
            "wrong_move\n",
            "  1003/50000: episode: 999, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2059.000 [2059.000, 2059.000],  loss: 12500880.000000, mae: 1.405459, mean_q: 0.794996\n",
            "wrong_move\n",
            "  1004/50000: episode: 1000, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1203.000 [1203.000, 1203.000],  loss: 12109880.000000, mae: 1.370208, mean_q: 0.826853\n",
            "wrong_move\n",
            "  1005/50000: episode: 1001, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2197.000 [2197.000, 2197.000],  loss: 12500744.000000, mae: 1.401982, mean_q: 0.812808\n",
            "wrong_move\n",
            "  1006/50000: episode: 1002, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3235.000 [3235.000, 3235.000],  loss: 12501084.000000, mae: 1.406503, mean_q: 0.818697\n",
            "wrong_move\n",
            "  1007/50000: episode: 1003, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 325.000 [325.000, 325.000],  loss: 12500830.000000, mae: 1.407793, mean_q: 0.829936\n",
            "wrong_move\n",
            "  1008/50000: episode: 1004, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2300.000 [2300.000, 2300.000],  loss: 12500789.000000, mae: 1.410132, mean_q: 0.841770\n",
            "wrong_move\n",
            "  1009/50000: episode: 1005, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: 12109913.000000, mae: 1.368966, mean_q: 0.828668\n",
            "wrong_move\n",
            "  1010/50000: episode: 1006, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: 12500742.000000, mae: 1.411229, mean_q: 0.845102\n",
            "wrong_move\n",
            "  1011/50000: episode: 1007, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3998.000 [3998.000, 3998.000],  loss: 12500798.000000, mae: 1.407085, mean_q: 0.834240\n",
            "wrong_move\n",
            "  1012/50000: episode: 1008, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: 12500625.000000, mae: 1.409504, mean_q: 0.831635\n",
            "wrong_move\n",
            "  1013/50000: episode: 1009, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1940.000 [1940.000, 1940.000],  loss: 12500616.000000, mae: 1.411197, mean_q: 0.843618\n",
            "wrong_move\n",
            "  1014/50000: episode: 1010, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: 12500436.000000, mae: 1.402821, mean_q: 0.810302\n",
            "wrong_move\n",
            "  1015/50000: episode: 1011, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: 12109737.000000, mae: 1.371072, mean_q: 0.835841\n",
            "wrong_move\n",
            "  1016/50000: episode: 1012, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: 12500486.000000, mae: 1.405356, mean_q: 0.826450\n",
            "wrong_move\n",
            "  1017/50000: episode: 1013, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: 12500332.000000, mae: 1.407503, mean_q: 0.839449\n",
            "wrong_move\n",
            "  1018/50000: episode: 1014, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 114.000 [114.000, 114.000],  loss: 12500305.000000, mae: 1.409781, mean_q: 0.839471\n",
            "wrong_move\n",
            "  1019/50000: episode: 1015, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1629.000 [1629.000, 1629.000],  loss: 12500445.000000, mae: 1.409991, mean_q: 0.842448\n",
            "wrong_move\n",
            "  1020/50000: episode: 1016, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1228.000 [1228.000, 1228.000],  loss: 12500148.000000, mae: 1.403309, mean_q: 0.798586\n",
            "wrong_move\n",
            "  1021/50000: episode: 1017, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2994.000 [2994.000, 2994.000],  loss: 12500393.000000, mae: 1.402961, mean_q: 0.813235\n",
            "wrong_move\n",
            "  1022/50000: episode: 1018, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1317.000 [1317.000, 1317.000],  loss: 12500136.000000, mae: 1.410268, mean_q: 0.812033\n",
            "wrong_move\n",
            "  1023/50000: episode: 1019, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: 12500367.000000, mae: 1.408155, mean_q: 0.812560\n",
            "wrong_move\n",
            "  1024/50000: episode: 1020, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 876.000 [876.000, 876.000],  loss: 12500751.000000, mae: 1.407976, mean_q: 0.809947\n",
            "wrong_move\n",
            "  1025/50000: episode: 1021, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: 12500013.000000, mae: 1.404105, mean_q: 0.796706\n",
            "wrong_move\n",
            "  1026/50000: episode: 1022, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: 12500198.000000, mae: 1.407062, mean_q: 0.823327\n",
            "wrong_move\n",
            "  1027/50000: episode: 1023, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: 12500374.000000, mae: 1.406594, mean_q: 0.833006\n",
            "wrong_move\n",
            "  1028/50000: episode: 1024, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: 12499997.000000, mae: 1.407443, mean_q: 0.821536\n",
            "wrong_move\n",
            "  1029/50000: episode: 1025, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 353.000 [353.000, 353.000],  loss: 12500078.000000, mae: 1.401431, mean_q: 0.794308\n",
            "wrong_move\n",
            "  1030/50000: episode: 1026, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 313.000 [313.000, 313.000],  loss: 12499638.000000, mae: 1.407521, mean_q: 0.815770\n",
            "wrong_move\n",
            "  1031/50000: episode: 1027, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: 12499924.000000, mae: 1.406102, mean_q: 0.814115\n",
            "wrong_move\n",
            "  1032/50000: episode: 1028, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1502.000 [1502.000, 1502.000],  loss: 12109333.000000, mae: 1.368981, mean_q: 0.826935\n",
            "wrong_move\n",
            "  1033/50000: episode: 1029, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1933.000 [1933.000, 1933.000],  loss: 12500297.000000, mae: 1.408053, mean_q: 0.819921\n",
            "wrong_move\n",
            "  1034/50000: episode: 1030, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3507.000 [3507.000, 3507.000],  loss: 12499761.000000, mae: 1.399974, mean_q: 0.791657\n",
            "wrong_move\n",
            "  1035/50000: episode: 1031, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1348.000 [1348.000, 1348.000],  loss: 12500158.000000, mae: 1.406113, mean_q: 0.819530\n",
            "wrong_move\n",
            "  1036/50000: episode: 1032, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1245.000 [1245.000, 1245.000],  loss: 12499986.000000, mae: 1.405329, mean_q: 0.831696\n",
            "wrong_move\n",
            "  1037/50000: episode: 1033, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2457.000 [2457.000, 2457.000],  loss: 12500090.000000, mae: 1.409262, mean_q: 0.822374\n",
            "wrong_move\n",
            "  1038/50000: episode: 1034, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1104.000 [1104.000, 1104.000],  loss: 12499935.000000, mae: 1.411457, mean_q: 0.833989\n",
            "wrong_move\n",
            "  1039/50000: episode: 1035, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: 12500049.000000, mae: 1.411412, mean_q: 0.845844\n",
            "wrong_move\n",
            "  1040/50000: episode: 1036, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1348.000 [1348.000, 1348.000],  loss: 12500167.000000, mae: 1.408486, mean_q: 0.840252\n",
            "wrong_move\n",
            "  1041/50000: episode: 1037, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 852.000 [852.000, 852.000],  loss: 12500019.000000, mae: 1.407983, mean_q: 0.844830\n",
            "wrong_move\n",
            "  1042/50000: episode: 1038, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1707.000 [1707.000, 1707.000],  loss: 12499825.000000, mae: 1.410556, mean_q: 0.824992\n",
            "wrong_move\n",
            "  1043/50000: episode: 1039, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: 12499992.000000, mae: 1.412993, mean_q: 0.842578\n",
            "wrong_move\n",
            "  1044/50000: episode: 1040, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: 12500050.000000, mae: 1.410239, mean_q: 0.854140\n",
            "wrong_move\n",
            "  1045/50000: episode: 1041, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: 12500105.000000, mae: 1.412712, mean_q: 0.849760\n",
            "wrong_move\n",
            "  1046/50000: episode: 1042, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2093.000 [2093.000, 2093.000],  loss: 12499632.000000, mae: 1.407877, mean_q: 0.849593\n",
            "wrong_move\n",
            "  1047/50000: episode: 1043, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1561.000 [1561.000, 1561.000],  loss: 12499920.000000, mae: 1.411317, mean_q: 0.846087\n",
            "wrong_move\n",
            "  1048/50000: episode: 1044, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 852.000 [852.000, 852.000],  loss: 12499503.000000, mae: 1.410621, mean_q: 0.823509\n",
            "wrong_move\n",
            "  1049/50000: episode: 1045, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 852.000 [852.000, 852.000],  loss: 12109191.000000, mae: 1.370635, mean_q: 0.846239\n",
            "wrong_move\n",
            "  1050/50000: episode: 1046, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 12500356.000000, mae: 1.410606, mean_q: 0.826675\n",
            "wrong_move\n",
            "  1051/50000: episode: 1047, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1771.000 [1771.000, 1771.000],  loss: 12499776.000000, mae: 1.409657, mean_q: 0.838696\n",
            "wrong_move\n",
            "  1052/50000: episode: 1048, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2281.000 [2281.000, 2281.000],  loss: 12500032.000000, mae: 1.408735, mean_q: 0.829985\n",
            "wrong_move\n",
            "  1053/50000: episode: 1049, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: 12108610.000000, mae: 1.366468, mean_q: 0.838544\n",
            "wrong_move\n",
            "  1054/50000: episode: 1050, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3563.000 [3563.000, 3563.000],  loss: 12499440.000000, mae: 1.408502, mean_q: 0.850493\n",
            "wrong_move\n",
            "  1055/50000: episode: 1051, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 852.000 [852.000, 852.000],  loss: 12499362.000000, mae: 1.412085, mean_q: 0.844522\n",
            "wrong_move\n",
            "  1056/50000: episode: 1052, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1418.000 [1418.000, 1418.000],  loss: 12499131.000000, mae: 1.405160, mean_q: 0.840678\n",
            "wrong_move\n",
            "  1058/50000: episode: 1053, duration: 1.083s, episode steps:   2, steps per second:   2, episode reward: -4981.000, mean reward: -2490.500 [-5000.000, 19.000], mean action: 2297.000 [852.000, 3742.000],  loss: 12499334.000000, mae: 1.407879, mean_q: 0.816562\n",
            "wrong_move\n",
            "  1059/50000: episode: 1054, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1104.000 [1104.000, 1104.000],  loss: 12499602.000000, mae: 1.405281, mean_q: 0.796227\n",
            "wrong_move\n",
            "  1060/50000: episode: 1055, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 852.000 [852.000, 852.000],  loss: 12499076.000000, mae: 1.404116, mean_q: 0.801194\n",
            "wrong_move\n",
            "  1061/50000: episode: 1056, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3563.000 [3563.000, 3563.000],  loss: 12499088.000000, mae: 1.410230, mean_q: 0.835808\n",
            "wrong_move\n",
            "  1062/50000: episode: 1057, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 852.000 [852.000, 852.000],  loss: 12499222.000000, mae: 1.406297, mean_q: 0.835219\n",
            "wrong_move\n",
            "  1063/50000: episode: 1058, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2073.000 [2073.000, 2073.000],  loss: 12498872.000000, mae: 1.410453, mean_q: 0.838565\n",
            "wrong_move\n",
            "  1064/50000: episode: 1059, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1929.000 [1929.000, 1929.000],  loss: 12499558.000000, mae: 1.405725, mean_q: 0.786997\n",
            "wrong_move\n",
            "  1065/50000: episode: 1060, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2073.000 [2073.000, 2073.000],  loss: 12498810.000000, mae: 1.405636, mean_q: 0.856673\n",
            "wrong_move\n",
            "  1066/50000: episode: 1061, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2930.000 [2930.000, 2930.000],  loss: 12108542.000000, mae: 1.370756, mean_q: 0.856107\n",
            "wrong_move\n",
            "  1067/50000: episode: 1062, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2073.000 [2073.000, 2073.000],  loss: 12498866.000000, mae: 1.410273, mean_q: 0.845036\n",
            "wrong_move\n",
            "  1068/50000: episode: 1063, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 852.000 [852.000, 852.000],  loss: 12498999.000000, mae: 1.411053, mean_q: 0.837698\n",
            "wrong_move\n",
            "  1069/50000: episode: 1064, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12499336.000000, mae: 1.404786, mean_q: 0.841207\n",
            "wrong_move\n",
            "  1070/50000: episode: 1065, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2073.000 [2073.000, 2073.000],  loss: 12499219.000000, mae: 1.411289, mean_q: 0.865005\n",
            "wrong_move\n",
            "  1071/50000: episode: 1066, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12499552.000000, mae: 1.403118, mean_q: 0.799474\n",
            "wrong_move\n",
            "  1072/50000: episode: 1067, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1725.000 [1725.000, 1725.000],  loss: 12499108.000000, mae: 1.412165, mean_q: 0.846758\n",
            "wrong_move\n",
            "  1073/50000: episode: 1068, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12498412.000000, mae: 1.410380, mean_q: 0.859317\n",
            "wrong_move\n",
            "  1074/50000: episode: 1069, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2073.000 [2073.000, 2073.000],  loss: 12499124.000000, mae: 1.410817, mean_q: 0.876468\n",
            "wrong_move\n",
            "  1075/50000: episode: 1070, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2073.000 [2073.000, 2073.000],  loss: 12498326.000000, mae: 1.409773, mean_q: 0.829970\n",
            "wrong_move\n",
            "  1076/50000: episode: 1071, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1424.000 [1424.000, 1424.000],  loss: 12499546.000000, mae: 1.412909, mean_q: 0.868146\n",
            "wrong_move\n",
            "  1077/50000: episode: 1072, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2921.000 [2921.000, 2921.000],  loss: 12498424.000000, mae: 1.405261, mean_q: 0.864021\n",
            "wrong_move\n",
            "  1078/50000: episode: 1073, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1647.000 [1647.000, 1647.000],  loss: 12498458.000000, mae: 1.408798, mean_q: 0.824079\n",
            "wrong_move\n",
            "  1079/50000: episode: 1074, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12499025.000000, mae: 1.412711, mean_q: 0.847108\n",
            "wrong_move\n",
            "  1080/50000: episode: 1075, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3563.000 [3563.000, 3563.000],  loss: 12498663.000000, mae: 1.405370, mean_q: 0.868709\n",
            "wrong_move\n",
            "  1081/50000: episode: 1076, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 11718257.000000, mae: 1.333657, mean_q: 0.839345\n",
            "wrong_move\n",
            "  1082/50000: episode: 1077, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 496.000 [496.000, 496.000],  loss: 12498880.000000, mae: 1.406934, mean_q: 0.841389\n",
            "wrong_move\n",
            "  1083/50000: episode: 1078, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1471.000 [1471.000, 1471.000],  loss: 12498709.000000, mae: 1.413841, mean_q: 0.881510\n",
            "wrong_move\n",
            "  1084/50000: episode: 1079, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1707.000 [1707.000, 1707.000],  loss: 12499327.000000, mae: 1.412131, mean_q: 0.870221\n",
            "wrong_move\n",
            "  1085/50000: episode: 1080, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2314.000 [2314.000, 2314.000],  loss: 12498610.000000, mae: 1.405428, mean_q: 0.861270\n",
            "wrong_move\n",
            "  1086/50000: episode: 1081, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2073.000 [2073.000, 2073.000],  loss: 11717452.000000, mae: 1.334993, mean_q: 0.876821\n",
            "wrong_move\n",
            "  1087/50000: episode: 1082, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12498106.000000, mae: 1.403600, mean_q: 0.848099\n",
            "wrong_move\n",
            "  1088/50000: episode: 1083, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2073.000 [2073.000, 2073.000],  loss: 12498476.000000, mae: 1.411193, mean_q: 0.855394\n",
            "wrong_move\n",
            "  1089/50000: episode: 1084, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12498004.000000, mae: 1.408406, mean_q: 0.891669\n",
            "wrong_move\n",
            "  1090/50000: episode: 1085, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3428.000 [3428.000, 3428.000],  loss: 12498440.000000, mae: 1.407886, mean_q: 0.863298\n",
            "wrong_move\n",
            "  1091/50000: episode: 1086, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 224.000 [224.000, 224.000],  loss: 12498531.000000, mae: 1.408413, mean_q: 0.878358\n",
            "wrong_move\n",
            "  1092/50000: episode: 1087, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 11717517.000000, mae: 1.333489, mean_q: 0.869786\n",
            "wrong_move\n",
            "  1093/50000: episode: 1088, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12497782.000000, mae: 1.405492, mean_q: 0.880059\n",
            "wrong_move\n",
            "  1094/50000: episode: 1089, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1869.000 [1869.000, 1869.000],  loss: 12498190.000000, mae: 1.410625, mean_q: 0.897663\n",
            "wrong_move\n",
            "  1095/50000: episode: 1090, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 12108533.000000, mae: 1.374750, mean_q: 0.885847\n",
            "wrong_move\n",
            "  1096/50000: episode: 1091, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1521.000 [1521.000, 1521.000],  loss: 12498488.000000, mae: 1.409022, mean_q: 0.890547\n",
            "wrong_move\n",
            "  1097/50000: episode: 1092, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1521.000 [1521.000, 1521.000],  loss: 12498194.000000, mae: 1.405854, mean_q: 0.876922\n",
            "wrong_move\n",
            "  1098/50000: episode: 1093, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1761.000 [1761.000, 1761.000],  loss: 12107126.000000, mae: 1.369223, mean_q: 0.887003\n",
            "wrong_move\n",
            "  1099/50000: episode: 1094, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12498226.000000, mae: 1.406467, mean_q: 0.917414\n",
            "wrong_move\n",
            "  1100/50000: episode: 1095, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1521.000 [1521.000, 1521.000],  loss: 12107244.000000, mae: 1.366553, mean_q: 0.925470\n",
            "wrong_move\n",
            "  1101/50000: episode: 1096, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12498092.000000, mae: 1.407716, mean_q: 0.919042\n",
            "wrong_move\n",
            "  1102/50000: episode: 1097, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2558.000 [2558.000, 2558.000],  loss: 12498544.000000, mae: 1.412890, mean_q: 0.894492\n",
            "wrong_move\n",
            "  1103/50000: episode: 1098, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 232.000 [232.000, 232.000],  loss: 12498389.000000, mae: 1.411175, mean_q: 0.929791\n",
            "wrong_move\n",
            "  1104/50000: episode: 1099, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2278.000 [2278.000, 2278.000],  loss: 12107984.000000, mae: 1.369365, mean_q: 0.917210\n",
            "wrong_move\n",
            "  1105/50000: episode: 1100, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1521.000 [1521.000, 1521.000],  loss: 12497768.000000, mae: 1.411839, mean_q: 0.911409\n",
            "wrong_move\n",
            "  1106/50000: episode: 1101, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1521.000 [1521.000, 1521.000],  loss: 12107828.000000, mae: 1.375933, mean_q: 0.922853\n",
            "wrong_move\n",
            "  1107/50000: episode: 1102, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2312.000 [2312.000, 2312.000],  loss: 12498086.000000, mae: 1.409954, mean_q: 0.904285\n",
            "wrong_move\n",
            "  1108/50000: episode: 1103, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12107400.000000, mae: 1.369722, mean_q: 0.930616\n",
            "wrong_move\n",
            "  1109/50000: episode: 1104, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 931.000 [931.000, 931.000],  loss: 12497721.000000, mae: 1.409842, mean_q: 0.946033\n",
            "wrong_move\n",
            "  1110/50000: episode: 1105, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 12497863.000000, mae: 1.410755, mean_q: 0.937510\n",
            "wrong_move\n",
            "  1111/50000: episode: 1106, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 12497427.000000, mae: 1.411676, mean_q: 0.942178\n",
            "wrong_move\n",
            "  1112/50000: episode: 1107, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1848.000 [1848.000, 1848.000],  loss: 12498617.000000, mae: 1.412103, mean_q: 0.941393\n",
            "wrong_move\n",
            "  1113/50000: episode: 1108, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: 12497580.000000, mae: 1.415284, mean_q: 0.903081\n",
            "wrong_move\n",
            "  1114/50000: episode: 1109, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12498028.000000, mae: 1.414622, mean_q: 0.930799\n",
            "wrong_move\n",
            "  1115/50000: episode: 1110, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1434.000 [1434.000, 1434.000],  loss: 12498888.000000, mae: 1.412935, mean_q: 0.925602\n",
            "wrong_move\n",
            "  1116/50000: episode: 1111, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1418.000 [1418.000, 1418.000],  loss: 12497094.000000, mae: 1.409577, mean_q: 0.988249\n",
            "wrong_move\n",
            "  1117/50000: episode: 1112, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12497698.000000, mae: 1.407159, mean_q: 0.971982\n",
            "wrong_move\n",
            "  1118/50000: episode: 1113, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 929.000 [929.000, 929.000],  loss: 12497528.000000, mae: 1.408375, mean_q: 1.007295\n",
            "wrong_move\n",
            "  1119/50000: episode: 1114, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2080.000 [2080.000, 2080.000],  loss: 12497554.000000, mae: 1.409078, mean_q: 0.957551\n",
            "wrong_move\n",
            "  1120/50000: episode: 1115, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1594.000 [1594.000, 1594.000],  loss: 12497168.000000, mae: 1.411833, mean_q: 0.971476\n",
            "wrong_move\n",
            "  1121/50000: episode: 1116, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1829.000 [1829.000, 1829.000],  loss: 12498358.000000, mae: 1.411286, mean_q: 0.955114\n",
            "wrong_move\n",
            "  1122/50000: episode: 1117, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 491.000 [491.000, 491.000],  loss: 12497173.000000, mae: 1.413047, mean_q: 1.019606\n",
            "wrong_move\n",
            "  1123/50000: episode: 1118, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3350.000 [3350.000, 3350.000],  loss: 12497776.000000, mae: 1.410618, mean_q: 0.977464\n",
            "wrong_move\n",
            "  1124/50000: episode: 1119, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3560.000 [3560.000, 3560.000],  loss: 12498409.000000, mae: 1.413499, mean_q: 0.922133\n",
            "wrong_move\n",
            "  1125/50000: episode: 1120, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12498444.000000, mae: 1.413146, mean_q: 0.929761\n",
            "wrong_move\n",
            "  1126/50000: episode: 1121, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3438.000 [3438.000, 3438.000],  loss: 12498538.000000, mae: 1.405176, mean_q: 0.996751\n",
            "wrong_move\n",
            "  1127/50000: episode: 1122, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3438.000 [3438.000, 3438.000],  loss: 12497676.000000, mae: 1.413096, mean_q: 0.989355\n",
            "wrong_move\n",
            "  1128/50000: episode: 1123, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1651.000 [1651.000, 1651.000],  loss: 12497574.000000, mae: 1.408917, mean_q: 1.102331\n",
            "wrong_move\n",
            "  1129/50000: episode: 1124, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2693.000 [2693.000, 2693.000],  loss: 12106898.000000, mae: 1.368794, mean_q: 1.023812\n",
            "wrong_move\n",
            "  1130/50000: episode: 1125, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 455.000 [455.000, 455.000],  loss: 12498010.000000, mae: 1.415820, mean_q: 0.977781\n",
            "wrong_move\n",
            "  1131/50000: episode: 1126, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1511.000 [1511.000, 1511.000],  loss: 12497945.000000, mae: 1.411185, mean_q: 1.042592\n",
            "wrong_move\n",
            "  1132/50000: episode: 1127, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12496828.000000, mae: 1.410275, mean_q: 1.005286\n",
            "wrong_move\n",
            "  1133/50000: episode: 1128, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3438.000 [3438.000, 3438.000],  loss: 12497910.000000, mae: 1.415136, mean_q: 0.993251\n",
            "wrong_move\n",
            "  1134/50000: episode: 1129, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2412.000 [2412.000, 2412.000],  loss: 12497096.000000, mae: 1.411958, mean_q: 1.103260\n",
            "wrong_move\n",
            "  1135/50000: episode: 1130, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12497251.000000, mae: 1.405896, mean_q: 1.025370\n",
            "wrong_move\n",
            "  1136/50000: episode: 1131, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12497400.000000, mae: 1.413393, mean_q: 1.014571\n",
            "wrong_move\n",
            "  1137/50000: episode: 1132, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2495.000 [2495.000, 2495.000],  loss: 12498523.000000, mae: 1.414155, mean_q: 1.025933\n",
            "wrong_move\n",
            "  1138/50000: episode: 1133, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12498495.000000, mae: 1.414764, mean_q: 1.032780\n",
            "wrong_move\n",
            "  1139/50000: episode: 1134, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1829.000 [1829.000, 1829.000],  loss: 12497492.000000, mae: 1.414910, mean_q: 1.002245\n",
            "wrong_move\n",
            "  1140/50000: episode: 1135, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2719.000 [2719.000, 2719.000],  loss: 12495970.000000, mae: 1.410360, mean_q: 1.097325\n",
            "wrong_move\n",
            "  1141/50000: episode: 1136, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12497616.000000, mae: 1.413364, mean_q: 1.030002\n",
            "wrong_move\n",
            "  1142/50000: episode: 1137, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2054.000 [2054.000, 2054.000],  loss: 12497250.000000, mae: 1.416063, mean_q: 0.947881\n",
            "wrong_move\n",
            "  1143/50000: episode: 1138, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: 12498333.000000, mae: 1.414866, mean_q: 1.008398\n",
            "wrong_move\n",
            "  1144/50000: episode: 1139, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: 12497181.000000, mae: 1.416066, mean_q: 1.050740\n",
            "wrong_move\n",
            "  1145/50000: episode: 1140, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12497594.000000, mae: 1.416736, mean_q: 1.035590\n",
            "wrong_move\n",
            "  1146/50000: episode: 1141, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12497202.000000, mae: 1.410598, mean_q: 1.153263\n",
            "wrong_move\n",
            "  1147/50000: episode: 1142, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12495926.000000, mae: 1.411888, mean_q: 1.114647\n",
            "wrong_move\n",
            "  1148/50000: episode: 1143, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: 12497720.000000, mae: 1.415817, mean_q: 1.008690\n",
            "wrong_move\n",
            "  1149/50000: episode: 1144, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12497572.000000, mae: 1.417932, mean_q: 0.909640\n",
            "wrong_move\n",
            "  1150/50000: episode: 1145, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12497398.000000, mae: 1.413902, mean_q: 1.069434\n",
            "wrong_move\n",
            "  1151/50000: episode: 1146, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: 12498432.000000, mae: 1.412657, mean_q: 1.132162\n",
            "wrong_move\n",
            "  1152/50000: episode: 1147, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12107672.000000, mae: 1.371350, mean_q: 1.234738\n",
            "wrong_move\n",
            "  1153/50000: episode: 1148, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 327.000 [327.000, 327.000],  loss: 12497700.000000, mae: 1.411721, mean_q: 1.122710\n",
            "wrong_move\n",
            "  1154/50000: episode: 1149, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1081.000 [1081.000, 1081.000],  loss: 12496798.000000, mae: 1.414549, mean_q: 1.070223\n",
            "wrong_move\n",
            "  1155/50000: episode: 1150, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12496238.000000, mae: 1.411672, mean_q: 1.109787\n",
            "wrong_move\n",
            "  1156/50000: episode: 1151, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: 12105576.000000, mae: 1.371692, mean_q: 1.154239\n",
            "wrong_move\n",
            "  1157/50000: episode: 1152, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 12495962.000000, mae: 1.415959, mean_q: 1.096971\n",
            "wrong_move\n",
            "  1158/50000: episode: 1153, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 12497100.000000, mae: 1.415371, mean_q: 1.095851\n",
            "wrong_move\n",
            "  1159/50000: episode: 1154, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12496875.000000, mae: 1.412003, mean_q: 1.165872\n",
            "wrong_move\n",
            "  1160/50000: episode: 1155, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12496898.000000, mae: 1.413003, mean_q: 1.169798\n",
            "wrong_move\n",
            "  1161/50000: episode: 1156, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: 12498308.000000, mae: 1.411849, mean_q: 1.167298\n",
            "wrong_move\n",
            "  1162/50000: episode: 1157, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3211.000 [3211.000, 3211.000],  loss: 12497672.000000, mae: 1.416970, mean_q: 1.079253\n",
            "wrong_move\n",
            "  1163/50000: episode: 1158, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12496097.000000, mae: 1.416044, mean_q: 1.181160\n",
            "wrong_move\n",
            "  1164/50000: episode: 1159, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3530.000 [3530.000, 3530.000],  loss: 12107954.000000, mae: 1.381279, mean_q: 1.127548\n",
            "wrong_move\n",
            "  1165/50000: episode: 1160, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12495914.000000, mae: 1.418560, mean_q: 1.079917\n",
            "wrong_move\n",
            "  1166/50000: episode: 1161, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1175.000 [1175.000, 1175.000],  loss: 12496173.000000, mae: 1.408301, mean_q: 1.272529\n",
            "wrong_move\n",
            "  1167/50000: episode: 1162, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12498478.000000, mae: 1.417473, mean_q: 1.121023\n",
            "wrong_move\n",
            "  1168/50000: episode: 1163, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 626.000 [626.000, 626.000],  loss: 12496691.000000, mae: 1.416368, mean_q: 1.235225\n",
            "wrong_move\n",
            "  1169/50000: episode: 1164, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12105306.000000, mae: 1.378094, mean_q: 1.184531\n",
            "wrong_move\n",
            "  1170/50000: episode: 1165, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12496056.000000, mae: 1.418686, mean_q: 1.096331\n",
            "wrong_move\n",
            "  1171/50000: episode: 1166, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 823.000 [823.000, 823.000],  loss: 12496544.000000, mae: 1.414589, mean_q: 1.277449\n",
            "wrong_move\n",
            "  1172/50000: episode: 1167, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 130.000 [130.000, 130.000],  loss: 12496416.000000, mae: 1.416003, mean_q: 1.218305\n",
            "wrong_move\n",
            "  1173/50000: episode: 1168, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12106356.000000, mae: 1.377449, mean_q: 1.150811\n",
            "wrong_move\n",
            "  1174/50000: episode: 1169, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12497377.000000, mae: 1.416686, mean_q: 1.175606\n",
            "wrong_move\n",
            "  1175/50000: episode: 1170, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12495740.000000, mae: 1.419475, mean_q: 1.150227\n",
            "wrong_move\n",
            "  1176/50000: episode: 1171, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12498164.000000, mae: 1.415503, mean_q: 1.260351\n",
            "wrong_move\n",
            "  1177/50000: episode: 1172, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12495938.000000, mae: 1.414975, mean_q: 1.389272\n",
            "wrong_move\n",
            "  1178/50000: episode: 1173, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 454.000 [454.000, 454.000],  loss: 12495466.000000, mae: 1.413494, mean_q: 1.363409\n",
            "wrong_move\n",
            "  1179/50000: episode: 1174, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1608.000 [1608.000, 1608.000],  loss: 12496346.000000, mae: 1.409801, mean_q: 1.296576\n",
            "wrong_move\n",
            "  1180/50000: episode: 1175, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12496413.000000, mae: 1.417366, mean_q: 1.289018\n",
            "wrong_move\n",
            "  1181/50000: episode: 1176, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1246.000 [1246.000, 1246.000],  loss: 12497260.000000, mae: 1.415733, mean_q: 1.322149\n",
            "wrong_move\n",
            "  1182/50000: episode: 1177, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12495247.000000, mae: 1.417818, mean_q: 1.217157\n",
            "wrong_move\n",
            "  1183/50000: episode: 1178, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2754.000 [2754.000, 2754.000],  loss: 12496865.000000, mae: 1.417039, mean_q: 1.170740\n",
            "wrong_move\n",
            "  1184/50000: episode: 1179, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12496148.000000, mae: 1.419817, mean_q: 1.142422\n",
            "wrong_move\n",
            "  1185/50000: episode: 1180, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12495493.000000, mae: 1.417679, mean_q: 1.246510\n",
            "wrong_move\n",
            "  1186/50000: episode: 1181, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12494956.000000, mae: 1.419414, mean_q: 1.273326\n",
            "wrong_move\n",
            "  1187/50000: episode: 1182, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12494302.000000, mae: 1.416011, mean_q: 1.339288\n",
            "wrong_move\n",
            "  1188/50000: episode: 1183, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 12104281.000000, mae: 1.378506, mean_q: 1.361335\n",
            "wrong_move\n",
            "  1189/50000: episode: 1184, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2412.000 [2412.000, 2412.000],  loss: 12498069.000000, mae: 1.414937, mean_q: 1.355162\n",
            "wrong_move\n",
            "  1190/50000: episode: 1185, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494620.000000, mae: 1.420893, mean_q: 1.232430\n",
            "wrong_move\n",
            "  1191/50000: episode: 1186, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1148.000 [1148.000, 1148.000],  loss: 12496232.000000, mae: 1.417041, mean_q: 1.298531\n",
            "wrong_move\n",
            "  1192/50000: episode: 1187, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12496086.000000, mae: 1.420253, mean_q: 1.205404\n",
            "wrong_move\n",
            "  1193/50000: episode: 1188, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12496986.000000, mae: 1.409675, mean_q: 1.483859\n",
            "wrong_move\n",
            "  1194/50000: episode: 1189, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12495911.000000, mae: 1.416329, mean_q: 1.354776\n",
            "wrong_move\n",
            "  1195/50000: episode: 1190, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12496340.000000, mae: 1.420079, mean_q: 1.366640\n",
            "wrong_move\n",
            "  1196/50000: episode: 1191, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1675.000 [1675.000, 1675.000],  loss: 12104070.000000, mae: 1.375611, mean_q: 1.417836\n",
            "wrong_move\n",
            "  1197/50000: episode: 1192, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12105970.000000, mae: 1.381074, mean_q: 1.338049\n",
            "wrong_move\n",
            "  1198/50000: episode: 1193, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494734.000000, mae: 1.413905, mean_q: 1.511400\n",
            "wrong_move\n",
            "  1199/50000: episode: 1194, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12498321.000000, mae: 1.408981, mean_q: 1.476506\n",
            "wrong_move\n",
            "  1200/50000: episode: 1195, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12495389.000000, mae: 1.419275, mean_q: 1.400121\n",
            "wrong_move\n",
            "  1201/50000: episode: 1196, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3370.000 [3370.000, 3370.000],  loss: 12496235.000000, mae: 1.414347, mean_q: 1.499617\n",
            "wrong_move\n",
            "  1202/50000: episode: 1197, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12496174.000000, mae: 1.418266, mean_q: 1.405179\n",
            "wrong_move\n",
            "  1203/50000: episode: 1198, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1657.000 [1657.000, 1657.000],  loss: 12496657.000000, mae: 1.420295, mean_q: 1.390897\n",
            "wrong_move\n",
            "  1204/50000: episode: 1199, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 931.000 [931.000, 931.000],  loss: 12105492.000000, mae: 1.382190, mean_q: 1.308049\n",
            "wrong_move\n",
            "  1205/50000: episode: 1200, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2297.000 [2297.000, 2297.000],  loss: 12104762.000000, mae: 1.378813, mean_q: 1.528741\n",
            "wrong_move\n",
            "  1206/50000: episode: 1201, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2207.000 [2207.000, 2207.000],  loss: 12495757.000000, mae: 1.416531, mean_q: 1.578663\n",
            "wrong_move\n",
            "  1207/50000: episode: 1202, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494487.000000, mae: 1.418386, mean_q: 1.499321\n",
            "wrong_move\n",
            "  1208/50000: episode: 1203, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12495500.000000, mae: 1.422220, mean_q: 1.325943\n",
            "wrong_move\n",
            "  1209/50000: episode: 1204, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12494520.000000, mae: 1.422284, mean_q: 1.378463\n",
            "wrong_move\n",
            "  1210/50000: episode: 1205, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12495435.000000, mae: 1.417688, mean_q: 1.418654\n",
            "wrong_move\n",
            "  1211/50000: episode: 1206, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3783.000 [3783.000, 3783.000],  loss: 12103111.000000, mae: 1.378441, mean_q: 1.499121\n",
            "wrong_move\n",
            "  1212/50000: episode: 1207, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12494927.000000, mae: 1.422580, mean_q: 1.380239\n",
            "wrong_move\n",
            "  1213/50000: episode: 1208, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12495234.000000, mae: 1.422031, mean_q: 1.401824\n",
            "wrong_move\n",
            "  1214/50000: episode: 1209, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: 12495566.000000, mae: 1.419802, mean_q: 1.477190\n",
            "wrong_move\n",
            "  1215/50000: episode: 1210, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 590.000 [590.000, 590.000],  loss: 12105526.000000, mae: 1.377443, mean_q: 1.533951\n",
            "wrong_move\n",
            "  1216/50000: episode: 1211, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2447.000 [2447.000, 2447.000],  loss: 12496438.000000, mae: 1.424240, mean_q: 1.359558\n",
            "wrong_move\n",
            "  1217/50000: episode: 1212, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2906.000 [2906.000, 2906.000],  loss: 12495093.000000, mae: 1.422405, mean_q: 1.385121\n",
            "wrong_move\n",
            "  1218/50000: episode: 1213, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12104934.000000, mae: 1.384983, mean_q: 1.512721\n",
            "wrong_move\n",
            "  1219/50000: episode: 1214, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12496110.000000, mae: 1.418715, mean_q: 1.562699\n",
            "wrong_move\n",
            "  1220/50000: episode: 1215, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2896.000 [2896.000, 2896.000],  loss: 12496056.000000, mae: 1.419816, mean_q: 1.624142\n",
            "wrong_move\n",
            "  1221/50000: episode: 1216, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12496066.000000, mae: 1.419941, mean_q: 1.595831\n",
            "wrong_move\n",
            "  1222/50000: episode: 1217, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494617.000000, mae: 1.423968, mean_q: 1.323964\n",
            "wrong_move\n",
            "  1223/50000: episode: 1218, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12103254.000000, mae: 1.381052, mean_q: 1.626517\n",
            "wrong_move\n",
            "  1224/50000: episode: 1219, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12493398.000000, mae: 1.423104, mean_q: 1.577551\n",
            "wrong_move\n",
            "  1225/50000: episode: 1220, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494735.000000, mae: 1.419464, mean_q: 1.543739\n",
            "wrong_move\n",
            "  1226/50000: episode: 1221, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12495510.000000, mae: 1.421021, mean_q: 1.515175\n",
            "wrong_move\n",
            "  1227/50000: episode: 1222, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 734.000 [734.000, 734.000],  loss: 12495842.000000, mae: 1.420688, mean_q: 1.633271\n",
            "wrong_move\n",
            "  1228/50000: episode: 1223, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12495173.000000, mae: 1.418293, mean_q: 1.663217\n",
            "wrong_move\n",
            "  1229/50000: episode: 1224, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12493129.000000, mae: 1.418656, mean_q: 1.729479\n",
            "wrong_move\n",
            "  1230/50000: episode: 1225, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 734.000 [734.000, 734.000],  loss: 12495969.000000, mae: 1.421962, mean_q: 1.655746\n",
            "wrong_move\n",
            "  1231/50000: episode: 1226, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12494582.000000, mae: 1.422343, mean_q: 1.620367\n",
            "wrong_move\n",
            "  1232/50000: episode: 1227, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 945.000 [945.000, 945.000],  loss: 12496421.000000, mae: 1.424059, mean_q: 1.551483\n",
            "wrong_move\n",
            "  1233/50000: episode: 1228, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12494706.000000, mae: 1.424163, mean_q: 1.521794\n",
            "wrong_move\n",
            "  1234/50000: episode: 1229, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2613.000 [2613.000, 2613.000],  loss: 12103472.000000, mae: 1.386567, mean_q: 1.571897\n",
            "wrong_move\n",
            "  1235/50000: episode: 1230, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 734.000 [734.000, 734.000],  loss: 12493008.000000, mae: 1.421658, mean_q: 1.584226\n",
            "wrong_move\n",
            "  1236/50000: episode: 1231, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12102295.000000, mae: 1.384351, mean_q: 1.700683\n",
            "wrong_move\n",
            "  1237/50000: episode: 1232, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12496453.000000, mae: 1.424804, mean_q: 1.522837\n",
            "wrong_move\n",
            "  1238/50000: episode: 1233, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12495033.000000, mae: 1.424071, mean_q: 1.719682\n",
            "wrong_move\n",
            "  1239/50000: episode: 1234, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2231.000 [2231.000, 2231.000],  loss: 12495110.000000, mae: 1.417982, mean_q: 1.766958\n",
            "wrong_move\n",
            "  1240/50000: episode: 1235, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12493871.000000, mae: 1.422229, mean_q: 1.735589\n",
            "wrong_move\n",
            "  1241/50000: episode: 1236, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2629.000 [2629.000, 2629.000],  loss: 12494107.000000, mae: 1.427115, mean_q: 1.420516\n",
            "wrong_move\n",
            "  1242/50000: episode: 1237, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3457.000 [3457.000, 3457.000],  loss: 12495859.000000, mae: 1.421043, mean_q: 1.692543\n",
            "wrong_move\n",
            "  1243/50000: episode: 1238, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3221.000 [3221.000, 3221.000],  loss: 12495457.000000, mae: 1.426503, mean_q: 1.534951\n",
            "wrong_move\n",
            "  1244/50000: episode: 1239, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: 12493926.000000, mae: 1.420400, mean_q: 1.761107\n",
            "wrong_move\n",
            "  1245/50000: episode: 1240, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12496212.000000, mae: 1.424231, mean_q: 1.707240\n",
            "wrong_move\n",
            "  1246/50000: episode: 1241, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12493766.000000, mae: 1.424721, mean_q: 1.611152\n",
            "wrong_move\n",
            "  1247/50000: episode: 1242, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: 12494247.000000, mae: 1.423241, mean_q: 1.782159\n",
            "wrong_move\n",
            "  1248/50000: episode: 1243, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: 12495434.000000, mae: 1.427834, mean_q: 1.715541\n",
            "wrong_move\n",
            "  1249/50000: episode: 1244, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12493850.000000, mae: 1.426892, mean_q: 1.468553\n",
            "wrong_move\n",
            "  1250/50000: episode: 1245, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12495619.000000, mae: 1.427439, mean_q: 1.693107\n",
            "wrong_move\n",
            "  1251/50000: episode: 1246, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494169.000000, mae: 1.427386, mean_q: 1.656434\n",
            "wrong_move\n",
            "  1252/50000: episode: 1247, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1762.000 [1762.000, 1762.000],  loss: 12492684.000000, mae: 1.424004, mean_q: 1.662027\n",
            "wrong_move\n",
            "  1253/50000: episode: 1248, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: 12494766.000000, mae: 1.425174, mean_q: 1.800600\n",
            "wrong_move\n",
            "  1254/50000: episode: 1249, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494497.000000, mae: 1.429088, mean_q: 1.721855\n",
            "wrong_move\n",
            "  1255/50000: episode: 1250, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494616.000000, mae: 1.428960, mean_q: 1.609020\n",
            "wrong_move\n",
            "  1256/50000: episode: 1251, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: 12492301.000000, mae: 1.427160, mean_q: 1.668836\n",
            "wrong_move\n",
            "  1257/50000: episode: 1252, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12491309.000000, mae: 1.426742, mean_q: 1.679756\n",
            "wrong_move\n",
            "  1258/50000: episode: 1253, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12495880.000000, mae: 1.428550, mean_q: 1.491762\n",
            "wrong_move\n",
            "  1259/50000: episode: 1254, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: 12496100.000000, mae: 1.425870, mean_q: 1.736771\n",
            "wrong_move\n",
            "  1260/50000: episode: 1255, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494220.000000, mae: 1.424519, mean_q: 1.890470\n",
            "wrong_move\n",
            "  1261/50000: episode: 1256, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12493951.000000, mae: 1.423172, mean_q: 1.835176\n",
            "wrong_move\n",
            "  1262/50000: episode: 1257, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494152.000000, mae: 1.428158, mean_q: 1.765216\n",
            "wrong_move\n",
            "  1263/50000: episode: 1258, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 737.000 [737.000, 737.000],  loss: 12493046.000000, mae: 1.424823, mean_q: 1.770367\n",
            "wrong_move\n",
            "  1264/50000: episode: 1259, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 24.000 [24.000, 24.000],  loss: 12102837.000000, mae: 1.386224, mean_q: 1.750906\n",
            "wrong_move\n",
            "  1265/50000: episode: 1260, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12102140.000000, mae: 1.388377, mean_q: 1.929443\n",
            "wrong_move\n",
            "  1266/50000: episode: 1261, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12493258.000000, mae: 1.428228, mean_q: 1.849755\n",
            "wrong_move\n",
            "  1267/50000: episode: 1262, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12494072.000000, mae: 1.429168, mean_q: 1.906977\n",
            "wrong_move\n",
            "  1268/50000: episode: 1263, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12494774.000000, mae: 1.429388, mean_q: 1.818836\n",
            "wrong_move\n",
            "  1269/50000: episode: 1264, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3394.000 [3394.000, 3394.000],  loss: 12493559.000000, mae: 1.422957, mean_q: 2.021109\n",
            "wrong_move\n",
            "  1270/50000: episode: 1265, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: 12492276.000000, mae: 1.426343, mean_q: 1.816803\n",
            "wrong_move\n",
            "  1271/50000: episode: 1266, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3304.000 [3304.000, 3304.000],  loss: 12494064.000000, mae: 1.430198, mean_q: 1.827161\n",
            "wrong_move\n",
            "  1272/50000: episode: 1267, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 941.000 [941.000, 941.000],  loss: 12492275.000000, mae: 1.426876, mean_q: 1.933295\n",
            "wrong_move\n",
            "  1273/50000: episode: 1268, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 673.000 [673.000, 673.000],  loss: 12103743.000000, mae: 1.391392, mean_q: 1.955854\n",
            "wrong_move\n",
            "  1274/50000: episode: 1269, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12102286.000000, mae: 1.391761, mean_q: 1.825275\n",
            "wrong_move\n",
            "  1275/50000: episode: 1270, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494610.000000, mae: 1.430658, mean_q: 1.875469\n",
            "wrong_move\n",
            "  1276/50000: episode: 1271, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12493346.000000, mae: 1.427386, mean_q: 1.985309\n",
            "wrong_move\n",
            "  1277/50000: episode: 1272, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12492912.000000, mae: 1.429986, mean_q: 1.660421\n",
            "wrong_move\n",
            "  1278/50000: episode: 1273, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12494265.000000, mae: 1.429369, mean_q: 1.922145\n",
            "wrong_move\n",
            "  1279/50000: episode: 1274, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3848.000 [3848.000, 3848.000],  loss: 12496396.000000, mae: 1.431888, mean_q: 2.001754\n",
            "wrong_move\n",
            "  1280/50000: episode: 1275, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12491807.000000, mae: 1.432687, mean_q: 1.821532\n",
            "wrong_move\n",
            "  1281/50000: episode: 1276, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12104159.000000, mae: 1.393949, mean_q: 1.998613\n",
            "wrong_move\n",
            "  1282/50000: episode: 1277, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1139.000 [1139.000, 1139.000],  loss: 12103292.000000, mae: 1.395090, mean_q: 1.900434\n",
            "wrong_move\n",
            "  1283/50000: episode: 1278, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3799.000 [3799.000, 3799.000],  loss: 12490160.000000, mae: 1.431669, mean_q: 2.079329\n",
            "wrong_move\n",
            "  1284/50000: episode: 1279, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12490879.000000, mae: 1.431599, mean_q: 2.081400\n",
            "wrong_move\n",
            "  1285/50000: episode: 1280, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12104807.000000, mae: 1.394379, mean_q: 1.845625\n",
            "wrong_move\n",
            "  1286/50000: episode: 1281, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1579.000 [1579.000, 1579.000],  loss: 12101762.000000, mae: 1.394103, mean_q: 1.970238\n",
            "wrong_move\n",
            "  1287/50000: episode: 1282, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12492220.000000, mae: 1.430980, mean_q: 2.024735\n",
            "wrong_move\n",
            "  1288/50000: episode: 1283, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12104440.000000, mae: 1.395386, mean_q: 1.876003\n",
            "wrong_move\n",
            "  1289/50000: episode: 1284, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12493311.000000, mae: 1.433702, mean_q: 1.738497\n",
            "wrong_move\n",
            "  1290/50000: episode: 1285, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12493060.000000, mae: 1.433859, mean_q: 1.928827\n",
            "wrong_move\n",
            "  1291/50000: episode: 1286, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 726.000 [726.000, 726.000],  loss: 12491414.000000, mae: 1.434230, mean_q: 1.697446\n",
            "wrong_move\n",
            "  1292/50000: episode: 1287, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12495660.000000, mae: 1.434640, mean_q: 1.894413\n",
            "wrong_move\n",
            "  1293/50000: episode: 1288, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12491568.000000, mae: 1.431847, mean_q: 1.949168\n",
            "wrong_move\n",
            "  1294/50000: episode: 1289, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12492343.000000, mae: 1.430491, mean_q: 2.090229\n",
            "wrong_move\n",
            "  1295/50000: episode: 1290, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12492865.000000, mae: 1.434033, mean_q: 2.054277\n",
            "wrong_move\n",
            "  1296/50000: episode: 1291, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2461.000 [2461.000, 2461.000],  loss: 12490185.000000, mae: 1.434233, mean_q: 1.979565\n",
            "wrong_move\n",
            "  1297/50000: episode: 1292, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12491849.000000, mae: 1.434814, mean_q: 1.815516\n",
            "wrong_move\n",
            "  1298/50000: episode: 1293, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 11710728.000000, mae: 1.355048, mean_q: 2.083626\n",
            "wrong_move\n",
            "  1299/50000: episode: 1294, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12492748.000000, mae: 1.433664, mean_q: 2.203481\n",
            "wrong_move\n",
            "  1300/50000: episode: 1295, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3135.000 [3135.000, 3135.000],  loss: 12491852.000000, mae: 1.434828, mean_q: 2.006621\n",
            "wrong_move\n",
            "  1301/50000: episode: 1296, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12493241.000000, mae: 1.435549, mean_q: 2.005842\n",
            "wrong_move\n",
            "  1302/50000: episode: 1297, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1068.000 [1068.000, 1068.000],  loss: 12492686.000000, mae: 1.434677, mean_q: 2.258818\n",
            "wrong_move\n",
            "  1303/50000: episode: 1298, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494788.000000, mae: 1.436216, mean_q: 1.847243\n",
            "wrong_move\n",
            "  1304/50000: episode: 1299, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12102846.000000, mae: 1.390295, mean_q: 2.247081\n",
            "wrong_move\n",
            "  1305/50000: episode: 1300, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12492945.000000, mae: 1.430245, mean_q: 2.193205\n",
            "wrong_move\n",
            "  1306/50000: episode: 1301, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12492172.000000, mae: 1.436593, mean_q: 2.155209\n",
            "wrong_move\n",
            "  1307/50000: episode: 1302, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 869.000 [869.000, 869.000],  loss: 12494518.000000, mae: 1.434481, mean_q: 2.207981\n",
            "wrong_move\n",
            "  1308/50000: episode: 1303, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1576.000 [1576.000, 1576.000],  loss: 12492720.000000, mae: 1.435205, mean_q: 2.208794\n",
            "wrong_move\n",
            "  1309/50000: episode: 1304, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1770.000 [1770.000, 1770.000],  loss: 12492266.000000, mae: 1.436391, mean_q: 2.103689\n",
            "wrong_move\n",
            "  1310/50000: episode: 1305, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12492652.000000, mae: 1.429539, mean_q: 2.288481\n",
            "wrong_move\n",
            "  1311/50000: episode: 1306, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494106.000000, mae: 1.436818, mean_q: 2.010221\n",
            "wrong_move\n",
            "  1312/50000: episode: 1307, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12104782.000000, mae: 1.395726, mean_q: 2.286656\n",
            "wrong_move\n",
            "  1313/50000: episode: 1308, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12494028.000000, mae: 1.436303, mean_q: 2.160960\n",
            "wrong_move\n",
            "  1314/50000: episode: 1309, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12490907.000000, mae: 1.437366, mean_q: 2.064032\n",
            "wrong_move\n",
            "  1315/50000: episode: 1310, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12495076.000000, mae: 1.435163, mean_q: 2.397959\n",
            "wrong_move\n",
            "  1316/50000: episode: 1311, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 391.000 [391.000, 391.000],  loss: 12493177.000000, mae: 1.435943, mean_q: 2.359842\n",
            "wrong_move\n",
            "  1317/50000: episode: 1312, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12495227.000000, mae: 1.437685, mean_q: 1.755754\n",
            "wrong_move\n",
            "  1318/50000: episode: 1313, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12495258.000000, mae: 1.435819, mean_q: 2.169325\n",
            "wrong_move\n",
            "  1319/50000: episode: 1314, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1646.000 [1646.000, 1646.000],  loss: 12491282.000000, mae: 1.440130, mean_q: 2.150652\n",
            "wrong_move\n",
            "  1320/50000: episode: 1315, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494896.000000, mae: 1.437985, mean_q: 2.324299\n",
            "wrong_move\n",
            "  1321/50000: episode: 1316, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12492282.000000, mae: 1.439006, mean_q: 2.094731\n",
            "wrong_move\n",
            "  1322/50000: episode: 1317, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12493710.000000, mae: 1.438278, mean_q: 1.783897\n",
            "wrong_move\n",
            "  1323/50000: episode: 1318, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1745.000 [1745.000, 1745.000],  loss: 12493490.000000, mae: 1.439292, mean_q: 2.366911\n",
            "wrong_move\n",
            "  1324/50000: episode: 1319, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12491910.000000, mae: 1.439413, mean_q: 2.278551\n",
            "wrong_move\n",
            "  1325/50000: episode: 1320, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12492852.000000, mae: 1.438931, mean_q: 2.277011\n",
            "wrong_move\n",
            "  1326/50000: episode: 1321, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3710.000 [3710.000, 3710.000],  loss: 12495369.000000, mae: 1.441227, mean_q: 2.114411\n",
            "wrong_move\n",
            "  1327/50000: episode: 1322, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12492960.000000, mae: 1.440275, mean_q: 2.099317\n",
            "wrong_move\n",
            "  1328/50000: episode: 1323, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12495243.000000, mae: 1.441071, mean_q: 2.154682\n",
            "wrong_move\n",
            "  1329/50000: episode: 1324, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12496118.000000, mae: 1.442860, mean_q: 2.336980\n",
            "wrong_move\n",
            "  1330/50000: episode: 1325, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12491402.000000, mae: 1.433989, mean_q: 2.474264\n",
            "wrong_move\n",
            "  1331/50000: episode: 1326, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12490217.000000, mae: 1.440037, mean_q: 2.499771\n",
            "wrong_move\n",
            "  1332/50000: episode: 1327, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12493273.000000, mae: 1.439447, mean_q: 2.374242\n",
            "wrong_move\n",
            "  1333/50000: episode: 1328, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12101653.000000, mae: 1.403567, mean_q: 2.072150\n",
            "wrong_move\n",
            "  1334/50000: episode: 1329, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12491674.000000, mae: 1.440573, mean_q: 2.400465\n",
            "wrong_move\n",
            "  1335/50000: episode: 1330, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12492898.000000, mae: 1.441982, mean_q: 2.425724\n",
            "wrong_move\n",
            "  1336/50000: episode: 1331, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 851.000 [851.000, 851.000],  loss: 12493120.000000, mae: 1.442410, mean_q: 2.200346\n",
            "wrong_move\n",
            "  1337/50000: episode: 1332, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1816.000 [1816.000, 1816.000],  loss: 12489736.000000, mae: 1.441931, mean_q: 2.337693\n",
            "wrong_move\n",
            "  1338/50000: episode: 1333, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1536.000 [1536.000, 1536.000],  loss: 12494732.000000, mae: 1.435766, mean_q: 2.455426\n",
            "wrong_move\n",
            "  1339/50000: episode: 1334, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12101754.000000, mae: 1.404909, mean_q: 2.231289\n",
            "wrong_move\n",
            "  1340/50000: episode: 1335, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12491042.000000, mae: 1.440463, mean_q: 2.346839\n",
            "wrong_move\n",
            "  1341/50000: episode: 1336, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12491820.000000, mae: 1.440495, mean_q: 2.495893\n",
            "wrong_move\n",
            "  1342/50000: episode: 1337, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12492538.000000, mae: 1.441830, mean_q: 2.347852\n",
            "wrong_move\n",
            "  1343/50000: episode: 1338, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 246.000 [246.000, 246.000],  loss: 12491708.000000, mae: 1.444112, mean_q: 2.098131\n",
            "wrong_move\n",
            "  1344/50000: episode: 1339, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12491859.000000, mae: 1.444659, mean_q: 2.373537\n",
            "wrong_move\n",
            "  1345/50000: episode: 1340, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12492281.000000, mae: 1.443907, mean_q: 1.911669\n",
            "wrong_move\n",
            "  1346/50000: episode: 1341, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12492774.000000, mae: 1.443213, mean_q: 2.108074\n",
            "wrong_move\n",
            "  1347/50000: episode: 1342, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12492036.000000, mae: 1.444280, mean_q: 2.247797\n",
            "wrong_move\n",
            "  1348/50000: episode: 1343, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1657.000 [1657.000, 1657.000],  loss: 12491138.000000, mae: 1.443893, mean_q: 2.161907\n",
            "wrong_move\n",
            "  1349/50000: episode: 1344, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12103512.000000, mae: 1.405492, mean_q: 2.277224\n",
            "wrong_move\n",
            "  1350/50000: episode: 1345, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3347.000 [3347.000, 3347.000],  loss: 12495526.000000, mae: 1.440012, mean_q: 2.535378\n",
            "wrong_move\n",
            "  1351/50000: episode: 1346, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12489522.000000, mae: 1.445472, mean_q: 2.386069\n",
            "wrong_move\n",
            "  1352/50000: episode: 1347, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12100621.000000, mae: 1.408097, mean_q: 2.328561\n",
            "wrong_move\n",
            "  1353/50000: episode: 1348, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12491668.000000, mae: 1.445147, mean_q: 2.628232\n",
            "wrong_move\n",
            "  1354/50000: episode: 1349, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3499.000 [3499.000, 3499.000],  loss: 12493094.000000, mae: 1.445225, mean_q: 2.622706\n",
            "wrong_move\n",
            "  1355/50000: episode: 1350, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: 12491813.000000, mae: 1.444809, mean_q: 2.476074\n",
            "wrong_move\n",
            "  1356/50000: episode: 1351, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2850.000 [2850.000, 2850.000],  loss: 12498246.000000, mae: 1.447271, mean_q: 2.345182\n",
            "wrong_move\n",
            "  1357/50000: episode: 1352, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12490378.000000, mae: 1.448037, mean_q: 2.558334\n",
            "wrong_move\n",
            "  1358/50000: episode: 1353, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12489868.000000, mae: 1.447588, mean_q: 2.418348\n",
            "wrong_move\n",
            "  1359/50000: episode: 1354, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12489816.000000, mae: 1.436096, mean_q: 2.540564\n",
            "wrong_move\n",
            "  1360/50000: episode: 1355, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: 12489680.000000, mae: 1.446108, mean_q: 2.079189\n",
            "wrong_move\n",
            "  1361/50000: episode: 1356, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3002.000 [3002.000, 3002.000],  loss: 12490777.000000, mae: 1.444619, mean_q: 2.418695\n",
            "wrong_move\n",
            "  1362/50000: episode: 1357, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3857.000 [3857.000, 3857.000],  loss: 12494120.000000, mae: 1.447881, mean_q: 2.501761\n",
            "wrong_move\n",
            "  1363/50000: episode: 1358, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12490544.000000, mae: 1.443570, mean_q: 2.705011\n",
            "wrong_move\n",
            "  1364/50000: episode: 1359, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12490041.000000, mae: 1.446053, mean_q: 2.381615\n",
            "wrong_move\n",
            "  1365/50000: episode: 1360, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12490946.000000, mae: 1.447665, mean_q: 2.214127\n",
            "wrong_move\n",
            "  1366/50000: episode: 1361, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: 12493916.000000, mae: 1.447934, mean_q: 2.494050\n",
            "wrong_move\n",
            "  1367/50000: episode: 1362, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 555.000 [555.000, 555.000],  loss: 12492905.000000, mae: 1.448577, mean_q: 2.338357\n",
            "wrong_move\n",
            "  1368/50000: episode: 1363, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12491893.000000, mae: 1.447246, mean_q: 2.545182\n",
            "wrong_move\n",
            "  1369/50000: episode: 1364, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: 11713458.000000, mae: 1.373567, mean_q: 2.526066\n",
            "wrong_move\n",
            "  1370/50000: episode: 1365, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12493286.000000, mae: 1.446842, mean_q: 2.704062\n",
            "wrong_move\n",
            "  1371/50000: episode: 1366, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: 12494584.000000, mae: 1.442680, mean_q: 2.585294\n",
            "wrong_move\n",
            "  1372/50000: episode: 1367, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494674.000000, mae: 1.451747, mean_q: 2.603859\n",
            "wrong_move\n",
            "  1373/50000: episode: 1368, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3777.000 [3777.000, 3777.000],  loss: 12492944.000000, mae: 1.449471, mean_q: 2.372241\n",
            "wrong_move\n",
            "  1374/50000: episode: 1369, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1561.000 [1561.000, 1561.000],  loss: 12102750.000000, mae: 1.410772, mean_q: 2.636153\n",
            "wrong_move\n",
            "  1375/50000: episode: 1370, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3317.000 [3317.000, 3317.000],  loss: 12491328.000000, mae: 1.448836, mean_q: 2.369385\n",
            "wrong_move\n",
            "  1376/50000: episode: 1371, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12489795.000000, mae: 1.449842, mean_q: 2.466494\n",
            "wrong_move\n",
            "  1377/50000: episode: 1372, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 916.000 [916.000, 916.000],  loss: 12490686.000000, mae: 1.448487, mean_q: 2.752056\n",
            "wrong_move\n",
            "  1378/50000: episode: 1373, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1108.000 [1108.000, 1108.000],  loss: 12492721.000000, mae: 1.450621, mean_q: 2.677415\n",
            "wrong_move\n",
            "  1379/50000: episode: 1374, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12101381.000000, mae: 1.410470, mean_q: 2.287198\n",
            "wrong_move\n",
            "  1380/50000: episode: 1375, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12488346.000000, mae: 1.448949, mean_q: 2.242451\n",
            "wrong_move\n",
            "  1381/50000: episode: 1376, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: 12490773.000000, mae: 1.451197, mean_q: 2.501221\n",
            "wrong_move\n",
            "  1382/50000: episode: 1377, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12489912.000000, mae: 1.451076, mean_q: 2.577147\n",
            "wrong_move\n",
            "  1383/50000: episode: 1378, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2271.000 [2271.000, 2271.000],  loss: 12490108.000000, mae: 1.451255, mean_q: 2.557144\n",
            "wrong_move\n",
            "  1384/50000: episode: 1379, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12497225.000000, mae: 1.451671, mean_q: 2.570424\n",
            "wrong_move\n",
            "  1385/50000: episode: 1380, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12491522.000000, mae: 1.450090, mean_q: 2.222213\n",
            "wrong_move\n",
            "  1386/50000: episode: 1381, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: 12489688.000000, mae: 1.451992, mean_q: 2.747840\n",
            "wrong_move\n",
            "  1387/50000: episode: 1382, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494394.000000, mae: 1.450942, mean_q: 2.814121\n",
            "wrong_move\n",
            "  1388/50000: episode: 1383, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12493334.000000, mae: 1.452406, mean_q: 2.663348\n",
            "wrong_move\n",
            "  1389/50000: episode: 1384, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: 12494189.000000, mae: 1.446627, mean_q: 2.853212\n",
            "wrong_move\n",
            "  1390/50000: episode: 1385, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 59.000 [59.000, 59.000],  loss: 12495440.000000, mae: 1.440168, mean_q: 2.723916\n",
            "wrong_move\n",
            "  1391/50000: episode: 1386, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12103082.000000, mae: 1.411623, mean_q: 2.187254\n",
            "wrong_move\n",
            "  1392/50000: episode: 1387, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: 12493980.000000, mae: 1.453108, mean_q: 2.757277\n",
            "wrong_move\n",
            "  1393/50000: episode: 1388, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: 12494452.000000, mae: 1.453179, mean_q: 2.460093\n",
            "wrong_move\n",
            "  1394/50000: episode: 1389, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1434.000 [1434.000, 1434.000],  loss: 12493016.000000, mae: 1.451389, mean_q: 2.611821\n",
            "wrong_move\n",
            "  1395/50000: episode: 1390, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: 12493900.000000, mae: 1.452612, mean_q: 2.707549\n",
            "wrong_move\n",
            "  1396/50000: episode: 1391, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: 12493258.000000, mae: 1.453985, mean_q: 2.503623\n",
            "wrong_move\n",
            "  1397/50000: episode: 1392, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12492708.000000, mae: 1.455497, mean_q: 2.912862\n",
            "wrong_move\n",
            "  1398/50000: episode: 1393, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 607.000 [607.000, 607.000],  loss: 12490568.000000, mae: 1.453610, mean_q: 2.622915\n",
            "wrong_move\n",
            "  1399/50000: episode: 1394, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12490516.000000, mae: 1.453144, mean_q: 2.444021\n",
            "wrong_move\n",
            "  1400/50000: episode: 1395, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2503.000 [2503.000, 2503.000],  loss: 12489326.000000, mae: 1.453005, mean_q: 2.898708\n",
            "wrong_move\n",
            "  1401/50000: episode: 1396, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12492749.000000, mae: 1.454495, mean_q: 2.602388\n",
            "wrong_move\n",
            "  1402/50000: episode: 1397, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12101614.000000, mae: 1.417394, mean_q: 2.475523\n",
            "wrong_move\n",
            "  1403/50000: episode: 1398, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12493563.000000, mae: 1.454047, mean_q: 2.873748\n",
            "wrong_move\n",
            "  1404/50000: episode: 1399, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: 12493307.000000, mae: 1.453811, mean_q: 2.817799\n",
            "wrong_move\n",
            "  1405/50000: episode: 1400, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12487320.000000, mae: 1.456903, mean_q: 2.697683\n",
            "wrong_move\n",
            "  1406/50000: episode: 1401, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3156.000 [3156.000, 3156.000],  loss: 12493762.000000, mae: 1.455197, mean_q: 2.750484\n",
            "wrong_move\n",
            "  1407/50000: episode: 1402, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2104.000 [2104.000, 2104.000],  loss: 12489976.000000, mae: 1.457479, mean_q: 2.736285\n",
            "wrong_move\n",
            "  1408/50000: episode: 1403, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12488156.000000, mae: 1.456826, mean_q: 2.766861\n",
            "wrong_move\n",
            "  1409/50000: episode: 1404, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12490857.000000, mae: 1.454952, mean_q: 2.515085\n",
            "wrong_move\n",
            "  1410/50000: episode: 1405, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2439.000 [2439.000, 2439.000],  loss: 12100410.000000, mae: 1.415553, mean_q: 2.845767\n",
            "wrong_move\n",
            "  1411/50000: episode: 1406, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1062.000 [1062.000, 1062.000],  loss: 12496242.000000, mae: 1.454437, mean_q: 2.973253\n",
            "wrong_move\n",
            "  1412/50000: episode: 1407, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12487953.000000, mae: 1.455802, mean_q: 2.341802\n",
            "wrong_move\n",
            "  1413/50000: episode: 1408, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12104640.000000, mae: 1.419836, mean_q: 2.871537\n",
            "wrong_move\n",
            "  1414/50000: episode: 1409, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2661.000 [2661.000, 2661.000],  loss: 12493687.000000, mae: 1.455010, mean_q: 2.908402\n",
            "wrong_move\n",
            "  1415/50000: episode: 1410, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12492776.000000, mae: 1.455168, mean_q: 2.281994\n",
            "wrong_move\n",
            "  1416/50000: episode: 1411, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: 12489597.000000, mae: 1.458603, mean_q: 2.781451\n",
            "wrong_move\n",
            "  1417/50000: episode: 1412, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12491762.000000, mae: 1.458800, mean_q: 2.660065\n",
            "wrong_move\n",
            "  1418/50000: episode: 1413, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: 12488761.000000, mae: 1.455007, mean_q: 2.987100\n",
            "wrong_move\n",
            "  1419/50000: episode: 1414, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: 12488988.000000, mae: 1.458644, mean_q: 2.983184\n",
            "wrong_move\n",
            "  1420/50000: episode: 1415, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: 12489987.000000, mae: 1.461683, mean_q: 3.002395\n",
            "wrong_move\n",
            "  1421/50000: episode: 1416, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 825.000 [825.000, 825.000],  loss: 12489104.000000, mae: 1.460214, mean_q: 2.634761\n",
            "wrong_move\n",
            "  1422/50000: episode: 1417, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: 12492182.000000, mae: 1.456815, mean_q: 3.082289\n",
            "wrong_move\n",
            "  1423/50000: episode: 1418, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3327.000 [3327.000, 3327.000],  loss: 12488779.000000, mae: 1.460863, mean_q: 3.015193\n",
            "wrong_move\n",
            "  1424/50000: episode: 1419, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3928.000 [3928.000, 3928.000],  loss: 12493929.000000, mae: 1.461036, mean_q: 2.697195\n",
            "wrong_move\n",
            "  1425/50000: episode: 1420, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: 12491066.000000, mae: 1.463097, mean_q: 2.892259\n",
            "wrong_move\n",
            "  1426/50000: episode: 1421, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3335.000 [3335.000, 3335.000],  loss: 12491208.000000, mae: 1.462981, mean_q: 2.920793\n",
            "wrong_move\n",
            "  1427/50000: episode: 1422, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 394.000 [394.000, 394.000],  loss: 12489132.000000, mae: 1.461152, mean_q: 2.789057\n",
            "wrong_move\n",
            "  1428/50000: episode: 1423, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12490199.000000, mae: 1.459428, mean_q: 2.464523\n",
            "wrong_move\n",
            "  1429/50000: episode: 1424, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12487830.000000, mae: 1.461192, mean_q: 2.742730\n",
            "wrong_move\n",
            "  1430/50000: episode: 1425, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12492227.000000, mae: 1.463872, mean_q: 2.976045\n",
            "wrong_move\n",
            "  1431/50000: episode: 1426, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12492456.000000, mae: 1.457616, mean_q: 3.009136\n",
            "wrong_move\n",
            "  1432/50000: episode: 1427, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: 12489374.000000, mae: 1.461550, mean_q: 3.059550\n",
            "wrong_move\n",
            "  1433/50000: episode: 1428, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12489587.000000, mae: 1.461247, mean_q: 2.517553\n",
            "wrong_move\n",
            "  1434/50000: episode: 1429, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12490912.000000, mae: 1.459204, mean_q: 2.817446\n",
            "wrong_move\n",
            "  1435/50000: episode: 1430, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12488657.000000, mae: 1.461076, mean_q: 2.475391\n",
            "wrong_move\n",
            "  1436/50000: episode: 1431, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494098.000000, mae: 1.463031, mean_q: 2.857182\n",
            "wrong_move\n",
            "  1437/50000: episode: 1432, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: 12486845.000000, mae: 1.456045, mean_q: 3.048620\n",
            "wrong_move\n",
            "  1438/50000: episode: 1433, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12490712.000000, mae: 1.463533, mean_q: 2.772509\n",
            "wrong_move\n",
            "  1439/50000: episode: 1434, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: 12487906.000000, mae: 1.463501, mean_q: 2.805086\n",
            "wrong_move\n",
            "  1440/50000: episode: 1435, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 606.000 [606.000, 606.000],  loss: 12491967.000000, mae: 1.462926, mean_q: 2.899830\n",
            "wrong_move\n",
            "  1441/50000: episode: 1436, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12492291.000000, mae: 1.461565, mean_q: 2.421380\n",
            "wrong_move\n",
            "  1442/50000: episode: 1437, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12493883.000000, mae: 1.465461, mean_q: 2.961962\n",
            "wrong_move\n",
            "  1443/50000: episode: 1438, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12489620.000000, mae: 1.462779, mean_q: 3.136796\n",
            "wrong_move\n",
            "  1444/50000: episode: 1439, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12488848.000000, mae: 1.462519, mean_q: 3.011293\n",
            "wrong_move\n",
            "  1445/50000: episode: 1440, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: 12494112.000000, mae: 1.465799, mean_q: 3.077683\n",
            "wrong_move\n",
            "  1446/50000: episode: 1441, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12493978.000000, mae: 1.462570, mean_q: 2.380101\n",
            "wrong_move\n",
            "  1447/50000: episode: 1442, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 974.000 [974.000, 974.000],  loss: 12495684.000000, mae: 1.467083, mean_q: 3.111701\n",
            "wrong_move\n",
            "  1448/50000: episode: 1443, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12492422.000000, mae: 1.462715, mean_q: 3.070752\n",
            "wrong_move\n",
            "  1449/50000: episode: 1444, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: 12492888.000000, mae: 1.465010, mean_q: 2.925168\n",
            "wrong_move\n",
            "  1450/50000: episode: 1445, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 114.000 [114.000, 114.000],  loss: 12486044.000000, mae: 1.466858, mean_q: 3.213462\n",
            "wrong_move\n",
            "  1451/50000: episode: 1446, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12493089.000000, mae: 1.463825, mean_q: 3.232868\n",
            "wrong_move\n",
            "  1452/50000: episode: 1447, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3440.000 [3440.000, 3440.000],  loss: 12100419.000000, mae: 1.426531, mean_q: 2.475034\n",
            "wrong_move\n",
            "  1453/50000: episode: 1448, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: 12490850.000000, mae: 1.468230, mean_q: 2.825197\n",
            "wrong_move\n",
            "  1454/50000: episode: 1449, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12491657.000000, mae: 1.459521, mean_q: 3.178953\n",
            "wrong_move\n",
            "  1455/50000: episode: 1450, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 204.000 [204.000, 204.000],  loss: 12097908.000000, mae: 1.426295, mean_q: 2.554378\n",
            "wrong_move\n",
            "  1456/50000: episode: 1451, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 817.000 [817.000, 817.000],  loss: 12489762.000000, mae: 1.468650, mean_q: 3.149827\n",
            "wrong_move\n",
            "  1457/50000: episode: 1452, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494250.000000, mae: 1.469751, mean_q: 3.210336\n",
            "wrong_move\n",
            "  1458/50000: episode: 1453, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: 12486698.000000, mae: 1.470611, mean_q: 2.983868\n",
            "wrong_move\n",
            "  1459/50000: episode: 1454, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3425.000 [3425.000, 3425.000],  loss: 12493786.000000, mae: 1.469803, mean_q: 3.145683\n",
            "wrong_move\n",
            "  1460/50000: episode: 1455, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2932.000 [2932.000, 2932.000],  loss: 12490050.000000, mae: 1.467989, mean_q: 2.693669\n",
            "wrong_move\n",
            "  1461/50000: episode: 1456, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: 12490866.000000, mae: 1.466129, mean_q: 2.684460\n",
            "wrong_move\n",
            "  1462/50000: episode: 1457, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 863.000 [863.000, 863.000],  loss: 12488410.000000, mae: 1.463785, mean_q: 3.293669\n",
            "wrong_move\n",
            "  1463/50000: episode: 1458, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: 12490638.000000, mae: 1.469790, mean_q: 3.035373\n",
            "wrong_move\n",
            "  1464/50000: episode: 1459, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3165.000 [3165.000, 3165.000],  loss: 12488022.000000, mae: 1.471275, mean_q: 3.311002\n",
            "wrong_move\n",
            "  1465/50000: episode: 1460, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: 12490391.000000, mae: 1.463398, mean_q: 2.227657\n",
            "wrong_move\n",
            "  1466/50000: episode: 1461, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: 12492328.000000, mae: 1.469513, mean_q: 2.774944\n",
            "wrong_move\n",
            "  1467/50000: episode: 1462, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12492128.000000, mae: 1.467403, mean_q: 3.173328\n",
            "wrong_move\n",
            "  1468/50000: episode: 1463, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12488906.000000, mae: 1.467982, mean_q: 2.993465\n",
            "wrong_move\n",
            "  1469/50000: episode: 1464, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: 12486778.000000, mae: 1.466244, mean_q: 2.383693\n",
            "wrong_move\n",
            "  1470/50000: episode: 1465, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3462.000 [3462.000, 3462.000],  loss: 12488406.000000, mae: 1.467470, mean_q: 2.552055\n",
            "wrong_move\n",
            "  1471/50000: episode: 1466, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12489046.000000, mae: 1.469652, mean_q: 3.197360\n",
            "wrong_move\n",
            "  1472/50000: episode: 1467, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: 12491849.000000, mae: 1.470360, mean_q: 3.282624\n",
            "wrong_move\n",
            "  1473/50000: episode: 1468, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: 12492183.000000, mae: 1.470856, mean_q: 3.245886\n",
            "wrong_move\n",
            "  1474/50000: episode: 1469, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12486841.000000, mae: 1.474507, mean_q: 3.295378\n",
            "wrong_move\n",
            "  1475/50000: episode: 1470, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: 12492724.000000, mae: 1.465439, mean_q: 2.398143\n",
            "wrong_move\n",
            "  1476/50000: episode: 1471, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12492529.000000, mae: 1.470733, mean_q: 2.922911\n",
            "wrong_move\n",
            "  1477/50000: episode: 1472, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: 12488487.000000, mae: 1.471180, mean_q: 2.880256\n",
            "wrong_move\n",
            "  1478/50000: episode: 1473, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 89.000 [89.000, 89.000],  loss: 12103416.000000, mae: 1.434482, mean_q: 3.066981\n",
            "wrong_move\n",
            "  1479/50000: episode: 1474, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12485201.000000, mae: 1.471931, mean_q: 3.242799\n",
            "wrong_move\n",
            "  1480/50000: episode: 1475, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12487834.000000, mae: 1.472221, mean_q: 3.153009\n",
            "wrong_move\n",
            "  1481/50000: episode: 1476, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: 12493132.000000, mae: 1.473076, mean_q: 3.262724\n",
            "wrong_move\n",
            "  1482/50000: episode: 1477, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12493249.000000, mae: 1.472250, mean_q: 2.883280\n",
            "wrong_move\n",
            "  1483/50000: episode: 1478, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: 12099386.000000, mae: 1.433976, mean_q: 3.311981\n",
            "wrong_move\n",
            "  1484/50000: episode: 1479, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2342.000 [2342.000, 2342.000],  loss: 12487598.000000, mae: 1.472913, mean_q: 2.994845\n",
            "wrong_move\n",
            "  1485/50000: episode: 1480, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: 12491742.000000, mae: 1.471099, mean_q: 2.533507\n",
            "wrong_move\n",
            "  1486/50000: episode: 1481, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12486696.000000, mae: 1.475791, mean_q: 3.079903\n",
            "wrong_move\n",
            "  1487/50000: episode: 1482, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3507.000 [3507.000, 3507.000],  loss: 12097012.000000, mae: 1.434928, mean_q: 3.173662\n",
            "wrong_move\n",
            "  1488/50000: episode: 1483, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12493065.000000, mae: 1.475867, mean_q: 2.990897\n",
            "wrong_move\n",
            "  1489/50000: episode: 1484, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12097003.000000, mae: 1.435230, mean_q: 2.802819\n",
            "wrong_move\n",
            "  1490/50000: episode: 1485, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12488798.000000, mae: 1.473679, mean_q: 2.696397\n",
            "wrong_move\n",
            "  1491/50000: episode: 1486, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2240.000 [2240.000, 2240.000],  loss: 12492476.000000, mae: 1.473907, mean_q: 2.911473\n",
            "wrong_move\n",
            "  1492/50000: episode: 1487, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 11706232.000000, mae: 1.399549, mean_q: 2.974262\n",
            "wrong_move\n",
            "  1493/50000: episode: 1488, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: 12493655.000000, mae: 1.468316, mean_q: 2.297218\n",
            "wrong_move\n",
            "  1494/50000: episode: 1489, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12101144.000000, mae: 1.431267, mean_q: 2.262756\n",
            "wrong_move\n",
            "  1495/50000: episode: 1490, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1524.000 [1524.000, 1524.000],  loss: 12491970.000000, mae: 1.475748, mean_q: 2.888790\n",
            "wrong_move\n",
            "  1496/50000: episode: 1491, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12488356.000000, mae: 1.476272, mean_q: 3.102112\n",
            "wrong_move\n",
            "  1497/50000: episode: 1492, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12488182.000000, mae: 1.474046, mean_q: 2.759182\n",
            "wrong_move\n",
            "  1498/50000: episode: 1493, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12488280.000000, mae: 1.474332, mean_q: 3.280898\n",
            "wrong_move\n",
            "  1499/50000: episode: 1494, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12488026.000000, mae: 1.477705, mean_q: 3.217949\n",
            "wrong_move\n",
            "  1500/50000: episode: 1495, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2742.000 [2742.000, 2742.000],  loss: 12490488.000000, mae: 1.475181, mean_q: 2.743431\n",
            "wrong_move\n",
            "  1501/50000: episode: 1496, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3368.000 [3368.000, 3368.000],  loss: 12096816.000000, mae: 1.433430, mean_q: 2.478445\n",
            "wrong_move\n",
            "  1502/50000: episode: 1497, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2729.000 [2729.000, 2729.000],  loss: 12484844.000000, mae: 1.476122, mean_q: 2.933713\n",
            "wrong_move\n",
            "  1503/50000: episode: 1498, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3368.000 [3368.000, 3368.000],  loss: 12488193.000000, mae: 1.477498, mean_q: 3.209558\n",
            "wrong_move\n",
            "  1504/50000: episode: 1499, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12486248.000000, mae: 1.469768, mean_q: 3.367593\n",
            "wrong_move\n",
            "  1505/50000: episode: 1500, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12498175.000000, mae: 1.477845, mean_q: 2.838149\n",
            "wrong_move\n",
            "  1506/50000: episode: 1501, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3145.000 [3145.000, 3145.000],  loss: 12488033.000000, mae: 1.476235, mean_q: 2.821444\n",
            "wrong_move\n",
            "  1507/50000: episode: 1502, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 420.000 [420.000, 420.000],  loss: 12491758.000000, mae: 1.476898, mean_q: 3.277299\n",
            "wrong_move\n",
            "  1508/50000: episode: 1503, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 420.000 [420.000, 420.000],  loss: 12491509.000000, mae: 1.480630, mean_q: 3.230780\n",
            "wrong_move\n",
            "  1509/50000: episode: 1504, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: 12096863.000000, mae: 1.437414, mean_q: 3.450260\n",
            "wrong_move\n",
            "  1510/50000: episode: 1505, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2037.000 [2037.000, 2037.000],  loss: 12486284.000000, mae: 1.480073, mean_q: 3.388663\n",
            "wrong_move\n",
            "  1511/50000: episode: 1506, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1590.000 [1590.000, 1590.000],  loss: 12491646.000000, mae: 1.479839, mean_q: 3.546650\n",
            "wrong_move\n",
            "  1512/50000: episode: 1507, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 533.000 [533.000, 533.000],  loss: 12489708.000000, mae: 1.477181, mean_q: 2.743465\n",
            "wrong_move\n",
            "  1513/50000: episode: 1508, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 180.000 [180.000, 180.000],  loss: 12486284.000000, mae: 1.479930, mean_q: 3.115554\n",
            "wrong_move\n",
            "  1514/50000: episode: 1509, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1171.000 [1171.000, 1171.000],  loss: 12490994.000000, mae: 1.482159, mean_q: 3.274123\n",
            "wrong_move\n",
            "  1515/50000: episode: 1510, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12494639.000000, mae: 1.482555, mean_q: 3.258873\n",
            "wrong_move\n",
            "  1516/50000: episode: 1511, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 420.000 [420.000, 420.000],  loss: 12103810.000000, mae: 1.443528, mean_q: 3.361680\n",
            "wrong_move\n",
            "  1517/50000: episode: 1512, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1357.000 [1357.000, 1357.000],  loss: 12493136.000000, mae: 1.481757, mean_q: 3.153531\n",
            "wrong_move\n",
            "  1518/50000: episode: 1513, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12488775.000000, mae: 1.481915, mean_q: 3.569841\n",
            "wrong_move\n",
            "  1519/50000: episode: 1514, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12488455.000000, mae: 1.478859, mean_q: 2.903472\n",
            "wrong_move\n",
            "  1520/50000: episode: 1515, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12490859.000000, mae: 1.482237, mean_q: 3.130813\n",
            "wrong_move\n",
            "  1521/50000: episode: 1516, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: 12490592.000000, mae: 1.483549, mean_q: 3.216728\n",
            "wrong_move\n",
            "  1522/50000: episode: 1517, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2365.000 [2365.000, 2365.000],  loss: 12484486.000000, mae: 1.484301, mean_q: 3.370177\n",
            "wrong_move\n",
            "  1523/50000: episode: 1518, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12485156.000000, mae: 1.479054, mean_q: 3.477657\n",
            "wrong_move\n",
            "  1524/50000: episode: 1519, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12493188.000000, mae: 1.484238, mean_q: 3.395041\n",
            "wrong_move\n",
            "  1525/50000: episode: 1520, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 70.000 [70.000, 70.000],  loss: 12492288.000000, mae: 1.475450, mean_q: 2.510235\n",
            "wrong_move\n",
            "  1526/50000: episode: 1521, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12489180.000000, mae: 1.483602, mean_q: 3.121933\n",
            "wrong_move\n",
            "  1527/50000: episode: 1522, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12482839.000000, mae: 1.482060, mean_q: 3.045758\n",
            "wrong_move\n",
            "  1528/50000: episode: 1523, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12487990.000000, mae: 1.480437, mean_q: 2.820992\n",
            "wrong_move\n",
            "  1529/50000: episode: 1524, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3669.000 [3669.000, 3669.000],  loss: 12485630.000000, mae: 1.480708, mean_q: 2.742206\n",
            "wrong_move\n",
            "  1530/50000: episode: 1525, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: 12485640.000000, mae: 1.483740, mean_q: 3.112681\n",
            "wrong_move\n",
            "  1531/50000: episode: 1526, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 12491502.000000, mae: 1.485088, mean_q: 3.138881\n",
            "wrong_move\n",
            "  1532/50000: episode: 1527, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: 12491920.000000, mae: 1.487134, mean_q: 3.442612\n",
            "wrong_move\n",
            "  1533/50000: episode: 1528, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: 12100007.000000, mae: 1.443560, mean_q: 2.954136\n",
            "wrong_move\n",
            "  1534/50000: episode: 1529, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: 12489966.000000, mae: 1.483621, mean_q: 3.034595\n",
            "wrong_move\n",
            "  1535/50000: episode: 1530, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: 12484186.000000, mae: 1.484800, mean_q: 3.265501\n",
            "wrong_move\n",
            "done, took 46.840 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 50000 steps ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wrong_move\n",
            "     1/50000: episode: 1, duration: 0.399s, episode steps:   1, steps per second:   3, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "     2/50000: episode: 2, duration: 0.005s, episode steps:   1, steps per second: 187, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "     3/50000: episode: 3, duration: 0.005s, episode steps:   1, steps per second: 204, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "     4/50000: episode: 4, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "     5/50000: episode: 5, duration: 0.004s, episode steps:   1, steps per second: 225, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "     6/50000: episode: 6, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "     7/50000: episode: 7, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "     8/50000: episode: 8, duration: 0.006s, episode steps:   1, steps per second: 171, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "     9/50000: episode: 9, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3959.000 [3959.000, 3959.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    10/50000: episode: 10, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    11/50000: episode: 11, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    12/50000: episode: 12, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    13/50000: episode: 13, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 447.000 [447.000, 447.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    14/50000: episode: 14, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    15/50000: episode: 15, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    16/50000: episode: 16, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    17/50000: episode: 17, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    18/50000: episode: 18, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    19/50000: episode: 19, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    20/50000: episode: 20, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 504.000 [504.000, 504.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    21/50000: episode: 21, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    22/50000: episode: 22, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3857.000 [3857.000, 3857.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    23/50000: episode: 23, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2111.000 [2111.000, 2111.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    24/50000: episode: 24, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    25/50000: episode: 25, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    26/50000: episode: 26, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    27/50000: episode: 27, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    28/50000: episode: 28, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2086.000 [2086.000, 2086.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    29/50000: episode: 29, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    30/50000: episode: 30, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    31/50000: episode: 31, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    32/50000: episode: 32, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 958.000 [958.000, 958.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    33/50000: episode: 33, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    34/50000: episode: 34, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    35/50000: episode: 35, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    36/50000: episode: 36, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    37/50000: episode: 37, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2932.000 [2932.000, 2932.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    38/50000: episode: 38, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    39/50000: episode: 39, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    40/50000: episode: 40, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    41/50000: episode: 41, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1134.000 [1134.000, 1134.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    42/50000: episode: 42, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    43/50000: episode: 43, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    44/50000: episode: 44, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2710.000 [2710.000, 2710.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    45/50000: episode: 45, duration: 0.006s, episode steps:   1, steps per second: 165, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    46/50000: episode: 46, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1458.000 [1458.000, 1458.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    47/50000: episode: 47, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    48/50000: episode: 48, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    49/50000: episode: 49, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1298.000 [1298.000, 1298.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    50/50000: episode: 50, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    51/50000: episode: 51, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 46.000 [46.000, 46.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    52/50000: episode: 52, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 518.000 [518.000, 518.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    53/50000: episode: 53, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    54/50000: episode: 54, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    55/50000: episode: 55, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    56/50000: episode: 56, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    57/50000: episode: 57, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    58/50000: episode: 58, duration: 0.024s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    59/50000: episode: 59, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    60/50000: episode: 60, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    61/50000: episode: 61, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2336.000 [2336.000, 2336.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    62/50000: episode: 62, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    63/50000: episode: 63, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    64/50000: episode: 64, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    65/50000: episode: 65, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 846.000 [846.000, 846.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    66/50000: episode: 66, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    67/50000: episode: 67, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    68/50000: episode: 68, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    69/50000: episode: 69, duration: 0.007s, episode steps:   1, steps per second: 134, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2425.000 [2425.000, 2425.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    70/50000: episode: 70, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    71/50000: episode: 71, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    72/50000: episode: 72, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    73/50000: episode: 73, duration: 0.006s, episode steps:   1, steps per second: 178, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    74/50000: episode: 74, duration: 0.005s, episode steps:   1, steps per second: 214, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    75/50000: episode: 75, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    76/50000: episode: 76, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    77/50000: episode: 77, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    78/50000: episode: 78, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    79/50000: episode: 79, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    80/50000: episode: 80, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    81/50000: episode: 81, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    82/50000: episode: 82, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    83/50000: episode: 83, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    84/50000: episode: 84, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2463.000 [2463.000, 2463.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    85/50000: episode: 85, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    86/50000: episode: 86, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    87/50000: episode: 87, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    88/50000: episode: 88, duration: 0.008s, episode steps:   1, steps per second: 132, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 155.000 [155.000, 155.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    89/50000: episode: 89, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2138.000 [2138.000, 2138.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    90/50000: episode: 90, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    91/50000: episode: 91, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1423.000 [1423.000, 1423.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    92/50000: episode: 92, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1072.000 [1072.000, 1072.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    93/50000: episode: 93, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 402.000 [402.000, 402.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    94/50000: episode: 94, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1688.000 [1688.000, 1688.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    95/50000: episode: 95, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2279.000 [2279.000, 2279.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    96/50000: episode: 96, duration: 0.005s, episode steps:   1, steps per second: 183, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2822.000 [2822.000, 2822.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    97/50000: episode: 97, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    98/50000: episode: 98, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "    99/50000: episode: 99, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   100/50000: episode: 100, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 442.000 [442.000, 442.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   101/50000: episode: 101, duration: 0.005s, episode steps:   1, steps per second: 220, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   102/50000: episode: 102, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   103/50000: episode: 103, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   104/50000: episode: 104, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   105/50000: episode: 105, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   106/50000: episode: 106, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   107/50000: episode: 107, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   108/50000: episode: 108, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1773.000 [1773.000, 1773.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   109/50000: episode: 109, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   110/50000: episode: 110, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   111/50000: episode: 111, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   112/50000: episode: 112, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   113/50000: episode: 113, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   114/50000: episode: 114, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2389.000 [2389.000, 2389.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   115/50000: episode: 115, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   116/50000: episode: 116, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3111.000 [3111.000, 3111.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   117/50000: episode: 117, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   118/50000: episode: 118, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2849.000 [2849.000, 2849.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   119/50000: episode: 119, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   120/50000: episode: 120, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2289.000 [2289.000, 2289.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   121/50000: episode: 121, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   122/50000: episode: 122, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   123/50000: episode: 123, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 625.000 [625.000, 625.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   124/50000: episode: 124, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   125/50000: episode: 125, duration: 0.006s, episode steps:   1, steps per second: 157, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   126/50000: episode: 126, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   127/50000: episode: 127, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   128/50000: episode: 128, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1932.000 [1932.000, 1932.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   129/50000: episode: 129, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   130/50000: episode: 130, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1960.000 [1960.000, 1960.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   131/50000: episode: 131, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   132/50000: episode: 132, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   133/50000: episode: 133, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   134/50000: episode: 134, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   135/50000: episode: 135, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   136/50000: episode: 136, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   137/50000: episode: 137, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   138/50000: episode: 138, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   139/50000: episode: 139, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   140/50000: episode: 140, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 566.000 [566.000, 566.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   141/50000: episode: 141, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   142/50000: episode: 142, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2381.000 [2381.000, 2381.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   143/50000: episode: 143, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   144/50000: episode: 144, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1446.000 [1446.000, 1446.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   145/50000: episode: 145, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   146/50000: episode: 146, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 713.000 [713.000, 713.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   147/50000: episode: 147, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   148/50000: episode: 148, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   149/50000: episode: 149, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   150/50000: episode: 150, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   151/50000: episode: 151, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   152/50000: episode: 152, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   153/50000: episode: 153, duration: 0.007s, episode steps:   1, steps per second: 138, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   154/50000: episode: 154, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   155/50000: episode: 155, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   156/50000: episode: 156, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   157/50000: episode: 157, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   158/50000: episode: 158, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 841.000 [841.000, 841.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   159/50000: episode: 159, duration: 0.005s, episode steps:   1, steps per second: 191, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   160/50000: episode: 160, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   161/50000: episode: 161, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1213.000 [1213.000, 1213.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   162/50000: episode: 162, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 293.000 [293.000, 293.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   163/50000: episode: 163, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 218.000 [218.000, 218.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   164/50000: episode: 164, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   165/50000: episode: 165, duration: 0.006s, episode steps:   1, steps per second: 168, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   166/50000: episode: 166, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   167/50000: episode: 167, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   168/50000: episode: 168, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   169/50000: episode: 169, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2827.000 [2827.000, 2827.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   170/50000: episode: 170, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   171/50000: episode: 171, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   172/50000: episode: 172, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   173/50000: episode: 173, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   174/50000: episode: 174, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   175/50000: episode: 175, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   176/50000: episode: 176, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2682.000 [2682.000, 2682.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   177/50000: episode: 177, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3117.000 [3117.000, 3117.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   178/50000: episode: 178, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   179/50000: episode: 179, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3765.000 [3765.000, 3765.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   180/50000: episode: 180, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   181/50000: episode: 181, duration: 0.006s, episode steps:   1, steps per second: 165, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   182/50000: episode: 182, duration: 0.005s, episode steps:   1, steps per second: 210, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   183/50000: episode: 183, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2147.000 [2147.000, 2147.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   184/50000: episode: 184, duration: 0.006s, episode steps:   1, steps per second: 156, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   185/50000: episode: 185, duration: 0.003s, episode steps:   1, steps per second: 342, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   186/50000: episode: 186, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2708.000 [2708.000, 2708.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   187/50000: episode: 187, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2806.000 [2806.000, 2806.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   188/50000: episode: 188, duration: 0.006s, episode steps:   1, steps per second: 176, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   189/50000: episode: 189, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   190/50000: episode: 190, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 560.000 [560.000, 560.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   191/50000: episode: 191, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   192/50000: episode: 192, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   193/50000: episode: 193, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   194/50000: episode: 194, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   195/50000: episode: 195, duration: 0.006s, episode steps:   1, steps per second: 160, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2076.000 [2076.000, 2076.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   196/50000: episode: 196, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   197/50000: episode: 197, duration: 0.007s, episode steps:   1, steps per second: 138, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   198/50000: episode: 198, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1863.000 [1863.000, 1863.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   199/50000: episode: 199, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   200/50000: episode: 200, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 528.000 [528.000, 528.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   201/50000: episode: 201, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 858.000 [858.000, 858.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   202/50000: episode: 202, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   203/50000: episode: 203, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   204/50000: episode: 204, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   205/50000: episode: 205, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   206/50000: episode: 206, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   207/50000: episode: 207, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   208/50000: episode: 208, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   209/50000: episode: 209, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1986.000 [1986.000, 1986.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   210/50000: episode: 210, duration: 0.007s, episode steps:   1, steps per second: 145, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   211/50000: episode: 211, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   212/50000: episode: 212, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   213/50000: episode: 213, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   214/50000: episode: 214, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   215/50000: episode: 215, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1336.000 [1336.000, 1336.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   216/50000: episode: 216, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   217/50000: episode: 217, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3077.000 [3077.000, 3077.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   218/50000: episode: 218, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   219/50000: episode: 219, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   220/50000: episode: 220, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   221/50000: episode: 221, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3325.000 [3325.000, 3325.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   222/50000: episode: 222, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   223/50000: episode: 223, duration: 0.006s, episode steps:   1, steps per second: 160, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   224/50000: episode: 224, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   225/50000: episode: 225, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   226/50000: episode: 226, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   227/50000: episode: 227, duration: 0.006s, episode steps:   1, steps per second: 159, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   228/50000: episode: 228, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2436.000 [2436.000, 2436.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   229/50000: episode: 229, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   230/50000: episode: 230, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2452.000 [2452.000, 2452.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   231/50000: episode: 231, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   232/50000: episode: 232, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   233/50000: episode: 233, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   234/50000: episode: 234, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 799.000 [799.000, 799.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   235/50000: episode: 235, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   236/50000: episode: 236, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   237/50000: episode: 237, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   238/50000: episode: 238, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   239/50000: episode: 239, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   240/50000: episode: 240, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   241/50000: episode: 241, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3384.000 [3384.000, 3384.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   242/50000: episode: 242, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   243/50000: episode: 243, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3372.000 [3372.000, 3372.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   244/50000: episode: 244, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   245/50000: episode: 245, duration: 0.006s, episode steps:   1, steps per second: 156, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   246/50000: episode: 246, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   247/50000: episode: 247, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   248/50000: episode: 248, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   249/50000: episode: 249, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   250/50000: episode: 250, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   251/50000: episode: 251, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   252/50000: episode: 252, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   253/50000: episode: 253, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   254/50000: episode: 254, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   255/50000: episode: 255, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   256/50000: episode: 256, duration: 0.006s, episode steps:   1, steps per second: 181, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   257/50000: episode: 257, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   258/50000: episode: 258, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1051.000 [1051.000, 1051.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   259/50000: episode: 259, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3277.000 [3277.000, 3277.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   260/50000: episode: 260, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   261/50000: episode: 261, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   262/50000: episode: 262, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   263/50000: episode: 263, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 630.000 [630.000, 630.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   264/50000: episode: 264, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   265/50000: episode: 265, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   266/50000: episode: 266, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   267/50000: episode: 267, duration: 0.005s, episode steps:   1, steps per second: 185, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   268/50000: episode: 268, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   269/50000: episode: 269, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   270/50000: episode: 270, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1015.000 [1015.000, 1015.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   271/50000: episode: 271, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 786.000 [786.000, 786.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   272/50000: episode: 272, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1988.000 [1988.000, 1988.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   273/50000: episode: 273, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3752.000 [3752.000, 3752.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   274/50000: episode: 274, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   275/50000: episode: 275, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   276/50000: episode: 276, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   277/50000: episode: 277, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   278/50000: episode: 278, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   279/50000: episode: 279, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   280/50000: episode: 280, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   281/50000: episode: 281, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   282/50000: episode: 282, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   283/50000: episode: 283, duration: 0.009s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   284/50000: episode: 284, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2262.000 [2262.000, 2262.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   285/50000: episode: 285, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   286/50000: episode: 286, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2523.000 [2523.000, 2523.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   287/50000: episode: 287, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   288/50000: episode: 288, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   289/50000: episode: 289, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   290/50000: episode: 290, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2449.000 [2449.000, 2449.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   291/50000: episode: 291, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2827.000 [2827.000, 2827.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   292/50000: episode: 292, duration: 0.006s, episode steps:   1, steps per second: 156, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   293/50000: episode: 293, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   294/50000: episode: 294, duration: 0.006s, episode steps:   1, steps per second: 158, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   295/50000: episode: 295, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   296/50000: episode: 296, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2193.000 [2193.000, 2193.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   297/50000: episode: 297, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 689.000 [689.000, 689.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   298/50000: episode: 298, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   299/50000: episode: 299, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1023.000 [1023.000, 1023.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   300/50000: episode: 300, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   301/50000: episode: 301, duration: 0.005s, episode steps:   1, steps per second: 182, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   302/50000: episode: 302, duration: 0.004s, episode steps:   1, steps per second: 233, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3868.000 [3868.000, 3868.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   303/50000: episode: 303, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   304/50000: episode: 304, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   305/50000: episode: 305, duration: 0.006s, episode steps:   1, steps per second: 181, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   306/50000: episode: 306, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   307/50000: episode: 307, duration: 0.006s, episode steps:   1, steps per second: 177, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   308/50000: episode: 308, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2010.000 [2010.000, 2010.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   309/50000: episode: 309, duration: 0.009s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   310/50000: episode: 310, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 836.000 [836.000, 836.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   311/50000: episode: 311, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3661.000 [3661.000, 3661.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   312/50000: episode: 312, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   313/50000: episode: 313, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   314/50000: episode: 314, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   315/50000: episode: 315, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   316/50000: episode: 316, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   317/50000: episode: 317, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   318/50000: episode: 318, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3729.000 [3729.000, 3729.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   319/50000: episode: 319, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   320/50000: episode: 320, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   321/50000: episode: 321, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   322/50000: episode: 322, duration: 0.006s, episode steps:   1, steps per second: 162, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   323/50000: episode: 323, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   324/50000: episode: 324, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1382.000 [1382.000, 1382.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   325/50000: episode: 325, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1769.000 [1769.000, 1769.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   326/50000: episode: 326, duration: 0.006s, episode steps:   1, steps per second: 156, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2477.000 [2477.000, 2477.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   327/50000: episode: 327, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1209.000 [1209.000, 1209.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   328/50000: episode: 328, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   329/50000: episode: 329, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   330/50000: episode: 330, duration: 0.005s, episode steps:   1, steps per second: 216, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   331/50000: episode: 331, duration: 0.005s, episode steps:   1, steps per second: 183, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1986.000 [1986.000, 1986.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   332/50000: episode: 332, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1423.000 [1423.000, 1423.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   333/50000: episode: 333, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   334/50000: episode: 334, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 43.000 [43.000, 43.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   335/50000: episode: 335, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   336/50000: episode: 336, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   337/50000: episode: 337, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3407.000 [3407.000, 3407.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   338/50000: episode: 338, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2827.000 [2827.000, 2827.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   339/50000: episode: 339, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   340/50000: episode: 340, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   341/50000: episode: 341, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   342/50000: episode: 342, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   343/50000: episode: 343, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2199.000 [2199.000, 2199.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   344/50000: episode: 344, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   345/50000: episode: 345, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3826.000 [3826.000, 3826.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   346/50000: episode: 346, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   347/50000: episode: 347, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   348/50000: episode: 348, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1502.000 [1502.000, 1502.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   349/50000: episode: 349, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   350/50000: episode: 350, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   351/50000: episode: 351, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   352/50000: episode: 352, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   353/50000: episode: 353, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   354/50000: episode: 354, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   355/50000: episode: 355, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   356/50000: episode: 356, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   357/50000: episode: 357, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   358/50000: episode: 358, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1458.000 [1458.000, 1458.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   359/50000: episode: 359, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   360/50000: episode: 360, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   361/50000: episode: 361, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   362/50000: episode: 362, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   363/50000: episode: 363, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   364/50000: episode: 364, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   365/50000: episode: 365, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   366/50000: episode: 366, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   367/50000: episode: 367, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   368/50000: episode: 368, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   369/50000: episode: 369, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   370/50000: episode: 370, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   371/50000: episode: 371, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   372/50000: episode: 372, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   373/50000: episode: 373, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   374/50000: episode: 374, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   375/50000: episode: 375, duration: 0.005s, episode steps:   1, steps per second: 195, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   376/50000: episode: 376, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   377/50000: episode: 377, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2398.000 [2398.000, 2398.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   378/50000: episode: 378, duration: 0.007s, episode steps:   1, steps per second: 151, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   379/50000: episode: 379, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 717.000 [717.000, 717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   380/50000: episode: 380, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   381/50000: episode: 381, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 884.000 [884.000, 884.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   382/50000: episode: 382, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   383/50000: episode: 383, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   384/50000: episode: 384, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   385/50000: episode: 385, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 143.000 [143.000, 143.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   386/50000: episode: 386, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   387/50000: episode: 387, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   388/50000: episode: 388, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2658.000 [2658.000, 2658.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   389/50000: episode: 389, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   390/50000: episode: 390, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 814.000 [814.000, 814.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   391/50000: episode: 391, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   392/50000: episode: 392, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3537.000 [3537.000, 3537.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   393/50000: episode: 393, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   394/50000: episode: 394, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   395/50000: episode: 395, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   396/50000: episode: 396, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   397/50000: episode: 397, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2619.000 [2619.000, 2619.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   398/50000: episode: 398, duration: 0.006s, episode steps:   1, steps per second: 154, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3186.000 [3186.000, 3186.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   399/50000: episode: 399, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   400/50000: episode: 400, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   401/50000: episode: 401, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 697.000 [697.000, 697.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   402/50000: episode: 402, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   403/50000: episode: 403, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   404/50000: episode: 404, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3130.000 [3130.000, 3130.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   405/50000: episode: 405, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   406/50000: episode: 406, duration: 0.005s, episode steps:   1, steps per second: 186, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   407/50000: episode: 407, duration: 0.005s, episode steps:   1, steps per second: 206, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   408/50000: episode: 408, duration: 0.006s, episode steps:   1, steps per second: 156, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   409/50000: episode: 409, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   410/50000: episode: 410, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1496.000 [1496.000, 1496.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   411/50000: episode: 411, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3368.000 [3368.000, 3368.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   412/50000: episode: 412, duration: 0.006s, episode steps:   1, steps per second: 172, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   413/50000: episode: 413, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   414/50000: episode: 414, duration: 0.017s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 523.000 [523.000, 523.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   415/50000: episode: 415, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   416/50000: episode: 416, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   417/50000: episode: 417, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   418/50000: episode: 418, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   419/50000: episode: 419, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   420/50000: episode: 420, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2361.000 [2361.000, 2361.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   421/50000: episode: 421, duration: 0.003s, episode steps:   1, steps per second: 295, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   422/50000: episode: 422, duration: 0.005s, episode steps:   1, steps per second: 200, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   423/50000: episode: 423, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   424/50000: episode: 424, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   425/50000: episode: 425, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3706.000 [3706.000, 3706.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   426/50000: episode: 426, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   427/50000: episode: 427, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   428/50000: episode: 428, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2601.000 [2601.000, 2601.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   429/50000: episode: 429, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   430/50000: episode: 430, duration: 0.005s, episode steps:   1, steps per second: 182, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   431/50000: episode: 431, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3114.000 [3114.000, 3114.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   432/50000: episode: 432, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1821.000 [1821.000, 1821.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   433/50000: episode: 433, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2414.000 [2414.000, 2414.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   434/50000: episode: 434, duration: 0.006s, episode steps:   1, steps per second: 174, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   435/50000: episode: 435, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   436/50000: episode: 436, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3283.000 [3283.000, 3283.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   437/50000: episode: 437, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   438/50000: episode: 438, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   439/50000: episode: 439, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2601.000 [2601.000, 2601.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   440/50000: episode: 440, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   441/50000: episode: 441, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   442/50000: episode: 442, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2827.000 [2827.000, 2827.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   443/50000: episode: 443, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1658.000 [1658.000, 1658.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   444/50000: episode: 444, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   445/50000: episode: 445, duration: 0.007s, episode steps:   1, steps per second: 140, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   446/50000: episode: 446, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   447/50000: episode: 447, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   448/50000: episode: 448, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   449/50000: episode: 449, duration: 0.006s, episode steps:   1, steps per second: 174, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2825.000 [2825.000, 2825.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   450/50000: episode: 450, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   451/50000: episode: 451, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   452/50000: episode: 452, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 987.000 [987.000, 987.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   453/50000: episode: 453, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3697.000 [3697.000, 3697.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   454/50000: episode: 454, duration: 0.004s, episode steps:   1, steps per second: 230, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1117.000 [1117.000, 1117.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   455/50000: episode: 455, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 316.000 [316.000, 316.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   456/50000: episode: 456, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   457/50000: episode: 457, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   458/50000: episode: 458, duration: 0.004s, episode steps:   1, steps per second: 244, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   459/50000: episode: 459, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2110.000 [2110.000, 2110.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   460/50000: episode: 460, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 314.000 [314.000, 314.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   461/50000: episode: 461, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   462/50000: episode: 462, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   463/50000: episode: 463, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3252.000 [3252.000, 3252.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   464/50000: episode: 464, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   465/50000: episode: 465, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   466/50000: episode: 466, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 283.000 [283.000, 283.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   467/50000: episode: 467, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   468/50000: episode: 468, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   469/50000: episode: 469, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   470/50000: episode: 470, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 73.000 [73.000, 73.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   471/50000: episode: 471, duration: 0.007s, episode steps:   1, steps per second: 153, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   472/50000: episode: 472, duration: 0.007s, episode steps:   1, steps per second: 134, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   473/50000: episode: 473, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   474/50000: episode: 474, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   475/50000: episode: 475, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   476/50000: episode: 476, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   477/50000: episode: 477, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   478/50000: episode: 478, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1206.000 [1206.000, 1206.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   479/50000: episode: 479, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   480/50000: episode: 480, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3508.000 [3508.000, 3508.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   481/50000: episode: 481, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   482/50000: episode: 482, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   483/50000: episode: 483, duration: 0.006s, episode steps:   1, steps per second: 180, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   484/50000: episode: 484, duration: 0.006s, episode steps:   1, steps per second: 162, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   485/50000: episode: 485, duration: 0.006s, episode steps:   1, steps per second: 155, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   486/50000: episode: 486, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   487/50000: episode: 487, duration: 0.005s, episode steps:   1, steps per second: 184, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   488/50000: episode: 488, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   489/50000: episode: 489, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   490/50000: episode: 490, duration: 0.006s, episode steps:   1, steps per second: 169, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   491/50000: episode: 491, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   492/50000: episode: 492, duration: 0.005s, episode steps:   1, steps per second: 207, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3037.000 [3037.000, 3037.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   493/50000: episode: 493, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   494/50000: episode: 494, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 58.000 [58.000, 58.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   495/50000: episode: 495, duration: 0.007s, episode steps:   1, steps per second: 147, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   496/50000: episode: 496, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2827.000 [2827.000, 2827.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   497/50000: episode: 497, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   498/50000: episode: 498, duration: 0.009s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   499/50000: episode: 499, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   500/50000: episode: 500, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   501/50000: episode: 501, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   502/50000: episode: 502, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   503/50000: episode: 503, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   504/50000: episode: 504, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2827.000 [2827.000, 2827.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   505/50000: episode: 505, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2282.000 [2282.000, 2282.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   506/50000: episode: 506, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 997.000 [997.000, 997.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   507/50000: episode: 507, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   508/50000: episode: 508, duration: 0.005s, episode steps:   1, steps per second: 201, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 774.000 [774.000, 774.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   509/50000: episode: 509, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1105.000 [1105.000, 1105.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   510/50000: episode: 510, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   511/50000: episode: 511, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   512/50000: episode: 512, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 519.000 [519.000, 519.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   513/50000: episode: 513, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   514/50000: episode: 514, duration: 0.006s, episode steps:   1, steps per second: 161, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   515/50000: episode: 515, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1487.000 [1487.000, 1487.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   516/50000: episode: 516, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2235.000 [2235.000, 2235.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   517/50000: episode: 517, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   518/50000: episode: 518, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 307.000 [307.000, 307.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   519/50000: episode: 519, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1607.000 [1607.000, 1607.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   520/50000: episode: 520, duration: 0.006s, episode steps:   1, steps per second: 159, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   521/50000: episode: 521, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1845.000 [1845.000, 1845.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   522/50000: episode: 522, duration: 0.005s, episode steps:   1, steps per second: 183, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   523/50000: episode: 523, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   524/50000: episode: 524, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   525/50000: episode: 525, duration: 0.004s, episode steps:   1, steps per second: 245, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   526/50000: episode: 526, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   527/50000: episode: 527, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   528/50000: episode: 528, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   529/50000: episode: 529, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   530/50000: episode: 530, duration: 0.007s, episode steps:   1, steps per second: 140, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   531/50000: episode: 531, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   532/50000: episode: 532, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1137.000 [1137.000, 1137.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   533/50000: episode: 533, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   534/50000: episode: 534, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   535/50000: episode: 535, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   536/50000: episode: 536, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   537/50000: episode: 537, duration: 0.004s, episode steps:   1, steps per second: 231, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   538/50000: episode: 538, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   539/50000: episode: 539, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   540/50000: episode: 540, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   541/50000: episode: 541, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   542/50000: episode: 542, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   543/50000: episode: 543, duration: 0.007s, episode steps:   1, steps per second: 153, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   544/50000: episode: 544, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   545/50000: episode: 545, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   546/50000: episode: 546, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3387.000 [3387.000, 3387.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   547/50000: episode: 547, duration: 0.005s, episode steps:   1, steps per second: 183, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2603.000 [2603.000, 2603.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   548/50000: episode: 548, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   549/50000: episode: 549, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   550/50000: episode: 550, duration: 0.006s, episode steps:   1, steps per second: 181, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1231.000 [1231.000, 1231.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   551/50000: episode: 551, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   552/50000: episode: 552, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   553/50000: episode: 553, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   554/50000: episode: 554, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   555/50000: episode: 555, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   556/50000: episode: 556, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3308.000 [3308.000, 3308.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   557/50000: episode: 557, duration: 0.005s, episode steps:   1, steps per second: 187, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   558/50000: episode: 558, duration: 0.006s, episode steps:   1, steps per second: 162, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   559/50000: episode: 559, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   560/50000: episode: 560, duration: 0.003s, episode steps:   1, steps per second: 287, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   561/50000: episode: 561, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   562/50000: episode: 562, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   563/50000: episode: 563, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   564/50000: episode: 564, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2935.000 [2935.000, 2935.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   565/50000: episode: 565, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   566/50000: episode: 566, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 420.000 [420.000, 420.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   567/50000: episode: 567, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   568/50000: episode: 568, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   569/50000: episode: 569, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1471.000 [1471.000, 1471.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   570/50000: episode: 570, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   571/50000: episode: 571, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   572/50000: episode: 572, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 420.000 [420.000, 420.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   573/50000: episode: 573, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   574/50000: episode: 574, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   575/50000: episode: 575, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   576/50000: episode: 576, duration: 0.006s, episode steps:   1, steps per second: 170, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1636.000 [1636.000, 1636.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   577/50000: episode: 577, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   578/50000: episode: 578, duration: 0.006s, episode steps:   1, steps per second: 154, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   579/50000: episode: 579, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1440.000 [1440.000, 1440.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   580/50000: episode: 580, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3177.000 [3177.000, 3177.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   581/50000: episode: 581, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   582/50000: episode: 582, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   583/50000: episode: 583, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   584/50000: episode: 584, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 763.000 [763.000, 763.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   585/50000: episode: 585, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   586/50000: episode: 586, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   587/50000: episode: 587, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 57.000 [57.000, 57.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   588/50000: episode: 588, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   589/50000: episode: 589, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   590/50000: episode: 590, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 485.000 [485.000, 485.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   591/50000: episode: 591, duration: 0.004s, episode steps:   1, steps per second: 246, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   592/50000: episode: 592, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 726.000 [726.000, 726.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   593/50000: episode: 593, duration: 0.003s, episode steps:   1, steps per second: 313, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   594/50000: episode: 594, duration: 0.006s, episode steps:   1, steps per second: 157, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   595/50000: episode: 595, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   596/50000: episode: 596, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   597/50000: episode: 597, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   598/50000: episode: 598, duration: 0.007s, episode steps:   1, steps per second: 146, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2717.000 [2717.000, 2717.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   599/50000: episode: 599, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   600/50000: episode: 600, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   601/50000: episode: 601, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   602/50000: episode: 602, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   603/50000: episode: 603, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3085.000 [3085.000, 3085.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   604/50000: episode: 604, duration: 0.006s, episode steps:   1, steps per second: 158, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   605/50000: episode: 605, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 738.000 [738.000, 738.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   606/50000: episode: 606, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   607/50000: episode: 607, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2536.000 [2536.000, 2536.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   608/50000: episode: 608, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3447.000 [3447.000, 3447.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   609/50000: episode: 609, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1838.000 [1838.000, 1838.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   610/50000: episode: 610, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2295.000 [2295.000, 2295.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   611/50000: episode: 611, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2169.000 [2169.000, 2169.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   612/50000: episode: 612, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2350.000 [2350.000, 2350.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   613/50000: episode: 613, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   614/50000: episode: 614, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2874.000 [2874.000, 2874.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   615/50000: episode: 615, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2074.000 [2074.000, 2074.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   616/50000: episode: 616, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 710.000 [710.000, 710.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   617/50000: episode: 617, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   618/50000: episode: 618, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   619/50000: episode: 619, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   620/50000: episode: 620, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 986.000 [986.000, 986.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   621/50000: episode: 621, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   622/50000: episode: 622, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   623/50000: episode: 623, duration: 0.006s, episode steps:   1, steps per second: 168, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   624/50000: episode: 624, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   625/50000: episode: 625, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   626/50000: episode: 626, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   627/50000: episode: 627, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   628/50000: episode: 628, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 736.000 [736.000, 736.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   629/50000: episode: 629, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2274.000 [2274.000, 2274.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   630/50000: episode: 630, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4005.000 [4005.000, 4005.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   631/50000: episode: 631, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3590.000 [3590.000, 3590.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   632/50000: episode: 632, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1703.000 [1703.000, 1703.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   633/50000: episode: 633, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 278.000 [278.000, 278.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   634/50000: episode: 634, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   635/50000: episode: 635, duration: 0.006s, episode steps:   1, steps per second: 162, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   636/50000: episode: 636, duration: 0.005s, episode steps:   1, steps per second: 216, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2579.000 [2579.000, 2579.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   637/50000: episode: 637, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   638/50000: episode: 638, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3528.000 [3528.000, 3528.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   639/50000: episode: 639, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2748.000 [2748.000, 2748.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   640/50000: episode: 640, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   641/50000: episode: 641, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2295.000 [2295.000, 2295.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   642/50000: episode: 642, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   643/50000: episode: 643, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2502.000 [2502.000, 2502.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   644/50000: episode: 644, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3766.000 [3766.000, 3766.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   645/50000: episode: 645, duration: 0.006s, episode steps:   1, steps per second: 175, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2442.000 [2442.000, 2442.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   646/50000: episode: 646, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   647/50000: episode: 647, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3382.000 [3382.000, 3382.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   648/50000: episode: 648, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   649/50000: episode: 649, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 420.000 [420.000, 420.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   650/50000: episode: 650, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   651/50000: episode: 651, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3612.000 [3612.000, 3612.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   652/50000: episode: 652, duration: 0.008s, episode steps:   1, steps per second: 132, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 326.000 [326.000, 326.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   653/50000: episode: 653, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3951.000 [3951.000, 3951.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   654/50000: episode: 654, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   655/50000: episode: 655, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   656/50000: episode: 656, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   657/50000: episode: 657, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 121.000 [121.000, 121.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   658/50000: episode: 658, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   659/50000: episode: 659, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   660/50000: episode: 660, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 813.000 [813.000, 813.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   661/50000: episode: 661, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   662/50000: episode: 662, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 276.000 [276.000, 276.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   663/50000: episode: 663, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   664/50000: episode: 664, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   665/50000: episode: 665, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   666/50000: episode: 666, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2827.000 [2827.000, 2827.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   667/50000: episode: 667, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   668/50000: episode: 668, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   669/50000: episode: 669, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   670/50000: episode: 670, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 116.000 [116.000, 116.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   671/50000: episode: 671, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   672/50000: episode: 672, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3570.000 [3570.000, 3570.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   673/50000: episode: 673, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   674/50000: episode: 674, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   675/50000: episode: 675, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 735.000 [735.000, 735.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   676/50000: episode: 676, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   677/50000: episode: 677, duration: 0.007s, episode steps:   1, steps per second: 146, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   678/50000: episode: 678, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   679/50000: episode: 679, duration: 0.006s, episode steps:   1, steps per second: 175, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   680/50000: episode: 680, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   681/50000: episode: 681, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 706.000 [706.000, 706.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   682/50000: episode: 682, duration: 0.006s, episode steps:   1, steps per second: 178, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   683/50000: episode: 683, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   684/50000: episode: 684, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   685/50000: episode: 685, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   686/50000: episode: 686, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 841.000 [841.000, 841.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   687/50000: episode: 687, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1723.000 [1723.000, 1723.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   688/50000: episode: 688, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 420.000 [420.000, 420.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   689/50000: episode: 689, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   690/50000: episode: 690, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   691/50000: episode: 691, duration: 0.027s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   692/50000: episode: 692, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   693/50000: episode: 693, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   694/50000: episode: 694, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   695/50000: episode: 695, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   696/50000: episode: 696, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   697/50000: episode: 697, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   698/50000: episode: 698, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   699/50000: episode: 699, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1707.000 [1707.000, 1707.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   700/50000: episode: 700, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1667.000 [1667.000, 1667.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   701/50000: episode: 701, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   702/50000: episode: 702, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   703/50000: episode: 703, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   704/50000: episode: 704, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3962.000 [3962.000, 3962.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   705/50000: episode: 705, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   706/50000: episode: 706, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   707/50000: episode: 707, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   708/50000: episode: 708, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   709/50000: episode: 709, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3556.000 [3556.000, 3556.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   710/50000: episode: 710, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   711/50000: episode: 711, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   712/50000: episode: 712, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   713/50000: episode: 713, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   714/50000: episode: 714, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   715/50000: episode: 715, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   716/50000: episode: 716, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   717/50000: episode: 717, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   718/50000: episode: 718, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   719/50000: episode: 719, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   720/50000: episode: 720, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   721/50000: episode: 721, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1385.000 [1385.000, 1385.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   722/50000: episode: 722, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   723/50000: episode: 723, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   724/50000: episode: 724, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1608.000 [1608.000, 1608.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   725/50000: episode: 725, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   726/50000: episode: 726, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2492.000 [2492.000, 2492.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   727/50000: episode: 727, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   728/50000: episode: 728, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   729/50000: episode: 729, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   730/50000: episode: 730, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   731/50000: episode: 731, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3129.000 [3129.000, 3129.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   732/50000: episode: 732, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1552.000 [1552.000, 1552.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   733/50000: episode: 733, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   734/50000: episode: 734, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   735/50000: episode: 735, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   736/50000: episode: 736, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1926.000 [1926.000, 1926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   737/50000: episode: 737, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 781.000 [781.000, 781.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   738/50000: episode: 738, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   739/50000: episode: 739, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   740/50000: episode: 740, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   741/50000: episode: 741, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3216.000 [3216.000, 3216.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   742/50000: episode: 742, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 975.000 [975.000, 975.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   743/50000: episode: 743, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   744/50000: episode: 744, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   745/50000: episode: 745, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   746/50000: episode: 746, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   747/50000: episode: 747, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   748/50000: episode: 748, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   749/50000: episode: 749, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2797.000 [2797.000, 2797.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   750/50000: episode: 750, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   751/50000: episode: 751, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2937.000 [2937.000, 2937.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   752/50000: episode: 752, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   753/50000: episode: 753, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   754/50000: episode: 754, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   755/50000: episode: 755, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   756/50000: episode: 756, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   757/50000: episode: 757, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3643.000 [3643.000, 3643.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   758/50000: episode: 758, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1376.000 [1376.000, 1376.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   759/50000: episode: 759, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   760/50000: episode: 760, duration: 0.027s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   761/50000: episode: 761, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   762/50000: episode: 762, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   763/50000: episode: 763, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   764/50000: episode: 764, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 492.000 [492.000, 492.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   765/50000: episode: 765, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2168.000 [2168.000, 2168.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   766/50000: episode: 766, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   767/50000: episode: 767, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   768/50000: episode: 768, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   769/50000: episode: 769, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   770/50000: episode: 770, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   771/50000: episode: 771, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   772/50000: episode: 772, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   773/50000: episode: 773, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1835.000 [1835.000, 1835.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   774/50000: episode: 774, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1338.000 [1338.000, 1338.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   775/50000: episode: 775, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   776/50000: episode: 776, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   777/50000: episode: 777, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   778/50000: episode: 778, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   779/50000: episode: 779, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4012.000 [4012.000, 4012.000],  loss: --, mae: --, mean_q: --\n",
            "wrong_move\n",
            "   780/50000: episode: 780, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 74.000 [74.000, 74.000],  loss: --, mae: --, mean_q: --\n"
          ]
        }
      ],
      "source": [
        "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
        "# even the metrics!\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "for i in range (10):\n",
        "  policy = EpsGreedyQPolicy(0.01)\n",
        "  dqn = DQNAgent(model=model, nb_actions=NB_ACTIONS, memory=memory,\n",
        "                target_model_update=1e-2, policy=policy)\n",
        "  dqn.compile(Adam(lr=1e-4), metrics=['mae'])\n",
        "\n",
        "  # Okay, now it's time to learn something! We visualize the training here for show, but this\n",
        "  # slows down training quite a lot. You can always safely abort the training prematurely using\n",
        "  # Ctrl + C.\n",
        "  his = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
        "  \n",
        "  model.save('chess_model.h5')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "rl_dqn_vs_sf_train.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
