{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install keras-rl2\n",
    "# ! pip install chess\n",
    "# ! pip install python-chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 09:03:52.705863: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-14 09:03:52.705970: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten,\\\n",
    "     Input,BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# import gym_chess\n",
    "\n",
    "import chess\n",
    "from sys import platform\n",
    "import os\n",
    "import chess.engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3963662281121590518\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 09:03:56.648040: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-14 09:03:56.658019: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-14 09:03:56.658073: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-14 09:03:56.658117: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LoG): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chmod: cannot access './algorithms/stockfish/stockfish_14.1_linux_x64': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    os.system('chmod +x ./algorithms/stockfish/stockfish_14.1_linux_x64')\n",
    "    engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish_14.1_linux_x64\")\n",
    "elif platform == \"win32\":\n",
    "    engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish_14.1_win_32bit.exe\")\n",
    "# elif platform == \"win64\":\n",
    "#     engine = chess.engine.SimpleEngine.popen_uci(r\"./algorithms/stockfish/stockfish_14.1_win_x64.exe\")\n",
    "# elif platform == \"darwin\":\n",
    "#     try:\n",
    "#         engine = chess.engine.SimpleEngine.popen_uci(r\"./algorithms/stockfish/stockfish-arm64\")\n",
    "#     except:\n",
    "#         try: \n",
    "#             engine = chess.engine.SimpleEngine.popen_uci(r\"./algorithms/stockfish/stockfish-x86-64-bmi2\")\n",
    "#         except:\n",
    "#             print(\"Don't support this OS for level 3\")\n",
    "# else:\n",
    "#     print(\"Don't support this OS for level 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_move(board):\n",
    "    result = engine.play(board.board, chess.engine.Limit(time=1))\n",
    "    return result.move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SHAPE = (65, )\n",
    "NB_ACTIONS = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessEnv:\n",
    "    '''\n",
    "    state - obser: ndarray - (65,): [:65] is flatten from int_board; [65] is color of bot; 1 is white and -1 is black\n",
    "    step: int. step_range = (0, 4096) , is encoded from square A to square B (64 x 64 val)\n",
    "    reward: int\n",
    "    '''\n",
    "\n",
    "    mapped = {\n",
    "            'P': 10,     # White Pawn\n",
    "            'p': -10,    # Black Pawn\n",
    "            'N': 20,     # White Knight\n",
    "            'n': -20,    # Black Knight\n",
    "            'B': 30,     # White Bishop\n",
    "            'b': -30,    # Black Bishop\n",
    "            'R': 40,     # White Rook\n",
    "            'r': -40,    # Black Rook\n",
    "            'Q': 50,     # White Queen\n",
    "            'q': -50,    # Black Queen\n",
    "            'K': 900,     # White King\n",
    "            'k': -900     # Black King\n",
    "    }\n",
    "    # state_shape = (8, 8)\n",
    "    # nb_actions = 4096\n",
    "    model = None\n",
    "    \n",
    "    def __init__(self, model: Sequential, neg_r_each_step = -1) -> None:\n",
    "        self.env = chess.Board()\n",
    "        self.state = self.reset()\n",
    "        # [-1] = 1 -> white, -1 -> black\n",
    "        self.bot_color = self.env.turn * 2 - 1\n",
    "        self.neg_r_each_step = neg_r_each_step\n",
    "        self.model = model\n",
    "\n",
    "    def is_draw(self):\n",
    "        if self.env.is_stalemate():\n",
    "            print(\"statlemate\")\n",
    "            return True\n",
    "        if self.env.is_fivefold_repetition():\n",
    "            print(\"fivefold repetition\")\n",
    "            return True\n",
    "        if self.env.is_seventyfive_moves():\n",
    "            print(\"75 moves\")\n",
    "            return True\n",
    "        if self.env.is_insufficient_material():\n",
    "            print(\"Insufficient Material\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_checkmate(self):\n",
    "        # If There is checkmate then it will be TRUE else FALSE.It will be a boolean value.\n",
    "        return self.env.is_checkmate()\n",
    "\n",
    "    def convert_board_to_int(self):\n",
    "        epd_string = self.env.epd()\n",
    "        list_int = np.empty((0, ))\n",
    "        for i in epd_string:\n",
    "            if i == \" \":\n",
    "                list_int = list_int.reshape((8, 8))\n",
    "                return list_int\n",
    "            elif i != \"/\":\n",
    "                if i in self.mapped:\n",
    "                    list_int = np.append(list_int, self.mapped[i])\n",
    "                else:\n",
    "                    for counter in range(0, int(i)):\n",
    "                        list_int = np.append(list_int, 0)\n",
    "        list_int = list_int.reshape((8, 8))\n",
    "        return list_int\n",
    "\n",
    "    def get_state(self) -> np.ndarray:\n",
    "        return np.append(self.convert_board_to_int().reshape(64,), self.env.turn * 2 - 1)\n",
    "\n",
    "    def legal_moves(self):\n",
    "        return list(self.env.legal_moves)\n",
    "\n",
    "    def encodeMove(self, move_uci:str):\n",
    "        if len(move_uci) != 4:\n",
    "            raise ValueError()\n",
    "        a, b = chess.parse_square(move_uci[:2]), chess.parse_square(move_uci[2:])\n",
    "        return a * 64 + b\n",
    "\n",
    "    def decodeMove(self, move_int:int):\n",
    "        a, b = move_int//64, move_int%64\n",
    "        # a, b = chess.square_name(a), chess.square_name(b)\n",
    "\n",
    "        move = self.env.find_move(from_square= a,to_square= b)\n",
    "        return move\n",
    "\n",
    "    def render(self):\n",
    "        print(self.env.unicode())\n",
    "\n",
    "    def reset(self):\n",
    "        # random state\n",
    "        redo = True\n",
    "        num_sample_steps = 0\n",
    "        while redo:\n",
    "            redo = False\n",
    "            self.env = chess.Board()\n",
    "            num_sample_steps = np.random.randint(0, 50)\n",
    "            for i in range (num_sample_steps):\n",
    "                lg_move = self.legal_moves()\n",
    "                if len(lg_move) != 0:\n",
    "                    move = np.random.choice(self.legal_moves())\n",
    "                    self.env.push(move)\n",
    "                else:\n",
    "                    redo = True\n",
    "                    break\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action: int):\n",
    "        reward = 0\n",
    "        done = True\n",
    "\n",
    "        try:\n",
    "            # move in legal move\n",
    "            move = self.decodeMove(action)\n",
    "\n",
    "            # neg reward each step\n",
    "            reward = self.neg_r_each_step\n",
    "\n",
    "            # location to_square\n",
    "            to_r, to_c = move.to_square//8, move.to_square%8\n",
    "            reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
    "\n",
    "            # action\n",
    "            self.env.push(move)\n",
    "            self.state = self.get_state()\n",
    "\n",
    "            # check end game\n",
    "            if self.is_checkmate():\n",
    "                reward += self.mapped['K']\n",
    "                done = True\n",
    "            elif self.is_draw():\n",
    "                reward += 300\n",
    "                done = True\n",
    "\n",
    "            # opponent's turn   \n",
    "            else:\n",
    "                move = find_move(self.env)\n",
    "\n",
    "                # location to_square\n",
    "                to_r, to_c = move.to_square//8, move.to_square%8\n",
    "                reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
    "\n",
    "                # action\n",
    "                self.env.push(move)\n",
    "                self.state = self.get_state()\n",
    "\n",
    "                # check end game\n",
    "                if self.is_checkmate():\n",
    "                    reward -= self.mapped['K']\n",
    "                    done = True\n",
    "                elif self.is_draw():\n",
    "                    reward += 300\n",
    "                    done = True\n",
    "        except:\n",
    "            # wrong move\n",
    "            reward = -5000\n",
    "            done = True\n",
    "            print('wrong_move')\n",
    "\n",
    "        return self.state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "WARNING:tensorflow:From /home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/layers/normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 65)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8448      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              528384    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4096)              0         \n",
      "=================================================================\n",
      "Total params: 554,368\n",
      "Trainable params: 553,856\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = Sequential()\n",
    "model.add(Input((1, ) + STATE_SHAPE))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(NB_ACTIONS))\n",
    "model.add(Activation('linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChessEnv(model, neg_r_each_step=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('chess_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "2021-12-14 09:04:16.675416: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 1999965000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong_move\n",
      "     1/50000: episode: 1, duration: 0.360s, episode steps:   1, steps per second:   3, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     2/50000: episode: 2, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3098.000 [3098.000, 3098.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     3/50000: episode: 3, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2996.000 [2996.000, 2996.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     4/50000: episode: 4, duration: 0.008s, episode steps:   1, steps per second: 132, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     5/50000: episode: 5, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     6/50000: episode: 6, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     7/50000: episode: 7, duration: 0.004s, episode steps:   1, steps per second: 251, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     8/50000: episode: 8, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     9/50000: episode: 9, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    10/50000: episode: 10, duration: 0.005s, episode steps:   1, steps per second: 194, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    11/50000: episode: 11, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    12/50000: episode: 12, duration: 0.006s, episode steps:   1, steps per second: 161, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    13/50000: episode: 13, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    14/50000: episode: 14, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 249.000 [249.000, 249.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    15/50000: episode: 15, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3623.000 [3623.000, 3623.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    16/50000: episode: 16, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    17/50000: episode: 17, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1517.000 [1517.000, 1517.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    18/50000: episode: 18, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3855.000 [3855.000, 3855.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    19/50000: episode: 19, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1890.000 [1890.000, 1890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    20/50000: episode: 20, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    21/50000: episode: 21, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    22/50000: episode: 22, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    23/50000: episode: 23, duration: 0.010s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    24/50000: episode: 24, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2734.000 [2734.000, 2734.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    25/50000: episode: 25, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    26/50000: episode: 26, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    27/50000: episode: 27, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2059.000 [2059.000, 2059.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    28/50000: episode: 28, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    29/50000: episode: 29, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    30/50000: episode: 30, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4032.000 [4032.000, 4032.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    31/50000: episode: 31, duration: 0.005s, episode steps:   1, steps per second: 208, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    32/50000: episode: 32, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    33/50000: episode: 33, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 446.000 [446.000, 446.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    34/50000: episode: 34, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    35/50000: episode: 35, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3212.000 [3212.000, 3212.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    36/50000: episode: 36, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 820.000 [820.000, 820.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    37/50000: episode: 37, duration: 0.005s, episode steps:   1, steps per second: 196, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    38/50000: episode: 38, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    39/50000: episode: 39, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    40/50000: episode: 40, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3648.000 [3648.000, 3648.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    41/50000: episode: 41, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    42/50000: episode: 42, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    43/50000: episode: 43, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    44/50000: episode: 44, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 134.000 [134.000, 134.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    45/50000: episode: 45, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    46/50000: episode: 46, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    47/50000: episode: 47, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    48/50000: episode: 48, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    49/50000: episode: 49, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    50/50000: episode: 50, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    51/50000: episode: 51, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    52/50000: episode: 52, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    53/50000: episode: 53, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1736.000 [1736.000, 1736.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    54/50000: episode: 54, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    55/50000: episode: 55, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    56/50000: episode: 56, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2146.000 [2146.000, 2146.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    57/50000: episode: 57, duration: 0.006s, episode steps:   1, steps per second: 176, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    58/50000: episode: 58, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    59/50000: episode: 59, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2773.000 [2773.000, 2773.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    60/50000: episode: 60, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2146.000 [2146.000, 2146.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    61/50000: episode: 61, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    62/50000: episode: 62, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    63/50000: episode: 63, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    64/50000: episode: 64, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    65/50000: episode: 65, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    66/50000: episode: 66, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2108.000 [2108.000, 2108.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    67/50000: episode: 67, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 429.000 [429.000, 429.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    68/50000: episode: 68, duration: 0.005s, episode steps:   1, steps per second: 191, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    69/50000: episode: 69, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    70/50000: episode: 70, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    71/50000: episode: 71, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    72/50000: episode: 72, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3017.000 [3017.000, 3017.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    73/50000: episode: 73, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2622.000 [2622.000, 2622.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    74/50000: episode: 74, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    75/50000: episode: 75, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2429.000 [2429.000, 2429.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    76/50000: episode: 76, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2590.000 [2590.000, 2590.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    77/50000: episode: 77, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1845.000 [1845.000, 1845.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    78/50000: episode: 78, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    79/50000: episode: 79, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    80/50000: episode: 80, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    81/50000: episode: 81, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2246.000 [2246.000, 2246.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    82/50000: episode: 82, duration: 0.007s, episode steps:   1, steps per second: 134, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    83/50000: episode: 83, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    84/50000: episode: 84, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    85/50000: episode: 85, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    86/50000: episode: 86, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    87/50000: episode: 87, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 544.000 [544.000, 544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    88/50000: episode: 88, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    89/50000: episode: 89, duration: 0.007s, episode steps:   1, steps per second: 138, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    90/50000: episode: 90, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    91/50000: episode: 91, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    92/50000: episode: 92, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    93/50000: episode: 93, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    94/50000: episode: 94, duration: 0.006s, episode steps:   1, steps per second: 181, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    95/50000: episode: 95, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 235.000 [235.000, 235.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    96/50000: episode: 96, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    97/50000: episode: 97, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    98/50000: episode: 98, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2286.000 [2286.000, 2286.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    99/50000: episode: 99, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   100/50000: episode: 100, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1935.000 [1935.000, 1935.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   101/50000: episode: 101, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   102/50000: episode: 102, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   103/50000: episode: 103, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3610.000 [3610.000, 3610.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   104/50000: episode: 104, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3922.000 [3922.000, 3922.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   105/50000: episode: 105, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 235.000 [235.000, 235.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   106/50000: episode: 106, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2322.000 [2322.000, 2322.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   107/50000: episode: 107, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   108/50000: episode: 108, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   109/50000: episode: 109, duration: 0.006s, episode steps:   1, steps per second: 170, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   110/50000: episode: 110, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   111/50000: episode: 111, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   112/50000: episode: 112, duration: 0.027s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   113/50000: episode: 113, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   114/50000: episode: 114, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   115/50000: episode: 115, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2322.000 [2322.000, 2322.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   116/50000: episode: 116, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   117/50000: episode: 117, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1665.000 [1665.000, 1665.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   118/50000: episode: 118, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1898.000 [1898.000, 1898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   119/50000: episode: 119, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   120/50000: episode: 120, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   121/50000: episode: 121, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 852.000 [852.000, 852.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   122/50000: episode: 122, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3098.000 [3098.000, 3098.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   123/50000: episode: 123, duration: 0.007s, episode steps:   1, steps per second: 134, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   124/50000: episode: 124, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2541.000 [2541.000, 2541.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   125/50000: episode: 125, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   126/50000: episode: 126, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   127/50000: episode: 127, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   128/50000: episode: 128, duration: 0.006s, episode steps:   1, steps per second: 167, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   129/50000: episode: 129, duration: 0.005s, episode steps:   1, steps per second: 185, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   130/50000: episode: 130, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   131/50000: episode: 131, duration: 0.005s, episode steps:   1, steps per second: 204, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   132/50000: episode: 132, duration: 0.004s, episode steps:   1, steps per second: 227, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   133/50000: episode: 133, duration: 0.004s, episode steps:   1, steps per second: 254, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   134/50000: episode: 134, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   135/50000: episode: 135, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   136/50000: episode: 136, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   137/50000: episode: 137, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   138/50000: episode: 138, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1856.000 [1856.000, 1856.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   139/50000: episode: 139, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 310.000 [310.000, 310.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   140/50000: episode: 140, duration: 0.005s, episode steps:   1, steps per second: 195, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   141/50000: episode: 141, duration: 0.006s, episode steps:   1, steps per second: 164, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   142/50000: episode: 142, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   143/50000: episode: 143, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   144/50000: episode: 144, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   145/50000: episode: 145, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   146/50000: episode: 146, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   147/50000: episode: 147, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 873.000 [873.000, 873.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   148/50000: episode: 148, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   149/50000: episode: 149, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   150/50000: episode: 150, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   151/50000: episode: 151, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2322.000 [2322.000, 2322.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   152/50000: episode: 152, duration: 0.005s, episode steps:   1, steps per second: 208, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   153/50000: episode: 153, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3263.000 [3263.000, 3263.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   154/50000: episode: 154, duration: 0.006s, episode steps:   1, steps per second: 171, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   155/50000: episode: 155, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   156/50000: episode: 156, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   157/50000: episode: 157, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   158/50000: episode: 158, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   159/50000: episode: 159, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   160/50000: episode: 160, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   161/50000: episode: 161, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   162/50000: episode: 162, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2148.000 [2148.000, 2148.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   163/50000: episode: 163, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1883.000 [1883.000, 1883.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   164/50000: episode: 164, duration: 0.004s, episode steps:   1, steps per second: 268, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   165/50000: episode: 165, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   166/50000: episode: 166, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3589.000 [3589.000, 3589.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   167/50000: episode: 167, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   168/50000: episode: 168, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3235.000 [3235.000, 3235.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   169/50000: episode: 169, duration: 0.017s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   170/50000: episode: 170, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   171/50000: episode: 171, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 503.000 [503.000, 503.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   172/50000: episode: 172, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   173/50000: episode: 173, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   174/50000: episode: 174, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   175/50000: episode: 175, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   176/50000: episode: 176, duration: 0.005s, episode steps:   1, steps per second: 185, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   177/50000: episode: 177, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   178/50000: episode: 178, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   179/50000: episode: 179, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   180/50000: episode: 180, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3424.000 [3424.000, 3424.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   181/50000: episode: 181, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   182/50000: episode: 182, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   183/50000: episode: 183, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   184/50000: episode: 184, duration: 0.006s, episode steps:   1, steps per second: 174, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1858.000 [1858.000, 1858.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   185/50000: episode: 185, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   186/50000: episode: 186, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   187/50000: episode: 187, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   188/50000: episode: 188, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   189/50000: episode: 189, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   190/50000: episode: 190, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   191/50000: episode: 191, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   192/50000: episode: 192, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 235.000 [235.000, 235.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   193/50000: episode: 193, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   194/50000: episode: 194, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   195/50000: episode: 195, duration: 0.006s, episode steps:   1, steps per second: 160, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   196/50000: episode: 196, duration: 0.005s, episode steps:   1, steps per second: 212, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   197/50000: episode: 197, duration: 0.004s, episode steps:   1, steps per second: 239, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2246.000 [2246.000, 2246.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   198/50000: episode: 198, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   199/50000: episode: 199, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3701.000 [3701.000, 3701.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   200/50000: episode: 200, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   201/50000: episode: 201, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 381.000 [381.000, 381.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   202/50000: episode: 202, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   203/50000: episode: 203, duration: 0.004s, episode steps:   1, steps per second: 234, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   204/50000: episode: 204, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   205/50000: episode: 205, duration: 0.004s, episode steps:   1, steps per second: 233, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 853.000 [853.000, 853.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   206/50000: episode: 206, duration: 0.004s, episode steps:   1, steps per second: 252, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2961.000 [2961.000, 2961.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   207/50000: episode: 207, duration: 0.003s, episode steps:   1, steps per second: 305, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   208/50000: episode: 208, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   209/50000: episode: 209, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   210/50000: episode: 210, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3669.000 [3669.000, 3669.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   211/50000: episode: 211, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   212/50000: episode: 212, duration: 0.006s, episode steps:   1, steps per second: 156, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   213/50000: episode: 213, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4030.000 [4030.000, 4030.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   214/50000: episode: 214, duration: 0.006s, episode steps:   1, steps per second: 175, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   215/50000: episode: 215, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3453.000 [3453.000, 3453.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   216/50000: episode: 216, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1856.000 [1856.000, 1856.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   217/50000: episode: 217, duration: 0.006s, episode steps:   1, steps per second: 174, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   218/50000: episode: 218, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   219/50000: episode: 219, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   220/50000: episode: 220, duration: 0.006s, episode steps:   1, steps per second: 167, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   221/50000: episode: 221, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2322.000 [2322.000, 2322.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   222/50000: episode: 222, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   223/50000: episode: 223, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 235.000 [235.000, 235.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   224/50000: episode: 224, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2059.000 [2059.000, 2059.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   225/50000: episode: 225, duration: 0.005s, episode steps:   1, steps per second: 211, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   226/50000: episode: 226, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   227/50000: episode: 227, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   228/50000: episode: 228, duration: 0.007s, episode steps:   1, steps per second: 147, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   229/50000: episode: 229, duration: 0.004s, episode steps:   1, steps per second: 252, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   230/50000: episode: 230, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   231/50000: episode: 231, duration: 0.005s, episode steps:   1, steps per second: 210, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   232/50000: episode: 232, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2322.000 [2322.000, 2322.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   233/50000: episode: 233, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1173.000 [1173.000, 1173.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   234/50000: episode: 234, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   235/50000: episode: 235, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1301.000 [1301.000, 1301.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   236/50000: episode: 236, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   237/50000: episode: 237, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   238/50000: episode: 238, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   239/50000: episode: 239, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   240/50000: episode: 240, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   241/50000: episode: 241, duration: 0.006s, episode steps:   1, steps per second: 160, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   242/50000: episode: 242, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   243/50000: episode: 243, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   244/50000: episode: 244, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   245/50000: episode: 245, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   246/50000: episode: 246, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 91.000 [91.000, 91.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   247/50000: episode: 247, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   248/50000: episode: 248, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   249/50000: episode: 249, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   250/50000: episode: 250, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   251/50000: episode: 251, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   252/50000: episode: 252, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   253/50000: episode: 253, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   254/50000: episode: 254, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   255/50000: episode: 255, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   256/50000: episode: 256, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3424.000 [3424.000, 3424.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   257/50000: episode: 257, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   258/50000: episode: 258, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   259/50000: episode: 259, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2773.000 [2773.000, 2773.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   260/50000: episode: 260, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   261/50000: episode: 261, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1147.000 [1147.000, 1147.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   262/50000: episode: 262, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   263/50000: episode: 263, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   264/50000: episode: 264, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   265/50000: episode: 265, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   266/50000: episode: 266, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3592.000 [3592.000, 3592.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   267/50000: episode: 267, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   268/50000: episode: 268, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   269/50000: episode: 269, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   270/50000: episode: 270, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   271/50000: episode: 271, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3205.000 [3205.000, 3205.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   272/50000: episode: 272, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   273/50000: episode: 273, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   274/50000: episode: 274, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   275/50000: episode: 275, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   276/50000: episode: 276, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   277/50000: episode: 277, duration: 0.007s, episode steps:   1, steps per second: 140, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   278/50000: episode: 278, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2349.000 [2349.000, 2349.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   279/50000: episode: 279, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2228.000 [2228.000, 2228.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   280/50000: episode: 280, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 873.000 [873.000, 873.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   281/50000: episode: 281, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   282/50000: episode: 282, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   283/50000: episode: 283, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   284/50000: episode: 284, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   285/50000: episode: 285, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   286/50000: episode: 286, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3389.000 [3389.000, 3389.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   287/50000: episode: 287, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   288/50000: episode: 288, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   289/50000: episode: 289, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   290/50000: episode: 290, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   291/50000: episode: 291, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   292/50000: episode: 292, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   293/50000: episode: 293, duration: 0.007s, episode steps:   1, steps per second: 147, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   294/50000: episode: 294, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   295/50000: episode: 295, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3220.000 [3220.000, 3220.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   296/50000: episode: 296, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1592.000 [1592.000, 1592.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   297/50000: episode: 297, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3419.000 [3419.000, 3419.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   298/50000: episode: 298, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1147.000 [1147.000, 1147.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   299/50000: episode: 299, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   300/50000: episode: 300, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   301/50000: episode: 301, duration: 0.006s, episode steps:   1, steps per second: 169, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   302/50000: episode: 302, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   303/50000: episode: 303, duration: 0.006s, episode steps:   1, steps per second: 162, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   304/50000: episode: 304, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   305/50000: episode: 305, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 381.000 [381.000, 381.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   306/50000: episode: 306, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3589.000 [3589.000, 3589.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   307/50000: episode: 307, duration: 0.007s, episode steps:   1, steps per second: 140, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1150.000 [1150.000, 1150.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   308/50000: episode: 308, duration: 0.004s, episode steps:   1, steps per second: 281, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   309/50000: episode: 309, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   310/50000: episode: 310, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   311/50000: episode: 311, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   312/50000: episode: 312, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   313/50000: episode: 313, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   314/50000: episode: 314, duration: 0.003s, episode steps:   1, steps per second: 358, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   315/50000: episode: 315, duration: 0.003s, episode steps:   1, steps per second: 356, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   316/50000: episode: 316, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   317/50000: episode: 317, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3993.000 [3993.000, 3993.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   318/50000: episode: 318, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   319/50000: episode: 319, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   320/50000: episode: 320, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   321/50000: episode: 321, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2981.000 [2981.000, 2981.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   322/50000: episode: 322, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   323/50000: episode: 323, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 235.000 [235.000, 235.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   324/50000: episode: 324, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   325/50000: episode: 325, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   326/50000: episode: 326, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   327/50000: episode: 327, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   328/50000: episode: 328, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   329/50000: episode: 329, duration: 0.004s, episode steps:   1, steps per second: 249, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   330/50000: episode: 330, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1856.000 [1856.000, 1856.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   331/50000: episode: 331, duration: 0.008s, episode steps:   1, steps per second: 133, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   332/50000: episode: 332, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3630.000 [3630.000, 3630.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   333/50000: episode: 333, duration: 0.004s, episode steps:   1, steps per second: 226, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1414.000 [1414.000, 1414.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   334/50000: episode: 334, duration: 0.006s, episode steps:   1, steps per second: 167, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   335/50000: episode: 335, duration: 0.003s, episode steps:   1, steps per second: 304, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1345.000 [1345.000, 1345.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   336/50000: episode: 336, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   337/50000: episode: 337, duration: 0.004s, episode steps:   1, steps per second: 229, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   338/50000: episode: 338, duration: 0.004s, episode steps:   1, steps per second: 270, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   339/50000: episode: 339, duration: 0.004s, episode steps:   1, steps per second: 233, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   340/50000: episode: 340, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3922.000 [3922.000, 3922.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   341/50000: episode: 341, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   342/50000: episode: 342, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1856.000 [1856.000, 1856.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   343/50000: episode: 343, duration: 0.005s, episode steps:   1, steps per second: 212, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   344/50000: episode: 344, duration: 0.006s, episode steps:   1, steps per second: 163, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   345/50000: episode: 345, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3942.000 [3942.000, 3942.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   346/50000: episode: 346, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   347/50000: episode: 347, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2608.000 [2608.000, 2608.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   348/50000: episode: 348, duration: 0.009s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1150.000 [1150.000, 1150.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   349/50000: episode: 349, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   350/50000: episode: 350, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2748.000 [2748.000, 2748.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   351/50000: episode: 351, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   352/50000: episode: 352, duration: 0.010s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   353/50000: episode: 353, duration: 0.004s, episode steps:   1, steps per second: 279, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2620.000 [2620.000, 2620.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   354/50000: episode: 354, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   355/50000: episode: 355, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2246.000 [2246.000, 2246.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   356/50000: episode: 356, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   357/50000: episode: 357, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   358/50000: episode: 358, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   359/50000: episode: 359, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   360/50000: episode: 360, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2889.000 [2889.000, 2889.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   361/50000: episode: 361, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   362/50000: episode: 362, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   363/50000: episode: 363, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   364/50000: episode: 364, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   365/50000: episode: 365, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   366/50000: episode: 366, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 232.000 [232.000, 232.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   367/50000: episode: 367, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   368/50000: episode: 368, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   369/50000: episode: 369, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   370/50000: episode: 370, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3098.000 [3098.000, 3098.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   371/50000: episode: 371, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   372/50000: episode: 372, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   373/50000: episode: 373, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3740.000 [3740.000, 3740.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   374/50000: episode: 374, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   375/50000: episode: 375, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   376/50000: episode: 376, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   377/50000: episode: 377, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   378/50000: episode: 378, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   379/50000: episode: 379, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3205.000 [3205.000, 3205.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   380/50000: episode: 380, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   381/50000: episode: 381, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   382/50000: episode: 382, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   383/50000: episode: 383, duration: 0.005s, episode steps:   1, steps per second: 222, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   384/50000: episode: 384, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   385/50000: episode: 385, duration: 0.006s, episode steps:   1, steps per second: 167, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   386/50000: episode: 386, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   387/50000: episode: 387, duration: 0.007s, episode steps:   1, steps per second: 146, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   388/50000: episode: 388, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   389/50000: episode: 389, duration: 0.003s, episode steps:   1, steps per second: 350, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   390/50000: episode: 390, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2146.000 [2146.000, 2146.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   391/50000: episode: 391, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   392/50000: episode: 392, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   393/50000: episode: 393, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   394/50000: episode: 394, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   395/50000: episode: 395, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   396/50000: episode: 396, duration: 0.005s, episode steps:   1, steps per second: 213, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   397/50000: episode: 397, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   398/50000: episode: 398, duration: 0.005s, episode steps:   1, steps per second: 208, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   399/50000: episode: 399, duration: 0.004s, episode steps:   1, steps per second: 232, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   400/50000: episode: 400, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   401/50000: episode: 401, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3922.000 [3922.000, 3922.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   402/50000: episode: 402, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1035.000 [1035.000, 1035.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   403/50000: episode: 403, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   404/50000: episode: 404, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   405/50000: episode: 405, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   406/50000: episode: 406, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   407/50000: episode: 407, duration: 0.005s, episode steps:   1, steps per second: 205, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   408/50000: episode: 408, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   409/50000: episode: 409, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   410/50000: episode: 410, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   411/50000: episode: 411, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 873.000 [873.000, 873.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   412/50000: episode: 412, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   413/50000: episode: 413, duration: 0.004s, episode steps:   1, steps per second: 226, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   414/50000: episode: 414, duration: 0.017s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   415/50000: episode: 415, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   416/50000: episode: 416, duration: 0.005s, episode steps:   1, steps per second: 200, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   417/50000: episode: 417, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 580.000 [580.000, 580.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   418/50000: episode: 418, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 873.000 [873.000, 873.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   419/50000: episode: 419, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   420/50000: episode: 420, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2402.000 [2402.000, 2402.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   421/50000: episode: 421, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   422/50000: episode: 422, duration: 0.005s, episode steps:   1, steps per second: 190, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   423/50000: episode: 423, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   424/50000: episode: 424, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3420.000 [3420.000, 3420.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   425/50000: episode: 425, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   426/50000: episode: 426, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1024.000 [1024.000, 1024.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   427/50000: episode: 427, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   428/50000: episode: 428, duration: 0.003s, episode steps:   1, steps per second: 302, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   429/50000: episode: 429, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   430/50000: episode: 430, duration: 0.004s, episode steps:   1, steps per second: 265, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   431/50000: episode: 431, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2479.000 [2479.000, 2479.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   432/50000: episode: 432, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4078.000 [4078.000, 4078.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   433/50000: episode: 433, duration: 0.006s, episode steps:   1, steps per second: 169, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   434/50000: episode: 434, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   435/50000: episode: 435, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   436/50000: episode: 436, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   437/50000: episode: 437, duration: 0.003s, episode steps:   1, steps per second: 356, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   438/50000: episode: 438, duration: 0.005s, episode steps:   1, steps per second: 217, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   439/50000: episode: 439, duration: 0.005s, episode steps:   1, steps per second: 206, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   440/50000: episode: 440, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   441/50000: episode: 441, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   442/50000: episode: 442, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   443/50000: episode: 443, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 545.000 [545.000, 545.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   444/50000: episode: 444, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   445/50000: episode: 445, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   446/50000: episode: 446, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3235.000 [3235.000, 3235.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   447/50000: episode: 447, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   448/50000: episode: 448, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   449/50000: episode: 449, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2407.000 [2407.000, 2407.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   450/50000: episode: 450, duration: 0.005s, episode steps:   1, steps per second: 192, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   451/50000: episode: 451, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   452/50000: episode: 452, duration: 0.005s, episode steps:   1, steps per second: 187, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3235.000 [3235.000, 3235.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   453/50000: episode: 453, duration: 0.006s, episode steps:   1, steps per second: 165, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   454/50000: episode: 454, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   455/50000: episode: 455, duration: 0.006s, episode steps:   1, steps per second: 178, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   456/50000: episode: 456, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   457/50000: episode: 457, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   458/50000: episode: 458, duration: 0.006s, episode steps:   1, steps per second: 158, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   459/50000: episode: 459, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   460/50000: episode: 460, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   461/50000: episode: 461, duration: 0.005s, episode steps:   1, steps per second: 221, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   462/50000: episode: 462, duration: 0.006s, episode steps:   1, steps per second: 178, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   463/50000: episode: 463, duration: 0.003s, episode steps:   1, steps per second: 300, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   464/50000: episode: 464, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   465/50000: episode: 465, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1517.000 [1517.000, 1517.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   466/50000: episode: 466, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2981.000 [2981.000, 2981.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   467/50000: episode: 467, duration: 0.006s, episode steps:   1, steps per second: 155, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   468/50000: episode: 468, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   469/50000: episode: 469, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1173.000 [1173.000, 1173.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   470/50000: episode: 470, duration: 0.005s, episode steps:   1, steps per second: 218, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   471/50000: episode: 471, duration: 0.005s, episode steps:   1, steps per second: 212, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   472/50000: episode: 472, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   473/50000: episode: 473, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   474/50000: episode: 474, duration: 0.004s, episode steps:   1, steps per second: 224, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   475/50000: episode: 475, duration: 0.005s, episode steps:   1, steps per second: 196, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   476/50000: episode: 476, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   477/50000: episode: 477, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3205.000 [3205.000, 3205.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   478/50000: episode: 478, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   479/50000: episode: 479, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   480/50000: episode: 480, duration: 0.005s, episode steps:   1, steps per second: 211, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   481/50000: episode: 481, duration: 0.004s, episode steps:   1, steps per second: 236, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   482/50000: episode: 482, duration: 0.005s, episode steps:   1, steps per second: 221, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   483/50000: episode: 483, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   484/50000: episode: 484, duration: 0.006s, episode steps:   1, steps per second: 157, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   485/50000: episode: 485, duration: 0.005s, episode steps:   1, steps per second: 209, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   486/50000: episode: 486, duration: 0.006s, episode steps:   1, steps per second: 175, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   487/50000: episode: 487, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   488/50000: episode: 488, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3223.000 [3223.000, 3223.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   489/50000: episode: 489, duration: 0.005s, episode steps:   1, steps per second: 196, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   490/50000: episode: 490, duration: 0.006s, episode steps:   1, steps per second: 156, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   491/50000: episode: 491, duration: 0.003s, episode steps:   1, steps per second: 336, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   492/50000: episode: 492, duration: 0.005s, episode steps:   1, steps per second: 211, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   493/50000: episode: 493, duration: 0.003s, episode steps:   1, steps per second: 352, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   494/50000: episode: 494, duration: 0.006s, episode steps:   1, steps per second: 159, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   495/50000: episode: 495, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1012.000 [1012.000, 1012.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   496/50000: episode: 496, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   497/50000: episode: 497, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   498/50000: episode: 498, duration: 0.007s, episode steps:   1, steps per second: 140, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   499/50000: episode: 499, duration: 0.004s, episode steps:   1, steps per second: 239, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   500/50000: episode: 500, duration: 0.006s, episode steps:   1, steps per second: 159, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1594.000 [1594.000, 1594.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   501/50000: episode: 501, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   502/50000: episode: 502, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   503/50000: episode: 503, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2876.000 [2876.000, 2876.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   504/50000: episode: 504, duration: 0.005s, episode steps:   1, steps per second: 193, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   505/50000: episode: 505, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2146.000 [2146.000, 2146.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   506/50000: episode: 506, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   507/50000: episode: 507, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   508/50000: episode: 508, duration: 0.004s, episode steps:   1, steps per second: 226, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   509/50000: episode: 509, duration: 0.007s, episode steps:   1, steps per second: 154, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1012.000 [1012.000, 1012.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   510/50000: episode: 510, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1144.000 [1144.000, 1144.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   511/50000: episode: 511, duration: 0.006s, episode steps:   1, steps per second: 161, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3235.000 [3235.000, 3235.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   512/50000: episode: 512, duration: 0.005s, episode steps:   1, steps per second: 192, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   513/50000: episode: 513, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   514/50000: episode: 514, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1338.000 [1338.000, 1338.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   515/50000: episode: 515, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   516/50000: episode: 516, duration: 0.003s, episode steps:   1, steps per second: 300, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   517/50000: episode: 517, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2110.000 [2110.000, 2110.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   518/50000: episode: 518, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2590.000 [2590.000, 2590.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   519/50000: episode: 519, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   520/50000: episode: 520, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   521/50000: episode: 521, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   522/50000: episode: 522, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   523/50000: episode: 523, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   524/50000: episode: 524, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   525/50000: episode: 525, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   526/50000: episode: 526, duration: 0.005s, episode steps:   1, steps per second: 183, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   527/50000: episode: 527, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   528/50000: episode: 528, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   529/50000: episode: 529, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   530/50000: episode: 530, duration: 0.004s, episode steps:   1, steps per second: 224, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   531/50000: episode: 531, duration: 0.005s, episode steps:   1, steps per second: 208, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3760.000 [3760.000, 3760.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   532/50000: episode: 532, duration: 0.003s, episode steps:   1, steps per second: 291, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3097.000 [3097.000, 3097.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   533/50000: episode: 533, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   534/50000: episode: 534, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   535/50000: episode: 535, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   536/50000: episode: 536, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   537/50000: episode: 537, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1898.000 [1898.000, 1898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   538/50000: episode: 538, duration: 0.005s, episode steps:   1, steps per second: 189, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   539/50000: episode: 539, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   540/50000: episode: 540, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   541/50000: episode: 541, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   542/50000: episode: 542, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   543/50000: episode: 543, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   544/50000: episode: 544, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   545/50000: episode: 545, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   546/50000: episode: 546, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   547/50000: episode: 547, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   548/50000: episode: 548, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   549/50000: episode: 549, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   550/50000: episode: 550, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   551/50000: episode: 551, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2773.000 [2773.000, 2773.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   552/50000: episode: 552, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   553/50000: episode: 553, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   554/50000: episode: 554, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   555/50000: episode: 555, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3657.000 [3657.000, 3657.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   556/50000: episode: 556, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   557/50000: episode: 557, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   558/50000: episode: 558, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   559/50000: episode: 559, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   560/50000: episode: 560, duration: 0.005s, episode steps:   1, steps per second: 185, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   561/50000: episode: 561, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   562/50000: episode: 562, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   563/50000: episode: 563, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   564/50000: episode: 564, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   565/50000: episode: 565, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   566/50000: episode: 566, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3098.000 [3098.000, 3098.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   567/50000: episode: 567, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   568/50000: episode: 568, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1787.000 [1787.000, 1787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   569/50000: episode: 569, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   570/50000: episode: 570, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   571/50000: episode: 571, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   572/50000: episode: 572, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   573/50000: episode: 573, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   574/50000: episode: 574, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   575/50000: episode: 575, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   576/50000: episode: 576, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   577/50000: episode: 577, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 995.000 [995.000, 995.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   578/50000: episode: 578, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   579/50000: episode: 579, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   580/50000: episode: 580, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   581/50000: episode: 581, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   582/50000: episode: 582, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3706.000 [3706.000, 3706.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   583/50000: episode: 583, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1173.000 [1173.000, 1173.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   584/50000: episode: 584, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   585/50000: episode: 585, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   586/50000: episode: 586, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   587/50000: episode: 587, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   588/50000: episode: 588, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 606.000 [606.000, 606.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   589/50000: episode: 589, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   590/50000: episode: 590, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   591/50000: episode: 591, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   592/50000: episode: 592, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3336.000 [3336.000, 3336.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   593/50000: episode: 593, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   594/50000: episode: 594, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   595/50000: episode: 595, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   596/50000: episode: 596, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   597/50000: episode: 597, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   598/50000: episode: 598, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   599/50000: episode: 599, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   600/50000: episode: 600, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   601/50000: episode: 601, duration: 0.027s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   602/50000: episode: 602, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   603/50000: episode: 603, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   604/50000: episode: 604, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   605/50000: episode: 605, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1649.000 [1649.000, 1649.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   606/50000: episode: 606, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   607/50000: episode: 607, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   608/50000: episode: 608, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   609/50000: episode: 609, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   610/50000: episode: 610, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   611/50000: episode: 611, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   612/50000: episode: 612, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   613/50000: episode: 613, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: --, mae: --, mean_q: --\n",
      "done, took 9.457 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong_move\n",
      "     1/50000: episode: 1, duration: 0.716s, episode steps:   1, steps per second:   1, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     2/50000: episode: 2, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     3/50000: episode: 3, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     4/50000: episode: 4, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     5/50000: episode: 5, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3776.000 [3776.000, 3776.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     6/50000: episode: 6, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     7/50000: episode: 7, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     8/50000: episode: 8, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     9/50000: episode: 9, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 341.000 [341.000, 341.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    10/50000: episode: 10, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2525.000 [2525.000, 2525.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    11/50000: episode: 11, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    12/50000: episode: 12, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1517.000 [1517.000, 1517.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    13/50000: episode: 13, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    14/50000: episode: 14, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    15/50000: episode: 15, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3387.000 [3387.000, 3387.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    16/50000: episode: 16, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    17/50000: episode: 17, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    18/50000: episode: 18, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    19/50000: episode: 19, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    20/50000: episode: 20, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    21/50000: episode: 21, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    22/50000: episode: 22, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    23/50000: episode: 23, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    24/50000: episode: 24, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    25/50000: episode: 25, duration: 0.006s, episode steps:   1, steps per second: 164, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    26/50000: episode: 26, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1527.000 [1527.000, 1527.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    27/50000: episode: 27, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2146.000 [2146.000, 2146.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    28/50000: episode: 28, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    29/50000: episode: 29, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    30/50000: episode: 30, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    31/50000: episode: 31, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    32/50000: episode: 32, duration: 0.005s, episode steps:   1, steps per second: 195, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    33/50000: episode: 33, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    34/50000: episode: 34, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3266.000 [3266.000, 3266.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    35/50000: episode: 35, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2612.000 [2612.000, 2612.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    36/50000: episode: 36, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    37/50000: episode: 37, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    38/50000: episode: 38, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1030.000 [1030.000, 1030.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    39/50000: episode: 39, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    40/50000: episode: 40, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    41/50000: episode: 41, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    42/50000: episode: 42, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    43/50000: episode: 43, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    44/50000: episode: 44, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    45/50000: episode: 45, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    46/50000: episode: 46, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    47/50000: episode: 47, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    48/50000: episode: 48, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    49/50000: episode: 49, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    50/50000: episode: 50, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    51/50000: episode: 51, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2981.000 [2981.000, 2981.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    52/50000: episode: 52, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    53/50000: episode: 53, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    54/50000: episode: 54, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    55/50000: episode: 55, duration: 0.007s, episode steps:   1, steps per second: 134, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    56/50000: episode: 56, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3266.000 [3266.000, 3266.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    57/50000: episode: 57, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    58/50000: episode: 58, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    59/50000: episode: 59, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    60/50000: episode: 60, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    61/50000: episode: 61, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 353.000 [353.000, 353.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    62/50000: episode: 62, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    63/50000: episode: 63, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    64/50000: episode: 64, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    65/50000: episode: 65, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1012.000 [1012.000, 1012.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    66/50000: episode: 66, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    67/50000: episode: 67, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    68/50000: episode: 68, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    69/50000: episode: 69, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    70/50000: episode: 70, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    71/50000: episode: 71, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    72/50000: episode: 72, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    73/50000: episode: 73, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    74/50000: episode: 74, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    75/50000: episode: 75, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    76/50000: episode: 76, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    77/50000: episode: 77, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    78/50000: episode: 78, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    79/50000: episode: 79, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    80/50000: episode: 80, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 873.000 [873.000, 873.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    81/50000: episode: 81, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3317.000 [3317.000, 3317.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    82/50000: episode: 82, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    83/50000: episode: 83, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    84/50000: episode: 84, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    85/50000: episode: 85, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    86/50000: episode: 86, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2104.000 [2104.000, 2104.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    87/50000: episode: 87, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    88/50000: episode: 88, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    89/50000: episode: 89, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    90/50000: episode: 90, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    91/50000: episode: 91, duration: 0.010s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    92/50000: episode: 92, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    93/50000: episode: 93, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    94/50000: episode: 94, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    95/50000: episode: 95, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2981.000 [2981.000, 2981.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    96/50000: episode: 96, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2034.000 [2034.000, 2034.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    97/50000: episode: 97, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    98/50000: episode: 98, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    99/50000: episode: 99, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   100/50000: episode: 100, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   101/50000: episode: 101, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 235.000 [235.000, 235.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   102/50000: episode: 102, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1517.000 [1517.000, 1517.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   103/50000: episode: 103, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3922.000 [3922.000, 3922.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   104/50000: episode: 104, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   105/50000: episode: 105, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   106/50000: episode: 106, duration: 0.007s, episode steps:   1, steps per second: 134, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   107/50000: episode: 107, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3220.000 [3220.000, 3220.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   108/50000: episode: 108, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   109/50000: episode: 109, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   110/50000: episode: 110, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   111/50000: episode: 111, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   112/50000: episode: 112, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1517.000 [1517.000, 1517.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   113/50000: episode: 113, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   114/50000: episode: 114, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 407.000 [407.000, 407.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   115/50000: episode: 115, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   116/50000: episode: 116, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   117/50000: episode: 117, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2610.000 [2610.000, 2610.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   118/50000: episode: 118, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4021.000 [4021.000, 4021.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   119/50000: episode: 119, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   120/50000: episode: 120, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   121/50000: episode: 121, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   122/50000: episode: 122, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   123/50000: episode: 123, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   124/50000: episode: 124, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   125/50000: episode: 125, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   126/50000: episode: 126, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   127/50000: episode: 127, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2731.000 [2731.000, 2731.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   128/50000: episode: 128, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   129/50000: episode: 129, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   130/50000: episode: 130, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   131/50000: episode: 131, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   132/50000: episode: 132, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   133/50000: episode: 133, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 318.000 [318.000, 318.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   134/50000: episode: 134, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   135/50000: episode: 135, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   136/50000: episode: 136, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3404.000 [3404.000, 3404.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   137/50000: episode: 137, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   138/50000: episode: 138, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3719.000 [3719.000, 3719.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   139/50000: episode: 139, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   140/50000: episode: 140, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   141/50000: episode: 141, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   142/50000: episode: 142, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   143/50000: episode: 143, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3098.000 [3098.000, 3098.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   144/50000: episode: 144, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   145/50000: episode: 145, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   146/50000: episode: 146, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   147/50000: episode: 147, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   148/50000: episode: 148, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   149/50000: episode: 149, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   150/50000: episode: 150, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 490.000 [490.000, 490.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   151/50000: episode: 151, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   152/50000: episode: 152, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   153/50000: episode: 153, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   154/50000: episode: 154, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1517.000 [1517.000, 1517.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   155/50000: episode: 155, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3098.000 [3098.000, 3098.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   156/50000: episode: 156, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   157/50000: episode: 157, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 795.000 [795.000, 795.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   158/50000: episode: 158, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   159/50000: episode: 159, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   160/50000: episode: 160, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   161/50000: episode: 161, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2889.000 [2889.000, 2889.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   162/50000: episode: 162, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1388.000 [1388.000, 1388.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   163/50000: episode: 163, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   164/50000: episode: 164, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   165/50000: episode: 165, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   166/50000: episode: 166, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   167/50000: episode: 167, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   168/50000: episode: 168, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1952.000 [1952.000, 1952.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   169/50000: episode: 169, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2322.000 [2322.000, 2322.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   170/50000: episode: 170, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2983.000 [2983.000, 2983.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   171/50000: episode: 171, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   172/50000: episode: 172, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   173/50000: episode: 173, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   174/50000: episode: 174, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   175/50000: episode: 175, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1288.000 [1288.000, 1288.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   176/50000: episode: 176, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   177/50000: episode: 177, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   178/50000: episode: 178, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 470.000 [470.000, 470.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   179/50000: episode: 179, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   180/50000: episode: 180, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   181/50000: episode: 181, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   182/50000: episode: 182, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   183/50000: episode: 183, duration: 0.007s, episode steps:   1, steps per second: 134, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   184/50000: episode: 184, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 585.000 [585.000, 585.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   185/50000: episode: 185, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   186/50000: episode: 186, duration: 0.005s, episode steps:   1, steps per second: 185, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2421.000 [2421.000, 2421.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   187/50000: episode: 187, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   188/50000: episode: 188, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   189/50000: episode: 189, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   190/50000: episode: 190, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   191/50000: episode: 191, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2252.000 [2252.000, 2252.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   192/50000: episode: 192, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   193/50000: episode: 193, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   194/50000: episode: 194, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   195/50000: episode: 195, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   196/50000: episode: 196, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 606.000 [606.000, 606.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   197/50000: episode: 197, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   198/50000: episode: 198, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   199/50000: episode: 199, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1012.000 [1012.000, 1012.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   200/50000: episode: 200, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   201/50000: episode: 201, duration: 0.004s, episode steps:   1, steps per second: 277, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   202/50000: episode: 202, duration: 0.006s, episode steps:   1, steps per second: 160, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   203/50000: episode: 203, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   204/50000: episode: 204, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   205/50000: episode: 205, duration: 0.006s, episode steps:   1, steps per second: 167, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   206/50000: episode: 206, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   207/50000: episode: 207, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   208/50000: episode: 208, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 522.000 [522.000, 522.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   209/50000: episode: 209, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2622.000 [2622.000, 2622.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   210/50000: episode: 210, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2331.000 [2331.000, 2331.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   211/50000: episode: 211, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   212/50000: episode: 212, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   213/50000: episode: 213, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1517.000 [1517.000, 1517.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   214/50000: episode: 214, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   215/50000: episode: 215, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   216/50000: episode: 216, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   217/50000: episode: 217, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   218/50000: episode: 218, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   219/50000: episode: 219, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 571.000 [571.000, 571.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   220/50000: episode: 220, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   221/50000: episode: 221, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4030.000 [4030.000, 4030.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   222/50000: episode: 222, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   223/50000: episode: 223, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   224/50000: episode: 224, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1423.000 [1423.000, 1423.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   225/50000: episode: 225, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   226/50000: episode: 226, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 585.000 [585.000, 585.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   227/50000: episode: 227, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   228/50000: episode: 228, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   229/50000: episode: 229, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3718.000 [3718.000, 3718.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   230/50000: episode: 230, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   231/50000: episode: 231, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2935.000 [2935.000, 2935.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   232/50000: episode: 232, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   233/50000: episode: 233, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   234/50000: episode: 234, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   235/50000: episode: 235, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   236/50000: episode: 236, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   237/50000: episode: 237, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3922.000 [3922.000, 3922.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   238/50000: episode: 238, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1143.000 [1143.000, 1143.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   239/50000: episode: 239, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 875.000 [875.000, 875.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   240/50000: episode: 240, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   241/50000: episode: 241, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   242/50000: episode: 242, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   243/50000: episode: 243, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   244/50000: episode: 244, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3775.000 [3775.000, 3775.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   245/50000: episode: 245, duration: 0.008s, episode steps:   1, steps per second: 132, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   246/50000: episode: 246, duration: 0.008s, episode steps:   1, steps per second: 132, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   247/50000: episode: 247, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   248/50000: episode: 248, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   249/50000: episode: 249, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   250/50000: episode: 250, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3098.000 [3098.000, 3098.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   251/50000: episode: 251, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   252/50000: episode: 252, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   253/50000: episode: 253, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4021.000 [4021.000, 4021.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   254/50000: episode: 254, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   255/50000: episode: 255, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   256/50000: episode: 256, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1297.000 [1297.000, 1297.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   257/50000: episode: 257, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   258/50000: episode: 258, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2719.000 [2719.000, 2719.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   259/50000: episode: 259, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   260/50000: episode: 260, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3147.000 [3147.000, 3147.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   261/50000: episode: 261, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2146.000 [2146.000, 2146.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   262/50000: episode: 262, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 287.000 [287.000, 287.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   263/50000: episode: 263, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   264/50000: episode: 264, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   265/50000: episode: 265, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   266/50000: episode: 266, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   267/50000: episode: 267, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1282.000 [1282.000, 1282.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   268/50000: episode: 268, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   269/50000: episode: 269, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   270/50000: episode: 270, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   271/50000: episode: 271, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   272/50000: episode: 272, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   273/50000: episode: 273, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   274/50000: episode: 274, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   275/50000: episode: 275, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   276/50000: episode: 276, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   277/50000: episode: 277, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   278/50000: episode: 278, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 705.000 [705.000, 705.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   279/50000: episode: 279, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   280/50000: episode: 280, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   281/50000: episode: 281, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1418.000 [1418.000, 1418.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   282/50000: episode: 282, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1856.000 [1856.000, 1856.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   283/50000: episode: 283, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   284/50000: episode: 284, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   285/50000: episode: 285, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   286/50000: episode: 286, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3886.000 [3886.000, 3886.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   287/50000: episode: 287, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   288/50000: episode: 288, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   289/50000: episode: 289, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   290/50000: episode: 290, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   291/50000: episode: 291, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   292/50000: episode: 292, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   293/50000: episode: 293, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 235.000 [235.000, 235.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   294/50000: episode: 294, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   295/50000: episode: 295, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   296/50000: episode: 296, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2773.000 [2773.000, 2773.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   297/50000: episode: 297, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   298/50000: episode: 298, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3922.000 [3922.000, 3922.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   299/50000: episode: 299, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   300/50000: episode: 300, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3098.000 [3098.000, 3098.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   301/50000: episode: 301, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   302/50000: episode: 302, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   303/50000: episode: 303, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   304/50000: episode: 304, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   305/50000: episode: 305, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   306/50000: episode: 306, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2527.000 [2527.000, 2527.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   307/50000: episode: 307, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   308/50000: episode: 308, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   309/50000: episode: 309, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   310/50000: episode: 310, duration: 0.047s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   311/50000: episode: 311, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   312/50000: episode: 312, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3147.000 [3147.000, 3147.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   313/50000: episode: 313, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   314/50000: episode: 314, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   315/50000: episode: 315, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   316/50000: episode: 316, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   317/50000: episode: 317, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2608.000 [2608.000, 2608.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   318/50000: episode: 318, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   319/50000: episode: 319, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   320/50000: episode: 320, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   321/50000: episode: 321, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1624.000 [1624.000, 1624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   322/50000: episode: 322, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   323/50000: episode: 323, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   324/50000: episode: 324, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   325/50000: episode: 325, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   326/50000: episode: 326, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   327/50000: episode: 327, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3424.000 [3424.000, 3424.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   328/50000: episode: 328, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   329/50000: episode: 329, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   330/50000: episode: 330, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3424.000 [3424.000, 3424.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   331/50000: episode: 331, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   332/50000: episode: 332, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   333/50000: episode: 333, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   334/50000: episode: 334, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   335/50000: episode: 335, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   336/50000: episode: 336, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   337/50000: episode: 337, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   338/50000: episode: 338, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3098.000 [3098.000, 3098.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   339/50000: episode: 339, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   340/50000: episode: 340, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3147.000 [3147.000, 3147.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   341/50000: episode: 341, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   342/50000: episode: 342, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2539.000 [2539.000, 2539.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   343/50000: episode: 343, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   344/50000: episode: 344, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   345/50000: episode: 345, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   346/50000: episode: 346, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1622.000 [1622.000, 1622.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   347/50000: episode: 347, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 661.000 [661.000, 661.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   348/50000: episode: 348, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   349/50000: episode: 349, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   350/50000: episode: 350, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   351/50000: episode: 351, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 923.000 [923.000, 923.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   352/50000: episode: 352, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   353/50000: episode: 353, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   354/50000: episode: 354, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   355/50000: episode: 355, duration: 0.022s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   356/50000: episode: 356, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   357/50000: episode: 357, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2863.000 [2863.000, 2863.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   358/50000: episode: 358, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   359/50000: episode: 359, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   360/50000: episode: 360, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   361/50000: episode: 361, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   362/50000: episode: 362, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   363/50000: episode: 363, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   364/50000: episode: 364, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   365/50000: episode: 365, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   366/50000: episode: 366, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   367/50000: episode: 367, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   368/50000: episode: 368, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3235.000 [3235.000, 3235.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   369/50000: episode: 369, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   370/50000: episode: 370, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3941.000 [3941.000, 3941.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   371/50000: episode: 371, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   372/50000: episode: 372, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4021.000 [4021.000, 4021.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   373/50000: episode: 373, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   374/50000: episode: 374, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1559.000 [1559.000, 1559.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   375/50000: episode: 375, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2773.000 [2773.000, 2773.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   376/50000: episode: 376, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   377/50000: episode: 377, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   378/50000: episode: 378, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   379/50000: episode: 379, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3570.000 [3570.000, 3570.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   380/50000: episode: 380, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   381/50000: episode: 381, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   382/50000: episode: 382, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2146.000 [2146.000, 2146.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   383/50000: episode: 383, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   384/50000: episode: 384, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   385/50000: episode: 385, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   386/50000: episode: 386, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   387/50000: episode: 387, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1095.000 [1095.000, 1095.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   388/50000: episode: 388, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3908.000 [3908.000, 3908.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   389/50000: episode: 389, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   390/50000: episode: 390, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3235.000 [3235.000, 3235.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   391/50000: episode: 391, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 429.000 [429.000, 429.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   392/50000: episode: 392, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   393/50000: episode: 393, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   394/50000: episode: 394, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   395/50000: episode: 395, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1744.000 [1744.000, 1744.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   396/50000: episode: 396, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   397/50000: episode: 397, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1173.000 [1173.000, 1173.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   398/50000: episode: 398, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   399/50000: episode: 399, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   400/50000: episode: 400, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   401/50000: episode: 401, duration: 0.044s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1012.000 [1012.000, 1012.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   402/50000: episode: 402, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   403/50000: episode: 403, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   404/50000: episode: 404, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   405/50000: episode: 405, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   406/50000: episode: 406, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 326.000 [326.000, 326.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   407/50000: episode: 407, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   408/50000: episode: 408, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3235.000 [3235.000, 3235.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   409/50000: episode: 409, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   410/50000: episode: 410, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   411/50000: episode: 411, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   412/50000: episode: 412, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3922.000 [3922.000, 3922.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   413/50000: episode: 413, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3878.000 [3878.000, 3878.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   414/50000: episode: 414, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   415/50000: episode: 415, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   416/50000: episode: 416, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2622.000 [2622.000, 2622.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   417/50000: episode: 417, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   418/50000: episode: 418, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   419/50000: episode: 419, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 585.000 [585.000, 585.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   420/50000: episode: 420, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3424.000 [3424.000, 3424.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   421/50000: episode: 421, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   422/50000: episode: 422, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   423/50000: episode: 423, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   424/50000: episode: 424, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   425/50000: episode: 425, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   426/50000: episode: 426, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   427/50000: episode: 427, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1411.000 [1411.000, 1411.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   428/50000: episode: 428, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   429/50000: episode: 429, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 951.000 [951.000, 951.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   430/50000: episode: 430, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1658.000 [1658.000, 1658.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   431/50000: episode: 431, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2479.000 [2479.000, 2479.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   432/50000: episode: 432, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   433/50000: episode: 433, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   434/50000: episode: 434, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   435/50000: episode: 435, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 287.000 [287.000, 287.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   436/50000: episode: 436, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   437/50000: episode: 437, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   438/50000: episode: 438, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 606.000 [606.000, 606.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   439/50000: episode: 439, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 965.000 [965.000, 965.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   440/50000: episode: 440, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1856.000 [1856.000, 1856.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   441/50000: episode: 441, duration: 0.027s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   442/50000: episode: 442, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   443/50000: episode: 443, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3922.000 [3922.000, 3922.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   444/50000: episode: 444, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   445/50000: episode: 445, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   446/50000: episode: 446, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   447/50000: episode: 447, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   448/50000: episode: 448, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1288.000 [1288.000, 1288.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   449/50000: episode: 449, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1279.000 [1279.000, 1279.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   450/50000: episode: 450, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   451/50000: episode: 451, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   452/50000: episode: 452, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   453/50000: episode: 453, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   454/50000: episode: 454, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   455/50000: episode: 455, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   456/50000: episode: 456, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   457/50000: episode: 457, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   458/50000: episode: 458, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   459/50000: episode: 459, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   460/50000: episode: 460, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   461/50000: episode: 461, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4034.000 [4034.000, 4034.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   462/50000: episode: 462, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1717.000 [1717.000, 1717.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   463/50000: episode: 463, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   464/50000: episode: 464, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   465/50000: episode: 465, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   466/50000: episode: 466, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   467/50000: episode: 467, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   468/50000: episode: 468, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   469/50000: episode: 469, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   470/50000: episode: 470, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 235.000 [235.000, 235.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   471/50000: episode: 471, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   472/50000: episode: 472, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   473/50000: episode: 473, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   474/50000: episode: 474, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   475/50000: episode: 475, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   476/50000: episode: 476, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   477/50000: episode: 477, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   478/50000: episode: 478, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2043.000 [2043.000, 2043.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   479/50000: episode: 479, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2094.000 [2094.000, 2094.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   480/50000: episode: 480, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   481/50000: episode: 481, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 571.000 [571.000, 571.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   482/50000: episode: 482, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   483/50000: episode: 483, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   484/50000: episode: 484, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   485/50000: episode: 485, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   486/50000: episode: 486, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   487/50000: episode: 487, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3922.000 [3922.000, 3922.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   488/50000: episode: 488, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1856.000 [1856.000, 1856.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   489/50000: episode: 489, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   490/50000: episode: 490, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   491/50000: episode: 491, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3878.000 [3878.000, 3878.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   492/50000: episode: 492, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2783.000 [2783.000, 2783.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   493/50000: episode: 493, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3098.000 [3098.000, 3098.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   494/50000: episode: 494, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   495/50000: episode: 495, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   496/50000: episode: 496, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   497/50000: episode: 497, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   498/50000: episode: 498, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   499/50000: episode: 499, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   500/50000: episode: 500, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2402.000 [2402.000, 2402.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   501/50000: episode: 501, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   502/50000: episode: 502, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   503/50000: episode: 503, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   504/50000: episode: 504, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 186.000 [186.000, 186.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   505/50000: episode: 505, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   506/50000: episode: 506, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 603.000 [603.000, 603.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   507/50000: episode: 507, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   508/50000: episode: 508, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   509/50000: episode: 509, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1173.000 [1173.000, 1173.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   510/50000: episode: 510, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   511/50000: episode: 511, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   512/50000: episode: 512, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   513/50000: episode: 513, duration: 0.027s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3205.000 [3205.000, 3205.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   514/50000: episode: 514, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 917.000 [917.000, 917.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   515/50000: episode: 515, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   516/50000: episode: 516, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2246.000 [2246.000, 2246.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   517/50000: episode: 517, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   518/50000: episode: 518, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2799.000 [2799.000, 2799.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   519/50000: episode: 519, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   520/50000: episode: 520, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   521/50000: episode: 521, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   522/50000: episode: 522, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   523/50000: episode: 523, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   524/50000: episode: 524, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3511.000 [3511.000, 3511.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   525/50000: episode: 525, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   526/50000: episode: 526, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2322.000 [2322.000, 2322.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   527/50000: episode: 527, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   528/50000: episode: 528, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   529/50000: episode: 529, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   530/50000: episode: 530, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 585.000 [585.000, 585.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   531/50000: episode: 531, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   532/50000: episode: 532, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2191.000 [2191.000, 2191.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   533/50000: episode: 533, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   534/50000: episode: 534, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   535/50000: episode: 535, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   536/50000: episode: 536, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   537/50000: episode: 537, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   538/50000: episode: 538, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   539/50000: episode: 539, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   540/50000: episode: 540, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   541/50000: episode: 541, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1279.000 [1279.000, 1279.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   542/50000: episode: 542, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   543/50000: episode: 543, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   544/50000: episode: 544, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   545/50000: episode: 545, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3773.000 [3773.000, 3773.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   546/50000: episode: 546, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1779.000 [1779.000, 1779.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   547/50000: episode: 547, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   548/50000: episode: 548, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 397.000 [397.000, 397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   549/50000: episode: 549, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   550/50000: episode: 550, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   551/50000: episode: 551, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   552/50000: episode: 552, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   553/50000: episode: 553, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   554/50000: episode: 554, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   555/50000: episode: 555, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   556/50000: episode: 556, duration: 0.044s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2479.000 [2479.000, 2479.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   557/50000: episode: 557, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   558/50000: episode: 558, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   559/50000: episode: 559, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   560/50000: episode: 560, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1517.000 [1517.000, 1517.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   561/50000: episode: 561, duration: 0.027s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2124.000 [2124.000, 2124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n"
     ]
    }
   ],
   "source": [
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "for i in range (10):\n",
    "  policy = EpsGreedyQPolicy(0.01)\n",
    "  dqn = DQNAgent(model=model, nb_actions=NB_ACTIONS, memory=memory,\n",
    "                target_model_update=1e-2, policy=policy)\n",
    "  dqn.compile(Adam(lr=1e-4), metrics=['mae'])\n",
    "\n",
    "  # Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "  # slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "  # Ctrl + C.\n",
    "  his = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
    "  \n",
    "  model.save('chess_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save dqn\n",
    "# dqn.save_weights('dqn_{}_weights.h5f'.format('chess'), overwrite=True)\n",
    "\n",
    "# # save model\n",
    "model.save('chess_model.h5')\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "files.download('chess_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# model = keras.models.load_model('chess_model.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
