{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEsaesKwwFnf",
        "outputId": "c020eac5-a8d3-42da-a1c8-a1ef30ecce3b"
      },
      "outputs": [],
      "source": [
        "# ! pip install keras-rl2\n",
        "# ! pip install chess\n",
        "# ! pip install python-chess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ocqViokxFxM",
        "outputId": "a32fdb04-daf7-474b-ab41-be5bee6850bd"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INImGNcRyKhX",
        "outputId": "afe25e4d-72d3-4bc4-88df-4bb6be1b84d1"
      },
      "outputs": [],
      "source": [
        "# ls drive/MyDrive/Data/Chess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-12-15 10:53:57.082631: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2021-12-15 10:53:57.082689: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten,\\\n",
        "     Input,BatchNormalization, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "\n",
        "# import gym_chess\n",
        "\n",
        "import chess\n",
        "from sys import platform\n",
        "import os\n",
        "import chess.engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 15746801220104432718\n",
            "]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-12-15 10:54:00.376272: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-12-15 10:54:00.378666: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2021-12-15 10:54:00.378686: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2021-12-15 10:54:00.378749: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LoG): /proc/driver/nvidia/version does not exist\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "# os.system('chmod +x stockfish_14.1_linux_x64')\n",
        "# engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish_14.1_linux_x64\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "if platform == \"linux\" or platform == \"linux2\":\n",
        "    os.system('chmod +x ../stockfish/stockfish_14.1_linux_x64')\n",
        "    engine = chess.engine.SimpleEngine.popen_uci(r\"../stockfish/stockfish_14.1_linux_x64\")\n",
        "elif platform == \"win32\":\n",
        "    engine = chess.engine.SimpleEngine.popen_uci(r\"../stockfish/stockfish_14.1_win_32bit.exe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_move(env):\n",
        "    result = engine.play(env.env, chess.engine.Limit(time=0.05))\n",
        "    return result.move"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "STATE_SHAPE = (65, )\n",
        "NB_ACTIONS = 4096"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChessEnv:\n",
        "    '''\n",
        "    state - obser: ndarray - (65,): [:65] is flatten from int_board; [65] is color of bot; 1 is white and -1 is black\n",
        "    step: int. step_range = (0, 4096) , is encoded from square A to square B (64 x 64 val)\n",
        "    reward: int\n",
        "    '''\n",
        "\n",
        "    mapped = {\n",
        "            'P': 10,     # White Pawn\n",
        "            'p': -10,    # Black Pawn\n",
        "            'N': 20,     # White Knight\n",
        "            'n': -20,    # Black Knight\n",
        "            'B': 30,     # White Bishop\n",
        "            'b': -30,    # Black Bishop\n",
        "            'R': 40,     # White Rook\n",
        "            'r': -40,    # Black Rook\n",
        "            'Q': 50,     # White Queen\n",
        "            'q': -50,    # Black Queen\n",
        "            'K': 900,     # White King\n",
        "            'k': -900     # Black King\n",
        "    }\n",
        "    # state_shape = (8, 8)\n",
        "    # nb_actions = 4096\n",
        "    model = None\n",
        "    \n",
        "    def __init__(self, model: Sequential, neg_r_each_step = -1) -> None:\n",
        "        self.env = chess.Board()\n",
        "        self.model = model\n",
        "        self.state = self.reset()\n",
        "        # [-1] = 1 -> white, -1 -> black\n",
        "        self.bot_color = self.env.turn * 2 - 1\n",
        "        self.neg_r_each_step = neg_r_each_step\n",
        "\n",
        "    def is_draw(self):\n",
        "        if self.env.is_stalemate():\n",
        "            print(\"statlemate\")\n",
        "            return True\n",
        "        if self.env.is_fivefold_repetition():\n",
        "            print(\"fivefold repetition\")\n",
        "            return True\n",
        "        if self.env.is_seventyfive_moves():\n",
        "            print(\"75 moves\")\n",
        "            return True\n",
        "        if self.env.is_insufficient_material():\n",
        "            print(\"Insufficient Material\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def is_checkmate(self):\n",
        "        # If There is checkmate then it will be TRUE else FALSE.It will be a boolean value.\n",
        "        return self.env.is_checkmate()\n",
        "\n",
        "    def convert_board_to_int(self):\n",
        "        epd_string = self.env.epd()\n",
        "        list_int = np.empty((0, ))\n",
        "        for i in epd_string:\n",
        "            if i == \" \":\n",
        "                list_int = list_int.reshape((8, 8))\n",
        "                return list_int\n",
        "            elif i != \"/\":\n",
        "                if i in self.mapped:\n",
        "                    list_int = np.append(list_int, self.mapped[i])\n",
        "                else:\n",
        "                    for counter in range(0, int(i)):\n",
        "                        list_int = np.append(list_int, 0)\n",
        "        list_int = list_int.reshape((8, 8))\n",
        "        return list_int\n",
        "\n",
        "    def get_state(self) -> np.ndarray:\n",
        "        return np.append(self.convert_board_to_int().reshape(64,), self.env.turn * 2 - 1)\n",
        "\n",
        "    def legal_moves(self):\n",
        "        return list(self.env.legal_moves)\n",
        "\n",
        "    def encodeMove(self, move_uci:str):\n",
        "        if len(move_uci) != 4:\n",
        "            raise ValueError()\n",
        "        a, b = chess.parse_square(move_uci[:2]), chess.parse_square(move_uci[2:])\n",
        "        return a * 64 + b\n",
        "\n",
        "    def decodeMove(self, move_int:int):\n",
        "        a, b = move_int//64, move_int%64\n",
        "        # a, b = chess.square_name(a), chess.square_name(b)\n",
        "\n",
        "        move = self.env.find_move(from_square= a,to_square= b)\n",
        "        return move\n",
        "\n",
        "    def render(self):\n",
        "        print(self.env.unicode())\n",
        "\n",
        "    def reset(self):\n",
        "        # random state\n",
        "        redo = True\n",
        "        num_sample_steps = 0\n",
        "        while redo:\n",
        "            redo = False\n",
        "            self.env = chess.Board()\n",
        "            num_sample_steps = np.random.randint(0, 50)\n",
        "            for i in range (num_sample_steps):\n",
        "                lg_move = self.legal_moves()\n",
        "                if len(lg_move) != 0:\n",
        "                    move = np.random.choice(self.legal_moves())\n",
        "                    self.env.push(move)\n",
        "                else:\n",
        "                    redo = True\n",
        "                    break\n",
        "\n",
        "        Q_val = self.model.predict(self.get_state().reshape((1, 1) + STATE_SHAPE)).reshape(-1, )\n",
        "        print('Val:', min(Q_val), max(Q_val))\n",
        "        return self.get_state()\n",
        "\n",
        "    def step(self, action: int):\n",
        "        reward = 0\n",
        "        done = True\n",
        "        \n",
        "        try:\n",
        "            # move in legal move\n",
        "            move = self.decodeMove(action)\n",
        "\n",
        "            # neg reward each step\n",
        "            reward = self.neg_r_each_step\n",
        "\n",
        "            # location to_square\n",
        "            to_r, to_c = move.to_square//8, move.to_square%8\n",
        "            reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
        "\n",
        "            # action\n",
        "            self.env.push(move)\n",
        "            self.state = self.get_state()\n",
        "\n",
        "            # check end game\n",
        "            if self.is_checkmate():\n",
        "                reward += self.mapped['K']\n",
        "                done = True\n",
        "                print('Win')\n",
        "            elif self.is_draw():\n",
        "                reward += 300\n",
        "                done = True\n",
        "\n",
        "            # opponent's turn   \n",
        "            else:\n",
        "                done = False\n",
        "\n",
        "                move = find_move(self)\n",
        "\n",
        "                # location to_square\n",
        "                to_r, to_c = move.to_square//8, move.to_square%8\n",
        "                reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
        "\n",
        "                # action\n",
        "                self.env.push(move)\n",
        "                self.state = self.get_state()\n",
        "\n",
        "                # check end game\n",
        "                if self.is_checkmate():\n",
        "                    reward -= self.mapped['K']\n",
        "                    done = True\n",
        "                    print(\"Lose\")\n",
        "                elif self.is_draw():\n",
        "                    reward += 300\n",
        "                    done = True\n",
        "\n",
        "        except:\n",
        "            # wrong move\n",
        "            reward = -5000\n",
        "            done = True\n",
        "            print('wrong_move')\n",
        "\n",
        "        return self.state, reward, done, {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "WARNING:tensorflow:From /home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/layers/normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 65)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               8448      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4096)              528384    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 4096)              0         \n",
            "=================================================================\n",
            "Total params: 554,368\n",
            "Trainable params: 553,856\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# model\n",
        "model = Sequential()\n",
        "model.add(Input((1, ) + STATE_SHAPE))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(NB_ACTIONS))\n",
        "model.add(Activation('linear'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
            "2021-12-15 10:54:01.711754: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 1999965000 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val: -50.72309 50.238934\n"
          ]
        }
      ],
      "source": [
        "env = ChessEnv(model, neg_r_each_step=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_weights('chess_model_sf.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 50000 steps ...\n",
            "Val: -8451.044 -4143.8154\n",
            "wrong_move\n",
            "     1/50000: episode: 1, duration: 0.306s, episode steps:   1, steps per second:   3, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8949.764 -3992.084\n",
            "wrong_move\n",
            "     2/50000: episode: 2, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9106.13 -3976.3955\n",
            "wrong_move\n",
            "     3/50000: episode: 3, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8688.658 -4032.4834\n",
            "wrong_move\n",
            "     4/50000: episode: 4, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8502.805 -3788.4563\n",
            "wrong_move\n",
            "     5/50000: episode: 5, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8451.632 -3788.9963\n",
            "wrong_move\n",
            "     6/50000: episode: 6, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8378.78 -4150.065\n",
            "wrong_move\n",
            "     7/50000: episode: 7, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9085.127 -3984.9683\n",
            "wrong_move\n",
            "     8/50000: episode: 8, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9030.09 -3972.9482\n",
            "wrong_move\n",
            "     9/50000: episode: 9, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8637.461 -4109.9385\n",
            "wrong_move\n",
            "    10/50000: episode: 10, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9074.525 -4012.614\n",
            "wrong_move\n",
            "    11/50000: episode: 11, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8730.76 -3796.0679\n",
            "wrong_move\n",
            "    12/50000: episode: 12, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8668.971 -3798.1174\n",
            "wrong_move\n",
            "    13/50000: episode: 13, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8611.463 -4159.184\n",
            "wrong_move\n",
            "    14/50000: episode: 14, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8168.646 -3779.1133\n",
            "wrong_move\n",
            "    15/50000: episode: 15, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8921.079 -4039.5703\n",
            "wrong_move\n",
            "    16/50000: episode: 16, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8502.835 -4104.178\n",
            "wrong_move\n",
            "    17/50000: episode: 17, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8686.53 -3427.6997\n",
            "wrong_move\n",
            "    18/50000: episode: 18, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9478.858 -3665.138\n",
            "wrong_move\n",
            "    19/50000: episode: 19, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1191.000 [1191.000, 1191.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8181.9834 -3779.5764\n",
            "wrong_move\n",
            "    20/50000: episode: 20, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9114.588 -3930.4863\n",
            "wrong_move\n",
            "    21/50000: episode: 21, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8596.174 -4076.8074\n",
            "wrong_move\n",
            "    22/50000: episode: 22, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8984.448 -3928.93\n",
            "wrong_move\n",
            "    23/50000: episode: 23, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8377.715 -3785.0754\n",
            "wrong_move\n",
            "    24/50000: episode: 24, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8537.242 -3792.4644\n",
            "wrong_move\n",
            "    25/50000: episode: 25, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8877.405 -4083.8948\n",
            "wrong_move\n",
            "    26/50000: episode: 26, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8742.699 -4097.4497\n",
            "wrong_move\n",
            "    27/50000: episode: 27, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8359.204 -3789.3125\n",
            "wrong_move\n",
            "    28/50000: episode: 28, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2454.000 [2454.000, 2454.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8914.759 -4049.462\n",
            "wrong_move\n",
            "    29/50000: episode: 29, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8458.539 -3788.7212\n",
            "wrong_move\n",
            "    30/50000: episode: 30, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9198.051 -3885.417\n",
            "wrong_move\n",
            "    31/50000: episode: 31, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9148.533 -3978.9016\n",
            "wrong_move\n",
            "    32/50000: episode: 32, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9182.675 -3922.2192\n",
            "wrong_move\n",
            "    33/50000: episode: 33, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8459.617 -3792.3994\n",
            "wrong_move\n",
            "    34/50000: episode: 34, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9059.41 -4017.4692\n",
            "wrong_move\n",
            "    35/50000: episode: 35, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9077.849 -3987.0327\n",
            "wrong_move\n",
            "    36/50000: episode: 36, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8303.554 -3779.8745\n",
            "wrong_move\n",
            "    37/50000: episode: 37, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9079.844 -4000.0813\n",
            "wrong_move\n",
            "    38/50000: episode: 38, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8719.614 -3800.1855\n",
            "wrong_move\n",
            "    39/50000: episode: 39, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9097.72 -3933.669\n",
            "wrong_move\n",
            "    40/50000: episode: 40, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8454.76 -3789.9023\n",
            "wrong_move\n",
            "    41/50000: episode: 41, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9095.552 -3992.1538\n",
            "wrong_move\n",
            "    42/50000: episode: 42, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8244.605 -4114.2754\n",
            "wrong_move\n",
            "    43/50000: episode: 43, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8675.17 -4143.1245\n",
            "wrong_move\n",
            "    44/50000: episode: 44, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10101.988 -2580.22\n",
            "wrong_move\n",
            "    45/50000: episode: 45, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8934.387 -4067.7605\n",
            "wrong_move\n",
            "    46/50000: episode: 46, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8322.125 -4149.0103\n",
            "wrong_move\n",
            "    47/50000: episode: 47, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8815.028 -4096.802\n",
            "wrong_move\n",
            "    48/50000: episode: 48, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8714.845 -3934.424\n",
            "wrong_move\n",
            "    49/50000: episode: 49, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2464.000 [2464.000, 2464.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8159.4585 -3778.9373\n",
            "wrong_move\n",
            "    50/50000: episode: 50, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8803.648 -3983.2705\n",
            "wrong_move\n",
            "    51/50000: episode: 51, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9112.097 -3975.4126\n",
            "wrong_move\n",
            "    52/50000: episode: 52, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9022.698 -4001.5803\n",
            "wrong_move\n",
            "    53/50000: episode: 53, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8717.92 -3791.7922\n",
            "wrong_move\n",
            "    54/50000: episode: 54, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8747.976 -4074.1792\n",
            "wrong_move\n",
            "    55/50000: episode: 55, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9086.121 -4006.4866\n",
            "wrong_move\n",
            "    56/50000: episode: 56, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8716.791 -3533.431\n",
            "wrong_move\n",
            "    57/50000: episode: 57, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9097.078 -3996.0134\n",
            "wrong_move\n",
            "    58/50000: episode: 58, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8918.052 -4056.7297\n",
            "wrong_move\n",
            "    59/50000: episode: 59, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9065.165 -4012.935\n",
            "wrong_move\n",
            "    60/50000: episode: 60, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9057.053 -4014.126\n",
            "wrong_move\n",
            "    61/50000: episode: 61, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8929.73 -4015.2166\n",
            "wrong_move\n",
            "    62/50000: episode: 62, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9062.577 -4018.3044\n",
            "wrong_move\n",
            "    63/50000: episode: 63, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8462.322 -3897.9768\n",
            "wrong_move\n",
            "    64/50000: episode: 64, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "    65/50000: episode: 65, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9121.455 -3944.3545\n",
            "wrong_move\n",
            "    66/50000: episode: 66, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8413.674 -3790.5188\n",
            "wrong_move\n",
            "    67/50000: episode: 67, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9171.216 -3904.9429\n",
            "wrong_move\n",
            "    68/50000: episode: 68, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8850.468 -4027.7637\n",
            "wrong_move\n",
            "    69/50000: episode: 69, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8592.278 -3794.515\n",
            "wrong_move\n",
            "    70/50000: episode: 70, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8420.112 -3785.5232\n",
            "wrong_move\n",
            "    71/50000: episode: 71, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8726.382 -4123.32\n",
            "wrong_move\n",
            "    72/50000: episode: 72, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9304.794 -2913.1165\n",
            "wrong_move\n",
            "    73/50000: episode: 73, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8406.878 -3786.5637\n",
            "wrong_move\n",
            "    74/50000: episode: 74, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8410.17 -4235.87\n",
            "wrong_move\n",
            "    75/50000: episode: 75, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9052.676 -3973.4214\n",
            "wrong_move\n",
            "    76/50000: episode: 76, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8907.619 -4075.0208\n",
            "wrong_move\n",
            "    77/50000: episode: 77, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9080.531 -4016.9846\n",
            "wrong_move\n",
            "    78/50000: episode: 78, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8767.638 -3801.2942\n",
            "wrong_move\n",
            "    79/50000: episode: 79, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9070.442 -4020.3604\n",
            "wrong_move\n",
            "    80/50000: episode: 80, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9161.892 -3655.4653\n",
            "wrong_move\n",
            "    81/50000: episode: 81, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8786.267 -4069.2249\n",
            "wrong_move\n",
            "    82/50000: episode: 82, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9073.611 -3979.9438\n",
            "wrong_move\n",
            "    83/50000: episode: 83, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9028.871 -3922.7207\n",
            "wrong_move\n",
            "    84/50000: episode: 84, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8179.202 -4134.4624\n",
            "wrong_move\n",
            "    85/50000: episode: 85, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8813.498 -4060.1704\n",
            "wrong_move\n",
            "    86/50000: episode: 86, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9586.8 -3089.096\n",
            "wrong_move\n",
            "    87/50000: episode: 87, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9071.254 -3994.9094\n",
            "wrong_move\n",
            "    88/50000: episode: 88, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8938.277 -4031.917\n",
            "wrong_move\n",
            "    89/50000: episode: 89, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9576.741 -3423.554\n",
            "wrong_move\n",
            "    90/50000: episode: 90, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8826.368 -4058.494\n",
            "wrong_move\n",
            "    91/50000: episode: 91, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8655.247 -4166.3423\n",
            "wrong_move\n",
            "    92/50000: episode: 92, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8357.388 -3785.6018\n",
            "wrong_move\n",
            "    93/50000: episode: 93, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8430.531 -3789.2454\n",
            "wrong_move\n",
            "    94/50000: episode: 94, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8243.423 -3776.9688\n",
            "wrong_move\n",
            "    95/50000: episode: 95, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9050.457 -4025.365\n",
            "wrong_move\n",
            "    96/50000: episode: 96, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9086.509 -4008.3699\n",
            "wrong_move\n",
            "    97/50000: episode: 97, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8897.947 -3801.4583\n",
            "wrong_move\n",
            "    98/50000: episode: 98, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8303.58 -3784.1926\n",
            "wrong_move\n",
            "    99/50000: episode: 99, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8547.168 -3793.5657\n",
            "wrong_move\n",
            "   100/50000: episode: 100, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8526.36 -3793.8726\n",
            "wrong_move\n",
            "   101/50000: episode: 101, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8357.147 -3785.5972\n",
            "wrong_move\n",
            "   102/50000: episode: 102, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9176.661 -3916.8325\n",
            "wrong_move\n",
            "   103/50000: episode: 103, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8766.306 -4076.3457\n",
            "wrong_move\n",
            "   104/50000: episode: 104, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8726.918 -4073.521\n",
            "wrong_move\n",
            "   105/50000: episode: 105, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8966.749 -3985.4526\n",
            "wrong_move\n",
            "   106/50000: episode: 106, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9095.03 -3995.958\n",
            "wrong_move\n",
            "   107/50000: episode: 107, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8754.265 -4026.8389\n",
            "wrong_move\n",
            "   108/50000: episode: 108, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8416.354 -3788.2068\n",
            "wrong_move\n",
            "   109/50000: episode: 109, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9118.232 -3983.8496\n",
            "wrong_move\n",
            "   110/50000: episode: 110, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8456.255 -3787.3428\n",
            "wrong_move\n",
            "   111/50000: episode: 111, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8345.681 -3785.041\n",
            "wrong_move\n",
            "   112/50000: episode: 112, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8364.339 -4041.175\n",
            "wrong_move\n",
            "   113/50000: episode: 113, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8564.602 -4165.765\n",
            "wrong_move\n",
            "   114/50000: episode: 114, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8449.323 -4129.4287\n",
            "wrong_move\n",
            "   115/50000: episode: 115, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8667.011 -3975.371\n",
            "wrong_move\n",
            "   116/50000: episode: 116, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8957.444 -3479.3557\n",
            "wrong_move\n",
            "   117/50000: episode: 117, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8550.064 -3795.2627\n",
            "wrong_move\n",
            "   118/50000: episode: 118, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2454.000 [2454.000, 2454.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   119/50000: episode: 119, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8632.584 -3796.158\n",
            "wrong_move\n",
            "   120/50000: episode: 120, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9091.936 -4001.7256\n",
            "wrong_move\n",
            "   121/50000: episode: 121, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9097.039 -3989.3098\n",
            "wrong_move\n",
            "   122/50000: episode: 122, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8548.549 -3793.3003\n",
            "wrong_move\n",
            "   123/50000: episode: 123, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9083.434 -3999.212\n",
            "wrong_move\n",
            "   124/50000: episode: 124, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8432.492 -3789.53\n",
            "wrong_move\n",
            "   125/50000: episode: 125, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9091.239 -4019.7686\n",
            "wrong_move\n",
            "   126/50000: episode: 126, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9079.982 -3947.775\n",
            "wrong_move\n",
            "   127/50000: episode: 127, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8398.587 -3785.0305\n",
            "wrong_move\n",
            "   128/50000: episode: 128, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8411.732 -3788.496\n",
            "wrong_move\n",
            "   129/50000: episode: 129, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9086.1045 -3924.9165\n",
            "wrong_move\n",
            "   130/50000: episode: 130, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8686.288 -3797.6147\n",
            "wrong_move\n",
            "   131/50000: episode: 131, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9067.549 -4019.7983\n",
            "wrong_move\n",
            "   132/50000: episode: 132, duration: 0.010s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   133/50000: episode: 133, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8852.828 -4026.656\n",
            "wrong_move\n",
            "   134/50000: episode: 134, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8915.249 -4013.2803\n",
            "wrong_move\n",
            "   135/50000: episode: 135, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9055.286 -4028.463\n",
            "wrong_move\n",
            "   136/50000: episode: 136, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8783.454 -4104.999\n",
            "wrong_move\n",
            "   137/50000: episode: 137, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8805.021 -4070.3074\n",
            "wrong_move\n",
            "   138/50000: episode: 138, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8666.989 -3790.5256\n",
            "wrong_move\n",
            "   139/50000: episode: 139, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8505.115 -4103.266\n",
            "wrong_move\n",
            "   140/50000: episode: 140, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9590.397 -2783.7205\n",
            "wrong_move\n",
            "   141/50000: episode: 141, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8785.497 -3159.6873\n",
            "wrong_move\n",
            "   142/50000: episode: 142, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8552.445 -4137.3276\n",
            "wrong_move\n",
            "   143/50000: episode: 143, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2760.000 [2760.000, 2760.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9069.276 -4024.6516\n",
            "wrong_move\n",
            "   144/50000: episode: 144, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9072.459 -4015.1357\n",
            "wrong_move\n",
            "   145/50000: episode: 145, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8753.88 -3295.1606\n",
            "wrong_move\n",
            "   146/50000: episode: 146, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8969.631 -3793.7722\n",
            "wrong_move\n",
            "   147/50000: episode: 147, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8697.005 -4149.709\n",
            "wrong_move\n",
            "   148/50000: episode: 148, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8523.623 -3793.859\n",
            "wrong_move\n",
            "   149/50000: episode: 149, duration: 0.027s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1401.000 [1401.000, 1401.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8424.879 -4214.304\n",
            "wrong_move\n",
            "   150/50000: episode: 150, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8847.17 -3799.082\n",
            "wrong_move\n",
            "   151/50000: episode: 151, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8531.523 -3794.3423\n",
            "wrong_move\n",
            "   152/50000: episode: 152, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2454.000 [2454.000, 2454.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8430.144 -3785.1624\n",
            "wrong_move\n",
            "   153/50000: episode: 153, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8811.169 -3800.3484\n",
            "wrong_move\n",
            "   154/50000: episode: 154, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9053.472 -4029.4666\n",
            "wrong_move\n",
            "   155/50000: episode: 155, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9212.292 -3664.5803\n",
            "wrong_move\n",
            "   156/50000: episode: 156, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9088.891 -3986.0215\n",
            "wrong_move\n",
            "   157/50000: episode: 157, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9332.89 -3913.8135\n",
            "wrong_move\n",
            "   158/50000: episode: 158, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9085.76 -4002.5913\n",
            "wrong_move\n",
            "   159/50000: episode: 159, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8332.677 -3782.67\n",
            "wrong_move\n",
            "   160/50000: episode: 160, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9145.84 -3926.3525\n",
            "wrong_move\n",
            "   161/50000: episode: 161, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8638.321 -3796.6418\n",
            "wrong_move\n",
            "   162/50000: episode: 162, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8301.942 -3784.0166\n",
            "wrong_move\n",
            "   163/50000: episode: 163, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8473.364 -3789.306\n",
            "wrong_move\n",
            "   164/50000: episode: 164, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8394.373 -3786.9792\n",
            "wrong_move\n",
            "   165/50000: episode: 165, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8767.44 -4034.3428\n",
            "wrong_move\n",
            "   166/50000: episode: 166, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9063.804 -4017.0774\n",
            "wrong_move\n",
            "   167/50000: episode: 167, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8439.689 -3788.1624\n",
            "wrong_move\n",
            "   168/50000: episode: 168, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9080.647 -4003.8647\n",
            "wrong_move\n",
            "   169/50000: episode: 169, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9120.818 -3995.9263\n",
            "wrong_move\n",
            "   170/50000: episode: 170, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8458.144 -3790.2996\n",
            "wrong_move\n",
            "   171/50000: episode: 171, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3337.000 [3337.000, 3337.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9075.764 -4023.7485\n",
            "wrong_move\n",
            "   172/50000: episode: 172, duration: 0.006s, episode steps:   1, steps per second: 167, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9073.052 -4014.2786\n",
            "wrong_move\n",
            "   173/50000: episode: 173, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9068.916 -4013.6714\n",
            "wrong_move\n",
            "   174/50000: episode: 174, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8387.771 -4133.208\n",
            "wrong_move\n",
            "   175/50000: episode: 175, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9055.192 -3982.656\n",
            "wrong_move\n",
            "   176/50000: episode: 176, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10029.5625 -2841.6897\n",
            "wrong_move\n",
            "   177/50000: episode: 177, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8865.15 -4034.0576\n",
            "wrong_move\n",
            "   178/50000: episode: 178, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9169.7295 -3839.6768\n",
            "wrong_move\n",
            "   179/50000: episode: 179, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9060.546 -3980.1252\n",
            "wrong_move\n",
            "   180/50000: episode: 180, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8456.937 -3788.1143\n",
            "wrong_move\n",
            "   181/50000: episode: 181, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9148.791 -3964.1675\n",
            "wrong_move\n",
            "   182/50000: episode: 182, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8790.4795 -4078.553\n",
            "wrong_move\n",
            "   183/50000: episode: 183, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9069.54 -4017.2861\n",
            "wrong_move\n",
            "   184/50000: episode: 184, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8747.173 -4068.2517\n",
            "wrong_move\n",
            "   185/50000: episode: 185, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8651.159 -4058.002\n",
            "wrong_move\n",
            "   186/50000: episode: 186, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   187/50000: episode: 187, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9126.677 -3990.8594\n",
            "wrong_move\n",
            "   188/50000: episode: 188, duration: 0.027s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8120.0864 -4119.972\n",
            "wrong_move\n",
            "   189/50000: episode: 189, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2239.000 [2239.000, 2239.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9057.387 -4023.1418\n",
            "wrong_move\n",
            "   190/50000: episode: 190, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8888.396 -4075.7178\n",
            "wrong_move\n",
            "   191/50000: episode: 191, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9060.089 -4024.8145\n",
            "wrong_move\n",
            "   192/50000: episode: 192, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9074.968 -4017.8936\n",
            "wrong_move\n",
            "   193/50000: episode: 193, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8807.169 -3802.066\n",
            "wrong_move\n",
            "   194/50000: episode: 194, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9114.221 -3976.814\n",
            "wrong_move\n",
            "   195/50000: episode: 195, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   196/50000: episode: 196, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8534.144 -3790.5312\n",
            "wrong_move\n",
            "   197/50000: episode: 197, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8758.241 -3030.861\n",
            "wrong_move\n",
            "   198/50000: episode: 198, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8202.998 -4104.1284\n",
            "wrong_move\n",
            "   199/50000: episode: 199, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8366.605 -3791.1367\n",
            "wrong_move\n",
            "   200/50000: episode: 200, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8502.136 -3794.1045\n",
            "wrong_move\n",
            "   201/50000: episode: 201, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8963.016 -4038.605\n",
            "wrong_move\n",
            "   202/50000: episode: 202, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8058.5586 -3784.5454\n",
            "wrong_move\n",
            "   203/50000: episode: 203, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8486.602 -4114.119\n",
            "wrong_move\n",
            "   204/50000: episode: 204, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9073.719 -4006.695\n",
            "wrong_move\n",
            "   205/50000: episode: 205, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8433.048 -3787.0828\n",
            "wrong_move\n",
            "   206/50000: episode: 206, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9589.42 -3406.8442\n",
            "wrong_move\n",
            "   207/50000: episode: 207, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9110.469 -3974.1277\n",
            "wrong_move\n",
            "   208/50000: episode: 208, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8724.124 -4072.594\n",
            "wrong_move\n",
            "   209/50000: episode: 209, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9100.445 -3968.8289\n",
            "wrong_move\n",
            "   210/50000: episode: 210, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3032.000 [3032.000, 3032.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.516 -3895.8608\n",
            "wrong_move\n",
            "   211/50000: episode: 211, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8351.233 -4126.1997\n",
            "wrong_move\n",
            "   212/50000: episode: 212, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8270.924 -3782.6785\n",
            "wrong_move\n",
            "   213/50000: episode: 213, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8225.359 -4158.709\n",
            "wrong_move\n",
            "   214/50000: episode: 214, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9057.93 -4031.1836\n",
            "wrong_move\n",
            "   215/50000: episode: 215, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9031.862 -3981.4177\n",
            "wrong_move\n",
            "   216/50000: episode: 216, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9100.647 -3986.5796\n",
            "wrong_move\n",
            "   217/50000: episode: 217, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8269.3545 -3664.246\n",
            "wrong_move\n",
            "   218/50000: episode: 218, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 985.000 [985.000, 985.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8165.6113 -4165.7485\n",
            "wrong_move\n",
            "   219/50000: episode: 219, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9056.87 -4025.2253\n",
            "wrong_move\n",
            "   220/50000: episode: 220, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8719.511 -4076.2793\n",
            "wrong_move\n",
            "   221/50000: episode: 221, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9067.983 -4008.9016\n",
            "wrong_move\n",
            "   222/50000: episode: 222, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8404.642 -3786.145\n",
            "wrong_move\n",
            "   223/50000: episode: 223, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8093.1626 -3863.5845\n",
            "wrong_move\n",
            "   224/50000: episode: 224, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8774.632 -4022.56\n",
            "wrong_move\n",
            "   225/50000: episode: 225, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8439.079 -3785.3975\n",
            "wrong_move\n",
            "   226/50000: episode: 226, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8546.571 -4076.2134\n",
            "wrong_move\n",
            "   227/50000: episode: 227, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8700.716 -3340.4265\n",
            "wrong_move\n",
            "   228/50000: episode: 228, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8428.057 -3785.2456\n",
            "wrong_move\n",
            "   229/50000: episode: 229, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8456.992 -3788.9192\n",
            "wrong_move\n",
            "   230/50000: episode: 230, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9055.961 -4025.3708\n",
            "wrong_move\n",
            "   231/50000: episode: 231, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9047.513 -3900.0994\n",
            "wrong_move\n",
            "   232/50000: episode: 232, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8625.57 -4190.6035\n",
            "wrong_move\n",
            "   233/50000: episode: 233, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7926.719 -3757.8035\n",
            "wrong_move\n",
            "   234/50000: episode: 234, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 592.000 [592.000, 592.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8937.146 -3978.2456\n",
            "wrong_move\n",
            "   235/50000: episode: 235, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9120.447 -3922.898\n",
            "wrong_move\n",
            "   236/50000: episode: 236, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8707.004 -4080.805\n",
            "wrong_move\n",
            "   237/50000: episode: 237, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8854.914 -4086.6787\n",
            "wrong_move\n",
            "   238/50000: episode: 238, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   239/50000: episode: 239, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8769.619 -3798.119\n",
            "wrong_move\n",
            "   240/50000: episode: 240, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8883.845 -4033.913\n",
            "wrong_move\n",
            "   241/50000: episode: 241, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8975.565 -3747.9187\n",
            "wrong_move\n",
            "   242/50000: episode: 242, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8221.504 -3780.8567\n",
            "wrong_move\n",
            "   243/50000: episode: 243, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9221.668 -3562.3389\n",
            "wrong_move\n",
            "   244/50000: episode: 244, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8736.778 -3798.3572\n",
            "wrong_move\n",
            "   245/50000: episode: 245, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8800.133 -4008.532\n",
            "wrong_move\n",
            "   246/50000: episode: 246, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8422.046 -3785.6099\n",
            "wrong_move\n",
            "   247/50000: episode: 247, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8305.981 -3783.2432\n",
            "wrong_move\n",
            "   248/50000: episode: 248, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8612.658 -3791.0273\n",
            "wrong_move\n",
            "   249/50000: episode: 249, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8420.751 -3788.343\n",
            "wrong_move\n",
            "   250/50000: episode: 250, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8520.3 -3790.6377\n",
            "wrong_move\n",
            "   251/50000: episode: 251, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9053.934 -4014.1135\n",
            "wrong_move\n",
            "   252/50000: episode: 252, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9120.73 -3983.5574\n",
            "wrong_move\n",
            "   253/50000: episode: 253, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8838.035 -4038.336\n",
            "wrong_move\n",
            "   254/50000: episode: 254, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9077.915 -4013.7925\n",
            "wrong_move\n",
            "   255/50000: episode: 255, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8836.145 -4099.214\n",
            "wrong_move\n",
            "   256/50000: episode: 256, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9119.905 -3978.336\n",
            "wrong_move\n",
            "   257/50000: episode: 257, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8770.539 -4095.4307\n",
            "wrong_move\n",
            "   258/50000: episode: 258, duration: 0.027s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9063.009 -4021.347\n",
            "wrong_move\n",
            "   259/50000: episode: 259, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8481.899 -3788.5881\n",
            "wrong_move\n",
            "   260/50000: episode: 260, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9098.407 -3973.819\n",
            "wrong_move\n",
            "   261/50000: episode: 261, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8569.563 -3791.0706\n",
            "wrong_move\n",
            "   262/50000: episode: 262, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8513.6 -3791.7192\n",
            "wrong_move\n",
            "   263/50000: episode: 263, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9103.976 -3981.8706\n",
            "wrong_move\n",
            "   264/50000: episode: 264, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9151.171 -3942.2698\n",
            "wrong_move\n",
            "   265/50000: episode: 265, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9091.606 -4003.9255\n",
            "wrong_move\n",
            "   266/50000: episode: 266, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8371.907 -3770.4382\n",
            "wrong_move\n",
            "   267/50000: episode: 267, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8663.203 -3797.8943\n",
            "wrong_move\n",
            "   269/50000: episode: 268, duration: 0.091s, episode steps:   2, steps per second:  22, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8691.315 -4138.6416\n",
            "wrong_move\n",
            "   270/50000: episode: 269, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9076.107 -3953.6995\n",
            "wrong_move\n",
            "   271/50000: episode: 270, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   272/50000: episode: 271, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8663.625 -3798.6672\n",
            "wrong_move\n",
            "   273/50000: episode: 272, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9070.205 -4018.9385\n",
            "wrong_move\n",
            "   274/50000: episode: 273, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9111.918 -3998.487\n",
            "wrong_move\n",
            "   275/50000: episode: 274, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9063.225 -4025.0083\n",
            "wrong_move\n",
            "   276/50000: episode: 275, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9877.597 -2655.8472\n",
            "wrong_move\n",
            "   277/50000: episode: 276, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8945.277 -4040.5625\n",
            "wrong_move\n",
            "   278/50000: episode: 277, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8406.372 -3785.836\n",
            "wrong_move\n",
            "   279/50000: episode: 278, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8861.577 -4022.8003\n",
            "wrong_move\n",
            "   280/50000: episode: 279, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9108.162 -3990.7808\n",
            "wrong_move\n",
            "   281/50000: episode: 280, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9066.281 -3988.92\n",
            "wrong_move\n",
            "   282/50000: episode: 281, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9098.876 -3987.2588\n",
            "wrong_move\n",
            "   283/50000: episode: 282, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8623.539 -4174.2954\n",
            "wrong_move\n",
            "   284/50000: episode: 283, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9065.086 -4023.5037\n",
            "wrong_move\n",
            "   285/50000: episode: 284, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8100.0645 -4088.9482\n",
            "wrong_move\n",
            "   286/50000: episode: 285, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9119.481 -3955.374\n",
            "wrong_move\n",
            "   287/50000: episode: 286, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8295.442 -3782.9263\n",
            "wrong_move\n",
            "   288/50000: episode: 287, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1401.000 [1401.000, 1401.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8619.545 -4144.233\n",
            "wrong_move\n",
            "   289/50000: episode: 288, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9110.082 -3982.3655\n",
            "wrong_move\n",
            "   290/50000: episode: 289, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9106.109 -3843.0632\n",
            "wrong_move\n",
            "   291/50000: episode: 290, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8920.857 -3883.7732\n",
            "wrong_move\n",
            "   292/50000: episode: 291, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9071.607 -4005.8171\n",
            "wrong_move\n",
            "   293/50000: episode: 292, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9088.594 -4000.5178\n",
            "wrong_move\n",
            "   294/50000: episode: 293, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8406.007 -4100.811\n",
            "wrong_move\n",
            "   295/50000: episode: 294, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9210.605 -3757.0981\n",
            "wrong_move\n",
            "   296/50000: episode: 295, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2461.000 [2461.000, 2461.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9034.646 -3733.7893\n",
            "wrong_move\n",
            "   297/50000: episode: 296, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8452.788 -3789.519\n",
            "wrong_move\n",
            "   298/50000: episode: 297, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8672.341 -4160.9556\n",
            "wrong_move\n",
            "   299/50000: episode: 298, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9127.074 -3984.43\n",
            "wrong_move\n",
            "   300/50000: episode: 299, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9067.065 -3996.1365\n",
            "wrong_move\n",
            "   301/50000: episode: 300, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9105.018 -4001.7163\n",
            "wrong_move\n",
            "   302/50000: episode: 301, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9064.098 -4018.239\n",
            "wrong_move\n",
            "   303/50000: episode: 302, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9075.222 -4008.4883\n",
            "wrong_move\n",
            "   304/50000: episode: 303, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8979.498 -3955.6917\n",
            "wrong_move\n",
            "   305/50000: episode: 304, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8517.328 -4217.9404\n",
            "wrong_move\n",
            "   306/50000: episode: 305, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9538.486 -2775.3838\n",
            "wrong_move\n",
            "   307/50000: episode: 306, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8669.431 -4025.0293\n",
            "wrong_move\n",
            "   308/50000: episode: 307, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9096.746 -3999.9426\n",
            "wrong_move\n",
            "   309/50000: episode: 308, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8873.45 -4043.4312\n",
            "wrong_move\n",
            "   310/50000: episode: 309, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8201.557 -4110.1997\n",
            "wrong_move\n",
            "   311/50000: episode: 310, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8432.873 -3786.332\n",
            "wrong_move\n",
            "   312/50000: episode: 311, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8620.3545 -3795.9075\n",
            "wrong_move\n",
            "   313/50000: episode: 312, duration: 0.027s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9056.727 -4015.541\n",
            "wrong_move\n",
            "   314/50000: episode: 313, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9060.2705 -4017.2332\n",
            "wrong_move\n",
            "   315/50000: episode: 314, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8559.556 -3784.675\n",
            "wrong_move\n",
            "   316/50000: episode: 315, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8826.079 -4099.7354\n",
            "wrong_move\n",
            "   317/50000: episode: 316, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8636.589 -4147.5947\n",
            "wrong_move\n",
            "   318/50000: episode: 317, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9062.356 -4026.2942\n",
            "wrong_move\n",
            "   319/50000: episode: 318, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8893.15 -4082.9778\n",
            "wrong_move\n",
            "   320/50000: episode: 319, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8566.305 -3788.4868\n",
            "wrong_move\n",
            "   321/50000: episode: 320, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8687.842 -3798.6582\n",
            "wrong_move\n",
            "   322/50000: episode: 321, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9054.735 -4026.134\n",
            "wrong_move\n",
            "   323/50000: episode: 322, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8425.993 -3791.5645\n",
            "wrong_move\n",
            "   324/50000: episode: 323, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8359.969 -3783.4944\n",
            "wrong_move\n",
            "   325/50000: episode: 324, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8605.056 -3795.3186\n",
            "wrong_move\n",
            "   326/50000: episode: 325, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8821.696 -4072.6934\n",
            "wrong_move\n",
            "   327/50000: episode: 326, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8324.7 -3784.6438\n",
            "wrong_move\n",
            "   328/50000: episode: 327, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9081.344 -4009.8298\n",
            "wrong_move\n",
            "   329/50000: episode: 328, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8760.179 -3789.2295\n",
            "wrong_move\n",
            "   330/50000: episode: 329, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8509.924 -4173.296\n",
            "wrong_move\n",
            "   331/50000: episode: 330, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8675.578 -4076.807\n",
            "wrong_move\n",
            "   332/50000: episode: 331, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8537.888 -3791.05\n",
            "wrong_move\n",
            "   333/50000: episode: 332, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8548.763 -3791.7263\n",
            "wrong_move\n",
            "   334/50000: episode: 333, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2066.000 [2066.000, 2066.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8911.829 -4040.0989\n",
            "wrong_move\n",
            "   335/50000: episode: 334, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9081.025 -4000.7778\n",
            "wrong_move\n",
            "   336/50000: episode: 335, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8575.246 -4208.485\n",
            "wrong_move\n",
            "   337/50000: episode: 336, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9344.884 -3880.3792\n",
            "wrong_move\n",
            "   338/50000: episode: 337, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9158.432 -3405.9192\n",
            "wrong_move\n",
            "   339/50000: episode: 338, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1191.000 [1191.000, 1191.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8464.985 -3795.1506\n",
            "wrong_move\n",
            "   340/50000: episode: 339, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9199.204 -3585.425\n",
            "wrong_move\n",
            "   341/50000: episode: 340, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8700.56 -3920.7935\n",
            "wrong_move\n",
            "   342/50000: episode: 341, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9131.024 -3956.2405\n",
            "wrong_move\n",
            "   343/50000: episode: 342, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8515.789 -3788.9065\n",
            "wrong_move\n",
            "   344/50000: episode: 343, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8429.872 -4136.8623\n",
            "wrong_move\n",
            "   345/50000: episode: 344, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8684.607 -4111.383\n",
            "wrong_move\n",
            "   346/50000: episode: 345, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9257.3545 -3771.0876\n",
            "wrong_move\n",
            "   347/50000: episode: 346, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1100.000 [1100.000, 1100.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8192.0 -4119.1577\n",
            "wrong_move\n",
            "   348/50000: episode: 347, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8411.622 -3786.267\n",
            "wrong_move\n",
            "   349/50000: episode: 348, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8489.555 -3787.5906\n",
            "wrong_move\n",
            "   350/50000: episode: 349, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8119.5527 -3783.9243\n",
            "wrong_move\n",
            "   351/50000: episode: 350, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9073.633 -4013.1375\n",
            "wrong_move\n",
            "   352/50000: episode: 351, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9077.963 -4005.227\n",
            "wrong_move\n",
            "   353/50000: episode: 352, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8832.535 -3807.4277\n",
            "wrong_move\n",
            "   354/50000: episode: 353, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8394.308 -3787.4983\n",
            "wrong_move\n",
            "   355/50000: episode: 354, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2454.000 [2454.000, 2454.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9075.176 -4019.8066\n",
            "wrong_move\n",
            "   356/50000: episode: 355, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8599.325 -3794.9648\n",
            "wrong_move\n",
            "   357/50000: episode: 356, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8443.778 -3788.8167\n",
            "wrong_move\n",
            "   358/50000: episode: 357, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9070.265 -3986.6907\n",
            "wrong_move\n",
            "   359/50000: episode: 358, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9075.218 -4002.361\n",
            "wrong_move\n",
            "   360/50000: episode: 359, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9083.972 -4008.0454\n",
            "wrong_move\n",
            "   361/50000: episode: 360, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9089.01 -3997.3892\n",
            "wrong_move\n",
            "   362/50000: episode: 361, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8959.563 -4025.9038\n",
            "wrong_move\n",
            "   363/50000: episode: 362, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8785.413 -3127.836\n",
            "wrong_move\n",
            "   364/50000: episode: 363, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9123.107 -3968.483\n",
            "wrong_move\n",
            "   365/50000: episode: 364, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8447.912 -3785.2834\n",
            "wrong_move\n",
            "   366/50000: episode: 365, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8368.164 -3783.1487\n",
            "wrong_move\n",
            "   367/50000: episode: 366, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9064.862 -4002.6182\n",
            "wrong_move\n",
            "   368/50000: episode: 367, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8278.632 -3783.762\n",
            "wrong_move\n",
            "   369/50000: episode: 368, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9064.699 -4024.3435\n",
            "wrong_move\n",
            "   370/50000: episode: 369, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8426.401 -3785.6086\n",
            "wrong_move\n",
            "   371/50000: episode: 370, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8402.759 -3787.1877\n",
            "wrong_move\n",
            "   372/50000: episode: 371, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9101.252 -3998.3208\n",
            "wrong_move\n",
            "   373/50000: episode: 372, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9119.961 -3966.902\n",
            "wrong_move\n",
            "   374/50000: episode: 373, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8544.846 -4074.8267\n",
            "wrong_move\n",
            "   375/50000: episode: 374, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9151.324 -3913.4722\n",
            "wrong_move\n",
            "   376/50000: episode: 375, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9079.554 -3990.393\n",
            "wrong_move\n",
            "   377/50000: episode: 376, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8683.935 -3797.0737\n",
            "wrong_move\n",
            "   378/50000: episode: 377, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9074.897 -4012.082\n",
            "wrong_move\n",
            "   379/50000: episode: 378, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8776.885 -3278.1245\n",
            "wrong_move\n",
            "   380/50000: episode: 379, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8713.444 -3799.422\n",
            "wrong_move\n",
            "   381/50000: episode: 380, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8805.893 -3802.2632\n",
            "wrong_move\n",
            "   382/50000: episode: 381, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9059.458 -4023.9836\n",
            "wrong_move\n",
            "   383/50000: episode: 382, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9083.48 -4009.4966\n",
            "wrong_move\n",
            "   384/50000: episode: 383, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.083 -4011.546\n",
            "wrong_move\n",
            "   385/50000: episode: 384, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8900.231 -4066.0398\n",
            "wrong_move\n",
            "   386/50000: episode: 385, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9686.741 -2932.9954\n",
            "wrong_move\n",
            "   387/50000: episode: 386, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8369.776 -3784.845\n",
            "wrong_move\n",
            "   388/50000: episode: 387, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8937.579 -4034.7148\n",
            "wrong_move\n",
            "   389/50000: episode: 388, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9165.827 -3922.5898\n",
            "wrong_move\n",
            "   390/50000: episode: 389, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8778.105 -4110.1216\n",
            "wrong_move\n",
            "   391/50000: episode: 390, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9204.344 -3769.4062\n",
            "wrong_move\n",
            "   392/50000: episode: 391, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9151.463 -3919.3203\n",
            "wrong_move\n",
            "   393/50000: episode: 392, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8149.0166 -3821.6838\n",
            "wrong_move\n",
            "   394/50000: episode: 393, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8932.671 -4042.2432\n",
            "wrong_move\n",
            "   395/50000: episode: 394, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8871.362 -3800.2402\n",
            "wrong_move\n",
            "   396/50000: episode: 395, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9208.416 -3267.7769\n",
            "wrong_move\n",
            "   397/50000: episode: 396, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8768.988 -3801.9517\n",
            "wrong_move\n",
            "   398/50000: episode: 397, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9049.542 -4009.221\n",
            "wrong_move\n",
            "   399/50000: episode: 398, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8397.777 -4147.7183\n",
            "wrong_move\n",
            "   400/50000: episode: 399, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7871.6553 -3908.7903\n",
            "wrong_move\n",
            "   401/50000: episode: 400, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2239.000 [2239.000, 2239.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8761.814 -4001.0588\n",
            "wrong_move\n",
            "   402/50000: episode: 401, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8945.703 -3801.754\n",
            "wrong_move\n",
            "   403/50000: episode: 402, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1100.000 [1100.000, 1100.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8467.6045 -3789.8047\n",
            "wrong_move\n",
            "   404/50000: episode: 403, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8538.077 -4200.1406\n",
            "wrong_move\n",
            "   405/50000: episode: 404, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9080.602 -4011.4482\n",
            "wrong_move\n",
            "   406/50000: episode: 405, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8316.357 -4156.904\n",
            "wrong_move\n",
            "   407/50000: episode: 406, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8475.538 -3787.7666\n",
            "wrong_move\n",
            "   408/50000: episode: 407, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9241.264 -3873.5022\n",
            "wrong_move\n",
            "   409/50000: episode: 408, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8494.185 -3788.6267\n",
            "wrong_move\n",
            "   410/50000: episode: 409, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9062.256 -3986.2874\n",
            "wrong_move\n",
            "   411/50000: episode: 410, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8738.069 -4102.523\n",
            "wrong_move\n",
            "   412/50000: episode: 411, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8079.956 -3708.6694\n",
            "wrong_move\n",
            "   413/50000: episode: 412, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1125.000 [1125.000, 1125.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9105.327 -3997.2861\n",
            "wrong_move\n",
            "   414/50000: episode: 413, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9130.399 -3923.6624\n",
            "wrong_move\n",
            "   415/50000: episode: 414, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8557.483 -3793.3135\n",
            "wrong_move\n",
            "   416/50000: episode: 415, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8399.274 -3793.3293\n",
            "wrong_move\n",
            "   417/50000: episode: 416, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9532.023 -3643.2131\n",
            "wrong_move\n",
            "   418/50000: episode: 417, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8657.245 -3797.3574\n",
            "wrong_move\n",
            "   419/50000: episode: 418, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8743.085 -4104.1416\n",
            "wrong_move\n",
            "   420/50000: episode: 419, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8172.425 -3780.8057\n",
            "wrong_move\n",
            "   421/50000: episode: 420, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9433.963 -3856.273\n",
            "wrong_move\n",
            "   422/50000: episode: 421, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8781.363 -3799.9456\n",
            "wrong_move\n",
            "   423/50000: episode: 422, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8468.602 -3788.5986\n",
            "wrong_move\n",
            "   424/50000: episode: 423, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8759.02 -4104.3257\n",
            "wrong_move\n",
            "   425/50000: episode: 424, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8870.795 -4087.3838\n",
            "wrong_move\n",
            "   426/50000: episode: 425, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8382.65 -3784.9739\n",
            "wrong_move\n",
            "   427/50000: episode: 426, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8578.575 -3795.1592\n",
            "wrong_move\n",
            "   428/50000: episode: 427, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8398.645 -3787.4973\n",
            "wrong_move\n",
            "   429/50000: episode: 428, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8385.155 -4131.408\n",
            "wrong_move\n",
            "   430/50000: episode: 429, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8436.001 -3788.7297\n",
            "wrong_move\n",
            "   431/50000: episode: 430, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9010.99 -3818.8293\n",
            "wrong_move\n",
            "   432/50000: episode: 431, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9076.313 -4000.899\n",
            "wrong_move\n",
            "   433/50000: episode: 432, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8713.417 -4092.147\n",
            "wrong_move\n",
            "   434/50000: episode: 433, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8917.32 -4033.3396\n",
            "wrong_move\n",
            "   435/50000: episode: 434, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8496.125 -3791.6816\n",
            "wrong_move\n",
            "   436/50000: episode: 435, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8465.05 -3786.516\n",
            "wrong_move\n",
            "   437/50000: episode: 436, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8584.587 -4092.3372\n",
            "wrong_move\n",
            "   438/50000: episode: 437, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8846.088 -3806.7268\n",
            "wrong_move\n",
            "   439/50000: episode: 438, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8523.907 -3792.06\n",
            "wrong_move\n",
            "   440/50000: episode: 439, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9046.303 -4014.2727\n",
            "wrong_move\n",
            "   441/50000: episode: 440, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9132.927 -3973.468\n",
            "wrong_move\n",
            "   442/50000: episode: 441, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9076.845 -4013.496\n",
            "wrong_move\n",
            "   443/50000: episode: 442, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9064.963 -4018.238\n",
            "wrong_move\n",
            "   444/50000: episode: 443, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8086.8438 -3775.0745\n",
            "wrong_move\n",
            "   445/50000: episode: 444, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9090.848 -4000.5105\n",
            "wrong_move\n",
            "   446/50000: episode: 445, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8847.976 -4077.6318\n",
            "wrong_move\n",
            "   447/50000: episode: 446, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8304.613 -3783.4329\n",
            "wrong_move\n",
            "   448/50000: episode: 447, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8525.311 -3790.9807\n",
            "wrong_move\n",
            "   449/50000: episode: 448, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8576.745 -3791.9968\n",
            "wrong_move\n",
            "   450/50000: episode: 449, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8640.232 -3797.4163\n",
            "wrong_move\n",
            "   451/50000: episode: 450, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9058.992 -4008.119\n",
            "wrong_move\n",
            "   452/50000: episode: 451, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8405.858 -4113.663\n",
            "wrong_move\n",
            "   453/50000: episode: 452, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   454/50000: episode: 453, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8473.635 -3787.3435\n",
            "wrong_move\n",
            "   455/50000: episode: 454, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8276.483 -3781.7825\n",
            "wrong_move\n",
            "   456/50000: episode: 455, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8614.728 -3793.1758\n",
            "wrong_move\n",
            "   457/50000: episode: 456, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8517.069 -3792.3728\n",
            "wrong_move\n",
            "   458/50000: episode: 457, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8253.876 -3774.703\n",
            "wrong_move\n",
            "   459/50000: episode: 458, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9116.712 -3984.0383\n",
            "wrong_move\n",
            "   460/50000: episode: 459, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9117.187 -3962.9216\n",
            "wrong_move\n",
            "   461/50000: episode: 460, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8398.428 -3786.8567\n",
            "wrong_move\n",
            "   462/50000: episode: 461, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9069.813 -4011.6833\n",
            "wrong_move\n",
            "   463/50000: episode: 462, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9042.933 -4028.5195\n",
            "wrong_move\n",
            "   464/50000: episode: 463, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8545.225 -4128.7324\n",
            "wrong_move\n",
            "   465/50000: episode: 464, duration: 0.027s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9169.579 -3893.5798\n",
            "wrong_move\n",
            "   466/50000: episode: 465, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8438.869 -3792.2744\n",
            "wrong_move\n",
            "   467/50000: episode: 466, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8387.809 -3784.2927\n",
            "wrong_move\n",
            "   468/50000: episode: 467, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9169.529 -3939.2046\n",
            "wrong_move\n",
            "   469/50000: episode: 468, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "done, took 11.618 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 50000 steps ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val: -8979.911 -3985.2292\n",
            "wrong_move\n",
            "     1/50000: episode: 1, duration: 0.455s, episode steps:   1, steps per second:   2, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8418.774 -3788.6082\n",
            "wrong_move\n",
            "     2/50000: episode: 2, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8365.691 -4024.56\n",
            "wrong_move\n",
            "     3/50000: episode: 3, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9145.995 -3936.1526\n",
            "wrong_move\n",
            "     4/50000: episode: 4, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8835.622 -4072.4512\n",
            "wrong_move\n",
            "     5/50000: episode: 5, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8576.651 -4117.2876\n",
            "wrong_move\n",
            "     6/50000: episode: 6, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8833.45 -3798.8762\n",
            "wrong_move\n",
            "     7/50000: episode: 7, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8511.63 -4139.2866\n",
            "wrong_move\n",
            "     8/50000: episode: 8, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8730.592 -3799.0396\n",
            "wrong_move\n",
            "     9/50000: episode: 9, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8702.853 -4003.5393\n",
            "wrong_move\n",
            "    10/50000: episode: 10, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9291.935 -3732.6833\n",
            "wrong_move\n",
            "    11/50000: episode: 11, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9058.006 -4006.592\n",
            "wrong_move\n",
            "    12/50000: episode: 12, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8966.451 -4028.7234\n",
            "wrong_move\n",
            "    13/50000: episode: 13, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8068.602 -3766.34\n",
            "wrong_move\n",
            "    14/50000: episode: 14, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8548.876 -4135.4097\n",
            "wrong_move\n",
            "    15/50000: episode: 15, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9132.875 -3935.0745\n",
            "wrong_move\n",
            "    16/50000: episode: 16, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8435.087 -3787.0103\n",
            "wrong_move\n",
            "    17/50000: episode: 17, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8331.946 -3783.3057\n",
            "wrong_move\n",
            "    18/50000: episode: 18, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9092.031 -4006.249\n",
            "wrong_move\n",
            "    19/50000: episode: 19, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8752.405 -4022.4812\n",
            "wrong_move\n",
            "    20/50000: episode: 20, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8522.2 -3793.6963\n",
            "wrong_move\n",
            "    21/50000: episode: 21, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8742.26 -4089.558\n",
            "wrong_move\n",
            "    22/50000: episode: 22, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8696.315 -3086.1462\n",
            "wrong_move\n",
            "    23/50000: episode: 23, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9052.574 -3974.2595\n",
            "wrong_move\n",
            "    24/50000: episode: 24, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9054.379 -4021.7705\n",
            "wrong_move\n",
            "    25/50000: episode: 25, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8990.37 -4026.189\n",
            "wrong_move\n",
            "    26/50000: episode: 26, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8611.114 -3797.487\n",
            "wrong_move\n",
            "    27/50000: episode: 27, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8883.461 -4060.4773\n",
            "wrong_move\n",
            "    28/50000: episode: 28, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9129.354 -3913.033\n",
            "wrong_move\n",
            "    29/50000: episode: 29, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9092.323 -3975.7517\n",
            "wrong_move\n",
            "    30/50000: episode: 30, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9075.73 -3977.8618\n",
            "wrong_move\n",
            "    31/50000: episode: 31, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8592.616 -3992.0066\n",
            "wrong_move\n",
            "    32/50000: episode: 32, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2672.000 [2672.000, 2672.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9279.978 -3812.2056\n",
            "wrong_move\n",
            "    33/50000: episode: 33, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9098.562 -3982.2815\n",
            "wrong_move\n",
            "    34/50000: episode: 34, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9054.738 -4009.7986\n",
            "wrong_move\n",
            "    35/50000: episode: 35, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9094.277 -3991.8696\n",
            "wrong_move\n",
            "    36/50000: episode: 36, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9113.333 -3966.654\n",
            "wrong_move\n",
            "    37/50000: episode: 37, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9099.869 -4001.7192\n",
            "wrong_move\n",
            "    38/50000: episode: 38, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8349.042 -3782.5369\n",
            "wrong_move\n",
            "    39/50000: episode: 39, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8463.583 -3791.2192\n",
            "wrong_move\n",
            "    40/50000: episode: 40, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8837.33 -4035.5327\n",
            "wrong_move\n",
            "    41/50000: episode: 41, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9070.297 -4019.261\n",
            "wrong_move\n",
            "    42/50000: episode: 42, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9069.727 -4011.475\n",
            "wrong_move\n",
            "    43/50000: episode: 43, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8369.767 -3783.2693\n",
            "wrong_move\n",
            "    44/50000: episode: 44, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8735.772 -3800.4814\n",
            "wrong_move\n",
            "    45/50000: episode: 45, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9092.317 -3982.6428\n",
            "wrong_move\n",
            "    46/50000: episode: 46, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8319.1045 -3784.741\n",
            "wrong_move\n",
            "    47/50000: episode: 47, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8582.241 -3792.276\n",
            "wrong_move\n",
            "    48/50000: episode: 48, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9080.659 -4011.21\n",
            "wrong_move\n",
            "    49/50000: episode: 49, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8685.494 -3785.5132\n",
            "wrong_move\n",
            "    50/50000: episode: 50, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8748.068 -3800.8882\n",
            "wrong_move\n",
            "    51/50000: episode: 51, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8815.948 -4036.712\n",
            "wrong_move\n",
            "    52/50000: episode: 52, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8407.238 -3787.741\n",
            "wrong_move\n",
            "    53/50000: episode: 53, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8944.509 -4051.621\n",
            "wrong_move\n",
            "    54/50000: episode: 54, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9022.655 -3983.2134\n",
            "wrong_move\n",
            "    55/50000: episode: 55, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8815.585 -4064.903\n",
            "wrong_move\n",
            "    56/50000: episode: 56, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9088.079 -4000.2815\n",
            "wrong_move\n",
            "    57/50000: episode: 57, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8209.274 -3891.4963\n",
            "wrong_move\n",
            "    58/50000: episode: 58, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9100.094 -3999.7676\n",
            "wrong_move\n",
            "    59/50000: episode: 59, duration: 0.017s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9139.119 -3926.7349\n",
            "wrong_move\n",
            "    60/50000: episode: 60, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "    61/50000: episode: 61, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8708.327 -4052.2922\n",
            "wrong_move\n",
            "    62/50000: episode: 62, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9252.649 -4063.017\n",
            "wrong_move\n",
            "    63/50000: episode: 63, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.048 -4024.9905\n",
            "wrong_move\n",
            "    64/50000: episode: 64, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9063.613 -4019.34\n",
            "wrong_move\n",
            "    65/50000: episode: 65, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9079.915 -4003.4102\n",
            "wrong_move\n",
            "    66/50000: episode: 66, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8555.079 -3788.7048\n",
            "wrong_move\n",
            "    67/50000: episode: 67, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9083.176 -3998.84\n",
            "wrong_move\n",
            "    68/50000: episode: 68, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8771.157 -4104.0635\n",
            "wrong_move\n",
            "    69/50000: episode: 69, duration: 0.024s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8308.298 -3783.5781\n",
            "wrong_move\n",
            "    70/50000: episode: 70, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8558.002 -3793.5413\n",
            "wrong_move\n",
            "    71/50000: episode: 71, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8763.84 -4101.449\n",
            "wrong_move\n",
            "    72/50000: episode: 72, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8447.065 -3788.0032\n",
            "wrong_move\n",
            "    73/50000: episode: 73, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8873.837 -4101.6206\n",
            "wrong_move\n",
            "    74/50000: episode: 74, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9069.835 -4011.968\n",
            "wrong_move\n",
            "    75/50000: episode: 75, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9071.036 -4012.4888\n",
            "wrong_move\n",
            "    76/50000: episode: 76, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9077.543 -4013.8696\n",
            "wrong_move\n",
            "    77/50000: episode: 77, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8788.93 -4041.9973\n",
            "wrong_move\n",
            "    78/50000: episode: 78, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9076.578 -4006.4207\n",
            "wrong_move\n",
            "    79/50000: episode: 79, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8583.555 -3795.0115\n",
            "wrong_move\n",
            "    80/50000: episode: 80, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8416.672 -3789.1792\n",
            "wrong_move\n",
            "    81/50000: episode: 81, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2454.000 [2454.000, 2454.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8521.961 -3793.5461\n",
            "wrong_move\n",
            "    82/50000: episode: 82, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8759.751 -4124.2954\n",
            "wrong_move\n",
            "    83/50000: episode: 83, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8420.365 -3787.9272\n",
            "wrong_move\n",
            "    84/50000: episode: 84, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8635.277 -3797.9802\n",
            "wrong_move\n",
            "    85/50000: episode: 85, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9112.16 -3958.1455\n",
            "wrong_move\n",
            "    86/50000: episode: 86, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8421.495 -3785.9604\n",
            "wrong_move\n",
            "    87/50000: episode: 87, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8611.179 -4136.1475\n",
            "wrong_move\n",
            "    88/50000: episode: 88, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8718.526 -4115.1562\n",
            "wrong_move\n",
            "    89/50000: episode: 89, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8451.288 -3789.2937\n",
            "wrong_move\n",
            "    90/50000: episode: 90, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "    91/50000: episode: 91, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8927.966 -3685.132\n",
            "wrong_move\n",
            "    92/50000: episode: 92, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8683.926 -3797.9646\n",
            "wrong_move\n",
            "    93/50000: episode: 93, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8800.159 -4114.656\n",
            "wrong_move\n",
            "    94/50000: episode: 94, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2461.000 [2461.000, 2461.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9067.55 -4012.675\n",
            "wrong_move\n",
            "    95/50000: episode: 95, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9077.254 -4004.2485\n",
            "wrong_move\n",
            "    96/50000: episode: 96, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8779.618 -3793.595\n",
            "wrong_move\n",
            "    97/50000: episode: 97, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9071.586 -4013.4202\n",
            "wrong_move\n",
            "    98/50000: episode: 98, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8434.42 -4082.2815\n",
            "wrong_move\n",
            "    99/50000: episode: 99, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9070.27 -3991.7148\n",
            "wrong_move\n",
            "   100/50000: episode: 100, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9109.583 -3974.37\n",
            "wrong_move\n",
            "   101/50000: episode: 101, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8588.684 -4169.421\n",
            "wrong_move\n",
            "   102/50000: episode: 102, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8456.252 -4082.5515\n",
            "wrong_move\n",
            "   103/50000: episode: 103, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8897.251 -4097.4883\n",
            "wrong_move\n",
            "   104/50000: episode: 104, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8348.712 -3782.1523\n",
            "wrong_move\n",
            "   105/50000: episode: 105, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8481.866 -3794.6477\n",
            "wrong_move\n",
            "   106/50000: episode: 106, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8420.529 -3788.3896\n",
            "wrong_move\n",
            "   107/50000: episode: 107, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8909.718 -4012.128\n",
            "wrong_move\n",
            "   108/50000: episode: 108, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9058.751 -4024.5164\n",
            "wrong_move\n",
            "   109/50000: episode: 109, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9099.193 -3990.0874\n",
            "wrong_move\n",
            "   110/50000: episode: 110, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8440.432 -3788.8516\n",
            "wrong_move\n",
            "   111/50000: episode: 111, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8969.708 -4010.8098\n",
            "wrong_move\n",
            "   112/50000: episode: 112, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   113/50000: episode: 113, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8491.324 -3789.8396\n",
            "wrong_move\n",
            "   114/50000: episode: 114, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8613.317 -4132.9775\n",
            "wrong_move\n",
            "   115/50000: episode: 115, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8417.03 -3788.1396\n",
            "wrong_move\n",
            "   116/50000: episode: 116, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9075.109 -4005.3452\n",
            "wrong_move\n",
            "   117/50000: episode: 117, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9069.54 -4017.2861\n",
            "wrong_move\n",
            "   118/50000: episode: 118, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8380.364 -3783.2805\n",
            "wrong_move\n",
            "   119/50000: episode: 119, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9137.415 -3969.57\n",
            "wrong_move\n",
            "   120/50000: episode: 120, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8606.89 -3796.3472\n",
            "wrong_move\n",
            "   121/50000: episode: 121, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9082.052 -4000.2524\n",
            "wrong_move\n",
            "   122/50000: episode: 122, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8869.476 -4060.2102\n",
            "wrong_move\n",
            "   123/50000: episode: 123, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8432.732 -3786.187\n",
            "wrong_move\n",
            "   124/50000: episode: 124, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9139.629 -3986.2305\n",
            "wrong_move\n",
            "   125/50000: episode: 125, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8685.456 -4140.233\n",
            "wrong_move\n",
            "   126/50000: episode: 126, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8524.875 -3789.7874\n",
            "wrong_move\n",
            "   127/50000: episode: 127, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8355.669 -3785.8047\n",
            "wrong_move\n",
            "   128/50000: episode: 128, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8991.348 -3966.4282\n",
            "wrong_move\n",
            "   129/50000: episode: 129, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9102.065 -3956.9722\n",
            "wrong_move\n",
            "   130/50000: episode: 130, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8795.669 -3993.1108\n",
            "wrong_move\n",
            "   131/50000: episode: 131, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8828.689 -4078.8618\n",
            "wrong_move\n",
            "   132/50000: episode: 132, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8320.805 -3781.8\n",
            "wrong_move\n",
            "   133/50000: episode: 133, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9065.617 -4022.5798\n",
            "wrong_move\n",
            "   134/50000: episode: 134, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9088.476 -4008.8557\n",
            "wrong_move\n",
            "   135/50000: episode: 135, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8896.868 -4082.4346\n",
            "wrong_move\n",
            "   136/50000: episode: 136, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9084.166 -4009.113\n",
            "wrong_move\n",
            "   137/50000: episode: 137, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8461.297 -3788.3562\n",
            "wrong_move\n",
            "   138/50000: episode: 138, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8388.013 -4122.4526\n",
            "wrong_move\n",
            "   139/50000: episode: 139, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9071.942 -4016.5332\n",
            "wrong_move\n",
            "   140/50000: episode: 140, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8506.533 -3791.7825\n",
            "wrong_move\n",
            "   141/50000: episode: 141, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8204.604 -3783.2454\n",
            "wrong_move\n",
            "   142/50000: episode: 142, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8410.056 -3779.6243\n",
            "wrong_move\n",
            "   143/50000: episode: 143, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2417.000 [2417.000, 2417.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8070.0938 -3726.6355\n",
            "wrong_move\n",
            "   144/50000: episode: 144, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2111.000 [2111.000, 2111.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9110.378 -3987.808\n",
            "wrong_move\n",
            "   145/50000: episode: 145, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9126.893 -3973.9888\n",
            "wrong_move\n",
            "   146/50000: episode: 146, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9066.726 -4021.2358\n",
            "wrong_move\n",
            "   147/50000: episode: 147, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9078.276 -3995.6536\n",
            "wrong_move\n",
            "   148/50000: episode: 148, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8326.725 -3786.9683\n",
            "wrong_move\n",
            "   149/50000: episode: 149, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9055.205 -4028.356\n",
            "wrong_move\n",
            "   150/50000: episode: 150, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9095.491 -4003.2297\n",
            "wrong_move\n",
            "   151/50000: episode: 151, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9069.183 -4014.5999\n",
            "wrong_move\n",
            "   152/50000: episode: 152, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.98 -4022.9922\n",
            "wrong_move\n",
            "   153/50000: episode: 153, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8791.337 -4072.7976\n",
            "wrong_move\n",
            "   154/50000: episode: 154, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9106.231 -3985.0652\n",
            "wrong_move\n",
            "   155/50000: episode: 155, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9080.196 -4012.2415\n",
            "wrong_move\n",
            "   156/50000: episode: 156, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8700.796 -3798.7783\n",
            "wrong_move\n",
            "   157/50000: episode: 157, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8885.46 -3793.0225\n",
            "wrong_move\n",
            "   158/50000: episode: 158, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9069.492 -4028.626\n",
            "wrong_move\n",
            "   159/50000: episode: 159, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8952.789 -4003.0889\n",
            "wrong_move\n",
            "   160/50000: episode: 160, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8793.301 -3801.5208\n",
            "wrong_move\n",
            "   161/50000: episode: 161, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8698.725 -3800.5283\n",
            "wrong_move\n",
            "   162/50000: episode: 162, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8599.448 -3796.8992\n",
            "wrong_move\n",
            "   163/50000: episode: 163, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9132.378 -3935.7737\n",
            "wrong_move\n",
            "   164/50000: episode: 164, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8520.438 -3792.0083\n",
            "wrong_move\n",
            "   165/50000: episode: 165, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9073.198 -4011.6292\n",
            "wrong_move\n",
            "   166/50000: episode: 166, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9100.939 -3989.105\n",
            "wrong_move\n",
            "   167/50000: episode: 167, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8954.305 -3985.8276\n",
            "wrong_move\n",
            "   168/50000: episode: 168, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8440.325 -3788.1155\n",
            "wrong_move\n",
            "   169/50000: episode: 169, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3531.000 [3531.000, 3531.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9060.624 -4025.737\n",
            "wrong_move\n",
            "   170/50000: episode: 170, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8384.045 -3786.561\n",
            "wrong_move\n",
            "   171/50000: episode: 171, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8347.67 -3789.9583\n",
            "wrong_move\n",
            "   172/50000: episode: 172, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2454.000 [2454.000, 2454.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9102.575 -3964.5566\n",
            "wrong_move\n",
            "   173/50000: episode: 173, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8471.56 -3788.5627\n",
            "wrong_move\n",
            "   174/50000: episode: 174, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8582.116 -3794.6848\n",
            "wrong_move\n",
            "   175/50000: episode: 175, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8969.145 -3958.5386\n",
            "wrong_move\n",
            "   176/50000: episode: 176, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9136.367 -3951.715\n",
            "wrong_move\n",
            "   177/50000: episode: 177, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8308.32 -3788.5593\n",
            "wrong_move\n",
            "   178/50000: episode: 178, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2778.000 [2778.000, 2778.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9087.307 -4000.0315\n",
            "wrong_move\n",
            "   179/50000: episode: 179, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8517.3 -4106.572\n",
            "wrong_move\n",
            "   180/50000: episode: 180, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8266.446 -3785.98\n",
            "wrong_move\n",
            "   181/50000: episode: 181, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8647.054 -3794.716\n",
            "wrong_move\n",
            "   182/50000: episode: 182, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9096.563 -3968.6704\n",
            "wrong_move\n",
            "   183/50000: episode: 183, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8979.644 -3929.2026\n",
            "wrong_move\n",
            "   184/50000: episode: 184, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2090.000 [2090.000, 2090.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8897.805 -4099.1094\n",
            "wrong_move\n",
            "   185/50000: episode: 185, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8805.035 -3798.8157\n",
            "wrong_move\n",
            "   186/50000: episode: 186, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9038.179 -4026.707\n",
            "wrong_move\n",
            "   187/50000: episode: 187, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9139.629 -3958.4055\n",
            "wrong_move\n",
            "   188/50000: episode: 188, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8485.349 -3787.0845\n",
            "wrong_move\n",
            "   189/50000: episode: 189, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8584.346 -4089.21\n",
            "wrong_move\n",
            "   190/50000: episode: 190, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8466.182 -3788.2126\n",
            "wrong_move\n",
            "   191/50000: episode: 191, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8553.177 -4083.3984\n",
            "wrong_move\n",
            "   192/50000: episode: 192, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8612.282 -3795.5002\n",
            "wrong_move\n",
            "   193/50000: episode: 193, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   194/50000: episode: 194, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8941.698 -4036.2844\n",
            "wrong_move\n",
            "   195/50000: episode: 195, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9052.01 -4025.764\n",
            "wrong_move\n",
            "   196/50000: episode: 196, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9065.737 -4023.6345\n",
            "wrong_move\n",
            "   197/50000: episode: 197, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8343.61 -3782.441\n",
            "wrong_move\n",
            "   198/50000: episode: 198, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8753.346 -4107.6997\n",
            "wrong_move\n",
            "   199/50000: episode: 199, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9089.793 -3986.7976\n",
            "wrong_move\n",
            "   200/50000: episode: 200, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   201/50000: episode: 201, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   202/50000: episode: 202, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8601.159 -3797.8425\n",
            "wrong_move\n",
            "   203/50000: episode: 203, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2454.000 [2454.000, 2454.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8250.736 -3780.491\n",
            "wrong_move\n",
            "   204/50000: episode: 204, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8500.457 -3707.4143\n",
            "wrong_move\n",
            "   205/50000: episode: 205, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8931.273 -4021.4631\n",
            "wrong_move\n",
            "   206/50000: episode: 206, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9091.55 -3684.09\n",
            "wrong_move\n",
            "   207/50000: episode: 207, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8393.873 -3985.161\n",
            "wrong_move\n",
            "   208/50000: episode: 208, duration: 0.027s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8770.701 -4085.0676\n",
            "wrong_move\n",
            "   209/50000: episode: 209, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9088.647 -4000.4907\n",
            "wrong_move\n",
            "   210/50000: episode: 210, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8902.957 -4056.552\n",
            "wrong_move\n",
            "   211/50000: episode: 211, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9113.552 -3943.4915\n",
            "wrong_move\n",
            "   212/50000: episode: 212, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8293.111 -3783.235\n",
            "wrong_move\n",
            "   213/50000: episode: 213, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9098.197 -3999.725\n",
            "wrong_move\n",
            "   214/50000: episode: 214, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9104.569 -3926.0208\n",
            "wrong_move\n",
            "   215/50000: episode: 215, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8461.099 -3788.4775\n",
            "wrong_move\n",
            "   216/50000: episode: 216, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9080.049 -4004.3242\n",
            "wrong_move\n",
            "   217/50000: episode: 217, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8651.788 -3797.01\n",
            "wrong_move\n",
            "   218/50000: episode: 218, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8461.661 -3793.301\n",
            "wrong_move\n",
            "   219/50000: episode: 219, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9017.231 -4004.5586\n",
            "wrong_move\n",
            "   220/50000: episode: 220, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9052.886 -4026.1338\n",
            "wrong_move\n",
            "   221/50000: episode: 221, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8414.251 -3787.8066\n",
            "wrong_move\n",
            "   222/50000: episode: 222, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8561.441 -3787.781\n",
            "wrong_move\n",
            "   223/50000: episode: 223, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9010.225 -4042.4136\n",
            "wrong_move\n",
            "   224/50000: episode: 224, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9267.099 -3519.8381\n",
            "wrong_move\n",
            "   225/50000: episode: 225, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9109.898 -3977.6868\n",
            "wrong_move\n",
            "   226/50000: episode: 226, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9071.878 -3993.9395\n",
            "wrong_move\n",
            "   227/50000: episode: 227, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9075.448 -4015.4492\n",
            "wrong_move\n",
            "   228/50000: episode: 228, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9069.839 -4017.4211\n",
            "wrong_move\n",
            "   229/50000: episode: 229, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8968.904 -4033.5073\n",
            "wrong_move\n",
            "   230/50000: episode: 230, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9070.602 -3999.008\n",
            "wrong_move\n",
            "   231/50000: episode: 231, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9097.183 -3998.3345\n",
            "wrong_move\n",
            "   232/50000: episode: 232, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9129.78 -3936.7312\n",
            "wrong_move\n",
            "   233/50000: episode: 233, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9080.706 -4010.1377\n",
            "wrong_move\n",
            "   234/50000: episode: 234, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9076.548 -4005.686\n",
            "wrong_move\n",
            "   235/50000: episode: 235, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8343.612 -3786.3376\n",
            "wrong_move\n",
            "   236/50000: episode: 236, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9008.927 -3976.477\n",
            "wrong_move\n",
            "   237/50000: episode: 237, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8410.828 -3784.8247\n",
            "wrong_move\n",
            "   238/50000: episode: 238, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8990.534 -3793.8713\n",
            "wrong_move\n",
            "   239/50000: episode: 239, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8920.14 -4010.7942\n",
            "wrong_move\n",
            "   240/50000: episode: 240, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8522.299 -3791.3345\n",
            "wrong_move\n",
            "   241/50000: episode: 241, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8810.571 -3799.4648\n",
            "wrong_move\n",
            "   242/50000: episode: 242, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8199.329 -3776.9573\n",
            "wrong_move\n",
            "   243/50000: episode: 243, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8702.349 -3937.0894\n",
            "wrong_move\n",
            "   244/50000: episode: 244, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8577.238 -3793.6494\n",
            "wrong_move\n",
            "   245/50000: episode: 245, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9079.172 -3988.2825\n",
            "wrong_move\n",
            "   246/50000: episode: 246, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9086.294 -4006.3914\n",
            "wrong_move\n",
            "   247/50000: episode: 247, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9065.617 -4022.5798\n",
            "wrong_move\n",
            "   248/50000: episode: 248, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8919.036 -4034.95\n",
            "wrong_move\n",
            "   249/50000: episode: 249, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9081.417 -4010.2888\n",
            "wrong_move\n",
            "   250/50000: episode: 250, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9071.913 -3994.4182\n",
            "wrong_move\n",
            "   251/50000: episode: 251, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9089.769 -3981.8228\n",
            "wrong_move\n",
            "   252/50000: episode: 252, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9073.841 -4001.0964\n",
            "wrong_move\n",
            "   253/50000: episode: 253, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9131.096 -3966.925\n",
            "wrong_move\n",
            "   254/50000: episode: 254, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8992.679 -3926.385\n",
            "wrong_move\n",
            "   255/50000: episode: 255, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9060.799 -4021.5469\n",
            "wrong_move\n",
            "   256/50000: episode: 256, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8133.0986 -3771.782\n",
            "wrong_move\n",
            "   257/50000: episode: 257, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9025.571 -3928.8906\n",
            "wrong_move\n",
            "   258/50000: episode: 258, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8939.296 -4057.5903\n",
            "wrong_move\n",
            "   259/50000: episode: 259, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9052.886 -4026.1338\n",
            "wrong_move\n",
            "   260/50000: episode: 260, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8530.8545 -3790.5645\n",
            "wrong_move\n",
            "   261/50000: episode: 261, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8828.46 -4062.263\n",
            "wrong_move\n",
            "   262/50000: episode: 262, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9111.02 -3970.4414\n",
            "wrong_move\n",
            "   263/50000: episode: 263, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8403.854 -3787.2954\n",
            "wrong_move\n",
            "   264/50000: episode: 264, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9106.882 -3974.6333\n",
            "wrong_move\n",
            "   265/50000: episode: 265, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9059.87 -4027.0212\n",
            "wrong_move\n",
            "   266/50000: episode: 266, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8690.579 -4117.9097\n",
            "wrong_move\n",
            "   267/50000: episode: 267, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8991.126 -3934.6267\n",
            "wrong_move\n",
            "   268/50000: episode: 268, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8389.149 -3784.5093\n",
            "wrong_move\n",
            "   269/50000: episode: 269, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8807.0625 -4105.014\n",
            "wrong_move\n",
            "   270/50000: episode: 270, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9100.813 -3996.1138\n",
            "wrong_move\n",
            "   271/50000: episode: 271, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8301.935 -3783.2832\n",
            "wrong_move\n",
            "   272/50000: episode: 272, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8766.565 -3799.665\n",
            "wrong_move\n",
            "   273/50000: episode: 273, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9028.041 -3532.116\n",
            "wrong_move\n",
            "   274/50000: episode: 274, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9067.634 -4020.818\n",
            "wrong_move\n",
            "   275/50000: episode: 275, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8607.245 -4077.4944\n",
            "wrong_move\n",
            "   276/50000: episode: 276, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8727.046 -3800.304\n",
            "wrong_move\n",
            "   277/50000: episode: 277, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8716.901 -3799.7444\n",
            "wrong_move\n",
            "   278/50000: episode: 278, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9480.059 -3052.0603\n",
            "wrong_move\n",
            "   279/50000: episode: 279, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9110.784 -3977.0137\n",
            "wrong_move\n",
            "   280/50000: episode: 280, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8622.449 -3794.2253\n",
            "wrong_move\n",
            "   281/50000: episode: 281, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9066.378 -4016.2866\n",
            "wrong_move\n",
            "   282/50000: episode: 282, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9102.65 -3983.3523\n",
            "wrong_move\n",
            "   283/50000: episode: 283, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9171.764 -3946.3215\n",
            "wrong_move\n",
            "   284/50000: episode: 284, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9113.933 -3945.3599\n",
            "wrong_move\n",
            "   285/50000: episode: 285, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9072.202 -4025.356\n",
            "wrong_move\n",
            "   286/50000: episode: 286, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8887.351 -3800.6836\n",
            "wrong_move\n",
            "   287/50000: episode: 287, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8405.156 -3784.5354\n",
            "wrong_move\n",
            "   288/50000: episode: 288, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9216.355 -3896.4424\n",
            "wrong_move\n",
            "   289/50000: episode: 289, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9148.647 -3945.598\n",
            "wrong_move\n",
            "   290/50000: episode: 290, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -2707725300.0 79286930000.0\n",
            "wrong_move\n",
            "   291/50000: episode: 291, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8486.029 -3790.1533\n",
            "wrong_move\n",
            "   292/50000: episode: 292, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9069.382 -4006.1143\n",
            "wrong_move\n",
            "   293/50000: episode: 293, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8786.863 -3677.0352\n",
            "wrong_move\n",
            "   294/50000: episode: 294, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9096.431 -3959.1382\n",
            "wrong_move\n",
            "   295/50000: episode: 295, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8677.065 -4154.8896\n",
            "wrong_move\n",
            "   296/50000: episode: 296, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8448.611 -3785.639\n",
            "wrong_move\n",
            "   297/50000: episode: 297, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8603.549 -3957.2678\n",
            "wrong_move\n",
            "   298/50000: episode: 298, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9064.053 -4019.2456\n",
            "wrong_move\n",
            "   299/50000: episode: 299, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8527.391 -4204.1265\n",
            "wrong_move\n",
            "   300/50000: episode: 300, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8391.26 -3788.265\n",
            "wrong_move\n",
            "   301/50000: episode: 301, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2454.000 [2454.000, 2454.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8554.865 -3791.12\n",
            "wrong_move\n",
            "   302/50000: episode: 302, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8955.54 -3985.4104\n",
            "wrong_move\n",
            "   303/50000: episode: 303, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8780.378 -4121.4766\n",
            "wrong_move\n",
            "   304/50000: episode: 304, duration: 0.007s, episode steps:   1, steps per second: 150, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8990.588 -3991.3704\n",
            "wrong_move\n",
            "   305/50000: episode: 305, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9080.081 -4010.0222\n",
            "wrong_move\n",
            "   306/50000: episode: 306, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8989.65 -4028.7114\n",
            "wrong_move\n",
            "   307/50000: episode: 307, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8507.132 -3791.627\n",
            "wrong_move\n",
            "   308/50000: episode: 308, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8444.825 -3442.4548\n",
            "wrong_move\n",
            "   309/50000: episode: 309, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8921.569 -4089.9722\n",
            "wrong_move\n",
            "   310/50000: episode: 310, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9105.023 -3988.9497\n",
            "wrong_move\n",
            "   311/50000: episode: 311, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9120.688 -3938.3123\n",
            "wrong_move\n",
            "   312/50000: episode: 312, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -10074.58 -3516.6152\n",
            "wrong_move\n",
            "   313/50000: episode: 313, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9076.051 -4011.9915\n",
            "wrong_move\n",
            "   314/50000: episode: 314, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8761.2705 -4078.3948\n",
            "wrong_move\n",
            "   315/50000: episode: 315, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8527.232 -3790.6045\n",
            "wrong_move\n",
            "   316/50000: episode: 316, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9069.692 -4022.1697\n",
            "wrong_move\n",
            "   317/50000: episode: 317, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9066.321 -4004.1064\n",
            "wrong_move\n",
            "   318/50000: episode: 318, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8295.08 -3783.6628\n",
            "wrong_move\n",
            "   319/50000: episode: 319, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9065.319 -4009.2744\n",
            "wrong_move\n",
            "   320/50000: episode: 320, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8351.894 -4178.1123\n",
            "wrong_move\n",
            "   321/50000: episode: 321, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8063.6704 -4081.237\n",
            "wrong_move\n",
            "   322/50000: episode: 322, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8422.661 -4042.065\n",
            "wrong_move\n",
            "   323/50000: episode: 323, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8477.831 -3789.798\n",
            "wrong_move\n",
            "   324/50000: episode: 324, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8417.669 -3788.4578\n",
            "wrong_move\n",
            "   325/50000: episode: 325, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9070.102 -4017.7024\n",
            "wrong_move\n",
            "   326/50000: episode: 326, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8812.78 -4066.5999\n",
            "wrong_move\n",
            "   327/50000: episode: 327, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8811.424 -4076.9473\n",
            "wrong_move\n",
            "   328/50000: episode: 328, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8509.266 -3790.2683\n",
            "wrong_move\n",
            "   329/50000: episode: 329, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8133.336 -4129.7695\n",
            "wrong_move\n",
            "   330/50000: episode: 330, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2239.000 [2239.000, 2239.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8568.351 -3795.725\n",
            "wrong_move\n",
            "   331/50000: episode: 331, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2454.000 [2454.000, 2454.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8800.625 -4103.3867\n",
            "wrong_move\n",
            "   332/50000: episode: 332, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8996.152 -3955.997\n",
            "wrong_move\n",
            "   333/50000: episode: 333, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9056.363 -4014.0212\n",
            "wrong_move\n",
            "   334/50000: episode: 334, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9151.358 -3925.6138\n",
            "wrong_move\n",
            "   335/50000: episode: 335, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7896.575 -3760.5696\n",
            "wrong_move\n",
            "   336/50000: episode: 336, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8746.997 -4053.7703\n",
            "wrong_move\n",
            "   337/50000: episode: 337, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8853.861 -4107.2446\n",
            "wrong_move\n",
            "   338/50000: episode: 338, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9066.456 -3975.8264\n",
            "wrong_move\n",
            "   339/50000: episode: 339, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9068.8125 -3993.4602\n",
            "wrong_move\n",
            "   340/50000: episode: 340, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8484.477 -3789.4802\n",
            "wrong_move\n",
            "   341/50000: episode: 341, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9138.335 -3947.0063\n",
            "wrong_move\n",
            "   342/50000: episode: 342, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8825.359 -3800.0015\n",
            "wrong_move\n",
            "   343/50000: episode: 343, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9092.349 -3997.0344\n",
            "wrong_move\n",
            "   344/50000: episode: 344, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8529.039 -3790.568\n",
            "wrong_move\n",
            "   345/50000: episode: 345, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8766.132 -4100.75\n",
            "wrong_move\n",
            "   346/50000: episode: 346, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8798.812 -4115.2935\n",
            "wrong_move\n",
            "   347/50000: episode: 347, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9059.773 -4017.1167\n",
            "wrong_move\n",
            "   348/50000: episode: 348, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8361.925 -3783.4285\n",
            "wrong_move\n",
            "   349/50000: episode: 349, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9066.829 -4011.567\n",
            "wrong_move\n",
            "   350/50000: episode: 350, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8680.55 -3667.588\n",
            "wrong_move\n",
            "   351/50000: episode: 351, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9050.212 -4021.57\n",
            "wrong_move\n",
            "   352/50000: episode: 352, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9065.435 -3991.4797\n",
            "wrong_move\n",
            "   353/50000: episode: 353, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8993.426 -3861.708\n",
            "wrong_move\n",
            "   354/50000: episode: 354, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8420.422 -3784.8066\n",
            "wrong_move\n",
            "   355/50000: episode: 355, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9143.646 -3919.661\n",
            "wrong_move\n",
            "   356/50000: episode: 356, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8951.209 -3995.0476\n",
            "wrong_move\n",
            "   357/50000: episode: 357, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8798.953 -4070.634\n",
            "wrong_move\n",
            "   358/50000: episode: 358, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8494.298 -3791.265\n",
            "wrong_move\n",
            "   359/50000: episode: 359, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9092.887 -4005.691\n",
            "wrong_move\n",
            "   360/50000: episode: 360, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8813.802 -3792.391\n",
            "wrong_move\n",
            "   361/50000: episode: 361, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8373.3125 -3786.1157\n",
            "wrong_move\n",
            "   362/50000: episode: 362, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9090.326 -4007.1736\n",
            "wrong_move\n",
            "   363/50000: episode: 363, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9112.1455 -3986.3237\n",
            "wrong_move\n",
            "   364/50000: episode: 364, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8514.536 -3788.7273\n",
            "wrong_move\n",
            "   365/50000: episode: 365, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8310.844 -3781.465\n",
            "wrong_move\n",
            "   366/50000: episode: 366, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8407.135 -3787.9646\n",
            "wrong_move\n",
            "   367/50000: episode: 367, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9104.109 -3966.4863\n",
            "wrong_move\n",
            "   368/50000: episode: 368, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9077.558 -4015.5771\n",
            "wrong_move\n",
            "   369/50000: episode: 369, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9067.461 -4022.7769\n",
            "wrong_move\n",
            "   370/50000: episode: 370, duration: 0.009s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9103.832 -3978.6445\n",
            "wrong_move\n",
            "   371/50000: episode: 371, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8194.854 -4098.2637\n",
            "wrong_move\n",
            "   372/50000: episode: 372, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9110.505 -3959.0195\n",
            "wrong_move\n",
            "   373/50000: episode: 373, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8549.639 -3793.1729\n",
            "wrong_move\n",
            "   374/50000: episode: 374, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8669.37 -4078.7175\n",
            "wrong_move\n",
            "   375/50000: episode: 375, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8688.996 -3929.7163\n",
            "wrong_move\n",
            "   376/50000: episode: 376, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8670.113 -3956.6292\n",
            "wrong_move\n",
            "   377/50000: episode: 377, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   378/50000: episode: 378, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8354.742 -3785.298\n",
            "wrong_move\n",
            "   379/50000: episode: 379, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9131.995 -3958.3682\n",
            "wrong_move\n",
            "   380/50000: episode: 380, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9152.651 -3598.0186\n",
            "wrong_move\n",
            "   381/50000: episode: 381, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8632.175 -4156.9453\n",
            "wrong_move\n",
            "   382/50000: episode: 382, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9015.45 -3801.8943\n",
            "wrong_move\n",
            "   383/50000: episode: 383, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1100.000 [1100.000, 1100.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8981.247 -3945.5027\n",
            "wrong_move\n",
            "   384/50000: episode: 384, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8500.694 -3791.8127\n",
            "wrong_move\n",
            "   385/50000: episode: 385, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8280.366 -3776.426\n",
            "wrong_move\n",
            "   386/50000: episode: 386, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9086.339 -4006.6982\n",
            "wrong_move\n",
            "   387/50000: episode: 387, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9054.179 -4024.232\n",
            "wrong_move\n",
            "   388/50000: episode: 388, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9084.134 -4010.8555\n",
            "wrong_move\n",
            "   389/50000: episode: 389, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8472.68 -3788.9531\n",
            "wrong_move\n",
            "   390/50000: episode: 390, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9063.861 -4017.7334\n",
            "wrong_move\n",
            "   391/50000: episode: 391, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9259.354 -3525.8633\n",
            "wrong_move\n",
            "   392/50000: episode: 392, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8255.1045 -3780.4878\n",
            "wrong_move\n",
            "   393/50000: episode: 393, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9074.028 -4012.8115\n",
            "wrong_move\n",
            "   394/50000: episode: 394, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9008.461 -4004.8533\n",
            "wrong_move\n",
            "   395/50000: episode: 395, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8589.312 -4167.4316\n",
            "wrong_move\n",
            "   396/50000: episode: 396, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8623.112 -3485.3994\n",
            "wrong_move\n",
            "   397/50000: episode: 397, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8384.015 -3691.0044\n",
            "wrong_move\n",
            "   398/50000: episode: 398, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8854.874 -4058.0503\n",
            "wrong_move\n",
            "   399/50000: episode: 399, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9062.108 -4026.019\n",
            "wrong_move\n",
            "   400/50000: episode: 400, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8852.859 -3798.7825\n",
            "wrong_move\n",
            "   401/50000: episode: 401, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.98 -4022.9922\n",
            "wrong_move\n",
            "   402/50000: episode: 402, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8134.9 -3771.2808\n",
            "wrong_move\n",
            "   403/50000: episode: 403, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9066.042 -4018.8894\n",
            "wrong_move\n",
            "   404/50000: episode: 404, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9086.715 -3949.3955\n",
            "wrong_move\n",
            "   405/50000: episode: 405, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8443.642 -3786.422\n",
            "wrong_move\n",
            "   406/50000: episode: 406, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9077.542 -3989.9224\n",
            "wrong_move\n",
            "   407/50000: episode: 407, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9134.627 -3963.839\n",
            "wrong_move\n",
            "   408/50000: episode: 408, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8511.342 -3789.809\n",
            "wrong_move\n",
            "   409/50000: episode: 409, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9080.118 -4001.3474\n",
            "wrong_move\n",
            "   410/50000: episode: 410, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9070.663 -3965.961\n",
            "wrong_move\n",
            "   411/50000: episode: 411, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8509.022 -3791.6326\n",
            "wrong_move\n",
            "   412/50000: episode: 412, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9073.801 -4006.5864\n",
            "wrong_move\n",
            "   413/50000: episode: 413, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   414/50000: episode: 414, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8662.632 -4019.1692\n",
            "wrong_move\n",
            "   415/50000: episode: 415, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2571.000 [2571.000, 2571.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8664.333 -4085.2444\n",
            "wrong_move\n",
            "   416/50000: episode: 416, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9087.403 -4002.519\n",
            "wrong_move\n",
            "   417/50000: episode: 417, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8418.394 -3785.839\n",
            "wrong_move\n",
            "   418/50000: episode: 418, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8450.339 -3791.7646\n",
            "wrong_move\n",
            "   419/50000: episode: 419, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9056.744 -4030.6348\n",
            "wrong_move\n",
            "   420/50000: episode: 420, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8614.84 -3794.212\n",
            "wrong_move\n",
            "   421/50000: episode: 421, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8464.715 -3789.9521\n",
            "wrong_move\n",
            "   422/50000: episode: 422, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8754.014 -4032.9146\n",
            "wrong_move\n",
            "   423/50000: episode: 423, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8912.453 -2917.1707\n",
            "wrong_move\n",
            "   424/50000: episode: 424, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9002.927 -3918.0469\n",
            "wrong_move\n",
            "   425/50000: episode: 425, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9047.021 -4001.7363\n",
            "wrong_move\n",
            "   426/50000: episode: 426, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9058.6875 -4013.6328\n",
            "wrong_move\n",
            "   427/50000: episode: 427, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9108.725 -3978.7747\n",
            "wrong_move\n",
            "   428/50000: episode: 428, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4087.000 [4087.000, 4087.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8466.194 -3790.1155\n",
            "wrong_move\n",
            "   429/50000: episode: 429, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8711.933 -3799.4792\n",
            "wrong_move\n",
            "   430/50000: episode: 430, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9130.809 -3979.5444\n",
            "wrong_move\n",
            "   431/50000: episode: 431, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8631.38 -4080.2373\n",
            "wrong_move\n",
            "   432/50000: episode: 432, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8967.491 -3967.9985\n",
            "wrong_move\n",
            "   433/50000: episode: 433, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9092.045 -3987.711\n",
            "wrong_move\n",
            "   434/50000: episode: 434, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9185.797 -3914.1785\n",
            "wrong_move\n",
            "   435/50000: episode: 435, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8481.729 -3790.5105\n",
            "wrong_move\n",
            "   436/50000: episode: 436, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8848.106 -3798.6335\n",
            "wrong_move\n",
            "   437/50000: episode: 437, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9095.632 -3984.9363\n",
            "wrong_move\n",
            "   438/50000: episode: 438, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8633.532 -3716.3032\n",
            "wrong_move\n",
            "   439/50000: episode: 439, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 677.000 [677.000, 677.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   440/50000: episode: 440, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9142.523 -3987.3723\n",
            "wrong_move\n",
            "   441/50000: episode: 441, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8932.99 -4021.5269\n",
            "wrong_move\n",
            "   442/50000: episode: 442, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9080.57 -4019.8164\n",
            "wrong_move\n",
            "   443/50000: episode: 443, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8570.257 -3794.4866\n",
            "wrong_move\n",
            "   444/50000: episode: 444, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8660.344 -4096.2905\n",
            "wrong_move\n",
            "   445/50000: episode: 445, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9302.055 -3440.041\n",
            "wrong_move\n",
            "   446/50000: episode: 446, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1191.000 [1191.000, 1191.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8434.453 -3787.1648\n",
            "wrong_move\n",
            "   447/50000: episode: 447, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9071.107 -4017.158\n",
            "wrong_move\n",
            "   448/50000: episode: 448, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8812.279 -3799.319\n",
            "wrong_move\n",
            "   449/50000: episode: 449, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9084.018 -4012.1008\n",
            "wrong_move\n",
            "   450/50000: episode: 450, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8562.857 -3792.2676\n",
            "wrong_move\n",
            "   451/50000: episode: 451, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8711.076 -3433.5542\n",
            "wrong_move\n",
            "   452/50000: episode: 452, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8968.319 -3942.0696\n",
            "wrong_move\n",
            "   453/50000: episode: 453, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9103.354 -3974.691\n",
            "wrong_move\n",
            "   454/50000: episode: 454, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9065.432 -4016.1013\n",
            "wrong_move\n",
            "   455/50000: episode: 455, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9008.583 -3919.9604\n",
            "wrong_move\n",
            "   456/50000: episode: 456, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9056.87 -4025.2253\n",
            "wrong_move\n",
            "   457/50000: episode: 457, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8864.81 -4090.6543\n",
            "wrong_move\n",
            "   458/50000: episode: 458, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9041.652 -3848.0103\n",
            "wrong_move\n",
            "   459/50000: episode: 459, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8727.038 -4103.6313\n",
            "wrong_move\n",
            "   460/50000: episode: 460, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8838.464 -4086.4033\n",
            "wrong_move\n",
            "   461/50000: episode: 461, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8473.405 -3786.8782\n",
            "wrong_move\n",
            "   462/50000: episode: 462, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9090.293 -3971.7078\n",
            "wrong_move\n",
            "   463/50000: episode: 463, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9316.273 -3437.999\n",
            "wrong_move\n",
            "   464/50000: episode: 464, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1191.000 [1191.000, 1191.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8826.556 -4035.817\n",
            "wrong_move\n",
            "   465/50000: episode: 465, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9057.822 -4005.0276\n",
            "wrong_move\n",
            "   466/50000: episode: 466, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8470.066 -3792.7126\n",
            "wrong_move\n",
            "   467/50000: episode: 467, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8269.787 -3779.869\n",
            "wrong_move\n",
            "   468/50000: episode: 468, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8664.483 -3799.032\n",
            "wrong_move\n",
            "   469/50000: episode: 469, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8405.526 -3784.4753\n",
            "wrong_move\n",
            "   470/50000: episode: 470, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9070.597 -4012.1213\n",
            "wrong_move\n",
            "   471/50000: episode: 471, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8572.641 -3795.8801\n",
            "wrong_move\n",
            "   472/50000: episode: 472, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8518.921 -3790.1343\n",
            "wrong_move\n",
            "   473/50000: episode: 473, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9110.7295 -3942.924\n",
            "wrong_move\n",
            "   474/50000: episode: 474, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -7937.242 -4075.2412\n",
            "wrong_move\n",
            "   475/50000: episode: 475, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3242.000 [3242.000, 3242.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8199.792 -3779.7097\n",
            "wrong_move\n",
            "   476/50000: episode: 476, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9632.321 -3415.6145\n",
            "wrong_move\n",
            "   477/50000: episode: 477, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8571.628 -4085.1643\n",
            "wrong_move\n",
            "   478/50000: episode: 478, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8527.845 -4139.354\n",
            "wrong_move\n",
            "   479/50000: episode: 479, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8611.492 -3916.0842\n",
            "wrong_move\n",
            "   480/50000: episode: 480, duration: 0.027s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9067.421 -4021.4326\n",
            "wrong_move\n",
            "   481/50000: episode: 481, duration: 0.017s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8533.128 -3791.4375\n",
            "wrong_move\n",
            "   482/50000: episode: 482, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8393.942 -3786.3403\n",
            "wrong_move\n",
            "   483/50000: episode: 483, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8696.279 -3798.786\n",
            "wrong_move\n",
            "   484/50000: episode: 484, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9060.2705 -4017.2332\n",
            "wrong_move\n",
            "   485/50000: episode: 485, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9053.9795 -4023.3467\n",
            "wrong_move\n",
            "   486/50000: episode: 486, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9081.636 -4017.9407\n",
            "wrong_move\n",
            "   487/50000: episode: 487, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8490.269 -3791.3076\n",
            "wrong_move\n",
            "   488/50000: episode: 488, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9051.138 -4007.0728\n",
            "wrong_move\n",
            "   489/50000: episode: 489, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9015.094 -3936.6064\n",
            "wrong_move\n",
            "   490/50000: episode: 490, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8829.804 -3801.2998\n",
            "wrong_move\n",
            "   491/50000: episode: 491, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8471.325 -3787.0674\n",
            "wrong_move\n",
            "   492/50000: episode: 492, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8811.205 -4068.0103\n",
            "wrong_move\n",
            "   493/50000: episode: 493, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8533.619 -3789.3562\n",
            "wrong_move\n",
            "   494/50000: episode: 494, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8823.987 -3800.2869\n",
            "wrong_move\n",
            "   495/50000: episode: 495, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8106.9375 -3775.64\n",
            "wrong_move\n",
            "   496/50000: episode: 496, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8748.334 -3796.664\n",
            "wrong_move\n",
            "   497/50000: episode: 497, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8274.784 -4145.6543\n",
            "wrong_move\n",
            "   498/50000: episode: 498, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3421.000 [3421.000, 3421.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8528.384 -3790.2664\n",
            "wrong_move\n",
            "   499/50000: episode: 499, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9056.607 -4004.444\n",
            "wrong_move\n",
            "   500/50000: episode: 500, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9127.529 -3965.331\n",
            "wrong_move\n",
            "   501/50000: episode: 501, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9061.221 -4018.8235\n",
            "wrong_move\n",
            "   502/50000: episode: 502, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8539.678 -3791.2603\n",
            "wrong_move\n",
            "   503/50000: episode: 503, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8895.756 -3998.1353\n",
            "wrong_move\n",
            "   504/50000: episode: 504, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8946.235 -3975.5793\n",
            "wrong_move\n",
            "   505/50000: episode: 505, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8481.756 -3787.447\n",
            "wrong_move\n",
            "   506/50000: episode: 506, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8778.066 -3802.2122\n",
            "wrong_move\n",
            "   507/50000: episode: 507, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8489.728 -3791.0942\n",
            "wrong_move\n",
            "   508/50000: episode: 508, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8875.024 -3802.7266\n",
            "wrong_move\n",
            "   509/50000: episode: 509, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1100.000 [1100.000, 1100.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9065.7295 -4015.163\n",
            "wrong_move\n",
            "   510/50000: episode: 510, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8368.7 -3784.1338\n",
            "wrong_move\n",
            "   511/50000: episode: 511, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8810.045 -3800.506\n",
            "wrong_move\n",
            "   512/50000: episode: 512, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2668.000 [2668.000, 2668.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9070.345 -4003.3354\n",
            "wrong_move\n",
            "   513/50000: episode: 513, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8462.65 -3787.814\n",
            "wrong_move\n",
            "   514/50000: episode: 514, duration: 0.027s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8039.9727 -3714.9832\n",
            "wrong_move\n",
            "   515/50000: episode: 515, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8402.82 -3788.234\n",
            "wrong_move\n",
            "   516/50000: episode: 516, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8470.62 -4242.0996\n",
            "wrong_move\n",
            "   517/50000: episode: 517, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9028.982 -4026.369\n",
            "wrong_move\n",
            "   518/50000: episode: 518, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 385.000 [385.000, 385.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8384.65 -3786.3071\n",
            "wrong_move\n",
            "   519/50000: episode: 519, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8849.45 -4027.2683\n",
            "wrong_move\n",
            "   520/50000: episode: 520, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8975.927 -3941.297\n",
            "wrong_move\n",
            "   521/50000: episode: 521, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9416.094 -3435.1497\n",
            "wrong_move\n",
            "   522/50000: episode: 522, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8227.153 -3781.406\n",
            "wrong_move\n",
            "   523/50000: episode: 523, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9030.737 -3966.8445\n",
            "wrong_move\n",
            "   524/50000: episode: 524, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8487.341 -3790.8286\n",
            "wrong_move\n",
            "   525/50000: episode: 525, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9070.597 -4018.2085\n",
            "wrong_move\n",
            "   526/50000: episode: 526, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8396.109 -3784.089\n",
            "wrong_move\n",
            "   527/50000: episode: 527, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8485.536 -3964.811\n",
            "wrong_move\n",
            "   528/50000: episode: 528, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8473.88 -3787.3054\n",
            "wrong_move\n",
            "   529/50000: episode: 529, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8617.024 -3796.641\n",
            "wrong_move\n",
            "   530/50000: episode: 530, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8162.3623 -3779.1611\n",
            "wrong_move\n",
            "   531/50000: episode: 531, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8483.93 -3791.6436\n",
            "wrong_move\n",
            "   532/50000: episode: 532, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9276.627 -2798.2585\n",
            "wrong_move\n",
            "   533/50000: episode: 533, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3999.000 [3999.000, 3999.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9070.891 -3998.0522\n",
            "wrong_move\n",
            "   534/50000: episode: 534, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8994.791 -3977.1045\n",
            "wrong_move\n",
            "   535/50000: episode: 535, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8952.114 -3943.133\n",
            "wrong_move\n",
            "   536/50000: episode: 536, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8667.692 -4134.6973\n",
            "wrong_move\n",
            "   537/50000: episode: 537, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8567.105 -3791.6235\n",
            "wrong_move\n",
            "   538/50000: episode: 538, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8843.87 -4079.364\n",
            "wrong_move\n",
            "   539/50000: episode: 539, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9090.972 -3993.7314\n",
            "wrong_move\n",
            "   540/50000: episode: 540, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8567.78 -3792.1775\n",
            "wrong_move\n",
            "   541/50000: episode: 541, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8967.75 -4028.9473\n",
            "wrong_move\n",
            "   542/50000: episode: 542, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8304.692 -3781.7732\n",
            "wrong_move\n",
            "   543/50000: episode: 543, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 18.000 [18.000, 18.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -9073.078 -4018.6677\n",
            "wrong_move\n",
            "   544/50000: episode: 544, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: --, mae: --, mean_q: --\n",
            "Val: -8842.053 -3759.5422\n",
            "wrong_move\n",
            "   545/50000: episode: 545, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1125.000 [1125.000, 1125.000],  loss: --, mae: --, mean_q: --\n"
          ]
        }
      ],
      "source": [
        "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
        "# even the metrics!\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "for i in range (10):\n",
        "  policy = EpsGreedyQPolicy(0.01)\n",
        "  dqn = DQNAgent(model=model, nb_actions=NB_ACTIONS, memory=memory,\n",
        "                target_model_update=1e-2, policy=policy)\n",
        "  dqn.compile(Adam(lr=1e-4), metrics=['mae'])\n",
        "\n",
        "  # Okay, now it's time to learn something! We visualize the training here for show, but this\n",
        "  # slows down training quite a lot. You can always safely abort the training prematurely using\n",
        "  # Ctrl + C.\n",
        "  his = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
        "  \n",
        "  model.save('chess_model_sf.h5')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "rl_dqn_vs_sf_train.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
