{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras-rl2 in /home/kienanh/.local/lib/python3.9/site-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in /home/kienanh/.local/lib/python3.9/site-packages (from keras-rl2) (2.5.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /usr/lib/python3/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.12.1)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.19.5)\n",
      "Requirement already satisfied: gast==0.4.0 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorflow->keras-rl2) (0.4.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.1.2)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /home/kienanh/.local/lib/python3.9/site-packages (from tensorflow->keras-rl2) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.34.1)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorflow->keras-rl2) (2.5.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorflow->keras-rl2) (0.36.2)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorflow->keras-rl2) (3.1.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorflow->keras-rl2) (0.13.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/lib/python3/dist-packages (from tensorflow->keras-rl2) (3.12.4)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorflow->keras-rl2) (2.7.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorflow->keras-rl2) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorflow->keras-rl2) (3.3.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorflow->keras-rl2) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (3.3.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (1.34.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (0.4.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (52.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (2.25.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/kienanh/.local/lib/python3.9/site-packages (from tensorboard~=2.5->tensorflow->keras-rl2) (2.0.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (4.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (4.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2) (0.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/kienanh/.local/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->keras-rl2) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->keras-rl2) (3.1.0)\n",
      "Installing collected packages: typing-extensions\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.0.1\n",
      "    Uninstalling typing-extensions-4.0.1:\n",
      "      Successfully uninstalled typing-extensions-4.0.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mypy 0.930 requires typing-extensions>=3.10, but you have typing-extensions 3.7.4.3 which is incompatible.\u001b[0m\n",
      "Successfully installed typing-extensions-3.7.4.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.2; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: chess in /home/kienanh/.local/lib/python3.9/site-packages (1.7.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.2; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-chess in /home/kienanh/.local/lib/python3.9/site-packages (1.999)\n",
      "Requirement already satisfied: chess<2,>=1 in /home/kienanh/.local/lib/python3.9/site-packages (from python-chess) (1.7.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.2; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install keras-rl2\n",
    "! pip install chess\n",
    "! pip install python-chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bot_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_564588/2577697367.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplatform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m  \u001b[0mbot_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel888\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_model888\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bot_model'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten,\\\n",
    "     Input,BatchNormalization, Dropout, Conv2D, Add, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import Policy\n",
    "\n",
    "import chess\n",
    "import chess.engine\n",
    "from sys import platform\n",
    "import os\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(0, '../../bot_model/')\n",
    "# from  model888 import get_model888\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.experimental.output_all_intermediates(True)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_562205/2258033288.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4430433725400181736\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-23 23:00:53.411448: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from drive.MyDrive.Data.Chess.model888 import get_model888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "os.system('chmod +x drive/MyDrive/Data/Chess/stockfish_14.1_linux_x64')\n",
    "engine = chess.engine.SimpleEngine.popen_uci(r\"drive/MyDrive/Data/Chess/stockfish_14.1_linux_x64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%\n",
    "# if platform == \"linux\" or platform == \"linux2\":\n",
    "#     os.system('chmod +x ../stockfish/stockfish_14.1_linux_x64')\n",
    "#     engine = chess.engine.SimpleEngine.popen_uci(r\"../stockfish/stockfish_14.1_linux_x64\")\n",
    "# elif platform == \"win32\":\n",
    "#     engine = chess.engine.SimpleEngine.popen_uci(r\"../stockfish/stockfish_14.1_win_32bit.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_move(env):\n",
    "    result = engine.play(env.env, chess.engine.Limit(time=0.05))\n",
    "    return result.move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SHAPE = (8, 8, 8)\n",
    "NB_ACTIONS = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(x,square,map):\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            if square[i][j]==map:\n",
    "                x[i][j]=1\n",
    "            elif square[i][j] == -map:\n",
    "                x[i][j]=-1   \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessEnv:\n",
    "    '''\n",
    "    state - obser: ndarray - (65,): [:65] is flatten from int_board; [65] is color of bot; 1 is white and -1 is black\n",
    "    step: int. step_range = (0, 4096) , is encoded from square A to square B (64 x 64 val)\n",
    "    reward: int\n",
    "    '''\n",
    "\n",
    "    mapped = {\n",
    "            'P': 10,     # White Pawn\n",
    "            'p': -10,    # Black Pawn\n",
    "            'N': 20,     # White Knight\n",
    "            'n': -20,    # Black Knight\n",
    "            'B': 30,     # White Bishop\n",
    "            'b': -30,    # Black Bishop\n",
    "            'R': 40,     # White Rook\n",
    "            'r': -40,    # Black Rook\n",
    "            'Q': 50,     # White Queen\n",
    "            'q': -50,    # Black Queen\n",
    "            'K': 900,     # White King\n",
    "            'k': -900     # Black King\n",
    "    }\n",
    "    point=[10,20,30,40,50,900]\n",
    "    state = None\n",
    "    model = None\n",
    "    neg_r_each_step = -1\n",
    "    \n",
    "    def __init__(self, model: Sequential, ReplayMem: SequentialMemory, neg_r_each_step = -1, stockfishMem = True) -> None:\n",
    "        self.env = chess.Board()\n",
    "        self.ReplayMem = ReplayMem\n",
    "        self.model = model\n",
    "        self.lastest_move=[0,0]\n",
    "        self.state = self.reset()\n",
    "        self.neg_r_each_step = neg_r_each_step\n",
    "        self.stockfishMem = stockfishMem\n",
    "\n",
    "    def is_draw(self):\n",
    "        if self.env.is_stalemate():\n",
    "            print(\"statlemate\")\n",
    "            return True\n",
    "        if self.env.is_fivefold_repetition():\n",
    "            print(\"fivefold repetition\")\n",
    "            return True\n",
    "        if self.env.is_fifty_moves():\n",
    "            print(\"50 moves\")\n",
    "            return True\n",
    "        if self.env.is_insufficient_material():\n",
    "            print(\"Insufficient Material\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_checkmate(self):\n",
    "        # If There is checkmate then it will be TRUE else FALSE.It will be a boolean value.\n",
    "        return self.env.is_checkmate()\n",
    "\n",
    "    def convert_board_to_int(self):\n",
    "        epd_string = self.env.epd()\n",
    "        list_int = np.empty((0, ))\n",
    "        for i in epd_string:\n",
    "            if i == \" \":\n",
    "                list_int = list_int.reshape((8, 8))\n",
    "                return list_int\n",
    "            elif i != \"/\":\n",
    "                if i in self.mapped:\n",
    "                    list_int = np.append(list_int, self.mapped[i])\n",
    "                else:\n",
    "                    for counter in range(0, int(i)):\n",
    "                        list_int = np.append(list_int, 0)\n",
    "        list_int = list_int.reshape((8, 8))\n",
    "        return list_int\n",
    "\n",
    "    def get_state(self) -> np.ndarray:\n",
    "        square=self.convert_board_to_int()\n",
    "        x=np.zeros([8,8,8])\n",
    "        for i in range(6):\n",
    "            convert(x[i],square,self.point[i])\n",
    "        moves=self.legal_moves()\n",
    "        for move in moves:\n",
    "            a= move.from_square\n",
    "            b= move.to_square\n",
    "            x[6][7-int(a /8)][a%8]=-1\n",
    "            x[6][7-int(b /8)][b%8]=1\n",
    "        a=self.lastest_move[0]\n",
    "        b=self.lastest_move[1]\n",
    "        if a!=b:\n",
    "            x[7][7-a //8][a%8]=-1\n",
    "            x[7][7-b//8][b%8] =1   \n",
    "        else: \n",
    "            x[7]=[[0]*8 for i in range(8)]\n",
    "        return x   \n",
    "        \n",
    "\n",
    "    def legal_moves(self):\n",
    "        return list(self.env.legal_moves)\n",
    "\n",
    "    def legal_moves_encoded(self):\n",
    "        lg_encoded = []\n",
    "        for move in (self.env.legal_moves):\n",
    "            from_square = move.from_square\n",
    "            to_square = move.to_square\n",
    "            \n",
    "            lg_encoded.append(from_square * 64 + to_square)\n",
    "        return np.array(lg_encoded)\n",
    "\n",
    "    def encodeMove(self, move_uci:str):\n",
    "        a, b = chess.parse_square(move_uci[:2]), chess.parse_square(move_uci[2:])\n",
    "        return a * 64 + b\n",
    "\n",
    "    def decodeMove(self, move_int:int):\n",
    "        a, b = move_int//64, move_int%64\n",
    "        move = self.env.find_move(from_square= a,to_square= b)\n",
    "        return move\n",
    "\n",
    "    def render(self):\n",
    "        print(self.env.unicode())\n",
    "\n",
    "    def reset(self):\n",
    "        # random state\n",
    "        redo = True\n",
    "        num_sample_steps = 0\n",
    "        while redo:\n",
    "            redo = False\n",
    "            self.env = chess.Board()\n",
    "            num_sample_steps = np.random.randint(0, 15)\n",
    "            for i in range (num_sample_steps):\n",
    "                lg_move = self.legal_moves()\n",
    "                if len(lg_move) != 0:\n",
    "                    move = np.random.choice(self.legal_moves())\n",
    "                    self.env.push(move)\n",
    "                else:\n",
    "                    redo = True\n",
    "                    break\n",
    "            if len(self.legal_moves()) == 0:\n",
    "                redo = True\n",
    "\n",
    "        if len(self.env.move_stack) !=0:\n",
    "            self.lastest_move[0]= self.env.move_stack[-1].from_square\n",
    "            self.lastest_move[1]= self.env.move_stack[-1].to_square\n",
    "        else:\n",
    "            self.lastest_move[0]=0\n",
    "            self.lastest_move[1]=0\n",
    "        if self.env.turn == False:\n",
    "            self.env=self.env.mirror() \n",
    "            for i in range(2):\n",
    "                a=7-self.lastest_move[i]//8\n",
    "                b=self.lastest_move[i]%8\n",
    "                self.lastest_move[i]=8*a+b\n",
    "\n",
    "        self.state =  self.get_state()\n",
    "\n",
    "        Q_val = np.sort(self.model.predict(self.state.reshape((1, 1) + STATE_SHAPE)).reshape(-1, ))\n",
    "        print('Val', num_sample_steps, ':', Q_val[0],  Q_val[-1], Q_val[4050], Q_val[4000], Q_val[3000],Q_val[2000])\n",
    "        return self.state\n",
    "\n",
    "    def ifStockfishTurn(self) -> None:\n",
    "        done = False\n",
    "        reward = self.neg_r_each_step\n",
    "\n",
    "        stf_move = find_move(self)\n",
    "\n",
    "        # location to_square\n",
    "        to_r, to_c = 7 - stf_move.to_square//8, stf_move.to_square%8\n",
    "        try:\n",
    "            reward += self.point[np.where(self.state[:6, to_r, to_c ] != 0)[0][0]]\n",
    "        except:\n",
    "            reward += 0\n",
    "\n",
    "        # action\n",
    "        self.env.push(stf_move)\n",
    "\n",
    "        #convert turn\n",
    "        self.lastest_move[0]= self.env.move_stack[-1].from_square\n",
    "        self.lastest_move[1]= self.env.move_stack[-1].to_square \n",
    "        self.env=self.env.mirror() \n",
    "        \n",
    "        for i in range(2):\n",
    "            a=7-self.lastest_move[i]//8\n",
    "            b=self.lastest_move[i]%8\n",
    "            self.lastest_move[i]=8*a+b\n",
    "\n",
    "        pseudo_state = self.get_state()\n",
    "\n",
    "        # check end game\n",
    "        if self.is_checkmate():\n",
    "            reward += 900\n",
    "            done = True\n",
    "        elif self.is_draw():\n",
    "            reward += 300\n",
    "            done = True\n",
    "        # opponent's turn   \n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "            move = find_move(self)\n",
    "\n",
    "            # location to_square\n",
    "            to_r, to_c = 7 - move.to_square//8, move.to_square%8\n",
    "            try:\n",
    "                reward -= self.point[np.where(pseudo_state[:6, to_r, to_c ] != 0)[0][0]]\n",
    "            except:\n",
    "                reward -= 0\n",
    "\n",
    "            # action\n",
    "            self.env.push(move)\n",
    "            self.env=self.env.mirror() \n",
    "\n",
    "            # check end game\n",
    "            if self.is_checkmate():\n",
    "                reward -= 900\n",
    "                done = True\n",
    "            elif self.is_draw():\n",
    "                reward += 300\n",
    "                done = True\n",
    "\n",
    "        self.ReplayMem.append(self.state, stf_move.from_square * 64 + stf_move.to_square, reward, done)\n",
    "\n",
    "    def step(self, action: int):\n",
    "        reward = 0\n",
    "        done = True\n",
    "        \n",
    "        try:\n",
    "            # move in legal move\n",
    "            move = self.decodeMove(action)\n",
    "        except:\n",
    "            # wrong move\n",
    "            reward = -5000\n",
    "            done = True\n",
    "            print('wrong_move')\n",
    "\n",
    "            # add in memory action if it was stockfish\n",
    "            if self.stockfishMem:\n",
    "                self.ifStockfishTurn()\n",
    "\n",
    "            return self.state, reward, done, {}\n",
    "\n",
    "        # neg reward each step\n",
    "        reward = self.neg_r_each_step\n",
    "\n",
    "        # location to_square\n",
    "        to_r, to_c = 7 - move.to_square//8, move.to_square%8\n",
    "        try:\n",
    "            reward += self.point[np.where(self.state[:6, to_r, to_c ] != 0)[0][0]]\n",
    "        except:\n",
    "            reward += 0\n",
    "\n",
    "        # action\n",
    "        self.env.push(move)\n",
    "\n",
    "        #convert turn\n",
    "        self.lastest_move[0]= self.env.move_stack[-1].from_square\n",
    "        self.lastest_move[1]= self.env.move_stack[-1].to_square\n",
    "        self.env=self.env.mirror() \n",
    "        for i in range(2):\n",
    "            a=7-self.lastest_move[i]//8\n",
    "            b=self.lastest_move[i]%8\n",
    "            self.lastest_move[i]=8*a+b\n",
    "\n",
    "        self.state = self.get_state()\n",
    "\n",
    "        # check end game\n",
    "        if self.is_checkmate():\n",
    "            reward += 900\n",
    "            done = True\n",
    "            print('Win')\n",
    "        elif self.is_draw():\n",
    "            reward += 300\n",
    "            done = True\n",
    "\n",
    "        # opponent's turn   \n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "            move = find_move(self)\n",
    "\n",
    "            # location to_square\n",
    "            to_r, to_c = 7 - move.to_square//8, move.to_square%8\n",
    "            try:\n",
    "                reward -= self.point[np.where(self.state[:6, to_r, to_c ] != 0)[0][0]]\n",
    "            except:\n",
    "                reward -= 0\n",
    "\n",
    "            # action\n",
    "            self.env.push(move)\n",
    "\n",
    "            #convert turn\n",
    "            self.lastest_move[0]= self.env.move_stack[-1].from_square\n",
    "            self.lastest_move[1]= self.env.move_stack[-1].to_square \n",
    "            self.env=self.env.mirror() \n",
    "            for i in range(2):\n",
    "                a=7-self.lastest_move[i]//8\n",
    "                b=self.lastest_move[i]%8\n",
    "                self.lastest_move[i]=8*a+b\n",
    "\n",
    "            self.state = self.get_state()\n",
    "\n",
    "            # check end game\n",
    "            if self.is_checkmate():\n",
    "                reward -= 900\n",
    "                done = True\n",
    "                print('Lose')\n",
    "            elif self.is_draw():\n",
    "                reward += 300\n",
    "                done = True\n",
    "\n",
    "        # if reward != -5000: \n",
    "        #     reward += 10000\n",
    "\n",
    "        return self.state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LegalMovesPolicy(Policy):\n",
    "    \"\"\"Implement the epsilon greedy policy\n",
    "\n",
    "    Eps Greedy policy either:\n",
    "\n",
    "    - takes a random action with probability epsilon\n",
    "    - takes current best action with prob (1 - epsilon)\n",
    "    \"\"\"\n",
    "    def __init__(self, env: ChessEnv, eps=.1 , randomPer = .4):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.env = env\n",
    "        self.randomPer = randomPer\n",
    "\n",
    "    def select_action(self, q_values):\n",
    "        \"\"\"Return the selected action\n",
    "\n",
    "        # Arguments\n",
    "            q_values (np.ndarray): List of the estimations of Q for each action\n",
    "\n",
    "        # Returns\n",
    "            Selection action\n",
    "        \"\"\"\n",
    "        assert q_values.ndim == 1\n",
    "        nb_actions = q_values.shape[0]\n",
    "\n",
    "        if np.random.uniform() < self.eps:\n",
    "            if np.random.uniform() < self.randomPer:\n",
    "                action = np.random.randint(0, nb_actions)\n",
    "                \n",
    "            else:\n",
    "                try:\n",
    "                  action = np.random.choice(self.env.legal_moves_encoded())\n",
    "                except:\n",
    "                  action = np.random.randint(0, nb_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_values)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/layers/normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1, 8, 8, 8)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 8, 8, 8)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 12, 12, 8)    0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 12, 12, 64)   32832       zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 12, 12, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 12, 12, 64)   0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 5, 5, 64)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 5, 5, 64)     36928       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 5, 5, 64)     256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 5, 5, 64)     0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 5, 5, 64)     36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 5, 5, 64)     36928       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 5, 5, 64)     256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 5, 5, 64)     256         res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 5, 5, 64)     0           bn2a_branch2b[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 5, 5, 64)     0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 5, 5, 64)     36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 5, 5, 64)     256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 5, 5, 64)     0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 5, 5, 64)     36928       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 5, 5, 64)     256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 5, 5, 64)     0           bn2b_branch2b[0][0]              \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 5, 5, 64)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 5, 5, 64)     36928       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 5, 5, 64)     256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 5, 5, 64)     0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 5, 5, 64)     36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 5, 5, 64)     256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 5, 5, 64)     0           bn2c_branch2b[0][0]              \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 5, 5, 64)     0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 3, 3, 128)    73856       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 3, 3, 128)    512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 3, 3, 128)    0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 3, 3, 128)    147584      activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 3, 3, 128)    73856       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 3, 3, 128)    512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 3, 3, 128)    512         res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 3, 3, 128)    0           bn3a_branch2b[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 3, 3, 128)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 3, 3, 128)    147584      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 3, 3, 128)    512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 3, 3, 128)    0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 3, 3, 128)    147584      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 3, 3, 128)    512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 3, 3, 128)    0           bn3b_branch2b[0][0]              \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 3, 3, 128)    0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 3, 3, 128)    147584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 3, 3, 128)    512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 3, 3, 128)    0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 3, 3, 128)    147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 3, 3, 128)    512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 3, 3, 128)    0           bn3c_branch2b[0][0]              \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 3, 3, 128)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 3, 3, 128)    147584      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 3, 3, 128)    512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 3, 3, 128)    0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 3, 3, 128)    147584      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 3, 3, 128)    512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 3, 3, 128)    0           bn3d_branch2b[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 3, 3, 128)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 3, 3, 256)    295168      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 3, 3, 256)    1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 3, 3, 256)    0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 3, 3, 256)    590080      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 3, 3, 256)    295168      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 3, 3, 256)    1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 3, 3, 256)    1024        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 3, 3, 256)    0           bn4a_branch2b[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 3, 3, 256)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 3, 3, 256)    590080      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 3, 3, 256)    1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 3, 3, 256)    0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 3, 3, 256)    590080      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 3, 3, 256)    1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 3, 3, 256)    0           bn4b_branch2b[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 3, 3, 256)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 3, 3, 256)    590080      activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 3, 3, 256)    1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 3, 3, 256)    0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 3, 3, 256)    590080      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 3, 3, 256)    1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 3, 3, 256)    0           bn4c_branch2b[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 3, 3, 256)    0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 3, 3, 256)    590080      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 3, 3, 256)    1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 3, 3, 256)    0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 3, 3, 256)    590080      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 3, 3, 256)    1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 3, 3, 256)    0           bn4d_branch2b[0][0]              \n",
      "                                                                 activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 3, 3, 256)    0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 3, 3, 256)    590080      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 3, 3, 256)    1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 3, 3, 256)    0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 3, 3, 256)    590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 3, 3, 256)    1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 3, 3, 256)    0           bn4e_branch2b[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 3, 3, 256)    0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 3, 3, 256)    590080      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 3, 3, 256)    1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 3, 3, 256)    0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 3, 3, 256)    590080      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 3, 3, 256)    1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 3, 3, 256)    0           bn4f_branch2b[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 3, 3, 256)    0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 2, 2, 512)    1180160     activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 2, 2, 512)    2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 2, 2, 512)    0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 2, 2, 512)    2359808     activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 2, 2, 512)    1180160     activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 2, 2, 512)    2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 2, 2, 512)    2048        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 2, 2, 512)    0           bn5a_branch2b[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 2, 2, 512)    0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 2, 2, 512)    2359808     activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 2, 2, 512)    2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 2, 2, 512)    0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 2, 2, 512)    2359808     activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 2, 2, 512)    2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 2, 2, 512)    0           bn5b_branch2b[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 2, 2, 512)    0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 2, 2, 512)    2359808     activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 2, 2, 512)    2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 2, 2, 512)    0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 2, 2, 512)    2359808     activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 2, 2, 512)    2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 2, 2, 512)    0           bn5c_branch2b[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 2, 2, 512)    0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 512)          0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "fc2 (Dense)                     (None, 1000)         513000      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "fc3 (Dense)                     (None, 4096)         4100096     fc2[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 27,360,104\n",
      "Trainable params: 27,342,952\n",
      "Non-trainable params: 17,152\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model and Mem\n",
    "memory = SequentialMemory(limit=10000, window_length=1)\n",
    "\n",
    "model = get_model888(STATE_SHAPE, NB_ACTIONS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "2021-12-28 07:58:31.851814: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 1999965000 Hz\n",
      "2021-12-28 07:58:33.129095: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 16384000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: -0.0020329154 0.0020397399 0.0014390318 0.0012643582 0.0004036182 -1.2418874e-05\n"
     ]
    }
   ],
   "source": [
    "#NOTE\n",
    "env = ChessEnv(model, memory, neg_r_each_step=-1, stockfishMem= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'drive/MyDrive/Data/Chess/superbot_888.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_562205/3383994977.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#NOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# model.load_weights('superbot_888.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/MyDrive/Data/Chess/superbot_888.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    230\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    231\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2317\u001b[0m           'first, then load the weights.')\n\u001b[1;32m   2318\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2320\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2321\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, **kwds)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nslots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_w0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0m\u001b[1;32m    425\u001b[0m                                fapl, fcpl=make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[1;32m    426\u001b[0m                                fs_persist=fs_persist, fs_threshold=fs_threshold),\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'drive/MyDrive/Data/Chess/superbot_888.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "#NOTE\n",
    "# model.load_weights('superbot_888.h5')\n",
    "model.load_weights('drive/MyDrive/Data/Chess/superbot_888.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n",
      "2021-12-28 08:05:13.896544: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 16384000 exceeds 10% of free system memory.\n",
      "2021-12-28 08:05:16.688173: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 16384000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 7500 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "    1/7500: episode: 1, duration: 3.615s, episode steps:   1, steps per second:   0, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020178945 0.002018991 0.0014392274 0.0012657917 0.00040250173 -1.3716555e-05\n",
      "wrong_move\n",
      "    2/7500: episode: 2, duration: 0.204s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020158396 0.0020249842 0.0014392474 0.001269595 0.00040233802 -1.2955999e-05\n",
      "wrong_move\n",
      "    3/7500: episode: 3, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020540871 0.0020592383 0.0014473652 0.0012624016 0.00040220044 -1.1630931e-05\n",
      "wrong_move\n",
      "    4/7500: episode: 4, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020522787 0.0020561435 0.0014468046 0.001260523 0.00040307082 -1.3784218e-05\n",
      "wrong_move\n",
      "    5/7500: episode: 5, duration: 0.169s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020097475 0.0020214627 0.0014293477 0.0012717372 0.00040270918 -1.3005858e-05\n",
      "wrong_move\n",
      "    6/7500: episode: 6, duration: 0.176s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.00201646 0.0020274683 0.0014411858 0.0012708016 0.0004021892 -1.3901517e-05\n",
      "wrong_move\n",
      "    7/7500: episode: 7, duration: 0.145s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 298.000 [298.000, 298.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020048644 0.002050563 0.0014228466 0.0012674269 0.000394645 -8.037667e-06\n",
      "wrong_move\n",
      "    8/7500: episode: 8, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020088067 0.002029314 0.0014298368 0.0012697849 0.0004009946 -1.2827957e-05\n",
      "wrong_move\n",
      "    9/7500: episode: 9, duration: 0.155s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020235844 0.0020333421 0.0014435736 0.001270263 0.00040328564 -1.2910001e-05\n",
      "wrong_move\n",
      "   10/7500: episode: 10, duration: 0.152s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020072395 0.0020327603 0.0014289608 0.0012713538 0.00040066903 -1.3152956e-05\n",
      "wrong_move\n",
      "   11/7500: episode: 11, duration: 0.149s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020220526 0.0020385617 0.0014457315 0.0012650954 0.00040253566 -1.4918005e-05\n",
      "wrong_move\n",
      "   12/7500: episode: 12, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2163.000 [2163.000, 2163.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020189483 0.002030942 0.0014391664 0.0012684078 0.0004016332 -1.31286215e-05\n",
      "wrong_move\n",
      "   13/7500: episode: 13, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020048097 0.002033203 0.0014310266 0.0012719013 0.0004020704 -1.3403551e-05\n",
      "wrong_move\n",
      "   14/7500: episode: 14, duration: 0.174s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002003965 0.0020229449 0.0014322561 0.0012697832 0.00040272725 -1.35159935e-05\n",
      "wrong_move\n",
      "   15/7500: episode: 15, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020177525 0.002025461 0.0014343122 0.0012708373 0.0004017771 -1.2664845e-05\n",
      "wrong_move\n",
      "   16/7500: episode: 16, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020348704 0.0020630984 0.0014483947 0.0012643966 0.00040164014 -1.0759322e-05\n",
      "wrong_move\n",
      "   17/7500: episode: 17, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2889.000 [2889.000, 2889.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002021499 0.0020392956 0.0014353748 0.0012674662 0.00040131353 -1.3085599e-05\n",
      "wrong_move\n",
      "   18/7500: episode: 18, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1639.000 [1639.000, 1639.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002017801 0.0020286955 0.0014421525 0.001262942 0.00040282536 -1.3400513e-05\n",
      "wrong_move\n",
      "   19/7500: episode: 19, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020193553 0.0020347305 0.001438716 0.0012725786 0.00040322682 -1.41927085e-05\n",
      "wrong_move\n",
      "   20/7500: episode: 20, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020317659 0.0020406237 0.0014356805 0.0012670157 0.00040415215 -1.4120347e-05\n",
      "wrong_move\n",
      "   21/7500: episode: 21, duration: 0.159s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1667.000 [1667.000, 1667.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "   22/7500: episode: 22, duration: 0.151s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.00202159 0.00203762 0.0014412748 0.001265928 0.0004034701 -1.4006255e-05\n",
      "wrong_move\n",
      "   23/7500: episode: 23, duration: 0.197s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020421077 0.0020475758 0.0014470886 0.0012701448 0.00040212515 -1.3076355e-05\n",
      "wrong_move\n",
      "   24/7500: episode: 24, duration: 0.146s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002032255 0.0020410581 0.0014378987 0.0012651513 0.000402716 -1.2685516e-05\n",
      "wrong_move\n",
      "   25/7500: episode: 25, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020145061 0.0020262029 0.001437539 0.0012707193 0.000403615 -1.4208461e-05\n",
      "wrong_move\n",
      "   26/7500: episode: 26, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002028639 0.0020411958 0.0014429638 0.0012610578 0.00040311902 -1.4344681e-05\n",
      "wrong_move\n",
      "   27/7500: episode: 27, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020377077 0.0020411077 0.00144401 0.001264001 0.00040005462 -1.292536e-05\n",
      "wrong_move\n",
      "   28/7500: episode: 28, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020175301 0.0020332881 0.001446617 0.0012688065 0.00040261523 -1.429196e-05\n",
      "wrong_move\n",
      "   29/7500: episode: 29, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020364837 0.0020528617 0.0014500775 0.0012649167 0.00040259995 -1.3689001e-05\n",
      "wrong_move\n",
      "   30/7500: episode: 30, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020028641 0.0020576285 0.0014195741 0.0012705685 0.0003936289 -7.5442667e-06\n",
      "wrong_move\n",
      "   31/7500: episode: 31, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020319219 0.0020388842 0.0014440326 0.0012649937 0.00040322533 -1.4800353e-05\n",
      "wrong_move\n",
      "   32/7500: episode: 32, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020345857 0.0020440721 0.0014486585 0.001263451 0.00040364018 -1.2785051e-05\n",
      "wrong_move\n",
      "   33/7500: episode: 33, duration: 0.154s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002003408 0.002031083 0.001428204 0.0012724303 0.00039954102 -1.1923723e-05\n",
      "wrong_move\n",
      "   36/7500: episode: 34, duration: 0.305s, episode steps:   3, steps per second:  10, episode reward: -5012.000, mean reward: -1670.667 [-5000.000, -1.000], mean action: 1596.667 [80.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020259975 0.0020284206 0.0014431756 0.0012626431 0.00040302522 -1.20129e-05\n",
      "wrong_move\n",
      "   37/7500: episode: 35, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002053603 0.0020494596 0.0014484993 0.0012609861 0.0004035621 -1.15364965e-05\n",
      "wrong_move\n",
      "   38/7500: episode: 36, duration: 0.176s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3928.000 [3928.000, 3928.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002022048 0.0020283766 0.0014356716 0.0012664915 0.00040098146 -1.1660115e-05\n",
      "wrong_move\n",
      "   39/7500: episode: 37, duration: 0.169s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020304623 0.0020420253 0.0014431861 0.0012648335 0.00040357633 -1.3550227e-05\n",
      "wrong_move\n",
      "   40/7500: episode: 38, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.00202378 0.0020424607 0.0014516109 0.0012668093 0.00040405695 -1.4488212e-05\n",
      "wrong_move\n",
      "   41/7500: episode: 39, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020240494 0.0020300131 0.0014388893 0.0012696121 0.00040279838 -1.32322675e-05\n",
      "wrong_move\n",
      "   42/7500: episode: 40, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 302.000 [302.000, 302.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020452072 0.0020609535 0.0014441833 0.001265851 0.00040342024 -1.0842603e-05\n",
      "wrong_move\n",
      "   44/7500: episode: 41, duration: 0.165s, episode steps:   2, steps per second:  12, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1862.500 [796.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020405357 0.0020563612 0.0014479613 0.0012618479 0.00040318948 -1.1952827e-05\n",
      "wrong_move\n",
      "   45/7500: episode: 42, duration: 0.174s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020302262 0.0020362537 0.0014395694 0.0012640338 0.00040360133 -1.2974218e-05\n",
      "wrong_move\n",
      "   46/7500: episode: 43, duration: 0.185s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002014074 0.002024019 0.0014291145 0.0012704765 0.00040089744 -1.3658297e-05\n",
      "wrong_move\n",
      "   47/7500: episode: 44, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020418945 0.002061518 0.0014474055 0.0012663911 0.00040179183 -1.1685846e-05\n",
      "wrong_move\n",
      "   48/7500: episode: 45, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002040045 0.002045215 0.0014454086 0.0012613165 0.00040317464 -1.3611962e-05\n",
      "wrong_move\n",
      "   49/7500: episode: 46, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020072185 0.0020288161 0.0014320227 0.001271769 0.00040176994 -1.158908e-05\n",
      "wrong_move\n",
      "   50/7500: episode: 47, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020442936 0.0020514422 0.0014489738 0.0012613036 0.00040269608 -1.3421901e-05\n",
      "wrong_move\n",
      "   52/7500: episode: 48, duration: 0.215s, episode steps:   2, steps per second:   9, episode reward: -5051.000, mean reward: -2525.500 [-5000.000, -51.000], mean action: 2128.500 [343.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "   53/7500: episode: 49, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019935758 0.0020351175 0.0014258093 0.0012710613 0.00039781834 -1.2173623e-05\n",
      "wrong_move\n",
      "   54/7500: episode: 50, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020304495 0.0020346404 0.0014364609 0.0012667903 0.00040330755 -1.3686906e-05\n",
      "wrong_move\n",
      "   55/7500: episode: 51, duration: 0.184s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020159225 0.0020288855 0.0014352781 0.0012640691 0.00040417325 -1.4519632e-05\n",
      "wrong_move\n",
      "   56/7500: episode: 52, duration: 0.166s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019980194 0.0020272557 0.0014409574 0.0012730709 0.00040157177 -1.3879944e-05\n",
      "wrong_move\n",
      "   58/7500: episode: 53, duration: 0.247s, episode steps:   2, steps per second:   8, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1960.000 [991.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020088067 0.002029314 0.0014298368 0.0012697849 0.0004009946 -1.2827957e-05\n",
      "wrong_move\n",
      "   59/7500: episode: 54, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020527549 0.002061576 0.0014408911 0.0012642979 0.00040201482 -1.073266e-05\n",
      "wrong_move\n",
      "   60/7500: episode: 55, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020294846 0.0020381939 0.0014442344 0.0012664082 0.00040223257 -1.16022275e-05\n",
      "wrong_move\n",
      "   61/7500: episode: 56, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002025565 0.0020331116 0.0014431571 0.001269097 0.00040389536 -1.4401128e-05\n",
      "wrong_move\n",
      "   62/7500: episode: 57, duration: 0.143s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020088393 0.0020264422 0.0014428744 0.001267779 0.00040243936 -1.4049474e-05\n",
      "wrong_move\n",
      "   63/7500: episode: 58, duration: 0.157s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020378695 0.0020333542 0.001439203 0.0012674994 0.0004011607 -1.3036399e-05\n",
      "wrong_move\n",
      "   64/7500: episode: 59, duration: 0.165s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020163148 0.0020237605 0.0014345159 0.0012667379 0.0004007932 -1.5845533e-05\n",
      "wrong_move\n",
      "   65/7500: episode: 60, duration: 0.152s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020238967 0.0020348635 0.0014384614 0.0012655166 0.0004034484 -1.4107543e-05\n",
      "wrong_move\n",
      "   66/7500: episode: 61, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020405722 0.0020466282 0.0014390708 0.0012637768 0.0004040706 -1.4176281e-05\n",
      "wrong_move\n",
      "   67/7500: episode: 62, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019995978 0.002033456 0.0014242401 0.0012708992 0.0003995149 -1.1964781e-05\n",
      "wrong_move\n",
      "   68/7500: episode: 63, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3534.000 [3534.000, 3534.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020231234 0.0020426179 0.0014364539 0.0012660974 0.00040073044 -1.41880155e-05\n",
      "wrong_move\n",
      "   69/7500: episode: 64, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020411238 0.0020393124 0.0014522241 0.0012601591 0.00040422723 -1.4292475e-05\n",
      "wrong_move\n",
      "   71/7500: episode: 65, duration: 0.285s, episode steps:   2, steps per second:   7, episode reward: -5041.000, mean reward: -2520.500 [-5000.000, -41.000], mean action: 1895.000 [861.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020223663 0.0020267705 0.0014351777 0.0012697265 0.0004036559 -1.2879791e-05\n",
      "wrong_move\n",
      "   72/7500: episode: 66, duration: 0.175s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3549.000 [3549.000, 3549.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020157588 0.0020311605 0.001437423 0.0012680762 0.0004020035 -1.4414407e-05\n",
      "wrong_move\n",
      "   73/7500: episode: 67, duration: 0.148s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020154691 0.0020201935 0.0014439626 0.0012698906 0.00040200652 -1.31913475e-05\n",
      "wrong_move\n",
      "   75/7500: episode: 68, duration: 0.227s, episode steps:   2, steps per second:   9, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1826.000 [723.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020004301 0.0020294217 0.0014286359 0.00127113 0.00040026678 -1.3347351e-05\n",
      "wrong_move\n",
      "   76/7500: episode: 69, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020301044 0.0020427578 0.0014459159 0.0012633529 0.00040308473 -1.5006452e-05\n",
      "wrong_move\n",
      "   77/7500: episode: 70, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020196314 0.0020294087 0.001436126 0.0012638697 0.0004022038 -1.4065732e-05\n",
      "wrong_move\n",
      "   78/7500: episode: 71, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020352814 0.0020500142 0.0014455623 0.0012619769 0.0004021182 -1.3304536e-05\n",
      "wrong_move\n",
      "   79/7500: episode: 72, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020356146 0.002042475 0.001446276 0.0012616077 0.00040125576 -1.4539692e-05\n",
      "wrong_move\n",
      "   80/7500: episode: 73, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002009958 0.0020297044 0.0014296635 0.0012724608 0.0004021875 -1.3604853e-05\n",
      "wrong_move\n",
      "   82/7500: episode: 74, duration: 0.262s, episode steps:   2, steps per second:   8, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 2322.500 [731.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002014939 0.0020366104 0.0014408584 0.0012673206 0.00040156321 -1.348116e-05\n",
      "wrong_move\n",
      "   83/7500: episode: 75, duration: 0.146s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002059834 0.0020786058 0.0014524672 0.0012586195 0.00040588333 -1.0858175e-05\n",
      "wrong_move\n",
      "   85/7500: episode: 76, duration: 0.224s, episode steps:   2, steps per second:   9, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1761.000 [593.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020334253 0.0020426172 0.0014427385 0.0012672705 0.00040260894 -1.3623223e-05\n",
      "wrong_move\n",
      "   86/7500: episode: 77, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020095601 0.002027832 0.0014329226 0.0012723691 0.00040177867 -1.3662178e-05\n",
      "wrong_move\n",
      "   87/7500: episode: 78, duration: 0.153s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020305512 0.0020288231 0.0014408658 0.0012684285 0.00040349158 -1.3325852e-05\n",
      "wrong_move\n",
      "   88/7500: episode: 79, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020059503 0.002026637 0.0014334486 0.0012714065 0.00039976975 -1.3239107e-05\n",
      "wrong_move\n",
      "   89/7500: episode: 80, duration: 0.151s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020319389 0.0020416551 0.0014367136 0.0012656093 0.00040238225 -1.4346093e-05\n",
      "wrong_move\n",
      "   90/7500: episode: 81, duration: 0.161s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020281477 0.0020441602 0.0014494301 0.0012672015 0.00040166453 -1.323074e-05\n",
      "wrong_move\n",
      "   91/7500: episode: 82, duration: 0.196s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020092982 0.0020232655 0.0014359043 0.0012715681 0.0004029524 -1.329893e-05\n",
      "wrong_move\n",
      "   92/7500: episode: 83, duration: 0.178s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019935758 0.0020351175 0.0014258093 0.0012710613 0.00039781834 -1.2173623e-05\n",
      "wrong_move\n",
      "   93/7500: episode: 84, duration: 0.145s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2958.000 [2958.000, 2958.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020075208 0.002030383 0.0014354354 0.0012692846 0.0004004954 -1.420128e-05\n",
      "wrong_move\n",
      "   94/7500: episode: 85, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "   95/7500: episode: 86, duration: 0.168s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.00201691 0.0020309417 0.0014357838 0.0012699482 0.00040196744 -1.4467441e-05\n",
      "wrong_move\n",
      "   96/7500: episode: 87, duration: 0.155s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020162258 0.0020283076 0.0014354159 0.001266454 0.000402132 -1.4207599e-05\n",
      "wrong_move\n",
      "   97/7500: episode: 88, duration: 0.149s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "   98/7500: episode: 89, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020055985 0.00202916 0.0014295161 0.0012755225 0.00039988116 -1.2448285e-05\n",
      "wrong_move\n",
      "   99/7500: episode: 90, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002000796 0.0020274306 0.0014286622 0.0012688234 0.000401342 -1.2644461e-05\n",
      "wrong_move\n",
      "  100/7500: episode: 91, duration: 0.160s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002052556 0.00205999 0.0014476575 0.0012583756 0.00040403756 -1.4349855e-05\n",
      "wrong_move\n",
      "  101/7500: episode: 92, duration: 0.158s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020338271 0.002038344 0.0014353858 0.0012639514 0.00040163708 -1.4048868e-05\n",
      "wrong_move\n",
      "  102/7500: episode: 93, duration: 0.142s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020310504 0.0020476284 0.0014417588 0.0012702658 0.00040165725 -1.2135377e-05\n",
      "wrong_move\n",
      "  103/7500: episode: 94, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020394078 0.0020420232 0.0014502269 0.0012607571 0.00040268406 -1.46443235e-05\n",
      "wrong_move\n",
      "  106/7500: episode: 95, duration: 0.417s, episode steps:   3, steps per second:   7, episode reward: -5072.000, mean reward: -1690.667 [-5000.000, -31.000], mean action: 1635.333 [334.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  107/7500: episode: 96, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3787.000 [3787.000, 3787.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  108/7500: episode: 97, duration: 0.167s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020082048 0.0020195355 0.0014405604 0.0012683695 0.0004029528 -1.4264782e-05\n",
      "wrong_move\n",
      "  110/7500: episode: 98, duration: 0.283s, episode steps:   2, steps per second:   7, episode reward: -5051.000, mean reward: -2525.500 [-5000.000, -51.000], mean action: 2124.000 [334.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020088067 0.002029314 0.0014298368 0.0012697849 0.0004009946 -1.2827957e-05\n",
      "wrong_move\n",
      "  111/7500: episode: 99, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020167823 0.0020264874 0.0014343491 0.0012687806 0.00040318235 -1.2570265e-05\n",
      "wrong_move\n",
      "  112/7500: episode: 100, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020544846 0.0020634867 0.0014471078 0.0012586904 0.00040106702 -9.5041905e-06\n",
      "wrong_move\n",
      "  113/7500: episode: 101, duration: 0.175s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019956625 0.0020378008 0.001425038 0.0012692508 0.00039957644 -1.3540939e-05\n",
      "wrong_move\n",
      "  114/7500: episode: 102, duration: 0.152s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020174673 0.0020250308 0.0014350221 0.0012669273 0.00040142416 -1.2628378e-05\n",
      "wrong_move\n",
      "  115/7500: episode: 103, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020188792 0.0020381273 0.001437378 0.0012645283 0.00040200152 -1.2118888e-05\n",
      "wrong_move\n",
      "  116/7500: episode: 104, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020197292 0.0020285065 0.0014424894 0.0012737605 0.0004029949 -1.4343375e-05\n",
      "wrong_move\n",
      "  117/7500: episode: 105, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.00200829 0.0020273826 0.0014377844 0.0012722217 0.00040308078 -1.3252837e-05\n",
      "wrong_move\n",
      "  118/7500: episode: 106, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020451618 0.002053949 0.0014516436 0.0012595968 0.00040141726 -1.1603988e-05\n",
      "wrong_move\n",
      "  119/7500: episode: 107, duration: 0.149s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.00200041 0.002028358 0.0014301969 0.0012720359 0.00040067968 -1.3035129e-05\n",
      "wrong_move\n",
      "  120/7500: episode: 108, duration: 0.184s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020147043 0.002022205 0.0014343422 0.0012687433 0.00040260627 -1.411718e-05\n",
      "wrong_move\n",
      "  121/7500: episode: 109, duration: 0.234s, episode steps:   1, steps per second:   4, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019893893 0.002033083 0.0014324805 0.0012697979 0.00039820955 -1.2118842e-05\n",
      "wrong_move\n",
      "  122/7500: episode: 110, duration: 0.142s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002025477 0.0020387215 0.0014424457 0.0012691973 0.0004031965 -1.3344257e-05\n",
      "wrong_move\n",
      "  123/7500: episode: 111, duration: 0.149s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  124/7500: episode: 112, duration: 0.174s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002026701 0.0020365445 0.0014407994 0.0012681119 0.00040397432 -1.376259e-05\n",
      "wrong_move\n",
      "  125/7500: episode: 113, duration: 0.145s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002033537 0.0020399885 0.0014466839 0.001269 0.000401027 -1.4310526e-05\n",
      "wrong_move\n",
      "  126/7500: episode: 114, duration: 0.118s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020452247 0.0020597691 0.0014453143 0.001260499 0.00040225475 -1.3678202e-05\n",
      "wrong_move\n",
      "  127/7500: episode: 115, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020316085 0.0020441997 0.0014455379 0.001264363 0.00040074805 -1.3532323e-05\n",
      "wrong_move\n",
      "  128/7500: episode: 116, duration: 0.168s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002032314 0.002031693 0.0014398261 0.001270612 0.00040360139 -1.3715313e-05\n",
      "wrong_move\n",
      "  131/7500: episode: 117, duration: 0.363s, episode steps:   3, steps per second:   8, episode reward: -5002.000, mean reward: -1667.333 [-5000.000, -1.000], mean action: 1745.667 [926.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002001594 0.0020326644 0.0014305834 0.0012702141 0.0003994357 -1.17030795e-05\n",
      "wrong_move\n",
      "  132/7500: episode: 118, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002015008 0.0020267812 0.0014305545 0.0012697831 0.00040029254 -1.3201643e-05\n",
      "wrong_move\n",
      "  133/7500: episode: 119, duration: 0.148s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020321957 0.0020404614 0.0014335441 0.0012682623 0.00040432956 -1.3944147e-05\n",
      "wrong_move\n",
      "  134/7500: episode: 120, duration: 0.149s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020471762 0.0020611822 0.0014477054 0.0012609685 0.0004008883 -1.1201664e-05\n",
      "wrong_move\n",
      "  135/7500: episode: 121, duration: 0.148s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020328313 0.0020342367 0.0014352207 0.0012710172 0.00040241514 -1.4105513e-05\n",
      "wrong_move\n",
      "  137/7500: episode: 122, duration: 0.209s, episode steps:   2, steps per second:  10, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1960.000 [991.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  138/7500: episode: 123, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  139/7500: episode: 124, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3753.000 [3753.000, 3753.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019956043 0.0020297435 0.0014250935 0.0012712643 0.0004000196 -1.2320634e-05\n",
      "wrong_move\n",
      "  140/7500: episode: 125, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 242.000 [242.000, 242.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020439934 0.0020568601 0.0014406758 0.0012678411 0.0004016022 -1.215058e-05\n",
      "wrong_move\n",
      "  142/7500: episode: 126, duration: 0.243s, episode steps:   2, steps per second:   8, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2476.000 [2023.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020158975 0.0020304585 0.0014317057 0.0012693533 0.00040314367 -1.2996978e-05\n",
      "wrong_move\n",
      "  144/7500: episode: 127, duration: 0.249s, episode steps:   2, steps per second:   8, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1960.000 [991.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020441962 0.0020518827 0.0014478588 0.001261739 0.00040219416 -1.3166158e-05\n",
      "wrong_move\n",
      "  145/7500: episode: 128, duration: 0.150s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  146/7500: episode: 129, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020124214 0.0020246233 0.001432048 0.0012732178 0.00040503696 -1.3794306e-05\n",
      "wrong_move\n",
      "  147/7500: episode: 130, duration: 0.149s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020278764 0.002035483 0.0014454338 0.0012661666 0.00040236575 -1.3820943e-05\n",
      "wrong_move\n",
      "  148/7500: episode: 131, duration: 0.177s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020403012 0.0020417871 0.001443312 0.0012635057 0.00040303273 -1.34742695e-05\n",
      "wrong_move\n",
      "  149/7500: episode: 132, duration: 0.178s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020371412 0.0020465967 0.001449465 0.0012640223 0.00040216834 -1.3734025e-05\n",
      "wrong_move\n",
      "  150/7500: episode: 133, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020088067 0.002029314 0.0014298368 0.0012697849 0.0004009946 -1.2827957e-05\n",
      "wrong_move\n",
      "  151/7500: episode: 134, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  152/7500: episode: 135, duration: 0.146s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020119292 0.0020243442 0.0014388232 0.0012735347 0.00040362714 -1.2959936e-05\n",
      "wrong_move\n",
      "  153/7500: episode: 136, duration: 0.157s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020059503 0.002026637 0.0014334486 0.0012714065 0.00039976975 -1.3239107e-05\n",
      "wrong_move\n",
      "  154/7500: episode: 137, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020182165 0.0020254499 0.0014359195 0.0012701805 0.00040060753 -1.4111101e-05\n",
      "wrong_move\n",
      "  155/7500: episode: 138, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020274618 0.0020389059 0.0014392213 0.0012655613 0.00040372758 -1.37677835e-05\n",
      "wrong_move\n",
      "  156/7500: episode: 139, duration: 0.147s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2607.000 [2607.000, 2607.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020118016 0.0020228322 0.0014300534 0.001271527 0.0004025388 -1.2987566e-05\n",
      "wrong_move\n",
      "  157/7500: episode: 140, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002033387 0.002038237 0.0014381405 0.0012624197 0.00040113166 -1.4480373e-05\n",
      "wrong_move\n",
      "  158/7500: episode: 141, duration: 0.163s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 524.000 [524.000, 524.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020308539 0.002039446 0.0014430255 0.0012687565 0.0004023307 -1.3371531e-05\n",
      "wrong_move\n",
      "  159/7500: episode: 142, duration: 0.141s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.001995153 0.002028733 0.0014284361 0.001272505 0.0004013913 -1.4172721e-05\n",
      "wrong_move\n",
      "  160/7500: episode: 143, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020135774 0.0020231153 0.0014359931 0.0012708866 0.0004008592 -1.2739681e-05\n",
      "wrong_move\n",
      "  161/7500: episode: 144, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020434503 0.0020555365 0.0014449668 0.0012612678 0.00040253304 -1.32898895e-05\n",
      "wrong_move\n",
      "  162/7500: episode: 145, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020302264 0.002034082 0.0014423074 0.0012654049 0.00040200533 -1.3594297e-05\n",
      "wrong_move\n",
      "  164/7500: episode: 146, duration: 0.233s, episode steps:   2, steps per second:   9, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1793.500 [658.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020177264 0.0020277821 0.0014360837 0.0012703504 0.00040454062 -1.3302622e-05\n",
      "wrong_move\n",
      "  165/7500: episode: 147, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002047236 0.0020501143 0.001445153 0.0012608605 0.00040214346 -1.2417091e-05\n",
      "wrong_move\n",
      "  166/7500: episode: 148, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020177613 0.002024934 0.0014346314 0.0012691859 0.00040260673 -1.3045341e-05\n",
      "wrong_move\n",
      "  167/7500: episode: 149, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  168/7500: episode: 150, duration: 0.147s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020135427 0.0020339175 0.0014368158 0.0012683783 0.00040333226 -1.3823101e-05\n",
      "wrong_move\n",
      "  169/7500: episode: 151, duration: 0.150s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  170/7500: episode: 152, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002038461 0.0020493541 0.0014423452 0.0012639473 0.00040246462 -1.1939275e-05\n",
      "wrong_move\n",
      "  171/7500: episode: 153, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020095895 0.0020287416 0.0014308749 0.0012679741 0.00040102843 -1.5796737e-05\n",
      "wrong_move\n",
      "  173/7500: episode: 154, duration: 0.247s, episode steps:   2, steps per second:   8, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1728.500 [528.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002032402 0.002031637 0.0014401978 0.0012686492 0.0004037093 -1.3258443e-05\n",
      "wrong_move\n",
      "  174/7500: episode: 155, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020126142 0.0020341584 0.0014376015 0.0012704796 0.0004042653 -1.2637554e-05\n",
      "wrong_move\n",
      "  175/7500: episode: 156, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019956043 0.0020297435 0.0014250935 0.0012712643 0.0004000196 -1.2320634e-05\n",
      "wrong_move\n",
      "  176/7500: episode: 157, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002020406 0.002032164 0.0014299431 0.0012683073 0.0004034123 -1.3901053e-05\n",
      "wrong_move\n",
      "  177/7500: episode: 158, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020342837 0.0020462011 0.0014419592 0.0012610927 0.00040444202 -1.3722647e-05\n",
      "wrong_move\n",
      "  178/7500: episode: 159, duration: 0.146s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020129012 0.0020258965 0.0014324749 0.0012695708 0.00040030057 -1.3381552e-05\n",
      "wrong_move\n",
      "  179/7500: episode: 160, duration: 0.147s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1789.000 [1789.000, 1789.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020089403 0.0020247498 0.0014410265 0.0012712876 0.00040139613 -1.4082027e-05\n",
      "wrong_move\n",
      "  180/7500: episode: 161, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002037373 0.0020382488 0.001440146 0.0012636125 0.00040384464 -1.2013988e-05\n",
      "wrong_move\n",
      "  182/7500: episode: 162, duration: 0.196s, episode steps:   2, steps per second:  10, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2041.000 [1153.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020245323 0.0020394514 0.0014393853 0.0012646281 0.00040263694 -1.39792355e-05\n",
      "wrong_move\n",
      "  183/7500: episode: 163, duration: 0.151s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020147928 0.002025424 0.0014401615 0.0012718124 0.0004020329 -1.32206405e-05\n",
      "wrong_move\n",
      "  184/7500: episode: 164, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020018192 0.0020330583 0.0014240639 0.0012725169 0.0004023076 -1.2154895e-05\n",
      "wrong_move\n",
      "  185/7500: episode: 165, duration: 0.160s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020252536 0.0020435702 0.0014393332 0.0012649911 0.00040151094 -1.1905433e-05\n",
      "wrong_move\n",
      "  186/7500: episode: 166, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002056224 0.002062076 0.0014417894 0.0012592671 0.00040332362 -1.1234297e-05\n",
      "wrong_move\n",
      "  187/7500: episode: 167, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020053226 0.0020231865 0.001437456 0.0012697286 0.00040138382 -1.4074314e-05\n",
      "wrong_move\n",
      "  188/7500: episode: 168, duration: 0.159s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020554627 0.0020642262 0.0014483926 0.0012609784 0.00040189645 -1.1572782e-05\n",
      "wrong_move\n",
      "  189/7500: episode: 169, duration: 0.158s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020290886 0.0020408453 0.0014427628 0.0012713982 0.00040265857 -1.3399014e-05\n",
      "wrong_move\n",
      "  190/7500: episode: 170, duration: 0.157s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3567.000 [3567.000, 3567.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002028942 0.0020359787 0.001445142 0.0012633275 0.00040172628 -1.3976314e-05\n",
      "wrong_move\n",
      "  192/7500: episode: 171, duration: 0.236s, episode steps:   2, steps per second:   8, episode reward: -5021.000, mean reward: -2510.500 [-5000.000, -21.000], mean action: 2416.500 [919.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020131655 0.002027819 0.0014350079 0.0012722533 0.00040383733 -1.2509992e-05\n",
      "wrong_move\n",
      "  193/7500: episode: 172, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 434.000 [434.000, 434.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020022625 0.002020697 0.001434533 0.0012717221 0.00040290193 -1.20302575e-05\n",
      "wrong_move\n",
      "  194/7500: episode: 173, duration: 0.154s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002037467 0.002052213 0.0014512036 0.0012666672 0.00040253336 -1.2477045e-05\n",
      "wrong_move\n",
      "  195/7500: episode: 174, duration: 0.162s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020419962 0.0020472119 0.0014433815 0.001263966 0.00039994126 -1.2583063e-05\n",
      "wrong_move\n",
      "  196/7500: episode: 175, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020525882 0.0020637957 0.001445021 0.0012597336 0.00040398564 -9.680771e-06\n",
      "wrong_move\n",
      "  197/7500: episode: 176, duration: 0.152s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002004463 0.002025344 0.0014378928 0.0012703873 0.00040075145 -1.368353e-05\n",
      "wrong_move\n",
      "  198/7500: episode: 177, duration: 0.172s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002036591 0.0020483383 0.00144661 0.0012667917 0.00040349568 -1.1447068e-05\n",
      "wrong_move\n",
      "  199/7500: episode: 178, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020334497 0.0020406728 0.0014401133 0.001268891 0.00040441327 -1.1734501e-05\n",
      "wrong_move\n",
      "  200/7500: episode: 179, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020445662 0.002049065 0.0014373076 0.001269177 0.00040208586 -1.23852915e-05\n",
      "wrong_move\n",
      "  201/7500: episode: 180, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020313826 0.0020486133 0.0014398843 0.0012642642 0.0004024795 -1.3695226e-05\n",
      "wrong_move\n",
      "  202/7500: episode: 181, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020283987 0.0020318215 0.001442392 0.0012679901 0.0004022253 -1.3064266e-05\n",
      "wrong_move\n",
      "  203/7500: episode: 182, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020421345 0.0020495856 0.0014426742 0.0012602553 0.00040349434 -1.2013558e-05\n",
      "wrong_move\n",
      "  204/7500: episode: 183, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002030265 0.0020252417 0.0014357559 0.0012709225 0.00040263587 -1.3133109e-05\n",
      "wrong_move\n",
      "  205/7500: episode: 184, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020155553 0.0020300366 0.0014342178 0.0012660869 0.00040126554 -1.2868015e-05\n",
      "wrong_move\n",
      "  206/7500: episode: 185, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002009875 0.0020333037 0.0014300795 0.0012681126 0.00040319524 -1.4548372e-05\n",
      "wrong_move\n",
      "  207/7500: episode: 186, duration: 0.165s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020052507 0.002029153 0.0014273042 0.0012707283 0.00040244093 -1.2941322e-05\n",
      "wrong_move\n",
      "  208/7500: episode: 187, duration: 0.162s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020134994 0.0020231043 0.0014285941 0.0012708809 0.0004033986 -1.469901e-05\n",
      "wrong_move\n",
      "  209/7500: episode: 188, duration: 0.167s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020061708 0.0020319535 0.0014243661 0.0012716996 0.0004005783 -1.2700857e-05\n",
      "wrong_move\n",
      "  210/7500: episode: 189, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020151217 0.002019358 0.0014334691 0.0012720049 0.00040199488 -1.0906457e-05\n",
      "wrong_move\n",
      "  211/7500: episode: 190, duration: 0.158s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020042786 0.002028476 0.0014314908 0.0012693357 0.00040107153 -1.3426514e-05\n",
      "wrong_move\n",
      "  212/7500: episode: 191, duration: 0.170s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020532552 0.0020550485 0.0014488738 0.0012593763 0.00040230347 -1.2117471e-05\n",
      "wrong_move\n",
      "  213/7500: episode: 192, duration: 0.217s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2692.000 [2692.000, 2692.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  214/7500: episode: 193, duration: 0.150s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  215/7500: episode: 194, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020414786 0.002058338 0.001439445 0.0012635966 0.00040186697 -1.27487365e-05\n",
      "wrong_move\n",
      "  216/7500: episode: 195, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002005761 0.0020221525 0.0014281091 0.0012699736 0.00040256348 -1.2753822e-05\n",
      "wrong_move\n",
      "  217/7500: episode: 196, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 907.000 [907.000, 907.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020132104 0.0020260918 0.0014371354 0.0012713752 0.00040238904 -1.4121787e-05\n",
      "wrong_move\n",
      "  218/7500: episode: 197, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020038404 0.0020262767 0.0014336101 0.0012721161 0.00040297338 -1.4475132e-05\n",
      "wrong_move\n",
      "  219/7500: episode: 198, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020092374 0.002025267 0.0014324741 0.0012679497 0.0004016847 -1.3847495e-05\n",
      "wrong_move\n",
      "  220/7500: episode: 199, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020162289 0.0020348865 0.0014380156 0.00126761 0.00040241546 -1.4431987e-05\n",
      "wrong_move\n",
      "  221/7500: episode: 200, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020277947 0.0020328886 0.0014436044 0.0012675135 0.00040322638 -1.33403755e-05\n",
      "wrong_move\n",
      "  222/7500: episode: 201, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020133597 0.0020213942 0.001435575 0.00126738 0.00040275106 -1.4290286e-05\n",
      "wrong_move\n",
      "  223/7500: episode: 202, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020449264 0.0020552136 0.0014525208 0.0012601318 0.00040048087 -1.4776843e-05\n",
      "wrong_move\n",
      "  224/7500: episode: 203, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1434.000 [1434.000, 1434.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019996502 0.0020322478 0.0014260988 0.0012694197 0.00039806686 -1.2077311e-05\n",
      "wrong_move\n",
      "  225/7500: episode: 204, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002029087 0.002043878 0.001441886 0.001265675 0.00040141612 -1.4119563e-05\n",
      "wrong_move\n",
      "  226/7500: episode: 205, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019911 0.002033099 0.0014319357 0.0012696709 0.00039767893 -1.3597557e-05\n",
      "wrong_move\n",
      "  227/7500: episode: 206, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020251179 0.0020297056 0.0014391491 0.0012693875 0.0004027512 -1.3926205e-05\n",
      "wrong_move\n",
      "  228/7500: episode: 207, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002031827 0.0020420384 0.0014384207 0.0012689845 0.0004027006 -1.2021864e-05\n",
      "wrong_move\n",
      "  229/7500: episode: 208, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020393962 0.0020449497 0.0014416443 0.0012653134 0.00040272257 -1.3607965e-05\n",
      "wrong_move\n",
      "  230/7500: episode: 209, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020247928 0.0020363806 0.0014428206 0.0012686533 0.0004024062 -1.44424885e-05\n",
      "wrong_move\n",
      "  231/7500: episode: 210, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020242117 0.0020451404 0.0014440434 0.0012617024 0.00040273363 -1.2113762e-05\n",
      "wrong_move\n",
      "  232/7500: episode: 211, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002008223 0.0020241602 0.001429607 0.0012703565 0.00040192172 -1.3472689e-05\n",
      "wrong_move\n",
      "  233/7500: episode: 212, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020496084 0.002053976 0.0014488228 0.0012620639 0.00040170742 -1.0732376e-05\n",
      "wrong_move\n",
      "  234/7500: episode: 213, duration: 0.152s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002014846 0.0020270688 0.0014373626 0.0012685602 0.00040183507 -1.4231942e-05\n",
      "wrong_move\n",
      "  235/7500: episode: 214, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020499914 0.0020594923 0.0014546644 0.0012594624 0.00040359684 -1.32923815e-05\n",
      "wrong_move\n",
      "  236/7500: episode: 215, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020357722 0.0020423075 0.0014530601 0.0012572532 0.00040405977 -1.4951234e-05\n",
      "wrong_move\n",
      "  237/7500: episode: 216, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020005337 0.0020299712 0.0014320927 0.001273315 0.00040157954 -1.2964694e-05\n",
      "wrong_move\n",
      "  238/7500: episode: 217, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1209.000 [1209.000, 1209.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019973696 0.0020342234 0.001425848 0.0012729695 0.00040054403 -1.2777789e-05\n",
      "wrong_move\n",
      "  239/7500: episode: 218, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020164773 0.0020279838 0.0014367597 0.0012705093 0.00040278863 -1.4593821e-05\n",
      "wrong_move\n",
      "  241/7500: episode: 219, duration: 0.183s, episode steps:   2, steps per second:  11, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 859.000 [405.000, 1313.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002022995 0.0020381492 0.0014404127 0.0012646548 0.00040179075 -1.4192425e-05\n",
      "wrong_move\n",
      "  242/7500: episode: 220, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020061613 0.0020206838 0.0014316083 0.001271102 0.00040150664 -1.4365345e-05\n",
      "wrong_move\n",
      "  243/7500: episode: 221, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020333512 0.0020354805 0.0014343368 0.0012685311 0.00040342857 -1.2350989e-05\n",
      "wrong_move\n",
      "  244/7500: episode: 222, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019996502 0.0020322478 0.0014260988 0.0012694197 0.00039806686 -1.2077311e-05\n",
      "wrong_move\n",
      "  245/7500: episode: 223, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020494964 0.0020553435 0.0014509778 0.0012621032 0.0004014255 -1.2029574e-05\n",
      "wrong_move\n",
      "  246/7500: episode: 224, duration: 0.145s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020265644 0.0020340197 0.0014439605 0.0012665301 0.00040365104 -1.3595374e-05\n",
      "wrong_move\n",
      "  247/7500: episode: 225, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020147336 0.0020316932 0.0014356491 0.0012724684 0.0003998847 -1.37136885e-05\n",
      "wrong_move\n",
      "  248/7500: episode: 226, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3124.000 [3124.000, 3124.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020247921 0.002028494 0.0014444338 0.0012639302 0.00040296587 -1.4000543e-05\n",
      "wrong_move\n",
      "  250/7500: episode: 227, duration: 0.210s, episode steps:   2, steps per second:  10, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1538.500 [148.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002044062 0.0020479218 0.0014377086 0.0012640136 0.0004023174 -1.2199744e-05\n",
      "wrong_move\n",
      "  251/7500: episode: 228, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020395827 0.0020562068 0.0014474437 0.0012630416 0.00040216852 -1.1963195e-05\n",
      "wrong_move\n",
      "  252/7500: episode: 229, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020057051 0.002029729 0.0014319221 0.0012702016 0.00040216313 -1.2391189e-05\n",
      "wrong_move\n",
      "  253/7500: episode: 230, duration: 0.163s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002038635 0.0020483336 0.0014385418 0.0012621741 0.00040091432 -1.3092234e-05\n",
      "wrong_move\n",
      "  254/7500: episode: 231, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020247684 0.0020297612 0.0014399049 0.0012714345 0.0004030573 -1.3330997e-05\n",
      "wrong_move\n",
      "  256/7500: episode: 232, duration: 0.194s, episode steps:   2, steps per second:  10, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 1956.000 [983.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  257/7500: episode: 233, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020044257 0.0020252964 0.0014325891 0.001271596 0.00040177803 -1.3636665e-05\n",
      "wrong_move\n",
      "  258/7500: episode: 234, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020411531 0.0020441352 0.0014424437 0.0012639777 0.00040054045 -1.34681595e-05\n",
      "wrong_move\n",
      "  259/7500: episode: 235, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002035063 0.0020389801 0.0014393202 0.0012685539 0.00040368334 -1.3380479e-05\n",
      "wrong_move\n",
      "  260/7500: episode: 236, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3466.000 [3466.000, 3466.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020511283 0.0020598024 0.0014504973 0.0012574707 0.0004026991 -1.398359e-05\n",
      "wrong_move\n",
      "  261/7500: episode: 237, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3015.000 [3015.000, 3015.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002001525 0.0020275954 0.0014269692 0.0012735393 0.00040052482 -1.31328e-05\n",
      "wrong_move\n",
      "  262/7500: episode: 238, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020211071 0.002028624 0.0014392838 0.0012703463 0.0004038413 -1.5184043e-05\n",
      "wrong_move\n",
      "  263/7500: episode: 239, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019986522 0.002032641 0.0014269258 0.0012705572 0.00039970138 -1.2442142e-05\n",
      "wrong_move\n",
      "  264/7500: episode: 240, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020125848 0.0020270927 0.0014358237 0.0012739444 0.0004042331 -1.3703677e-05\n",
      "wrong_move\n",
      "  266/7500: episode: 241, duration: 0.220s, episode steps:   2, steps per second:   9, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1504.500 [80.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019975777 0.0020326427 0.0014257554 0.0012724837 0.00040058306 -1.1251002e-05\n",
      "wrong_move\n",
      "  267/7500: episode: 242, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002025273 0.0020348001 0.0014390816 0.0012692095 0.00040451012 -1.3874953e-05\n",
      "wrong_move\n",
      "  268/7500: episode: 243, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020340087 0.0020360695 0.0014446765 0.0012656096 0.00040394775 -1.3261873e-05\n",
      "wrong_move\n",
      "  271/7500: episode: 244, duration: 0.331s, episode steps:   3, steps per second:   9, episode reward: -5022.000, mean reward: -1674.000 [-5000.000, -1.000], mean action: 1806.000 [454.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020327922 0.0020348632 0.0014380818 0.0012648508 0.00040361006 -1.3685243e-05\n",
      "wrong_move\n",
      "  272/7500: episode: 245, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002033771 0.0020417876 0.0014469305 0.0012699303 0.0004037005 -1.3591927e-05\n",
      "wrong_move\n",
      "  273/7500: episode: 246, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2361.000 [2361.000, 2361.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020044404 0.0020328495 0.0014277237 0.0012704778 0.00039969344 -1.20904115e-05\n",
      "wrong_move\n",
      "  274/7500: episode: 247, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020081175 0.002022792 0.0014388781 0.0012696311 0.00040110294 -1.1956065e-05\n",
      "wrong_move\n",
      "  275/7500: episode: 248, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020103364 0.002027531 0.0014306317 0.0012748474 0.00040226895 -1.3255616e-05\n",
      "wrong_move\n",
      "  278/7500: episode: 249, duration: 0.274s, episode steps:   3, steps per second:  11, episode reward: -5042.000, mean reward: -1680.667 [-5000.000, -1.000], mean action: 1501.667 [723.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020386593 0.0020370982 0.0014452776 0.0012645946 0.00040322074 -1.4907706e-05\n",
      "wrong_move\n",
      "  279/7500: episode: 250, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2287.000 [2287.000, 2287.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002014912 0.0020247386 0.001436368 0.0012710623 0.00040383742 -1.3774927e-05\n",
      "wrong_move\n",
      "  280/7500: episode: 251, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020061973 0.0020303067 0.0014300605 0.001274693 0.00040038794 -1.33010035e-05\n",
      "wrong_move\n",
      "  281/7500: episode: 252, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020085464 0.0020235525 0.0014416625 0.0012712857 0.0004027326 -1.2318138e-05\n",
      "wrong_move\n",
      "  282/7500: episode: 253, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002002416 0.0020271754 0.0014268534 0.001270958 0.00039929376 -1.29452455e-05\n",
      "wrong_move\n",
      "  283/7500: episode: 254, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020146912 0.0020379054 0.0014320785 0.0012691471 0.00040382723 -1.2529046e-05\n",
      "wrong_move\n",
      "  284/7500: episode: 255, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020284392 0.002029041 0.0014374683 0.0012650011 0.00040092654 -1.2503068e-05\n",
      "wrong_move\n",
      "  285/7500: episode: 256, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020343019 0.002046519 0.0014458485 0.0012664383 0.00040278537 -1.3892276e-05\n",
      "wrong_move\n",
      "  286/7500: episode: 257, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020233684 0.0020309058 0.0014361159 0.0012645607 0.0004015604 -1.4509635e-05\n",
      "wrong_move\n",
      "  287/7500: episode: 258, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002035623 0.0020499306 0.0014490705 0.001261897 0.00040272396 -1.4453792e-05\n",
      "wrong_move\n",
      "  289/7500: episode: 259, duration: 0.202s, episode steps:   2, steps per second:  10, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1793.500 [658.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020290008 0.0020387706 0.0014417532 0.0012638464 0.00040206796 -1.4347159e-05\n",
      "wrong_move\n",
      "  290/7500: episode: 260, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020088824 0.0020270827 0.0014314266 0.0012709696 0.0004026394 -1.2770521e-05\n",
      "wrong_move\n",
      "  291/7500: episode: 261, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020168405 0.0020346334 0.0014406496 0.0012696215 0.00040370645 -1.3882784e-05\n",
      "wrong_move\n",
      "  293/7500: episode: 262, duration: 0.176s, episode steps:   2, steps per second:  11, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1761.000 [593.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020311314 0.0020382148 0.0014413325 0.0012699781 0.00040144427 -1.444784e-05\n",
      "wrong_move\n",
      "  294/7500: episode: 263, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  295/7500: episode: 264, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020071964 0.0020315503 0.0014275819 0.0012709369 0.00040047837 -1.3451225e-05\n",
      "wrong_move\n",
      "  296/7500: episode: 265, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020351524 0.0020466708 0.0014408053 0.0012696617 0.0004029416 -1.3238852e-05\n",
      "wrong_move\n",
      "  297/7500: episode: 266, duration: 0.153s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019946387 0.0020290895 0.0014247644 0.0012712762 0.0003983429 -1.1889017e-05\n",
      "wrong_move\n",
      "  298/7500: episode: 267, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020085152 0.0020295775 0.0014421523 0.0012733417 0.00040210952 -1.307087e-05\n",
      "wrong_move\n",
      "  299/7500: episode: 268, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019996767 0.002024186 0.0014316093 0.0012735339 0.00040288523 -1.26702325e-05\n",
      "wrong_move\n",
      "  300/7500: episode: 269, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020229276 0.0020324262 0.0014345777 0.0012717939 0.00039994586 -1.4511446e-05\n",
      "wrong_move\n",
      "  301/7500: episode: 270, duration: 0.145s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020218573 0.0020406547 0.0014412686 0.0012664751 0.0004015768 -1.4156858e-05\n",
      "wrong_move\n",
      "  302/7500: episode: 271, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2795.000 [2795.000, 2795.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019935758 0.0020351175 0.0014258093 0.0012710613 0.00039781834 -1.2173623e-05\n",
      "wrong_move\n",
      "  303/7500: episode: 272, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.00201407 0.002033458 0.0014321675 0.0012679072 0.00040208193 -1.344148e-05\n",
      "wrong_move\n",
      "  304/7500: episode: 273, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020396712 0.002040575 0.0014432381 0.001265567 0.0004013097 -1.5188779e-05\n",
      "wrong_move\n",
      "  305/7500: episode: 274, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1838.000 [1838.000, 1838.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020375198 0.0020497055 0.0014475456 0.0012645622 0.00040254276 -1.42469435e-05\n",
      "wrong_move\n",
      "  306/7500: episode: 275, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.00200663 0.002025443 0.0014356893 0.0012717304 0.00040216342 -1.4794394e-05\n",
      "wrong_move\n",
      "  307/7500: episode: 276, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  309/7500: episode: 277, duration: 0.163s, episode steps:   2, steps per second:  12, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1761.000 [593.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020225693 0.0020303484 0.0014339532 0.0012682047 0.00040393864 -1.2042117e-05\n",
      "wrong_move\n",
      "  310/7500: episode: 278, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020026823 0.0020346704 0.0014259495 0.0012686523 0.00039859276 -1.3174253e-05\n",
      "wrong_move\n",
      "  311/7500: episode: 279, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020150058 0.0020304662 0.00143439 0.0012722887 0.0004031383 -1.3075925e-05\n",
      "wrong_move\n",
      "  312/7500: episode: 280, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020088067 0.002029314 0.0014298368 0.0012697849 0.0004009946 -1.2827957e-05\n",
      "wrong_move\n",
      "  314/7500: episode: 281, duration: 0.138s, episode steps:   2, steps per second:  14, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1797.500 [666.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002014966 0.0020260706 0.0014333706 0.001269197 0.00040203205 -1.4041434e-05\n",
      "wrong_move\n",
      "  315/7500: episode: 282, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002027179 0.002035274 0.0014396448 0.0012663256 0.00040292778 -1.4199613e-05\n",
      "wrong_move\n",
      "  316/7500: episode: 283, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020079503 0.0020343165 0.0014301362 0.0012735326 0.00040266235 -1.263251e-05\n",
      "wrong_move\n",
      "  318/7500: episode: 284, duration: 0.211s, episode steps:   2, steps per second:   9, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2053.500 [1178.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020343347 0.002037307 0.0014426408 0.001267844 0.00040109752 -1.3293749e-05\n",
      "wrong_move\n",
      "  319/7500: episode: 285, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020354462 0.0020519877 0.0014454294 0.0012606012 0.00040417543 -1.4658195e-05\n",
      "wrong_move\n",
      "  320/7500: episode: 286, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020262492 0.0020376435 0.0014437023 0.0012672017 0.00040119328 -1.4074758e-05\n",
      "wrong_move\n",
      "  321/7500: episode: 287, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020496028 0.0020588066 0.0014501655 0.001258976 0.00040284282 -1.2599641e-05\n",
      "wrong_move\n",
      "  322/7500: episode: 288, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020149886 0.002032742 0.0014331301 0.0012628096 0.00040154936 -1.4347076e-05\n",
      "wrong_move\n",
      "  323/7500: episode: 289, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019935758 0.0020351175 0.0014258093 0.0012710613 0.00039781834 -1.2173623e-05\n",
      "wrong_move\n",
      "  325/7500: episode: 290, duration: 0.169s, episode steps:   2, steps per second:  12, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1830.000 [731.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020199083 0.0020225716 0.0014346173 0.0012665229 0.00040028946 -1.31998895e-05\n",
      "wrong_move\n",
      "  326/7500: episode: 291, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2084.000 [2084.000, 2084.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020311505 0.0020371678 0.0014400056 0.0012661841 0.0004024518 -1.3409208e-05\n",
      "wrong_move\n",
      "  327/7500: episode: 292, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2287.000 [2287.000, 2287.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020379398 0.0020452647 0.0014432814 0.0012657872 0.00040348078 -1.3978033e-05\n",
      "wrong_move\n",
      "  328/7500: episode: 293, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020426132 0.0020481471 0.0014453755 0.0012669063 0.00040241625 -1.4164232e-05\n",
      "wrong_move\n",
      "  329/7500: episode: 294, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020219055 0.0020390628 0.001444323 0.0012646145 0.00040385936 -1.3324267e-05\n",
      "wrong_move\n",
      "  330/7500: episode: 295, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020199216 0.0020321654 0.0014412956 0.0012702949 0.00040332723 -1.5073936e-05\n",
      "wrong_move\n",
      "  331/7500: episode: 296, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020249872 0.0020729457 0.0014123156 0.0012632028 0.0003915245 -7.871975e-06\n",
      "wrong_move\n",
      "  332/7500: episode: 297, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020180698 0.0020297447 0.0014301669 0.0012735218 0.00039913727 -1.2545759e-05\n",
      "wrong_move\n",
      "  333/7500: episode: 298, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2465.000 [2465.000, 2465.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020302257 0.0020362241 0.001443388 0.0012680691 0.00040383908 -1.3427758e-05\n",
      "wrong_move\n",
      "  334/7500: episode: 299, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002040761 0.002053871 0.001452047 0.0012625403 0.00040222617 -1.5063315e-05\n",
      "wrong_move\n",
      "  335/7500: episode: 300, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  336/7500: episode: 301, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002051222 0.0020627393 0.0014470392 0.001261359 0.00040219905 -1.2855984e-05\n",
      "wrong_move\n",
      "  337/7500: episode: 302, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002023228 0.002031788 0.0014409745 0.0012739684 0.00040461496 -1.27393105e-05\n",
      "wrong_move\n",
      "  338/7500: episode: 303, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020239889 0.002031172 0.0014393681 0.0012702476 0.0004035733 -1.2596991e-05\n",
      "wrong_move\n",
      "  340/7500: episode: 304, duration: 0.192s, episode steps:   2, steps per second:  10, episode reward: -5021.000, mean reward: -2510.500 [-5000.000, -21.000], mean action: 2257.500 [601.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020318334 0.0020398549 0.0014475143 0.0012648511 0.00040304207 -1.4387402e-05\n",
      "wrong_move\n",
      "  341/7500: episode: 305, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020182093 0.00203057 0.0014393078 0.0012680247 0.00040459944 -1.4307243e-05\n",
      "wrong_move\n",
      "  343/7500: episode: 306, duration: 0.165s, episode steps:   2, steps per second:  12, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1504.500 [80.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020064279 0.0020299775 0.001429819 0.0012703608 0.0004000725 -1.2027915e-05\n",
      "wrong_move\n",
      "  345/7500: episode: 307, duration: 0.150s, episode steps:   2, steps per second:  13, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1895.000 [861.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002018129 0.002027567 0.0014351949 0.0012661039 0.00040509796 -1.3621106e-05\n",
      "wrong_move\n",
      "  346/7500: episode: 308, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020037417 0.0020286026 0.0014303642 0.0012705933 0.00040060346 -1.3881077e-05\n",
      "wrong_move\n",
      "  347/7500: episode: 309, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020376695 0.0020407534 0.0014423747 0.001260568 0.00040162588 -1.3313826e-05\n",
      "wrong_move\n",
      "  348/7500: episode: 310, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020403166 0.0020520685 0.0014480312 0.0012601719 0.0004022428 -1.2679751e-05\n",
      "wrong_move\n",
      "  349/7500: episode: 311, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020772414 0.0020735804 0.0014498025 0.0012603288 0.00040745945 -1.0976126e-05\n",
      "wrong_move\n",
      "  350/7500: episode: 312, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002053063 0.0020607605 0.0014431323 0.0012653131 0.0004011956 -1.1539378e-05\n",
      "wrong_move\n",
      "  351/7500: episode: 313, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002036766 0.0020419487 0.001442761 0.0012650104 0.00040317536 -1.374976e-05\n",
      "wrong_move\n",
      "  354/7500: episode: 314, duration: 0.291s, episode steps:   3, steps per second:  10, episode reward: -5042.000, mean reward: -1680.667 [-5000.000, -11.000], mean action: 1762.000 [666.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020369177 0.0020414651 0.0014415106 0.0012674215 0.0004020372 -1.2698518e-05\n",
      "wrong_move\n",
      "  355/7500: episode: 315, duration: 0.164s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002034691 0.002044265 0.0014455924 0.0012681762 0.00040393934 -1.4346908e-05\n",
      "wrong_move\n",
      "  356/7500: episode: 316, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019986522 0.002032641 0.0014269258 0.0012705572 0.00039970138 -1.2442142e-05\n",
      "wrong_move\n",
      "  358/7500: episode: 317, duration: 0.162s, episode steps:   2, steps per second:  12, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1895.000 [861.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020632683 0.002063633 0.0014486134 0.0012622133 0.000404419 -1.3560311e-05\n",
      "wrong_move\n",
      "  359/7500: episode: 318, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020075564 0.0020183364 0.0014362108 0.0012706479 0.000401936 -1.31011075e-05\n",
      "wrong_move\n",
      "  360/7500: episode: 319, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020133553 0.002023905 0.0014406703 0.0012726531 0.00040279658 -1.5012025e-05\n",
      "wrong_move\n",
      "  361/7500: episode: 320, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020153397 0.002029587 0.0014336695 0.0012722987 0.00040377642 -1.3763747e-05\n",
      "wrong_move\n",
      "  362/7500: episode: 321, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020102027 0.0020310062 0.0014286576 0.0012716757 0.00039940843 -1.1336069e-05\n",
      "wrong_move\n",
      "  364/7500: episode: 322, duration: 0.198s, episode steps:   2, steps per second:  10, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2383.500 [853.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020063908 0.0020245516 0.0014350126 0.0012686779 0.00040161854 -1.509428e-05\n",
      "wrong_move\n",
      "  365/7500: episode: 323, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002026912 0.0020345512 0.001437267 0.0012720153 0.00040256875 -1.5618192e-05\n",
      "wrong_move\n",
      "  367/7500: episode: 324, duration: 0.190s, episode steps:   2, steps per second:  11, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1504.500 [80.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002004409 0.0020593812 0.0014151878 0.0012718372 0.00039303183 -7.365055e-06\n",
      "wrong_move\n",
      "  369/7500: episode: 325, duration: 0.221s, episode steps:   2, steps per second:   9, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1505.500 [82.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020057051 0.002029729 0.0014319221 0.0012702016 0.00040216313 -1.2391189e-05\n",
      "wrong_move\n",
      "  370/7500: episode: 326, duration: 0.151s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020057051 0.002029729 0.0014319221 0.0012702016 0.00040216313 -1.2391189e-05\n",
      "wrong_move\n",
      "  373/7500: episode: 327, duration: 0.389s, episode steps:   3, steps per second:   8, episode reward: -5002.000, mean reward: -1667.333 [-5000.000, -1.000], mean action: 1352.667 [536.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020244156 0.0020371596 0.0014430019 0.0012671816 0.00040450852 -1.3979501e-05\n",
      "wrong_move\n",
      "  374/7500: episode: 328, duration: 0.145s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020071731 0.002028165 0.001430329 0.0012719089 0.00039947414 -1.358139e-05\n",
      "wrong_move\n",
      "  375/7500: episode: 329, duration: 0.150s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2152.000 [2152.000, 2152.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019955628 0.0020344774 0.0014278451 0.0012697906 0.00039946166 -1.2447252e-05\n",
      "wrong_move\n",
      "  376/7500: episode: 330, duration: 0.163s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020411452 0.002048879 0.0014467577 0.0012650928 0.00040054545 -1.25195875e-05\n",
      "wrong_move\n",
      "  377/7500: episode: 331, duration: 0.155s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002009517 0.0020266704 0.0014339678 0.0012691581 0.0004017279 -1.2398428e-05\n",
      "wrong_move\n",
      "  379/7500: episode: 332, duration: 0.192s, episode steps:   2, steps per second:  10, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1728.500 [528.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020182133 0.0020262692 0.0014356293 0.0012744467 0.00040181316 -1.6224965e-05\n",
      "wrong_move\n",
      "  380/7500: episode: 333, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020391806 0.0020448095 0.0014485741 0.0012658405 0.00040327403 -1.3140358e-05\n",
      "wrong_move\n",
      "  381/7500: episode: 334, duration: 0.174s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020467495 0.0020478042 0.0014452905 0.0012625634 0.00040154712 -1.2452505e-05\n",
      "wrong_move\n",
      "  382/7500: episode: 335, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002031546 0.0020432076 0.0014488214 0.0012638292 0.000402092 -1.4169658e-05\n",
      "wrong_move\n",
      "  383/7500: episode: 336, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020132663 0.0020271167 0.0014333915 0.0012739524 0.00040294192 -1.3508052e-05\n",
      "wrong_move\n",
      "  384/7500: episode: 337, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020359159 0.0020459236 0.0014486751 0.001265173 0.00040232704 -1.13921415e-05\n",
      "wrong_move\n",
      "  385/7500: episode: 338, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2110.000 [2110.000, 2110.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020517486 0.0020629254 0.0014519972 0.0012613512 0.00040148394 -1.167084e-05\n",
      "wrong_move\n",
      "  386/7500: episode: 339, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020268909 0.0020376972 0.0014467287 0.0012679201 0.00040493012 -1.2091962e-05\n",
      "wrong_move\n",
      "  387/7500: episode: 340, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019986522 0.002032641 0.0014269258 0.0012705572 0.00039970138 -1.2442142e-05\n",
      "wrong_move\n",
      "  388/7500: episode: 341, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020238955 0.0020259738 0.0014391802 0.0012649007 0.00040295592 -1.5286918e-05\n",
      "wrong_move\n",
      "  389/7500: episode: 342, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020259353 0.0020421157 0.0014411218 0.0012657226 0.00040290275 -1.3827455e-05\n",
      "wrong_move\n",
      "  392/7500: episode: 343, duration: 0.358s, episode steps:   3, steps per second:   8, episode reward: -5022.000, mean reward: -1674.000 [-5000.000, -1.000], mean action: 1357.000 [346.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002024529 0.0020384067 0.0014375843 0.0012703406 0.0004043224 -1.2667198e-05\n",
      "wrong_move\n",
      "  393/7500: episode: 344, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002048346 0.0020557404 0.0014447718 0.0012620983 0.0004009589 -1.1660653e-05\n",
      "wrong_move\n",
      "  395/7500: episode: 345, duration: 0.165s, episode steps:   2, steps per second:  12, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1794.000 [659.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020113497 0.0020224245 0.001433956 0.0012711469 0.00040267681 -1.3741992e-05\n",
      "wrong_move\n",
      "  397/7500: episode: 346, duration: 0.178s, episode steps:   2, steps per second:  11, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1956.000 [983.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020315924 0.0020384882 0.0014431701 0.0012676539 0.00040226823 -1.364253e-05\n",
      "wrong_move\n",
      "  398/7500: episode: 347, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 906.000 [906.000, 906.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020338919 0.002047572 0.0014469207 0.001260835 0.00040295778 -1.3949295e-05\n",
      "wrong_move\n",
      "  399/7500: episode: 348, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  400/7500: episode: 349, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.00203235 0.0020450556 0.0014381066 0.0012681329 0.00040256535 -1.4502897e-05\n",
      "wrong_move\n",
      "  401/7500: episode: 350, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020621286 0.0020675003 0.0014499426 0.0012582694 0.0004051907 -1.1277843e-05\n",
      "wrong_move\n",
      "  402/7500: episode: 351, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020152032 0.0020343715 0.0014325337 0.001273297 0.00040202233 -1.3884128e-05\n",
      "wrong_move\n",
      "  404/7500: episode: 352, duration: 0.240s, episode steps:   2, steps per second:   8, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1830.000 [731.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002021006 0.0020354153 0.0014352752 0.001268677 0.0004031924 -1.3163946e-05\n",
      "wrong_move\n",
      "  406/7500: episode: 353, duration: 0.144s, episode steps:   2, steps per second:  14, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1505.500 [82.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002014607 0.0020311957 0.0014344729 0.0012683349 0.00040091164 -1.2837898e-05\n",
      "wrong_move\n",
      "  407/7500: episode: 354, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020221092 0.0020346462 0.0014438961 0.0012659178 0.0004027576 -1.4055737e-05\n",
      "wrong_move\n",
      "  408/7500: episode: 355, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  409/7500: episode: 356, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002008608 0.0020235332 0.0014335247 0.0012698756 0.00040452357 -1.352666e-05\n",
      "wrong_move\n",
      "  411/7500: episode: 357, duration: 0.221s, episode steps:   2, steps per second:   9, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2633.500 [1350.000, 3917.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020166712 0.0020315633 0.0014396674 0.0012671883 0.00040274378 -1.3328656e-05\n",
      "wrong_move\n",
      "  412/7500: episode: 358, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020049897 0.0020252801 0.0014292671 0.0012710479 0.0004011922 -1.3723664e-05\n",
      "wrong_move\n",
      "  413/7500: episode: 359, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002038712 0.0020465092 0.0014460129 0.0012579716 0.00040158996 -1.2139564e-05\n",
      "wrong_move\n",
      "  414/7500: episode: 360, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002027579 0.0020391014 0.0014435663 0.0012699323 0.00040288348 -1.2767954e-05\n",
      "wrong_move\n",
      "  416/7500: episode: 361, duration: 0.174s, episode steps:   2, steps per second:  11, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1720.500 [512.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020187446 0.002023621 0.0014366168 0.0012715362 0.0004022901 -1.283844e-05\n",
      "wrong_move\n",
      "  417/7500: episode: 362, duration: 0.166s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002034174 0.00204536 0.0014517533 0.0012628231 0.00040159235 -1.2033681e-05\n",
      "wrong_move\n",
      "  418/7500: episode: 363, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020428186 0.002059952 0.0014507256 0.0012629393 0.00040212573 -1.2993427e-05\n",
      "wrong_move\n",
      "  421/7500: episode: 364, duration: 0.325s, episode steps:   3, steps per second:   9, episode reward: -5012.000, mean reward: -1670.667 [-5000.000, -1.000], mean action: 1352.333 [202.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002013897 0.0020253824 0.0014325993 0.0012692022 0.00040324862 -1.3552992e-05\n",
      "wrong_move\n",
      "  422/7500: episode: 365, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.00202901 0.002047499 0.0014436862 0.0012631995 0.00040280382 -1.3734498e-05\n",
      "wrong_move\n",
      "  423/7500: episode: 366, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020160442 0.0020265367 0.0014331132 0.0012731 0.00040383244 -1.29218715e-05\n",
      "wrong_move\n",
      "  424/7500: episode: 367, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020352262 0.0020405457 0.0014501677 0.0012667646 0.00040271258 -1.3916273e-05\n",
      "wrong_move\n",
      "  425/7500: episode: 368, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  426/7500: episode: 369, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020230466 0.0020356989 0.0014376387 0.001271505 0.00040230635 -1.3958204e-05\n",
      "wrong_move\n",
      "  427/7500: episode: 370, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020730456 0.0020735427 0.001441317 0.001260273 0.0004013928 -1.2815913e-05\n",
      "wrong_move\n",
      "  428/7500: episode: 371, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2034.000 [2034.000, 2034.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002025129 0.0020309607 0.0014325097 0.0012690956 0.00040342537 -1.3076344e-05\n",
      "wrong_move\n",
      "  429/7500: episode: 372, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002015543 0.0020297095 0.0014275253 0.0012701734 0.0004019404 -1.3486459e-05\n",
      "wrong_move\n",
      "  430/7500: episode: 373, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  431/7500: episode: 374, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020230643 0.0020315587 0.0014426996 0.0012668372 0.00040325476 -1.319584e-05\n",
      "wrong_move\n",
      "  432/7500: episode: 375, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020252736 0.0020385436 0.0014424848 0.0012705028 0.00040346544 -1.4322894e-05\n",
      "wrong_move\n",
      "  434/7500: episode: 376, duration: 0.141s, episode steps:   2, steps per second:  14, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1630.500 [332.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002024902 0.0020347126 0.0014487911 0.0012651214 0.00040466452 -1.359276e-05\n",
      "wrong_move\n",
      "  435/7500: episode: 377, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020118752 0.0020324448 0.0014363398 0.0012686264 0.00040287193 -1.4787056e-05\n",
      "wrong_move\n",
      "  436/7500: episode: 378, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002008224 0.0020257027 0.001430523 0.0012680846 0.00040380572 -1.3646648e-05\n",
      "wrong_move\n",
      "  437/7500: episode: 379, duration: 0.153s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002011058 0.0020276452 0.0014343915 0.0012714016 0.00040321215 -1.4309844e-05\n",
      "wrong_move\n",
      "  438/7500: episode: 380, duration: 0.143s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002046118 0.0020500424 0.0014412004 0.0012688079 0.0004041446 -1.2380202e-05\n",
      "wrong_move\n",
      "  439/7500: episode: 381, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3422.000 [3422.000, 3422.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002016995 0.0020307077 0.0014313953 0.001271794 0.00040255755 -1.39102285e-05\n",
      "wrong_move\n",
      "  440/7500: episode: 382, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020203055 0.0020291514 0.0014353137 0.0012669129 0.00040186054 -1.317342e-05\n",
      "wrong_move\n",
      "  441/7500: episode: 383, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  442/7500: episode: 384, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  443/7500: episode: 385, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020413292 0.002045679 0.0014383758 0.0012648503 0.0004028103 -1.2329319e-05\n",
      "wrong_move\n",
      "  444/7500: episode: 386, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002015088 0.0020250168 0.0014427978 0.0012677171 0.00040240516 -1.4648802e-05\n",
      "wrong_move\n",
      "  446/7500: episode: 387, duration: 0.249s, episode steps:   2, steps per second:   8, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 2420.000 [926.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020337556 0.0020387855 0.0014436478 0.0012710512 0.00040176543 -1.3996454e-05\n",
      "wrong_move\n",
      "  447/7500: episode: 388, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020004301 0.0020294217 0.0014286359 0.00127113 0.00040026678 -1.3347351e-05\n",
      "wrong_move\n",
      "  448/7500: episode: 389, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020302467 0.00203392 0.0014500355 0.0012636322 0.0004041007 -1.2512541e-05\n",
      "wrong_move\n",
      "  449/7500: episode: 390, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020331992 0.0020369827 0.0014374845 0.0012618554 0.00040141173 -1.5444131e-05\n",
      "wrong_move\n",
      "  450/7500: episode: 391, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2989.000 [2989.000, 2989.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002020264 0.002028038 0.0014357379 0.0012688172 0.00040233912 -1.4402081e-05\n",
      "wrong_move\n",
      "  452/7500: episode: 392, duration: 0.214s, episode steps:   2, steps per second:   9, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 1634.000 [339.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020249935 0.002040888 0.0014389062 0.0012664521 0.00040379784 -1.3722129e-05\n",
      "wrong_move\n",
      "  454/7500: episode: 393, duration: 0.258s, episode steps:   2, steps per second:   8, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 1538.500 [148.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020376684 0.0020459678 0.0014391964 0.0012684604 0.00040256188 -1.4312733e-05\n",
      "wrong_move\n",
      "  455/7500: episode: 394, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002025953 0.0020466007 0.001438193 0.0012628487 0.00040281026 -1.3347177e-05\n",
      "wrong_move\n",
      "  456/7500: episode: 395, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020273943 0.002038341 0.0014342549 0.0012655343 0.00040371792 -1.3699573e-05\n",
      "wrong_move\n",
      "  459/7500: episode: 396, duration: 0.225s, episode steps:   3, steps per second:  13, episode reward: -5042.000, mean reward: -1680.667 [-5000.000, -1.000], mean action: 1561.333 [405.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020112474 0.002023315 0.0014384376 0.0012665405 0.00040215917 -1.4051453e-05\n",
      "wrong_move\n",
      "  460/7500: episode: 397, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002026034 0.0020353384 0.0014364689 0.0012684725 0.0004042046 -1.3602323e-05\n",
      "wrong_move\n",
      "  461/7500: episode: 398, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002054806 0.0020463348 0.0014489817 0.0012575919 0.00040253173 -1.4094796e-05\n",
      "wrong_move\n",
      "  462/7500: episode: 399, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020130344 0.0020231896 0.0014366696 0.0012699235 0.00040284393 -1.206245e-05\n",
      "wrong_move\n",
      "  465/7500: episode: 400, duration: 0.313s, episode steps:   3, steps per second:  10, episode reward: -5002.000, mean reward: -1667.333 [-5000.000, -1.000], mean action: 1349.667 [454.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020568604 0.002065034 0.0014435516 0.0012643555 0.00040104802 -1.1043478e-05\n",
      "wrong_move\n",
      "  466/7500: episode: 401, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020355736 0.002048296 0.001437483 0.0012648595 0.000402491 -1.300381e-05\n",
      "wrong_move\n",
      "  467/7500: episode: 402, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020150426 0.0020289265 0.0014466925 0.0012712813 0.0004011362 -1.5006909e-05\n",
      "wrong_move\n",
      "  469/7500: episode: 403, duration: 0.180s, episode steps:   2, steps per second:  11, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1761.000 [593.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002039778 0.0020507947 0.0014507392 0.0012617209 0.00040142343 -1.2913571e-05\n",
      "wrong_move\n",
      "  470/7500: episode: 404, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020388681 0.0020458712 0.0014419019 0.0012659057 0.00040250123 -1.4199235e-05\n",
      "wrong_move\n",
      "  471/7500: episode: 405, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020422274 0.002052801 0.0014489682 0.00126867 0.00040321445 -1.2329932e-05\n",
      "wrong_move\n",
      "  472/7500: episode: 406, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020103867 0.0020307975 0.001427791 0.0012714133 0.00040426932 -1.2813045e-05\n",
      "wrong_move\n",
      "  473/7500: episode: 407, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020218508 0.0020294785 0.0014384228 0.0012707632 0.00040220565 -1.37557e-05\n",
      "wrong_move\n",
      "  474/7500: episode: 408, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002014567 0.0020217975 0.0014346854 0.0012670022 0.00040356914 -1.4475074e-05\n",
      "wrong_move\n",
      "  475/7500: episode: 409, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020359792 0.0020389576 0.0014433913 0.0012622022 0.00040309745 -1.6104634e-05\n",
      "wrong_move\n",
      "  476/7500: episode: 410, duration: 0.176s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020409417 0.0020546834 0.0014440314 0.001265953 0.0004014293 -1.3403998e-05\n",
      "wrong_move\n",
      "  477/7500: episode: 411, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020254033 0.0020310832 0.0014323504 0.0012740961 0.00040302784 -1.3925426e-05\n",
      "wrong_move\n",
      "  478/7500: episode: 412, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020309284 0.0020361375 0.0014411187 0.0012688979 0.00040260731 -1.3095458e-05\n",
      "wrong_move\n",
      "  479/7500: episode: 413, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  480/7500: episode: 414, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002045686 0.002061889 0.0014470126 0.0012603449 0.00040391614 -9.4104325e-06\n",
      "wrong_move\n",
      "  482/7500: episode: 415, duration: 0.184s, episode steps:   2, steps per second:  11, episode reward: -5021.000, mean reward: -2510.500 [-5000.000, -21.000], mean action: 2494.000 [2059.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002036975 0.0020397794 0.0014390333 0.0012683916 0.00040362676 -1.3091134e-05\n",
      "wrong_move\n",
      "  483/7500: episode: 416, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020092994 0.0020636113 0.0014116495 0.0012715078 0.00039432914 -7.693328e-06\n",
      "wrong_move\n",
      "  484/7500: episode: 417, duration: 0.156s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020088067 0.002029314 0.0014298368 0.0012697849 0.0004009946 -1.2827957e-05\n",
      "wrong_move\n",
      "  485/7500: episode: 418, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020098663 0.002028472 0.001431183 0.0012702668 0.000401632 -1.3754005e-05\n",
      "wrong_move\n",
      "  486/7500: episode: 419, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020283887 0.002031777 0.0014452633 0.0012660349 0.00040121566 -1.348164e-05\n",
      "wrong_move\n",
      "  487/7500: episode: 420, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020042753 0.0020279398 0.0014298972 0.0012721057 0.00040234133 -1.2547554e-05\n",
      "wrong_move\n",
      "  488/7500: episode: 421, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020310895 0.0020538839 0.0014422459 0.0012639799 0.00040155428 -1.1485723e-05\n",
      "wrong_move\n",
      "  489/7500: episode: 422, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002031804 0.002045199 0.001437082 0.0012676533 0.00040215734 -1.4295743e-05\n",
      "wrong_move\n",
      "  490/7500: episode: 423, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002048384 0.002048637 0.0014530596 0.0012562745 0.00040280898 -1.2626548e-05\n",
      "wrong_move\n",
      "  491/7500: episode: 424, duration: 0.224s, episode steps:   1, steps per second:   4, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020126926 0.0020248361 0.0014388347 0.0012652099 0.0004019005 -1.3765995e-05\n",
      "wrong_move\n",
      "  492/7500: episode: 425, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0019940718 0.00203544 0.0014244869 0.0012702666 0.00039733647 -1.2557255e-05\n",
      "wrong_move\n",
      "  493/7500: episode: 426, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.00200041 0.002028358 0.0014301969 0.0012720359 0.00040067968 -1.3035129e-05\n",
      "wrong_move\n",
      "  494/7500: episode: 427, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002021285 0.0020279752 0.0014389821 0.0012694479 0.0004029901 -1.1432152e-05\n",
      "wrong_move\n",
      "  495/7500: episode: 428, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020195183 0.0020281486 0.0014421057 0.0012663461 0.00040354402 -1.27535895e-05\n",
      "wrong_move\n",
      "  496/7500: episode: 429, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020271528 0.0020399399 0.0014396711 0.0012655739 0.00040391955 -1.3984973e-05\n",
      "wrong_move\n",
      "  497/7500: episode: 430, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1179.000 [1179.000, 1179.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020207325 0.0020318876 0.0014381226 0.0012671404 0.0004000424 -1.43069665e-05\n",
      "wrong_move\n",
      "  498/7500: episode: 431, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020261493 0.0020388488 0.001436627 0.001270602 0.00040250388 -1.3030671e-05\n",
      "wrong_move\n",
      "  499/7500: episode: 432, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.002035956 0.002047545 0.0014432411 0.0012647578 0.00040063102 -1.3532481e-05\n",
      "wrong_move\n",
      "  500/7500: episode: 433, duration: 0.163s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2576.000 [2576.000, 2576.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.0020061708 0.0020319535 0.0014243661 0.0012716996 0.0004005783 -1.2700857e-05\n",
      "wrong_move\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "2021-12-28 08:06:29.017496: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 16384000 exceeds 10% of free system memory.\n",
      "2021-12-28 08:06:29.336557: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 16384000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  501/7500: episode: 434, duration: 20.293s, episode steps:   1, steps per second:   0, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -0.23171139 0.03430933 0.033482727 0.032636482 0.0154920705 -0.000684537\n",
      "wrong_move\n",
      "  502/7500: episode: 435, duration: 3.651s, episode steps:   1, steps per second:   0, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 337.000 [337.000, 337.000],  loss: 10936640.000000, mae: 1.072042, mean_q: 0.010483\n",
      "Val: -0.5948721 0.03430253 0.033530165 0.03231619 0.015904713 -0.0013957694\n",
      "wrong_move\n",
      "  503/7500: episode: 436, duration: 3.121s, episode steps:   1, steps per second:   0, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1005.000 [1005.000, 1005.000],  loss: 10935292.000000, mae: 1.078588, mean_q: 0.027074\n",
      "Val: -0.9495462 0.034303736 0.033629034 0.03292987 0.01618065 -0.0006429292\n",
      "wrong_move\n",
      "  504/7500: episode: 437, duration: 3.277s, episode steps:   1, steps per second:   0, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2811.000 [2811.000, 2811.000],  loss: 10152460.000000, mae: 1.009454, mean_q: 0.033478\n",
      "Val: -1.3213379 0.03430398 0.03341795 0.0325961 0.015897196 -0.0019729212\n",
      "wrong_move\n",
      "done, took 94.750 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    }
   ],
   "source": [
    "# # Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# # even the metrics!\n",
    "# memory = SequentialMemory(limit=10000, window_length=1)\n",
    "for i in range (10):\n",
    "  policy = LegalMovesPolicy(env, 0.3, 0.1)\n",
    "  dqn = DQNAgent(model=model, nb_actions=NB_ACTIONS, memory=memory, batch_size = 16, gamma = 0.001,\n",
    "                target_model_update=1e-2, policy=policy, nb_steps_warmup = 500)\n",
    "  dqn.compile(Adam(lr=1e-1), metrics=['mae'])\n",
    "\n",
    "  his = dqn.fit(env, nb_steps=7500, visualize=False, verbose=2)\n",
    "  \n",
    "  #NOTE\n",
    "  model.save('superbot_888.h5')\n",
    "  !cp -r superbot_888.h5 /content/drive/MyDrive/Data/Chess\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
