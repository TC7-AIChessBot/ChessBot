{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install keras-rl2\n",
    "# ! pip install chess\n",
    "# ! pip install python-chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 21:12:44.201208: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-10 21:12:44.201237: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten,\\\n",
    "     Input, Conv2D, BatchNormalization, MaxPool2D, Reshape, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# import gym_chess\n",
    "\n",
    "import chess\n",
    "# import sys\n",
    "# sys.path.insert(0, '../')\n",
    "# sys.path.insert(0, '../alpha_beta')\n",
    "# from MyChessBoard import MyChessBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11868930409909203330\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-10 21:12:45.704301: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-10 21:12:45.705063: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-10 21:12:45.705078: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-10 21:12:45.705098: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LoG): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SHAPE = (65, )\n",
    "NB_ACTIONS = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessEnv:\n",
    "    '''\n",
    "    state - obser: ndarray - (65,): [:65] is flatten from int_board; [65] is color of bot; 1 is white and -1 is black\n",
    "    step: int. step_range = (0, 4096) , is encoded from square A to square B (64 x 64 val)\n",
    "    reward: int\n",
    "    '''\n",
    "\n",
    "    mapped = {\n",
    "            'P': 10,     # White Pawn\n",
    "            'p': -10,    # Black Pawn\n",
    "            'N': 20,     # White Knight\n",
    "            'n': -20,    # Black Knight\n",
    "            'B': 30,     # White Bishop\n",
    "            'b': -30,    # Black Bishop\n",
    "            'R': 40,     # White Rook\n",
    "            'r': -40,    # Black Rook\n",
    "            'Q': 50,     # White Queen\n",
    "            'q': -50,    # Black Queen\n",
    "            'K': 900,     # White King\n",
    "            'k': -900     # Black King\n",
    "    }\n",
    "    # state_shape = (8, 8)\n",
    "    # nb_actions = 4096\n",
    "    model = None\n",
    "    \n",
    "    def __init__(self, model: Sequential, neg_r_each_step = -1) -> None:\n",
    "        self.env = chess.Board()\n",
    "        self.state = self.reset()\n",
    "        # [-1] = 1 -> white, -1 -> black\n",
    "        self.bot_color = self.env.turn * 2 - 1\n",
    "        self.neg_r_each_step = neg_r_each_step\n",
    "        self.model = model\n",
    "\n",
    "    def is_draw(self):\n",
    "        if self.env.is_stalemate():\n",
    "            print(\"statlemate\")\n",
    "            return True\n",
    "        if self.env.is_fivefold_repetition():\n",
    "            print(\"fivefold repetition\")\n",
    "            return True\n",
    "        if self.env.is_seventyfive_moves():\n",
    "            print(\"75 moves\")\n",
    "            return True\n",
    "        if self.env.is_insufficient_material():\n",
    "            print(\"Insufficient Material\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_checkmate(self):\n",
    "        # If There is checkmate then it will be TRUE else FALSE.It will be a boolean value.\n",
    "        return self.env.is_checkmate()\n",
    "\n",
    "    def convert_board_to_int(self):\n",
    "        epd_string = self.env.epd()\n",
    "        list_int = np.empty((0, ))\n",
    "        for i in epd_string:\n",
    "            if i == \" \":\n",
    "                list_int = list_int.reshape((8, 8))\n",
    "                return list_int\n",
    "            elif i != \"/\":\n",
    "                if i in self.mapped:\n",
    "                    list_int = np.append(list_int, self.mapped[i])\n",
    "                else:\n",
    "                    for counter in range(0, int(i)):\n",
    "                        list_int = np.append(list_int, 0)\n",
    "        list_int = list_int.reshape((8, 8))\n",
    "        return list_int\n",
    "\n",
    "    def get_state(self) -> np.ndarray:\n",
    "        return np.append(self.convert_board_to_int().reshape(64,), self.env.turn * 2 - 1)\n",
    "\n",
    "    def legal_moves(self):\n",
    "        return list(self.env.legal_moves)\n",
    "\n",
    "    def encodeMove(self, move_uci:str):\n",
    "        if len(move_uci) != 4:\n",
    "            raise ValueError()\n",
    "        a, b = chess.parse_square(move_uci[:2]), chess.parse_square(move_uci[2:])\n",
    "        return a * 64 + b\n",
    "\n",
    "    def decodeMove(self, move_int:int):\n",
    "        a, b = move_int//64, move_int%64\n",
    "        # a, b = chess.square_name(a), chess.square_name(b)\n",
    "\n",
    "        move = self.env.find_move(from_square= a,to_square= b)\n",
    "        return move\n",
    "\n",
    "    def render(self):\n",
    "        print(self.env.unicode())\n",
    "\n",
    "    def reset(self):\n",
    "        # random state\n",
    "        redo = True\n",
    "        num_sample_steps = 0\n",
    "        while redo:\n",
    "            redo = False\n",
    "            self.env = chess.Board()\n",
    "            num_sample_steps = np.random.randint(0, 50)\n",
    "            for i in range (num_sample_steps):\n",
    "                lg_move = self.legal_moves()\n",
    "                if len(lg_move) != 0:\n",
    "                    move = np.random.choice(self.legal_moves())\n",
    "                    self.env.push(move)\n",
    "                else:\n",
    "                    redo = True\n",
    "                    break\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action: int):\n",
    "        reward = 0\n",
    "        done = True\n",
    "\n",
    "        try:\n",
    "            # move in legal move\n",
    "            move = self.decodeMove(action)\n",
    "\n",
    "            # neg reward each step\n",
    "            reward = self.neg_r_each_step\n",
    "\n",
    "            # location to_square\n",
    "            to_r, to_c = move.to_square//8, move.to_square%8\n",
    "            reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
    "\n",
    "            # action\n",
    "            self.env.push(move)\n",
    "            self.state = self.get_state()\n",
    "\n",
    "            # check end game\n",
    "            if self.is_checkmate():\n",
    "                reward += self.mapped['K']\n",
    "                done = True\n",
    "            elif self.is_draw():\n",
    "                reward += 300\n",
    "                done = True\n",
    "\n",
    "            # opponent's turn   \n",
    "            else:\n",
    "                done = False\n",
    "                Q_val = self.model.predict(self.state.reshape((1, 1) + STATE_SHAPE)).reshape(-1, )\n",
    "                idx_sorted = np.argsort(Q_val)\n",
    "\n",
    "                for act in idx_sorted:\n",
    "                    try:\n",
    "                        move = self.decodeMove(act)\n",
    "\n",
    "                        # location to_square\n",
    "                        to_r, to_c = move.to_square//8, move.to_square%8\n",
    "                        reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
    "\n",
    "                        # action\n",
    "                        self.env.push(move)\n",
    "                        self.state = self.get_state()\n",
    "\n",
    "                        # check end game\n",
    "                        if self.is_checkmate():\n",
    "                            reward -= self.mapped['K']\n",
    "                            done = True\n",
    "                        elif self.is_draw():\n",
    "                            reward += 300\n",
    "                            done = True\n",
    "                        \n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "        except:\n",
    "            # wrong move\n",
    "            reward = -5000\n",
    "            done = True\n",
    "            print('wrong_move')\n",
    "\n",
    "        return self.state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "WARNING:tensorflow:From /home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/layers/normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 65)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8448      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              528384    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4096)              0         \n",
      "=================================================================\n",
      "Total params: 554,368\n",
      "Trainable params: 553,856\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = Sequential()\n",
    "model.add(Input((1, ) + STATE_SHAPE))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(NB_ACTIONS))\n",
    "model.add(Activation('linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChessEnv(model, neg_r_each_step=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong_move\n",
      "     1/50000: episode: 1, duration: 0.531s, episode steps:   1, steps per second:   2, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3337.000 [3337.000, 3337.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     2/50000: episode: 2, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 714.000 [714.000, 714.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     3/50000: episode: 3, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 821.000 [821.000, 821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     4/50000: episode: 4, duration: 0.005s, episode steps:   1, steps per second: 194, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2151.000 [2151.000, 2151.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     5/50000: episode: 5, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     6/50000: episode: 6, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     7/50000: episode: 7, duration: 0.006s, episode steps:   1, steps per second: 158, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     8/50000: episode: 8, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1134.000 [1134.000, 1134.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     9/50000: episode: 9, duration: 0.005s, episode steps:   1, steps per second: 220, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    10/50000: episode: 10, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    11/50000: episode: 11, duration: 2.039s, episode steps:   1, steps per second:   0, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    12/50000: episode: 12, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 367.000 [367.000, 367.000],  loss: 12884251.000000, mae: 145.099060, mean_q: 246.914200\n",
      "wrong_move\n",
      "    13/50000: episode: 13, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 404.000 [404.000, 404.000],  loss: 12843257.000000, mae: 147.297974, mean_q: 261.431335\n",
      "wrong_move\n",
      "    14/50000: episode: 14, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1835.000 [1835.000, 1835.000],  loss: 12932100.000000, mae: 145.459930, mean_q: 254.938095\n",
      "wrong_move\n",
      "    15/50000: episode: 15, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: 12714786.000000, mae: 145.610413, mean_q: 260.247192\n",
      "wrong_move\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/home/kienanh/.local/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/home/kienanh/.local/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/home/kienanh/.local/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/home/kienanh/.local/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    16/50000: episode: 16, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 12726982.000000, mae: 145.589767, mean_q: 251.247650\n",
      "wrong_move\n",
      "    17/50000: episode: 17, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2130.000 [2130.000, 2130.000],  loss: 12819499.000000, mae: 147.727295, mean_q: 260.152527\n",
      "wrong_move\n",
      "    18/50000: episode: 18, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2130.000 [2130.000, 2130.000],  loss: 12692618.000000, mae: 149.395477, mean_q: 265.783813\n",
      "wrong_move\n",
      "    19/50000: episode: 19, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3205.000 [3205.000, 3205.000],  loss: 13009026.000000, mae: 145.472229, mean_q: 236.638474\n",
      "wrong_move\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    20/50000: episode: 20, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 12799388.000000, mae: 149.282928, mean_q: 254.349869\n",
      "wrong_move\n",
      "    21/50000: episode: 21, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: 12727290.000000, mae: 145.682129, mean_q: 235.561615\n",
      "wrong_move\n",
      "    22/50000: episode: 22, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2130.000 [2130.000, 2130.000],  loss: 12787268.000000, mae: 147.561737, mean_q: 250.121490\n",
      "wrong_move\n",
      "    23/50000: episode: 23, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3227.000 [3227.000, 3227.000],  loss: 12832900.000000, mae: 146.660828, mean_q: 230.245300\n",
      "wrong_move\n",
      "    24/50000: episode: 24, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 12787985.000000, mae: 148.705185, mean_q: 237.399017\n",
      "wrong_move\n",
      "    25/50000: episode: 25, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2130.000 [2130.000, 2130.000],  loss: 12720282.000000, mae: 147.249023, mean_q: 231.634003\n",
      "wrong_move\n",
      "    26/50000: episode: 26, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1579.000 [1579.000, 1579.000],  loss: 12921041.000000, mae: 145.203720, mean_q: 217.213257\n",
      "wrong_move\n",
      "    27/50000: episode: 27, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2747.000 [2747.000, 2747.000],  loss: 12771702.000000, mae: 147.195923, mean_q: 221.350693\n",
      "wrong_move\n",
      "    28/50000: episode: 28, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2130.000 [2130.000, 2130.000],  loss: 12745542.000000, mae: 147.155121, mean_q: 231.428482\n",
      "wrong_move\n",
      "    29/50000: episode: 29, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2130.000 [2130.000, 2130.000],  loss: 12791184.000000, mae: 146.564972, mean_q: 223.632187\n",
      "wrong_move\n",
      "    30/50000: episode: 30, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 364.000 [364.000, 364.000],  loss: 12780784.000000, mae: 147.996857, mean_q: 224.538910\n",
      "wrong_move\n",
      "    31/50000: episode: 31, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1438.000 [1438.000, 1438.000],  loss: 12628732.000000, mae: 146.862045, mean_q: 219.817108\n",
      "wrong_move\n",
      "    32/50000: episode: 32, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3901.000 [3901.000, 3901.000],  loss: 12728174.000000, mae: 147.701202, mean_q: 222.881744\n",
      "wrong_move\n",
      "    33/50000: episode: 33, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3649.000 [3649.000, 3649.000],  loss: 12575964.000000, mae: 145.135803, mean_q: 209.539642\n",
      "wrong_move\n",
      "    34/50000: episode: 34, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 12861070.000000, mae: 144.929596, mean_q: 202.260635\n",
      "wrong_move\n",
      "    35/50000: episode: 35, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1400.000 [1400.000, 1400.000],  loss: 12555181.000000, mae: 144.662491, mean_q: 206.323425\n",
      "wrong_move\n",
      "    36/50000: episode: 36, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 80.000 [80.000, 80.000],  loss: 12619226.000000, mae: 144.978455, mean_q: 204.860016\n",
      "wrong_move\n",
      "    37/50000: episode: 37, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1404.000 [1404.000, 1404.000],  loss: 12644978.000000, mae: 145.097260, mean_q: 197.520981\n",
      "wrong_move\n",
      "    38/50000: episode: 38, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2130.000 [2130.000, 2130.000],  loss: 12472612.000000, mae: 148.325958, mean_q: 229.946228\n",
      "wrong_move\n",
      "    39/50000: episode: 39, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3183.000 [3183.000, 3183.000],  loss: 12629699.000000, mae: 146.402740, mean_q: 202.989990\n",
      "wrong_move\n",
      "    40/50000: episode: 40, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3773.000 [3773.000, 3773.000],  loss: 12495722.000000, mae: 147.497711, mean_q: 210.334076\n",
      "wrong_move\n",
      "    41/50000: episode: 41, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3787.000 [3787.000, 3787.000],  loss: 12432188.000000, mae: 146.836899, mean_q: 211.099564\n",
      "wrong_move\n",
      "    42/50000: episode: 42, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1835.000 [1835.000, 1835.000],  loss: 12567128.000000, mae: 145.550491, mean_q: 191.991882\n",
      "wrong_move\n",
      "    43/50000: episode: 43, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3122.000 [3122.000, 3122.000],  loss: 12090316.000000, mae: 146.113083, mean_q: 200.355240\n",
      "wrong_move\n",
      "    44/50000: episode: 44, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3632.000 [3632.000, 3632.000],  loss: 12368257.000000, mae: 147.166946, mean_q: 206.621155\n",
      "wrong_move\n",
      "    45/50000: episode: 45, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2458.000 [2458.000, 2458.000],  loss: 12110919.000000, mae: 147.414871, mean_q: 216.522522\n",
      "wrong_move\n",
      "    46/50000: episode: 46, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1463.000 [1463.000, 1463.000],  loss: 12046728.000000, mae: 147.004501, mean_q: 211.166107\n",
      "wrong_move\n",
      "    48/50000: episode: 47, duration: 0.083s, episode steps:   2, steps per second:  24, episode reward: -4961.000, mean reward: -2480.500 [-5000.000, 39.000], mean action: 3632.000 [3632.000, 3632.000],  loss: 12301510.000000, mae: 149.479980, mean_q: 225.476318\n",
      "wrong_move\n",
      "    49/50000: episode: 48, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 12203284.000000, mae: 148.502960, mean_q: 218.752319\n",
      "wrong_move\n",
      "    50/50000: episode: 49, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1768.000 [1768.000, 1768.000],  loss: 12198634.000000, mae: 152.293991, mean_q: 236.084457\n",
      "wrong_move\n",
      "    51/50000: episode: 50, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 12260753.000000, mae: 149.976166, mean_q: 233.192368\n",
      "wrong_move\n",
      "    52/50000: episode: 51, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 11743320.000000, mae: 149.622375, mean_q: 220.023163\n",
      "wrong_move\n",
      "    53/50000: episode: 52, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 12128543.000000, mae: 151.080231, mean_q: 226.699249\n",
      "wrong_move\n",
      "    54/50000: episode: 53, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2872.000 [2872.000, 2872.000],  loss: 12367808.000000, mae: 147.057083, mean_q: 192.153168\n",
      "wrong_move\n",
      "    55/50000: episode: 54, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 11567815.000000, mae: 152.592560, mean_q: 237.860260\n",
      "wrong_move\n",
      "    56/50000: episode: 55, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: 11454894.000000, mae: 152.279404, mean_q: 237.629456\n",
      "wrong_move\n",
      "    57/50000: episode: 56, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 80.000 [80.000, 80.000],  loss: 11731811.000000, mae: 154.984894, mean_q: 247.711151\n",
      "wrong_move\n",
      "    58/50000: episode: 57, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 12007705.000000, mae: 151.328506, mean_q: 205.432892\n",
      "wrong_move\n",
      "    59/50000: episode: 58, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2637.000 [2637.000, 2637.000],  loss: 11717384.000000, mae: 147.007538, mean_q: 214.511658\n",
      "wrong_move\n",
      "    60/50000: episode: 59, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 11558374.000000, mae: 148.705765, mean_q: 221.912354\n",
      "wrong_move\n",
      "    61/50000: episode: 60, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 80.000 [80.000, 80.000],  loss: 12323168.000000, mae: 149.983704, mean_q: 214.999878\n",
      "wrong_move\n",
      "    62/50000: episode: 61, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3321.000 [3321.000, 3321.000],  loss: 11682608.000000, mae: 150.500229, mean_q: 213.701813\n",
      "wrong_move\n",
      "    63/50000: episode: 62, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 11967958.000000, mae: 152.596191, mean_q: 226.266144\n",
      "wrong_move\n",
      "    64/50000: episode: 63, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1438.000 [1438.000, 1438.000],  loss: 12176123.000000, mae: 152.620544, mean_q: 231.741943\n",
      "wrong_move\n",
      "    65/50000: episode: 64, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 685.000 [685.000, 685.000],  loss: 11894240.000000, mae: 151.237961, mean_q: 217.904755\n",
      "wrong_move\n",
      "    66/50000: episode: 65, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1405.000 [1405.000, 1405.000],  loss: 11755028.000000, mae: 150.604477, mean_q: 211.141861\n",
      "wrong_move\n",
      "    67/50000: episode: 66, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2073.000 [2073.000, 2073.000],  loss: 12356834.000000, mae: 150.433868, mean_q: 223.400085\n",
      "wrong_move\n",
      "    69/50000: episode: 67, duration: 0.082s, episode steps:   2, steps per second:  24, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 848.500 [259.000, 1438.000],  loss: 12295290.000000, mae: 150.723358, mean_q: 202.388123\n",
      "wrong_move\n",
      "    70/50000: episode: 68, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3861.000 [3861.000, 3861.000],  loss: 11919398.000000, mae: 152.660294, mean_q: 221.313385\n",
      "wrong_move\n",
      "    71/50000: episode: 69, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 80.000 [80.000, 80.000],  loss: 11628122.000000, mae: 154.330780, mean_q: 210.884445\n",
      "wrong_move\n",
      "    72/50000: episode: 70, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 12132624.000000, mae: 150.745453, mean_q: 227.495026\n",
      "wrong_move\n",
      "    73/50000: episode: 71, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3000.000 [3000.000, 3000.000],  loss: 11512810.000000, mae: 152.403564, mean_q: 224.678833\n",
      "wrong_move\n",
      "    74/50000: episode: 72, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1727.000 [1727.000, 1727.000],  loss: 12136852.000000, mae: 149.101257, mean_q: 216.158600\n",
      "wrong_move\n",
      "    75/50000: episode: 73, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11918091.000000, mae: 153.414703, mean_q: 235.761719\n",
      "wrong_move\n",
      "    76/50000: episode: 74, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11967238.000000, mae: 152.229675, mean_q: 214.328033\n",
      "wrong_move\n",
      "    77/50000: episode: 75, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 897.000 [897.000, 897.000],  loss: 12226589.000000, mae: 154.495560, mean_q: 211.234070\n",
      "wrong_move\n",
      "    78/50000: episode: 76, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: 11683876.000000, mae: 150.761108, mean_q: 224.076202\n",
      "wrong_move\n",
      "    79/50000: episode: 77, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 11497454.000000, mae: 151.319427, mean_q: 191.734940\n",
      "wrong_move\n",
      "    80/50000: episode: 78, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2804.000 [2804.000, 2804.000],  loss: 11590664.000000, mae: 152.279785, mean_q: 228.095764\n",
      "wrong_move\n",
      "    81/50000: episode: 79, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 12089942.000000, mae: 150.006409, mean_q: 192.792297\n",
      "wrong_move\n",
      "    82/50000: episode: 80, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 11265145.000000, mae: 154.319794, mean_q: 214.832077\n",
      "wrong_move\n",
      "    83/50000: episode: 81, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: 11695665.000000, mae: 147.083755, mean_q: 177.645111\n",
      "wrong_move\n",
      "    84/50000: episode: 82, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: 11116926.000000, mae: 149.390900, mean_q: 203.329178\n",
      "wrong_move\n",
      "    85/50000: episode: 83, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 12044984.000000, mae: 148.978943, mean_q: 186.246796\n",
      "wrong_move\n",
      "    86/50000: episode: 84, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3429.000 [3429.000, 3429.000],  loss: 11237074.000000, mae: 149.146576, mean_q: 206.682098\n",
      "wrong_move\n",
      "    87/50000: episode: 85, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: 12474226.000000, mae: 154.011063, mean_q: 219.115585\n",
      "wrong_move\n",
      "    88/50000: episode: 86, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 12091337.000000, mae: 153.417053, mean_q: 196.549530\n",
      "wrong_move\n",
      "    89/50000: episode: 87, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 271.000 [271.000, 271.000],  loss: 11464244.000000, mae: 149.715912, mean_q: 196.215225\n",
      "wrong_move\n",
      "    90/50000: episode: 88, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 465.000 [465.000, 465.000],  loss: 11511725.000000, mae: 150.767929, mean_q: 195.342743\n",
      "wrong_move\n",
      "    91/50000: episode: 89, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 12017066.000000, mae: 149.784256, mean_q: 178.859329\n",
      "wrong_move\n",
      "    92/50000: episode: 90, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1242.000 [1242.000, 1242.000],  loss: 11797153.000000, mae: 153.868225, mean_q: 201.025284\n",
      "wrong_move\n",
      "    94/50000: episode: 91, duration: 0.127s, episode steps:   2, steps per second:  16, episode reward: -4971.000, mean reward: -2485.500 [-5000.000, 29.000], mean action: 528.000 [528.000, 528.000],  loss: 11905979.000000, mae: 149.679062, mean_q: 185.561478\n",
      "wrong_move\n",
      "    95/50000: episode: 92, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 11369486.000000, mae: 149.458633, mean_q: 194.710754\n",
      "wrong_move\n",
      "    96/50000: episode: 93, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1438.000 [1438.000, 1438.000],  loss: 11399600.000000, mae: 150.909409, mean_q: 187.726776\n",
      "wrong_move\n",
      "    97/50000: episode: 94, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1364.000 [1364.000, 1364.000],  loss: 11265302.000000, mae: 153.186722, mean_q: 205.897095\n",
      "wrong_move\n",
      "    98/50000: episode: 95, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3040.000 [3040.000, 3040.000],  loss: 11858058.000000, mae: 154.126419, mean_q: 207.869965\n",
      "wrong_move\n",
      "    99/50000: episode: 96, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3429.000 [3429.000, 3429.000],  loss: 11466540.000000, mae: 156.015045, mean_q: 227.970001\n",
      "wrong_move\n",
      "   100/50000: episode: 97, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1438.000 [1438.000, 1438.000],  loss: 11801084.000000, mae: 153.608917, mean_q: 181.260712\n",
      "wrong_move\n",
      "   101/50000: episode: 98, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3572.000 [3572.000, 3572.000],  loss: 11529913.000000, mae: 149.467590, mean_q: 174.500504\n",
      "wrong_move\n",
      "   102/50000: episode: 99, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2069.000 [2069.000, 2069.000],  loss: 11118425.000000, mae: 150.192673, mean_q: 190.068237\n",
      "wrong_move\n",
      "   104/50000: episode: 100, duration: 0.107s, episode steps:   2, steps per second:  19, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 3429.000 [3429.000, 3429.000],  loss: 11494321.000000, mae: 151.222778, mean_q: 187.031097\n",
      "wrong_move\n",
      "   106/50000: episode: 101, duration: 0.070s, episode steps:   2, steps per second:  29, episode reward: -5021.000, mean reward: -2510.500 [-5000.000, -21.000], mean action: 3233.000 [3037.000, 3429.000],  loss: 11190149.000000, mae: 154.661789, mean_q: 200.891846\n",
      "wrong_move\n",
      "   107/50000: episode: 102, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2763.000 [2763.000, 2763.000],  loss: 12112860.000000, mae: 148.631470, mean_q: 162.668716\n",
      "wrong_move\n",
      "   108/50000: episode: 103, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 671.000 [671.000, 671.000],  loss: 11681656.000000, mae: 156.408112, mean_q: 223.591660\n",
      "wrong_move\n",
      "   109/50000: episode: 104, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 11127146.000000, mae: 149.408356, mean_q: 189.795639\n",
      "wrong_move\n",
      "   110/50000: episode: 105, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 11028848.000000, mae: 150.419250, mean_q: 192.937698\n",
      "wrong_move\n",
      "   111/50000: episode: 106, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1683.000 [1683.000, 1683.000],  loss: 11366388.000000, mae: 152.692657, mean_q: 209.073837\n",
      "wrong_move\n",
      "   112/50000: episode: 107, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 48.000 [48.000, 48.000],  loss: 11076161.000000, mae: 148.856812, mean_q: 182.811584\n",
      "wrong_move\n",
      "   113/50000: episode: 108, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: 11147022.000000, mae: 152.659058, mean_q: 191.159271\n",
      "wrong_move\n",
      "   115/50000: episode: 109, duration: 0.063s, episode steps:   2, steps per second:  32, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 138.000 [137.000, 139.000],  loss: 11529428.000000, mae: 151.349625, mean_q: 196.615524\n",
      "wrong_move\n",
      "   116/50000: episode: 110, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 62.000 [62.000, 62.000],  loss: 11749113.000000, mae: 149.370117, mean_q: 180.273102\n",
      "wrong_move\n",
      "   117/50000: episode: 111, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 11891706.000000, mae: 154.829071, mean_q: 207.575470\n",
      "wrong_move\n",
      "   118/50000: episode: 112, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3429.000 [3429.000, 3429.000],  loss: 10443062.000000, mae: 151.412735, mean_q: 184.283875\n",
      "wrong_move\n",
      "   119/50000: episode: 113, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3698.000 [3698.000, 3698.000],  loss: 11684781.000000, mae: 151.946152, mean_q: 163.745667\n",
      "wrong_move\n",
      "   120/50000: episode: 114, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 970.000 [970.000, 970.000],  loss: 11946402.000000, mae: 148.529938, mean_q: 154.893372\n",
      "wrong_move\n",
      "   121/50000: episode: 115, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 48.000 [48.000, 48.000],  loss: 11719453.000000, mae: 156.430450, mean_q: 196.729233\n",
      "wrong_move\n",
      "   122/50000: episode: 116, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 11059008.000000, mae: 151.823486, mean_q: 171.189590\n",
      "wrong_move\n",
      "   123/50000: episode: 117, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 485.000 [485.000, 485.000],  loss: 11629814.000000, mae: 149.923645, mean_q: 187.962402\n",
      "wrong_move\n",
      "   124/50000: episode: 118, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: 11372013.000000, mae: 150.734039, mean_q: 168.438049\n",
      "wrong_move\n",
      "   125/50000: episode: 119, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 10696241.000000, mae: 151.731323, mean_q: 175.393585\n",
      "wrong_move\n",
      "   126/50000: episode: 120, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 127.000 [127.000, 127.000],  loss: 10645448.000000, mae: 151.589264, mean_q: 168.321106\n",
      "wrong_move\n",
      "   127/50000: episode: 121, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2931.000 [2931.000, 2931.000],  loss: 11547439.000000, mae: 151.317993, mean_q: 172.348495\n",
      "wrong_move\n",
      "   128/50000: episode: 122, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: 11612096.000000, mae: 150.901093, mean_q: 175.037811\n",
      "wrong_move\n",
      "   129/50000: episode: 123, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 98.000 [98.000, 98.000],  loss: 11668134.000000, mae: 152.607803, mean_q: 172.958374\n",
      "wrong_move\n",
      "   130/50000: episode: 124, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: 10858385.000000, mae: 153.916931, mean_q: 182.043381\n",
      "wrong_move\n",
      "   131/50000: episode: 125, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 11568418.000000, mae: 151.250427, mean_q: 171.706009\n",
      "wrong_move\n",
      "   132/50000: episode: 126, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: 11589330.000000, mae: 150.934753, mean_q: 162.054825\n",
      "wrong_move\n",
      "   133/50000: episode: 127, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2097.000 [2097.000, 2097.000],  loss: 10731816.000000, mae: 154.101822, mean_q: 180.532166\n",
      "wrong_move\n",
      "   134/50000: episode: 128, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 528.000 [528.000, 528.000],  loss: 10529345.000000, mae: 152.856506, mean_q: 172.356964\n",
      "wrong_move\n",
      "   135/50000: episode: 129, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: 11246066.000000, mae: 153.287003, mean_q: 164.381531\n",
      "wrong_move\n",
      "   136/50000: episode: 130, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 11197154.000000, mae: 154.939697, mean_q: 179.865417\n",
      "wrong_move\n",
      "   137/50000: episode: 131, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 11008595.000000, mae: 150.913589, mean_q: 150.564529\n",
      "wrong_move\n",
      "   138/50000: episode: 132, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 11457028.000000, mae: 153.061005, mean_q: 159.599686\n",
      "wrong_move\n",
      "   139/50000: episode: 133, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2097.000 [2097.000, 2097.000],  loss: 12044024.000000, mae: 154.909805, mean_q: 161.947372\n",
      "wrong_move\n",
      "   140/50000: episode: 134, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1031.000 [1031.000, 1031.000],  loss: 11376386.000000, mae: 154.615021, mean_q: 164.298721\n",
      "wrong_move\n",
      "   141/50000: episode: 135, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 10547210.000000, mae: 149.973190, mean_q: 142.931915\n",
      "wrong_move\n",
      "   142/50000: episode: 136, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2097.000 [2097.000, 2097.000],  loss: 11281102.000000, mae: 150.803879, mean_q: 158.974960\n",
      "wrong_move\n",
      "   143/50000: episode: 137, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: 11958427.000000, mae: 151.943314, mean_q: 147.034561\n",
      "wrong_move\n",
      "   144/50000: episode: 138, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1438.000 [1438.000, 1438.000],  loss: 11503419.000000, mae: 151.184952, mean_q: 152.740936\n",
      "wrong_move\n",
      "   145/50000: episode: 139, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 529.000 [529.000, 529.000],  loss: 11649202.000000, mae: 151.957138, mean_q: 156.034119\n",
      "wrong_move\n",
      "   146/50000: episode: 140, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11473270.000000, mae: 150.576813, mean_q: 137.917175\n",
      "wrong_move\n",
      "   147/50000: episode: 141, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 11632955.000000, mae: 150.761658, mean_q: 137.895920\n",
      "wrong_move\n",
      "   148/50000: episode: 142, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2349.000 [2349.000, 2349.000],  loss: 10972665.000000, mae: 154.267609, mean_q: 134.838013\n",
      "wrong_move\n",
      "   149/50000: episode: 143, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3429.000 [3429.000, 3429.000],  loss: 10803640.000000, mae: 151.001877, mean_q: 150.158585\n",
      "wrong_move\n",
      "   150/50000: episode: 144, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 11718129.000000, mae: 152.685272, mean_q: 161.033829\n",
      "wrong_move\n",
      "   153/50000: episode: 145, duration: 0.301s, episode steps:   3, steps per second:  10, episode reward: -4102.000, mean reward: -1367.333 [-5000.000, 899.000], mean action: 272.667 [196.000, 363.000],  loss: 11055784.000000, mae: 152.159592, mean_q: 140.321533\n",
      "wrong_move\n",
      "   154/50000: episode: 146, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 10903387.000000, mae: 154.669159, mean_q: 148.310120\n",
      "wrong_move\n",
      "   155/50000: episode: 147, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1100.000 [1100.000, 1100.000],  loss: 10633638.000000, mae: 149.626007, mean_q: 131.798645\n",
      "wrong_move\n",
      "   156/50000: episode: 148, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 379.000 [379.000, 379.000],  loss: 11799632.000000, mae: 149.606110, mean_q: 137.670044\n",
      "wrong_move\n",
      "   157/50000: episode: 149, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 10927601.000000, mae: 152.703506, mean_q: 151.046478\n",
      "wrong_move\n",
      "   159/50000: episode: 150, duration: 0.140s, episode steps:   2, steps per second:  14, episode reward: -4091.000, mean reward: -2045.500 [-5000.000, 909.000], mean action: 797.000 [259.000, 1335.000],  loss: 10638677.000000, mae: 152.110321, mean_q: 144.576599\n",
      "wrong_move\n",
      "   160/50000: episode: 151, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2496.000 [2496.000, 2496.000],  loss: 10765338.000000, mae: 158.929977, mean_q: 180.644775\n",
      "wrong_move\n",
      "   161/50000: episode: 152, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11195884.000000, mae: 150.999008, mean_q: 136.226685\n",
      "wrong_move\n",
      "   162/50000: episode: 153, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 9685327.000000, mae: 154.273499, mean_q: 171.940918\n",
      "wrong_move\n",
      "   164/50000: episode: 154, duration: 0.165s, episode steps:   2, steps per second:  12, episode reward: -4101.000, mean reward: -2050.500 [-5000.000, 899.000], mean action: 1790.000 [259.000, 3321.000],  loss: 11045858.000000, mae: 150.485855, mean_q: 148.181091\n",
      "wrong_move\n",
      "   165/50000: episode: 155, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2656.000 [2656.000, 2656.000],  loss: 10244942.000000, mae: 152.363495, mean_q: 152.526581\n",
      "wrong_move\n",
      "   166/50000: episode: 156, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 472.000 [472.000, 472.000],  loss: 10954016.000000, mae: 153.425980, mean_q: 152.780151\n",
      "wrong_move\n",
      "   167/50000: episode: 157, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2303.000 [2303.000, 2303.000],  loss: 11154172.000000, mae: 156.757309, mean_q: 150.154327\n",
      "wrong_move\n",
      "   168/50000: episode: 158, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3572.000 [3572.000, 3572.000],  loss: 10542860.000000, mae: 152.213135, mean_q: 129.875488\n",
      "wrong_move\n",
      "   169/50000: episode: 159, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 11658370.000000, mae: 151.959244, mean_q: 129.044403\n",
      "wrong_move\n",
      "   170/50000: episode: 160, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 11131195.000000, mae: 151.174896, mean_q: 111.355331\n",
      "wrong_move\n",
      "   172/50000: episode: 161, duration: 0.140s, episode steps:   2, steps per second:  14, episode reward: -4091.000, mean reward: -2045.500 [-5000.000, 909.000], mean action: 1129.000 [259.000, 1999.000],  loss: 10911302.000000, mae: 152.753784, mean_q: 135.843155\n",
      "wrong_move\n",
      "   173/50000: episode: 162, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 10841722.000000, mae: 153.832306, mean_q: 142.673004\n",
      "wrong_move\n",
      "   174/50000: episode: 163, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 11730016.000000, mae: 158.012177, mean_q: 138.840698\n",
      "wrong_move\n",
      "   175/50000: episode: 164, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 229.000 [229.000, 229.000],  loss: 11426213.000000, mae: 153.801727, mean_q: 128.479034\n",
      "wrong_move\n",
      "   176/50000: episode: 165, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 9769583.000000, mae: 152.763016, mean_q: 128.497498\n",
      "wrong_move\n",
      "   177/50000: episode: 166, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 10113382.000000, mae: 151.874435, mean_q: 118.712067\n",
      "wrong_move\n",
      "   178/50000: episode: 167, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 528.000 [528.000, 528.000],  loss: 10999293.000000, mae: 151.388367, mean_q: 112.633102\n",
      "wrong_move\n",
      "   179/50000: episode: 168, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: 11148071.000000, mae: 154.871368, mean_q: 118.883209\n",
      "wrong_move\n",
      "   180/50000: episode: 169, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3429.000 [3429.000, 3429.000],  loss: 10442548.000000, mae: 154.490387, mean_q: 104.822739\n",
      "wrong_move\n",
      "   181/50000: episode: 170, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1685.000 [1685.000, 1685.000],  loss: 10659694.000000, mae: 152.222382, mean_q: 99.700577\n",
      "wrong_move\n",
      "   182/50000: episode: 171, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 10350189.000000, mae: 152.272751, mean_q: 108.959137\n",
      "wrong_move\n",
      "   183/50000: episode: 172, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 10693064.000000, mae: 155.612671, mean_q: 112.810928\n",
      "wrong_move\n",
      "   185/50000: episode: 173, duration: 0.113s, episode steps:   2, steps per second:  18, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 528.000 [528.000, 528.000],  loss: 10820382.000000, mae: 152.240662, mean_q: 111.873306\n",
      "wrong_move\n",
      "   186/50000: episode: 174, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 11343664.000000, mae: 155.215851, mean_q: 103.082291\n",
      "wrong_move\n",
      "   187/50000: episode: 175, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: 11270392.000000, mae: 152.443863, mean_q: 88.310265\n",
      "wrong_move\n",
      "   188/50000: episode: 176, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3313.000 [3313.000, 3313.000],  loss: 11681581.000000, mae: 152.706573, mean_q: 89.026169\n",
      "wrong_move\n",
      "   189/50000: episode: 177, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2244.000 [2244.000, 2244.000],  loss: 10743166.000000, mae: 153.224243, mean_q: 96.480446\n",
      "wrong_move\n",
      "   190/50000: episode: 178, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 618.000 [618.000, 618.000],  loss: 11299201.000000, mae: 153.315338, mean_q: 96.920830\n",
      "wrong_move\n",
      "   191/50000: episode: 179, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1438.000 [1438.000, 1438.000],  loss: 11400500.000000, mae: 157.109344, mean_q: 123.588875\n",
      "wrong_move\n",
      "   192/50000: episode: 180, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 259.000 [259.000, 259.000],  loss: 11100466.000000, mae: 153.362976, mean_q: 102.741142\n",
      "wrong_move\n",
      "   193/50000: episode: 181, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1068.000 [1068.000, 1068.000],  loss: 11403711.000000, mae: 154.679642, mean_q: 101.758987\n",
      "wrong_move\n",
      "   194/50000: episode: 182, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1438.000 [1438.000, 1438.000],  loss: 10547088.000000, mae: 153.854507, mean_q: 97.388321\n",
      "wrong_move\n",
      "   196/50000: episode: 183, duration: 0.115s, episode steps:   2, steps per second:  17, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1273.500 [1109.000, 1438.000],  loss: 11071488.000000, mae: 153.222900, mean_q: 93.938393\n",
      "wrong_move\n",
      "   197/50000: episode: 184, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1438.000 [1438.000, 1438.000],  loss: 10862624.000000, mae: 156.657715, mean_q: 117.575012\n",
      "wrong_move\n",
      "   198/50000: episode: 185, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: 11364330.000000, mae: 154.948700, mean_q: 99.786324\n",
      "wrong_move\n",
      "   199/50000: episode: 186, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 10252228.000000, mae: 155.770050, mean_q: 101.425743\n",
      "wrong_move\n",
      "   200/50000: episode: 187, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1438.000 [1438.000, 1438.000],  loss: 11341264.000000, mae: 155.905640, mean_q: 98.165405\n",
      "wrong_move\n",
      "   201/50000: episode: 188, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1438.000 [1438.000, 1438.000],  loss: 10401714.000000, mae: 152.906097, mean_q: 80.997940\n",
      "wrong_move\n",
      "   202/50000: episode: 189, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1438.000 [1438.000, 1438.000],  loss: 11477106.000000, mae: 159.956924, mean_q: 128.647827\n",
      "wrong_move\n",
      "   203/50000: episode: 190, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2683.000 [2683.000, 2683.000],  loss: 11298025.000000, mae: 151.308960, mean_q: 101.934280\n",
      "wrong_move\n",
      "   204/50000: episode: 191, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 10478323.000000, mae: 155.109726, mean_q: 93.480209\n",
      "wrong_move\n",
      "   205/50000: episode: 192, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3366.000 [3366.000, 3366.000],  loss: 10669072.000000, mae: 152.418961, mean_q: 95.346558\n",
      "wrong_move\n",
      "   206/50000: episode: 193, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 80.000 [80.000, 80.000],  loss: 10164473.000000, mae: 151.506226, mean_q: 77.818611\n",
      "wrong_move\n",
      "   207/50000: episode: 194, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 80.000 [80.000, 80.000],  loss: 11349136.000000, mae: 154.869049, mean_q: 114.448120\n",
      "wrong_move\n",
      "   208/50000: episode: 195, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 816.000 [816.000, 816.000],  loss: 10691315.000000, mae: 153.813889, mean_q: 88.943008\n",
      "wrong_move\n",
      "   209/50000: episode: 196, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1825.000 [1825.000, 1825.000],  loss: 9834082.000000, mae: 155.173660, mean_q: 77.155525\n",
      "wrong_move\n",
      "   210/50000: episode: 197, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 179.000 [179.000, 179.000],  loss: 10100540.000000, mae: 155.691925, mean_q: 101.452621\n",
      "wrong_move\n",
      "   211/50000: episode: 198, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1825.000 [1825.000, 1825.000],  loss: 9869982.000000, mae: 155.017532, mean_q: 88.897202\n",
      "wrong_move\n",
      "   212/50000: episode: 199, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3207.000 [3207.000, 3207.000],  loss: 11082516.000000, mae: 156.952911, mean_q: 92.289139\n",
      "wrong_move\n",
      "   213/50000: episode: 200, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: 10331879.000000, mae: 157.702454, mean_q: 87.949768\n",
      "wrong_move\n",
      "   214/50000: episode: 201, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: 10629782.000000, mae: 153.103058, mean_q: 89.097084\n",
      "wrong_move\n",
      "   215/50000: episode: 202, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4009.000 [4009.000, 4009.000],  loss: 11838942.000000, mae: 154.258728, mean_q: 105.650085\n",
      "wrong_move\n",
      "   216/50000: episode: 203, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2762.000 [2762.000, 2762.000],  loss: 10604730.000000, mae: 153.403778, mean_q: 69.498734\n",
      "wrong_move\n",
      "   217/50000: episode: 204, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: 10887500.000000, mae: 157.582367, mean_q: 108.200577\n",
      "wrong_move\n",
      "   218/50000: episode: 205, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 229.000 [229.000, 229.000],  loss: 10826264.000000, mae: 156.699478, mean_q: 88.817108\n",
      "wrong_move\n",
      "   219/50000: episode: 206, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1304.000 [1304.000, 1304.000],  loss: 10989014.000000, mae: 153.674103, mean_q: 87.982040\n",
      "wrong_move\n",
      "   220/50000: episode: 207, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 779.000 [779.000, 779.000],  loss: 10317242.000000, mae: 155.037720, mean_q: 94.646545\n",
      "wrong_move\n",
      "   221/50000: episode: 208, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2335.000 [2335.000, 2335.000],  loss: 10995441.000000, mae: 152.862488, mean_q: 88.924759\n",
      "wrong_move\n",
      "   222/50000: episode: 209, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1081.000 [1081.000, 1081.000],  loss: 10173479.000000, mae: 157.959625, mean_q: 98.422623\n",
      "wrong_move\n",
      "   223/50000: episode: 210, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 11205685.000000, mae: 161.324875, mean_q: 100.626740\n",
      "wrong_move\n",
      "   224/50000: episode: 211, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3428.000 [3428.000, 3428.000],  loss: 10351654.000000, mae: 153.458893, mean_q: 83.407883\n",
      "wrong_move\n",
      "   225/50000: episode: 212, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2188.000 [2188.000, 2188.000],  loss: 11118178.000000, mae: 155.338364, mean_q: 89.815964\n",
      "wrong_move\n",
      "   226/50000: episode: 213, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2444.000 [2444.000, 2444.000],  loss: 10248689.000000, mae: 153.587433, mean_q: 80.396339\n",
      "wrong_move\n",
      "   227/50000: episode: 214, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 245.000 [245.000, 245.000],  loss: 11039646.000000, mae: 154.053162, mean_q: 67.260239\n",
      "wrong_move\n",
      "   228/50000: episode: 215, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1768.000 [1768.000, 1768.000],  loss: 9197030.000000, mae: 152.548737, mean_q: 68.557190\n",
      "wrong_move\n",
      "   229/50000: episode: 216, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1998.000 [1998.000, 1998.000],  loss: 11401835.000000, mae: 154.701080, mean_q: 82.942635\n",
      "wrong_move\n",
      "   230/50000: episode: 217, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 10829564.000000, mae: 157.025970, mean_q: 95.710602\n",
      "wrong_move\n",
      "   231/50000: episode: 218, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1649.000 [1649.000, 1649.000],  loss: 11108258.000000, mae: 154.715118, mean_q: 87.085953\n",
      "wrong_move\n",
      "   232/50000: episode: 219, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3966.000 [3966.000, 3966.000],  loss: 11141865.000000, mae: 158.426056, mean_q: 103.969498\n",
      "wrong_move\n",
      "   233/50000: episode: 220, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 553.000 [553.000, 553.000],  loss: 10374362.000000, mae: 154.739990, mean_q: 91.452400\n",
      "wrong_move\n",
      "   234/50000: episode: 221, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 952.000 [952.000, 952.000],  loss: 11588004.000000, mae: 154.327957, mean_q: 77.448654\n",
      "wrong_move\n",
      "   235/50000: episode: 222, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3060.000 [3060.000, 3060.000],  loss: 10806982.000000, mae: 157.873154, mean_q: 93.576561\n",
      "wrong_move\n",
      "   236/50000: episode: 223, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1245.000 [1245.000, 1245.000],  loss: 10693969.000000, mae: 154.024017, mean_q: 98.437119\n",
      "wrong_move\n",
      "   237/50000: episode: 224, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: 10347080.000000, mae: 155.943253, mean_q: 94.759071\n",
      "wrong_move\n",
      "   238/50000: episode: 225, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3469.000 [3469.000, 3469.000],  loss: 11162849.000000, mae: 158.351669, mean_q: 106.434769\n",
      "wrong_move\n",
      "   239/50000: episode: 226, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2975.000 [2975.000, 2975.000],  loss: 9890069.000000, mae: 161.518860, mean_q: 110.767410\n",
      "wrong_move\n",
      "   240/50000: episode: 227, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1245.000 [1245.000, 1245.000],  loss: 10695124.000000, mae: 155.703705, mean_q: 83.925224\n",
      "wrong_move\n",
      "   241/50000: episode: 228, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2849.000 [2849.000, 2849.000],  loss: 9460785.000000, mae: 159.200348, mean_q: 102.717422\n",
      "wrong_move\n",
      "   242/50000: episode: 229, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1245.000 [1245.000, 1245.000],  loss: 10268148.000000, mae: 154.282471, mean_q: 66.019791\n",
      "wrong_move\n",
      "   243/50000: episode: 230, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2657.000 [2657.000, 2657.000],  loss: 10353298.000000, mae: 155.465729, mean_q: 80.033890\n",
      "wrong_move\n",
      "   244/50000: episode: 231, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2657.000 [2657.000, 2657.000],  loss: 10081696.000000, mae: 155.920990, mean_q: 77.727364\n",
      "wrong_move\n",
      "   245/50000: episode: 232, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1726.000 [1726.000, 1726.000],  loss: 10985598.000000, mae: 156.891754, mean_q: 79.219818\n",
      "wrong_move\n",
      "   246/50000: episode: 233, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1498.000 [1498.000, 1498.000],  loss: 11577179.000000, mae: 155.689209, mean_q: 93.653999\n",
      "wrong_move\n",
      "   247/50000: episode: 234, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3492.000 [3492.000, 3492.000],  loss: 10484146.000000, mae: 158.004700, mean_q: 86.264877\n",
      "wrong_move\n",
      "   248/50000: episode: 235, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3715.000 [3715.000, 3715.000],  loss: 9313749.000000, mae: 156.082458, mean_q: 91.633446\n",
      "wrong_move\n",
      "   249/50000: episode: 236, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 580.000 [580.000, 580.000],  loss: 10269270.000000, mae: 160.209198, mean_q: 95.395004\n",
      "wrong_move\n",
      "   250/50000: episode: 237, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1225.000 [1225.000, 1225.000],  loss: 11075858.000000, mae: 156.330688, mean_q: 71.174881\n",
      "wrong_move\n",
      "   251/50000: episode: 238, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 10334416.000000, mae: 156.050293, mean_q: 83.896393\n",
      "wrong_move\n",
      "   252/50000: episode: 239, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 410.000 [410.000, 410.000],  loss: 10737114.000000, mae: 155.724762, mean_q: 68.885620\n",
      "wrong_move\n",
      "   253/50000: episode: 240, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3066.000 [3066.000, 3066.000],  loss: 9182994.000000, mae: 164.503937, mean_q: 123.207451\n",
      "wrong_move\n",
      "   254/50000: episode: 241, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2209.000 [2209.000, 2209.000],  loss: 11393861.000000, mae: 160.458771, mean_q: 106.333725\n",
      "wrong_move\n",
      "   255/50000: episode: 242, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2657.000 [2657.000, 2657.000],  loss: 10833809.000000, mae: 156.354279, mean_q: 85.142242\n",
      "wrong_move\n",
      "   256/50000: episode: 243, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1680.000 [1680.000, 1680.000],  loss: 10334394.000000, mae: 155.865784, mean_q: 59.662102\n",
      "wrong_move\n",
      "   257/50000: episode: 244, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 11757233.000000, mae: 157.025146, mean_q: 77.155396\n",
      "wrong_move\n",
      "   258/50000: episode: 245, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1679.000 [1679.000, 1679.000],  loss: 10436654.000000, mae: 155.668243, mean_q: 88.353851\n",
      "wrong_move\n",
      "   259/50000: episode: 246, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3766.000 [3766.000, 3766.000],  loss: 10543961.000000, mae: 158.222595, mean_q: 98.634354\n",
      "wrong_move\n",
      "   260/50000: episode: 247, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2242.000 [2242.000, 2242.000],  loss: 10706612.000000, mae: 158.772705, mean_q: 83.136139\n",
      "wrong_move\n",
      "   261/50000: episode: 248, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1183.000 [1183.000, 1183.000],  loss: 10726562.000000, mae: 161.361328, mean_q: 97.390503\n",
      "wrong_move\n",
      "   262/50000: episode: 249, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1701.000 [1701.000, 1701.000],  loss: 10734710.000000, mae: 158.319229, mean_q: 81.938721\n",
      "wrong_move\n",
      "   263/50000: episode: 250, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1183.000 [1183.000, 1183.000],  loss: 10756344.000000, mae: 156.154724, mean_q: 80.386772\n",
      "wrong_move\n",
      "   264/50000: episode: 251, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1193.000 [1193.000, 1193.000],  loss: 10498972.000000, mae: 157.812347, mean_q: 81.391022\n",
      "wrong_move\n",
      "   265/50000: episode: 252, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3619.000 [3619.000, 3619.000],  loss: 10956819.000000, mae: 157.577759, mean_q: 77.530075\n",
      "wrong_move\n",
      "   266/50000: episode: 253, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 153.000 [153.000, 153.000],  loss: 10961542.000000, mae: 159.080597, mean_q: 82.242653\n",
      "wrong_move\n",
      "   267/50000: episode: 254, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1719.000 [1719.000, 1719.000],  loss: 10913037.000000, mae: 161.333405, mean_q: 93.937271\n",
      "wrong_move\n",
      "   268/50000: episode: 255, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2810.000 [2810.000, 2810.000],  loss: 11656824.000000, mae: 158.061630, mean_q: 90.958511\n",
      "wrong_move\n",
      "   269/50000: episode: 256, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3148.000 [3148.000, 3148.000],  loss: 10611613.000000, mae: 161.594055, mean_q: 96.783134\n",
      "wrong_move\n",
      "   270/50000: episode: 257, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 730.000 [730.000, 730.000],  loss: 10433550.000000, mae: 158.699524, mean_q: 88.585663\n",
      "wrong_move\n",
      "   271/50000: episode: 258, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2865.000 [2865.000, 2865.000],  loss: 10010013.000000, mae: 158.872147, mean_q: 80.951645\n",
      "wrong_move\n",
      "   272/50000: episode: 259, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3790.000 [3790.000, 3790.000],  loss: 11122350.000000, mae: 158.437622, mean_q: 76.325172\n",
      "wrong_move\n",
      "   273/50000: episode: 260, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2865.000 [2865.000, 2865.000],  loss: 10665880.000000, mae: 160.079971, mean_q: 87.147949\n",
      "wrong_move\n",
      "   274/50000: episode: 261, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 193.000 [193.000, 193.000],  loss: 10985020.000000, mae: 166.007080, mean_q: 92.643204\n",
      "wrong_move\n",
      "   275/50000: episode: 262, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1964.000 [1964.000, 1964.000],  loss: 10922402.000000, mae: 160.670135, mean_q: 91.016312\n",
      "wrong_move\n",
      "   276/50000: episode: 263, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2468.000 [2468.000, 2468.000],  loss: 10047813.000000, mae: 161.699829, mean_q: 109.653122\n",
      "wrong_move\n",
      "   277/50000: episode: 264, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1560.000 [1560.000, 1560.000],  loss: 10459231.000000, mae: 161.900681, mean_q: 96.045349\n",
      "wrong_move\n",
      "   278/50000: episode: 265, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2865.000 [2865.000, 2865.000],  loss: 9475136.000000, mae: 162.670105, mean_q: 104.453079\n",
      "wrong_move\n",
      "   279/50000: episode: 266, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3690.000 [3690.000, 3690.000],  loss: 11021128.000000, mae: 165.145569, mean_q: 96.797577\n",
      "wrong_move\n",
      "   280/50000: episode: 267, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1846.000 [1846.000, 1846.000],  loss: 10659233.000000, mae: 162.918854, mean_q: 91.420654\n",
      "wrong_move\n",
      "   281/50000: episode: 268, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1244.000 [1244.000, 1244.000],  loss: 10970510.000000, mae: 158.919296, mean_q: 77.610756\n",
      "wrong_move\n",
      "   282/50000: episode: 269, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1560.000 [1560.000, 1560.000],  loss: 11404869.000000, mae: 159.994202, mean_q: 86.874176\n",
      "wrong_move\n",
      "   283/50000: episode: 270, duration: 0.044s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1082.000 [1082.000, 1082.000],  loss: 11774686.000000, mae: 159.233231, mean_q: 78.045815\n",
      "wrong_move\n",
      "   284/50000: episode: 271, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2144.000 [2144.000, 2144.000],  loss: 10488832.000000, mae: 159.482788, mean_q: 92.721329\n",
      "wrong_move\n",
      "   285/50000: episode: 272, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1910.000 [1910.000, 1910.000],  loss: 10996124.000000, mae: 162.981491, mean_q: 96.108429\n",
      "wrong_move\n",
      "   286/50000: episode: 273, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2470.000 [2470.000, 2470.000],  loss: 11260863.000000, mae: 157.571274, mean_q: 61.275471\n",
      "wrong_move\n",
      "   287/50000: episode: 274, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 524.000 [524.000, 524.000],  loss: 11357271.000000, mae: 162.724396, mean_q: 86.006294\n",
      "wrong_move\n",
      "   288/50000: episode: 275, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1313.000 [1313.000, 1313.000],  loss: 9684514.000000, mae: 160.187180, mean_q: 82.292633\n",
      "wrong_move\n",
      "   289/50000: episode: 276, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 9745062.000000, mae: 158.298080, mean_q: 68.066162\n",
      "wrong_move\n",
      "   290/50000: episode: 277, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2151.000 [2151.000, 2151.000],  loss: 10917481.000000, mae: 159.028275, mean_q: 89.685982\n",
      "wrong_move\n",
      "   291/50000: episode: 278, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: 10345254.000000, mae: 160.840790, mean_q: 108.115753\n",
      "wrong_move\n",
      "   292/50000: episode: 279, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 478.000 [478.000, 478.000],  loss: 11559402.000000, mae: 160.157623, mean_q: 81.468277\n",
      "wrong_move\n",
      "   293/50000: episode: 280, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 154.000 [154.000, 154.000],  loss: 10891879.000000, mae: 164.805115, mean_q: 110.219467\n",
      "wrong_move\n",
      "   294/50000: episode: 281, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 839.000 [839.000, 839.000],  loss: 10516714.000000, mae: 161.879578, mean_q: 93.712875\n",
      "wrong_move\n",
      "   295/50000: episode: 282, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 137.000 [137.000, 137.000],  loss: 10502674.000000, mae: 160.864960, mean_q: 91.125725\n",
      "wrong_move\n",
      "   296/50000: episode: 283, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 912.000 [912.000, 912.000],  loss: 10807931.000000, mae: 160.123474, mean_q: 88.078751\n",
      "wrong_move\n",
      "   297/50000: episode: 284, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 788.000 [788.000, 788.000],  loss: 11067476.000000, mae: 160.816299, mean_q: 93.989639\n",
      "wrong_move\n",
      "   298/50000: episode: 285, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3284.000 [3284.000, 3284.000],  loss: 10630964.000000, mae: 167.616547, mean_q: 101.714401\n",
      "wrong_move\n",
      "   299/50000: episode: 286, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 137.000 [137.000, 137.000],  loss: 11136668.000000, mae: 162.851013, mean_q: 74.099831\n",
      "wrong_move\n",
      "   301/50000: episode: 287, duration: 0.105s, episode steps:   2, steps per second:  19, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 3422.500 [3177.000, 3668.000],  loss: 9931781.000000, mae: 163.940887, mean_q: 98.072205\n",
      "wrong_move\n",
      "   302/50000: episode: 288, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4038.000 [4038.000, 4038.000],  loss: 10882396.000000, mae: 162.884033, mean_q: 104.269333\n",
      "wrong_move\n",
      "   303/50000: episode: 289, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3955.000 [3955.000, 3955.000],  loss: 11001464.000000, mae: 161.789810, mean_q: 101.071030\n",
      "wrong_move\n",
      "   304/50000: episode: 290, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1071.000 [1071.000, 1071.000],  loss: 10674402.000000, mae: 159.052399, mean_q: 80.518013\n",
      "wrong_move\n",
      "   305/50000: episode: 291, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 272.000 [272.000, 272.000],  loss: 10191393.000000, mae: 159.626709, mean_q: 69.138840\n",
      "wrong_move\n",
      "   306/50000: episode: 292, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2830.000 [2830.000, 2830.000],  loss: 10333414.000000, mae: 161.462143, mean_q: 78.517250\n",
      "wrong_move\n",
      "   307/50000: episode: 293, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3908.000 [3908.000, 3908.000],  loss: 9557798.000000, mae: 158.874191, mean_q: 74.134094\n",
      "wrong_move\n",
      "   308/50000: episode: 294, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3613.000 [3613.000, 3613.000],  loss: 10657803.000000, mae: 162.213730, mean_q: 98.060089\n",
      "wrong_move\n",
      "   309/50000: episode: 295, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 584.000 [584.000, 584.000],  loss: 11051058.000000, mae: 164.567245, mean_q: 88.062027\n",
      "wrong_move\n",
      "   310/50000: episode: 296, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1530.000 [1530.000, 1530.000],  loss: 10923061.000000, mae: 160.148651, mean_q: 71.841270\n",
      "wrong_move\n",
      "   311/50000: episode: 297, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 87.000 [87.000, 87.000],  loss: 10418040.000000, mae: 165.731354, mean_q: 105.730728\n",
      "wrong_move\n",
      "   312/50000: episode: 298, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1812.000 [1812.000, 1812.000],  loss: 10781212.000000, mae: 166.206802, mean_q: 111.654747\n",
      "wrong_move\n",
      "   313/50000: episode: 299, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 67.000 [67.000, 67.000],  loss: 10345080.000000, mae: 161.522583, mean_q: 84.744110\n",
      "wrong_move\n",
      "   314/50000: episode: 300, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1530.000 [1530.000, 1530.000],  loss: 10344828.000000, mae: 161.875854, mean_q: 92.619919\n",
      "wrong_move\n",
      "   315/50000: episode: 301, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1530.000 [1530.000, 1530.000],  loss: 10729689.000000, mae: 162.903778, mean_q: 95.913338\n",
      "wrong_move\n",
      "   316/50000: episode: 302, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 954.000 [954.000, 954.000],  loss: 10271340.000000, mae: 160.202728, mean_q: 81.078491\n",
      "wrong_move\n",
      "   317/50000: episode: 303, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1955.000 [1955.000, 1955.000],  loss: 10264536.000000, mae: 162.367050, mean_q: 75.732971\n",
      "wrong_move\n",
      "   318/50000: episode: 304, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3868.000 [3868.000, 3868.000],  loss: 10258783.000000, mae: 163.424652, mean_q: 88.263557\n",
      "wrong_move\n",
      "   319/50000: episode: 305, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3622.000 [3622.000, 3622.000],  loss: 11244502.000000, mae: 159.607758, mean_q: 63.144917\n",
      "wrong_move\n",
      "   320/50000: episode: 306, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3347.000 [3347.000, 3347.000],  loss: 11311183.000000, mae: 162.246078, mean_q: 89.589828\n",
      "wrong_move\n",
      "   322/50000: episode: 307, duration: 0.119s, episode steps:   2, steps per second:  17, episode reward: -4981.000, mean reward: -2490.500 [-5000.000, 19.000], mean action: 2538.500 [1153.000, 3924.000],  loss: 11213656.000000, mae: 163.640839, mean_q: 102.250839\n",
      "wrong_move\n",
      "   323/50000: episode: 308, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3073.000 [3073.000, 3073.000],  loss: 9390520.000000, mae: 163.116257, mean_q: 91.758514\n",
      "wrong_move\n",
      "   324/50000: episode: 309, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 363.000 [363.000, 363.000],  loss: 10877524.000000, mae: 159.277466, mean_q: 41.054344\n",
      "wrong_move\n",
      "   325/50000: episode: 310, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1357.000 [1357.000, 1357.000],  loss: 10202238.000000, mae: 163.022202, mean_q: 93.905090\n",
      "wrong_move\n",
      "   326/50000: episode: 311, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 318.000 [318.000, 318.000],  loss: 10274963.000000, mae: 163.242706, mean_q: 74.398796\n",
      "wrong_move\n",
      "   327/50000: episode: 312, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3547.000 [3547.000, 3547.000],  loss: 10397057.000000, mae: 168.302277, mean_q: 102.032608\n",
      "wrong_move\n",
      "   328/50000: episode: 313, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2482.000 [2482.000, 2482.000],  loss: 10618717.000000, mae: 166.030151, mean_q: 111.554932\n",
      "wrong_move\n",
      "   329/50000: episode: 314, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 478.000 [478.000, 478.000],  loss: 10627431.000000, mae: 162.248840, mean_q: 92.457520\n",
      "wrong_move\n",
      "   330/50000: episode: 315, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2805.000 [2805.000, 2805.000],  loss: 10805862.000000, mae: 164.121155, mean_q: 90.798325\n",
      "wrong_move\n",
      "   331/50000: episode: 316, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1153.000 [1153.000, 1153.000],  loss: 11432534.000000, mae: 163.084976, mean_q: 79.402969\n",
      "wrong_move\n",
      "   332/50000: episode: 317, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2704.000 [2704.000, 2704.000],  loss: 11135878.000000, mae: 164.269775, mean_q: 95.786072\n",
      "wrong_move\n",
      "   333/50000: episode: 318, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2002.000 [2002.000, 2002.000],  loss: 11032074.000000, mae: 164.916351, mean_q: 80.592056\n",
      "wrong_move\n",
      "   334/50000: episode: 319, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3075.000 [3075.000, 3075.000],  loss: 10912842.000000, mae: 165.402405, mean_q: 101.665298\n",
      "wrong_move\n",
      "   335/50000: episode: 320, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2804.000 [2804.000, 2804.000],  loss: 10960899.000000, mae: 162.757904, mean_q: 74.944977\n",
      "wrong_move\n",
      "   336/50000: episode: 321, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1306.000 [1306.000, 1306.000],  loss: 9845500.000000, mae: 164.646408, mean_q: 90.671593\n",
      "wrong_move\n",
      "   337/50000: episode: 322, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2365.000 [2365.000, 2365.000],  loss: 10469775.000000, mae: 161.484680, mean_q: 63.766853\n",
      "wrong_move\n",
      "   338/50000: episode: 323, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3682.000 [3682.000, 3682.000],  loss: 11183956.000000, mae: 161.881500, mean_q: 79.559700\n",
      "wrong_move\n",
      "   339/50000: episode: 324, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 139.000 [139.000, 139.000],  loss: 10440800.000000, mae: 164.602310, mean_q: 92.098236\n",
      "wrong_move\n",
      "   340/50000: episode: 325, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1401.000 [1401.000, 1401.000],  loss: 10592354.000000, mae: 169.237122, mean_q: 103.279083\n",
      "wrong_move\n",
      "   341/50000: episode: 326, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1705.000 [1705.000, 1705.000],  loss: 10538741.000000, mae: 163.029297, mean_q: 77.591614\n",
      "wrong_move\n",
      "   342/50000: episode: 327, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1450.000 [1450.000, 1450.000],  loss: 10674590.000000, mae: 166.639099, mean_q: 89.279663\n",
      "wrong_move\n",
      "   343/50000: episode: 328, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2305.000 [2305.000, 2305.000],  loss: 9910796.000000, mae: 166.286163, mean_q: 101.236542\n",
      "wrong_move\n",
      "   344/50000: episode: 329, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3808.000 [3808.000, 3808.000],  loss: 11058394.000000, mae: 161.363007, mean_q: 50.466057\n",
      "wrong_move\n",
      "   345/50000: episode: 330, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2197.000 [2197.000, 2197.000],  loss: 11389020.000000, mae: 166.524170, mean_q: 79.025810\n",
      "wrong_move\n",
      "   346/50000: episode: 331, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: 11206255.000000, mae: 162.401489, mean_q: 69.885986\n",
      "wrong_move\n",
      "   347/50000: episode: 332, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3599.000 [3599.000, 3599.000],  loss: 10492181.000000, mae: 163.554565, mean_q: 65.225372\n",
      "wrong_move\n",
      "   348/50000: episode: 333, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: 10624771.000000, mae: 163.811340, mean_q: 82.330620\n",
      "wrong_move\n",
      "   349/50000: episode: 334, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1085.000 [1085.000, 1085.000],  loss: 11071908.000000, mae: 164.046677, mean_q: 88.032585\n",
      "wrong_move\n",
      "   350/50000: episode: 335, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 418.000 [418.000, 418.000],  loss: 9893980.000000, mae: 162.902588, mean_q: 70.587929\n",
      "wrong_move\n",
      "   351/50000: episode: 336, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 395.000 [395.000, 395.000],  loss: 10220884.000000, mae: 169.868591, mean_q: 119.716660\n",
      "wrong_move\n",
      "   352/50000: episode: 337, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1683.000 [1683.000, 1683.000],  loss: 10722085.000000, mae: 164.413727, mean_q: 72.679352\n",
      "wrong_move\n",
      "   353/50000: episode: 338, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1731.000 [1731.000, 1731.000],  loss: 10329744.000000, mae: 165.265717, mean_q: 77.339836\n",
      "wrong_move\n",
      "   354/50000: episode: 339, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2826.000 [2826.000, 2826.000],  loss: 10180178.000000, mae: 164.222412, mean_q: 69.269760\n",
      "wrong_move\n",
      "   355/50000: episode: 340, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1421.000 [1421.000, 1421.000],  loss: 9763591.000000, mae: 162.960876, mean_q: 84.178192\n",
      "wrong_move\n",
      "   356/50000: episode: 341, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2876.000 [2876.000, 2876.000],  loss: 9649005.000000, mae: 163.096924, mean_q: 61.349785\n",
      "wrong_move\n",
      "   357/50000: episode: 342, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2305.000 [2305.000, 2305.000],  loss: 10861051.000000, mae: 167.428802, mean_q: 88.673264\n",
      "wrong_move\n",
      "   358/50000: episode: 343, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2659.000 [2659.000, 2659.000],  loss: 9964530.000000, mae: 164.439453, mean_q: 82.063957\n",
      "wrong_move\n",
      "   359/50000: episode: 344, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4004.000 [4004.000, 4004.000],  loss: 10436804.000000, mae: 166.791946, mean_q: 73.177460\n",
      "wrong_move\n",
      "   360/50000: episode: 345, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3360.000 [3360.000, 3360.000],  loss: 10456158.000000, mae: 166.083603, mean_q: 69.902451\n",
      "wrong_move\n",
      "   361/50000: episode: 346, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3313.000 [3313.000, 3313.000],  loss: 10503335.000000, mae: 165.269745, mean_q: 84.880432\n",
      "wrong_move\n",
      "   362/50000: episode: 347, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2903.000 [2903.000, 2903.000],  loss: 10930003.000000, mae: 167.902618, mean_q: 75.026321\n",
      "wrong_move\n",
      "   363/50000: episode: 348, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2659.000 [2659.000, 2659.000],  loss: 11003376.000000, mae: 163.668045, mean_q: 70.395721\n",
      "wrong_move\n",
      "   364/50000: episode: 349, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 10182763.000000, mae: 166.646591, mean_q: 82.074585\n",
      "wrong_move\n",
      "   365/50000: episode: 350, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3925.000 [3925.000, 3925.000],  loss: 10684311.000000, mae: 165.827423, mean_q: 69.227272\n",
      "wrong_move\n",
      "   366/50000: episode: 351, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3721.000 [3721.000, 3721.000],  loss: 10558832.000000, mae: 164.967346, mean_q: 73.281090\n",
      "wrong_move\n",
      "   367/50000: episode: 352, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2001.000 [2001.000, 2001.000],  loss: 11017066.000000, mae: 164.762756, mean_q: 61.963604\n",
      "wrong_move\n",
      "   368/50000: episode: 353, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2001.000 [2001.000, 2001.000],  loss: 10753928.000000, mae: 164.893204, mean_q: 41.668385\n",
      "wrong_move\n",
      "   369/50000: episode: 354, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1153.000 [1153.000, 1153.000],  loss: 10248674.000000, mae: 165.626617, mean_q: 79.389893\n",
      "wrong_move\n",
      "   370/50000: episode: 355, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1850.000 [1850.000, 1850.000],  loss: 10123158.000000, mae: 165.360321, mean_q: 85.902946\n",
      "wrong_move\n",
      "   371/50000: episode: 356, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 335.000 [335.000, 335.000],  loss: 10516678.000000, mae: 167.130020, mean_q: 89.020523\n",
      "wrong_move\n",
      "   372/50000: episode: 357, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2966.000 [2966.000, 2966.000],  loss: 10729064.000000, mae: 166.320892, mean_q: 89.857224\n",
      "wrong_move\n",
      "   373/50000: episode: 358, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 361.000 [361.000, 361.000],  loss: 10823777.000000, mae: 166.532928, mean_q: 83.017616\n",
      "wrong_move\n",
      "   374/50000: episode: 359, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3948.000 [3948.000, 3948.000],  loss: 10532982.000000, mae: 166.213440, mean_q: 77.834053\n",
      "wrong_move\n",
      "   375/50000: episode: 360, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 254.000 [254.000, 254.000],  loss: 10329424.000000, mae: 164.846405, mean_q: 93.224388\n",
      "wrong_move\n",
      "   376/50000: episode: 361, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 453.000 [453.000, 453.000],  loss: 9985509.000000, mae: 169.464935, mean_q: 93.876007\n",
      "wrong_move\n",
      "   377/50000: episode: 362, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 980.000 [980.000, 980.000],  loss: 10640940.000000, mae: 167.487915, mean_q: 99.778534\n",
      "wrong_move\n",
      "   378/50000: episode: 363, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 369.000 [369.000, 369.000],  loss: 9971440.000000, mae: 165.943085, mean_q: 67.634445\n",
      "wrong_move\n",
      "   379/50000: episode: 364, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2956.000 [2956.000, 2956.000],  loss: 10929909.000000, mae: 168.963409, mean_q: 89.825500\n",
      "wrong_move\n",
      "   380/50000: episode: 365, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1127.000 [1127.000, 1127.000],  loss: 10454100.000000, mae: 166.823792, mean_q: 75.516434\n",
      "wrong_move\n",
      "   381/50000: episode: 366, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1262.000 [1262.000, 1262.000],  loss: 9660623.000000, mae: 165.858719, mean_q: 80.795425\n",
      "wrong_move\n",
      "   382/50000: episode: 367, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2967.000 [2967.000, 2967.000],  loss: 9438536.000000, mae: 166.562622, mean_q: 57.976044\n",
      "wrong_move\n",
      "   383/50000: episode: 368, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2967.000 [2967.000, 2967.000],  loss: 10742690.000000, mae: 168.156967, mean_q: 80.064957\n",
      "wrong_move\n",
      "   384/50000: episode: 369, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 930.000 [930.000, 930.000],  loss: 10245328.000000, mae: 165.789825, mean_q: 60.786064\n",
      "wrong_move\n",
      "   385/50000: episode: 370, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: 10708028.000000, mae: 166.386108, mean_q: 59.692661\n",
      "wrong_move\n",
      "   386/50000: episode: 371, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3250.000 [3250.000, 3250.000],  loss: 10505636.000000, mae: 166.416473, mean_q: 71.654991\n",
      "wrong_move\n",
      "   387/50000: episode: 372, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2365.000 [2365.000, 2365.000],  loss: 9688768.000000, mae: 168.601608, mean_q: 107.519318\n",
      "wrong_move\n",
      "   388/50000: episode: 373, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3801.000 [3801.000, 3801.000],  loss: 10376798.000000, mae: 166.200012, mean_q: 77.971550\n",
      "wrong_move\n",
      "   389/50000: episode: 374, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2887.000 [2887.000, 2887.000],  loss: 10415696.000000, mae: 168.226318, mean_q: 69.188889\n",
      "wrong_move\n",
      "   390/50000: episode: 375, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 10597090.000000, mae: 172.024261, mean_q: 95.924316\n",
      "wrong_move\n",
      "   391/50000: episode: 376, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3593.000 [3593.000, 3593.000],  loss: 10759192.000000, mae: 166.438660, mean_q: 71.164627\n",
      "wrong_move\n",
      "   392/50000: episode: 377, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4084.000 [4084.000, 4084.000],  loss: 10660012.000000, mae: 167.317673, mean_q: 65.214996\n",
      "wrong_move\n",
      "   393/50000: episode: 378, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3544.000 [3544.000, 3544.000],  loss: 10163780.000000, mae: 167.557953, mean_q: 81.365189\n",
      "wrong_move\n",
      "   394/50000: episode: 379, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3101.000 [3101.000, 3101.000],  loss: 10004330.000000, mae: 166.797180, mean_q: 64.532661\n",
      "wrong_move\n",
      "   395/50000: episode: 380, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2464.000 [2464.000, 2464.000],  loss: 10472864.000000, mae: 170.486450, mean_q: 91.771202\n",
      "wrong_move\n",
      "   396/50000: episode: 381, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3630.000 [3630.000, 3630.000],  loss: 10106613.000000, mae: 167.584473, mean_q: 72.026291\n",
      "wrong_move\n",
      "   397/50000: episode: 382, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2081.000 [2081.000, 2081.000],  loss: 10895797.000000, mae: 166.343460, mean_q: 63.064384\n",
      "wrong_move\n",
      "   398/50000: episode: 383, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2569.000 [2569.000, 2569.000],  loss: 9982430.000000, mae: 167.894547, mean_q: 69.139023\n",
      "wrong_move\n",
      "   399/50000: episode: 384, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3720.000 [3720.000, 3720.000],  loss: 10644156.000000, mae: 166.845200, mean_q: 57.766399\n",
      "wrong_move\n",
      "   400/50000: episode: 385, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3888.000 [3888.000, 3888.000],  loss: 10810259.000000, mae: 168.066803, mean_q: 66.060608\n",
      "wrong_move\n",
      "   401/50000: episode: 386, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2081.000 [2081.000, 2081.000],  loss: 9900184.000000, mae: 172.383408, mean_q: 126.240143\n",
      "wrong_move\n",
      "   402/50000: episode: 387, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2094.000 [2094.000, 2094.000],  loss: 10651304.000000, mae: 168.614212, mean_q: 82.965477\n",
      "wrong_move\n",
      "   403/50000: episode: 388, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2774.000 [2774.000, 2774.000],  loss: 10517508.000000, mae: 167.747986, mean_q: 58.043358\n",
      "wrong_move\n",
      "   404/50000: episode: 389, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2598.000 [2598.000, 2598.000],  loss: 10420928.000000, mae: 169.215088, mean_q: 99.312256\n",
      "wrong_move\n",
      "   405/50000: episode: 390, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 817.000 [817.000, 817.000],  loss: 11304814.000000, mae: 168.369537, mean_q: 62.211082\n",
      "wrong_move\n",
      "   406/50000: episode: 391, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 494.000 [494.000, 494.000],  loss: 10503570.000000, mae: 170.869781, mean_q: 87.354012\n",
      "wrong_move\n",
      "   407/50000: episode: 392, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 353.000 [353.000, 353.000],  loss: 10101716.000000, mae: 168.656250, mean_q: 80.916016\n",
      "wrong_move\n",
      "   408/50000: episode: 393, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 249.000 [249.000, 249.000],  loss: 10178185.000000, mae: 174.839813, mean_q: 103.040436\n",
      "wrong_move\n",
      "   409/50000: episode: 394, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 556.000 [556.000, 556.000],  loss: 11189956.000000, mae: 168.795807, mean_q: 59.505196\n",
      "wrong_move\n",
      "   410/50000: episode: 395, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: 10327513.000000, mae: 170.285156, mean_q: 96.254402\n",
      "wrong_move\n",
      "   411/50000: episode: 396, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2687.000 [2687.000, 2687.000],  loss: 9820909.000000, mae: 169.851654, mean_q: 76.532471\n",
      "wrong_move\n",
      "   412/50000: episode: 397, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 450.000 [450.000, 450.000],  loss: 10608492.000000, mae: 173.346741, mean_q: 87.847015\n",
      "wrong_move\n",
      "   413/50000: episode: 398, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 967.000 [967.000, 967.000],  loss: 10536125.000000, mae: 169.606018, mean_q: 83.232399\n",
      "wrong_move\n",
      "   414/50000: episode: 399, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 300.000 [300.000, 300.000],  loss: 10810262.000000, mae: 170.593338, mean_q: 71.711807\n",
      "wrong_move\n",
      "   415/50000: episode: 400, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4025.000 [4025.000, 4025.000],  loss: 10083914.000000, mae: 168.098206, mean_q: 49.668060\n",
      "wrong_move\n",
      "   416/50000: episode: 401, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 343.000 [343.000, 343.000],  loss: 10953228.000000, mae: 169.646561, mean_q: 67.928650\n",
      "wrong_move\n",
      "   417/50000: episode: 402, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2355.000 [2355.000, 2355.000],  loss: 10136012.000000, mae: 169.843613, mean_q: 80.848938\n",
      "wrong_move\n",
      "   418/50000: episode: 403, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2895.000 [2895.000, 2895.000],  loss: 10103980.000000, mae: 172.007629, mean_q: 85.321892\n",
      "wrong_move\n",
      "   419/50000: episode: 404, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 802.000 [802.000, 802.000],  loss: 9352139.000000, mae: 174.766922, mean_q: 89.263687\n",
      "wrong_move\n",
      "   420/50000: episode: 405, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2355.000 [2355.000, 2355.000],  loss: 10794695.000000, mae: 170.498016, mean_q: 67.793083\n",
      "wrong_move\n",
      "   421/50000: episode: 406, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 407.000 [407.000, 407.000],  loss: 10402402.000000, mae: 172.212143, mean_q: 69.321220\n",
      "wrong_move\n",
      "   422/50000: episode: 407, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2752.000 [2752.000, 2752.000],  loss: 10509771.000000, mae: 169.183655, mean_q: 80.037140\n",
      "wrong_move\n",
      "   423/50000: episode: 408, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2161.000 [2161.000, 2161.000],  loss: 11242562.000000, mae: 172.216782, mean_q: 87.126236\n",
      "wrong_move\n",
      "   424/50000: episode: 409, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3458.000 [3458.000, 3458.000],  loss: 10356926.000000, mae: 173.792236, mean_q: 101.710777\n",
      "wrong_move\n",
      "   425/50000: episode: 410, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4059.000 [4059.000, 4059.000],  loss: 10344170.000000, mae: 172.146332, mean_q: 82.310944\n",
      "wrong_move\n",
      "   426/50000: episode: 411, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 242.000 [242.000, 242.000],  loss: 11083323.000000, mae: 170.411148, mean_q: 62.586166\n",
      "wrong_move\n",
      "   427/50000: episode: 412, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 376.000 [376.000, 376.000],  loss: 10592336.000000, mae: 173.312454, mean_q: 98.688759\n",
      "wrong_move\n",
      "   428/50000: episode: 413, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3445.000 [3445.000, 3445.000],  loss: 10279422.000000, mae: 175.285583, mean_q: 98.871010\n",
      "wrong_move\n",
      "   429/50000: episode: 414, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2609.000 [2609.000, 2609.000],  loss: 10588454.000000, mae: 170.183105, mean_q: 70.820648\n",
      "wrong_move\n",
      "   430/50000: episode: 415, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3218.000 [3218.000, 3218.000],  loss: 9873895.000000, mae: 176.937378, mean_q: 95.970589\n",
      "wrong_move\n",
      "   431/50000: episode: 416, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 10273442.000000, mae: 172.317108, mean_q: 80.660851\n",
      "wrong_move\n",
      "   432/50000: episode: 417, duration: 0.065s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2077.000 [2077.000, 2077.000],  loss: 10600677.000000, mae: 173.114258, mean_q: 99.397491\n",
      "wrong_move\n",
      "   433/50000: episode: 418, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1559.000 [1559.000, 1559.000],  loss: 11025360.000000, mae: 171.140564, mean_q: 69.739716\n",
      "wrong_move\n",
      "   434/50000: episode: 419, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1634.000 [1634.000, 1634.000],  loss: 10243528.000000, mae: 172.847595, mean_q: 75.031898\n",
      "wrong_move\n",
      "   435/50000: episode: 420, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 11061076.000000, mae: 174.285645, mean_q: 96.995247\n",
      "wrong_move\n",
      "   436/50000: episode: 421, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3761.000 [3761.000, 3761.000],  loss: 10668020.000000, mae: 176.751373, mean_q: 99.877777\n",
      "wrong_move\n",
      "   437/50000: episode: 422, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1233.000 [1233.000, 1233.000],  loss: 10587678.000000, mae: 173.512054, mean_q: 87.472511\n",
      "wrong_move\n",
      "   438/50000: episode: 423, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 389.000 [389.000, 389.000],  loss: 10046342.000000, mae: 171.985901, mean_q: 77.982880\n",
      "wrong_move\n",
      "   439/50000: episode: 424, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 972.000 [972.000, 972.000],  loss: 10195209.000000, mae: 170.861176, mean_q: 78.516472\n",
      "wrong_move\n",
      "   440/50000: episode: 425, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2996.000 [2996.000, 2996.000],  loss: 9475316.000000, mae: 171.802124, mean_q: 71.752983\n",
      "wrong_move\n",
      "   441/50000: episode: 426, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1819.000 [1819.000, 1819.000],  loss: 9061998.000000, mae: 171.911743, mean_q: 62.142776\n",
      "wrong_move\n",
      "   442/50000: episode: 427, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3679.000 [3679.000, 3679.000],  loss: 10829740.000000, mae: 174.239380, mean_q: 79.233665\n",
      "wrong_move\n",
      "   443/50000: episode: 428, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 11102426.000000, mae: 175.655975, mean_q: 123.947258\n",
      "wrong_move\n",
      "   444/50000: episode: 429, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3373.000 [3373.000, 3373.000],  loss: 10235434.000000, mae: 173.402298, mean_q: 70.247696\n",
      "wrong_move\n",
      "   445/50000: episode: 430, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2552.000 [2552.000, 2552.000],  loss: 10561832.000000, mae: 171.180725, mean_q: 60.249954\n",
      "wrong_move\n",
      "   446/50000: episode: 431, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3151.000 [3151.000, 3151.000],  loss: 9747715.000000, mae: 172.185043, mean_q: 56.313728\n",
      "wrong_move\n",
      "   447/50000: episode: 432, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 391.000 [391.000, 391.000],  loss: 10531620.000000, mae: 170.721024, mean_q: 58.912529\n",
      "wrong_move\n",
      "   448/50000: episode: 433, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 331.000 [331.000, 331.000],  loss: 10593246.000000, mae: 175.598083, mean_q: 84.426422\n",
      "wrong_move\n",
      "   449/50000: episode: 434, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3572.000 [3572.000, 3572.000],  loss: 10331758.000000, mae: 171.557846, mean_q: 57.588932\n",
      "wrong_move\n",
      "   450/50000: episode: 435, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3551.000 [3551.000, 3551.000],  loss: 10525343.000000, mae: 172.652008, mean_q: 59.582203\n",
      "wrong_move\n",
      "   451/50000: episode: 436, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2901.000 [2901.000, 2901.000],  loss: 10385975.000000, mae: 172.703735, mean_q: 87.200386\n",
      "wrong_move\n",
      "   452/50000: episode: 437, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3679.000 [3679.000, 3679.000],  loss: 11143633.000000, mae: 173.491455, mean_q: 79.761391\n",
      "wrong_move\n",
      "   453/50000: episode: 438, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 933.000 [933.000, 933.000],  loss: 10648400.000000, mae: 172.020645, mean_q: 80.126923\n",
      "wrong_move\n",
      "   454/50000: episode: 439, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 9986212.000000, mae: 174.173828, mean_q: 81.390533\n",
      "wrong_move\n",
      "   455/50000: episode: 440, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3701.000 [3701.000, 3701.000],  loss: 10029502.000000, mae: 176.338638, mean_q: 87.668167\n",
      "wrong_move\n",
      "   456/50000: episode: 441, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 9878386.000000, mae: 177.144409, mean_q: 82.059052\n",
      "wrong_move\n",
      "   457/50000: episode: 442, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 836.000 [836.000, 836.000],  loss: 10603336.000000, mae: 173.508820, mean_q: 78.138435\n",
      "wrong_move\n",
      "   458/50000: episode: 443, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3002.000 [3002.000, 3002.000],  loss: 10151033.000000, mae: 174.234787, mean_q: 69.093277\n",
      "wrong_move\n",
      "   459/50000: episode: 444, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4052.000 [4052.000, 4052.000],  loss: 10868212.000000, mae: 176.304581, mean_q: 100.663834\n",
      "wrong_move\n",
      "   460/50000: episode: 445, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1691.000 [1691.000, 1691.000],  loss: 10261452.000000, mae: 176.397644, mean_q: 82.939697\n",
      "wrong_move\n",
      "   461/50000: episode: 446, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2297.000 [2297.000, 2297.000],  loss: 10683216.000000, mae: 174.557388, mean_q: 73.013748\n",
      "wrong_move\n",
      "   462/50000: episode: 447, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2112.000 [2112.000, 2112.000],  loss: 9647774.000000, mae: 173.534058, mean_q: 80.919189\n",
      "wrong_move\n",
      "   463/50000: episode: 448, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1295.000 [1295.000, 1295.000],  loss: 10455330.000000, mae: 172.918884, mean_q: 52.458176\n",
      "wrong_move\n",
      "   464/50000: episode: 449, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 492.000 [492.000, 492.000],  loss: 10469933.000000, mae: 175.294830, mean_q: 84.245651\n",
      "wrong_move\n",
      "   465/50000: episode: 450, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3871.000 [3871.000, 3871.000],  loss: 9994146.000000, mae: 177.138336, mean_q: 97.301331\n",
      "wrong_move\n",
      "   466/50000: episode: 451, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3043.000 [3043.000, 3043.000],  loss: 10199776.000000, mae: 173.905807, mean_q: 54.889950\n",
      "wrong_move\n",
      "   467/50000: episode: 452, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 904.000 [904.000, 904.000],  loss: 10027816.000000, mae: 175.846970, mean_q: 88.103821\n",
      "wrong_move\n",
      "   468/50000: episode: 453, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 10740660.000000, mae: 176.602127, mean_q: 82.744942\n",
      "wrong_move\n",
      "   469/50000: episode: 454, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1996.000 [1996.000, 1996.000],  loss: 10509864.000000, mae: 176.969299, mean_q: 88.426491\n",
      "wrong_move\n",
      "   470/50000: episode: 455, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2286.000 [2286.000, 2286.000],  loss: 11023212.000000, mae: 175.589722, mean_q: 78.648369\n",
      "wrong_move\n",
      "   471/50000: episode: 456, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1571.000 [1571.000, 1571.000],  loss: 10563354.000000, mae: 174.901962, mean_q: 73.011017\n",
      "wrong_move\n",
      "   472/50000: episode: 457, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1221.000 [1221.000, 1221.000],  loss: 10940594.000000, mae: 175.678833, mean_q: 72.580154\n",
      "wrong_move\n",
      "   473/50000: episode: 458, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2286.000 [2286.000, 2286.000],  loss: 10024128.000000, mae: 175.933929, mean_q: 74.168762\n",
      "wrong_move\n",
      "   474/50000: episode: 459, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2229.000 [2229.000, 2229.000],  loss: 10959528.000000, mae: 175.473389, mean_q: 45.481815\n",
      "wrong_move\n",
      "   475/50000: episode: 460, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2286.000 [2286.000, 2286.000],  loss: 10072852.000000, mae: 177.071106, mean_q: 72.127495\n",
      "wrong_move\n",
      "   476/50000: episode: 461, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3601.000 [3601.000, 3601.000],  loss: 10534141.000000, mae: 176.893250, mean_q: 81.193291\n",
      "wrong_move\n",
      "   477/50000: episode: 462, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3348.000 [3348.000, 3348.000],  loss: 10002722.000000, mae: 179.277832, mean_q: 87.446327\n",
      "wrong_move\n",
      "   478/50000: episode: 463, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3011.000 [3011.000, 3011.000],  loss: 9604965.000000, mae: 183.563202, mean_q: 101.352676\n",
      "wrong_move\n",
      "   479/50000: episode: 464, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4091.000 [4091.000, 4091.000],  loss: 10896832.000000, mae: 178.609268, mean_q: 104.109024\n",
      "wrong_move\n",
      "   480/50000: episode: 465, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1820.000 [1820.000, 1820.000],  loss: 9768128.000000, mae: 178.673431, mean_q: 112.079948\n",
      "wrong_move\n",
      "   481/50000: episode: 466, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1622.000 [1622.000, 1622.000],  loss: 9854614.000000, mae: 177.337173, mean_q: 90.702209\n",
      "wrong_move\n",
      "   482/50000: episode: 467, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 295.000 [295.000, 295.000],  loss: 10255404.000000, mae: 178.623383, mean_q: 96.526108\n",
      "wrong_move\n",
      "   483/50000: episode: 468, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: 10531494.000000, mae: 176.258881, mean_q: 57.735382\n",
      "wrong_move\n",
      "   484/50000: episode: 469, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1.000 [1.000, 1.000],  loss: 11360470.000000, mae: 179.615540, mean_q: 80.622299\n",
      "wrong_move\n",
      "   485/50000: episode: 470, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 26.000 [26.000, 26.000],  loss: 10667216.000000, mae: 177.648605, mean_q: 62.477154\n",
      "wrong_move\n",
      "   486/50000: episode: 471, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2536.000 [2536.000, 2536.000],  loss: 10418603.000000, mae: 177.949890, mean_q: 85.702667\n",
      "wrong_move\n",
      "   487/50000: episode: 472, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3434.000 [3434.000, 3434.000],  loss: 9968256.000000, mae: 177.396210, mean_q: 103.556366\n",
      "wrong_move\n",
      "   488/50000: episode: 473, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3405.000 [3405.000, 3405.000],  loss: 11173944.000000, mae: 174.697678, mean_q: 36.948803\n",
      "wrong_move\n",
      "   489/50000: episode: 474, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1800.000 [1800.000, 1800.000],  loss: 10750282.000000, mae: 176.653488, mean_q: 77.945862\n",
      "wrong_move\n",
      "   490/50000: episode: 475, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1107.000 [1107.000, 1107.000],  loss: 10324761.000000, mae: 178.484863, mean_q: 72.609497\n",
      "wrong_move\n",
      "   491/50000: episode: 476, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 374.000 [374.000, 374.000],  loss: 10203884.000000, mae: 176.906738, mean_q: 63.087460\n",
      "wrong_move\n",
      "   492/50000: episode: 477, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3028.000 [3028.000, 3028.000],  loss: 10055646.000000, mae: 179.275330, mean_q: 73.956146\n",
      "wrong_move\n",
      "   493/50000: episode: 478, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 610.000 [610.000, 610.000],  loss: 10977506.000000, mae: 181.174316, mean_q: 85.748375\n",
      "wrong_move\n",
      "   494/50000: episode: 479, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3271.000 [3271.000, 3271.000],  loss: 10196574.000000, mae: 178.254425, mean_q: 67.657310\n",
      "wrong_move\n",
      "   495/50000: episode: 480, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1983.000 [1983.000, 1983.000],  loss: 10758862.000000, mae: 176.259430, mean_q: 47.967667\n",
      "wrong_move\n",
      "   496/50000: episode: 481, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 95.000 [95.000, 95.000],  loss: 11002648.000000, mae: 179.608063, mean_q: 78.390938\n",
      "wrong_move\n",
      "   497/50000: episode: 482, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 10278352.000000, mae: 180.950348, mean_q: 83.409538\n",
      "wrong_move\n",
      "   498/50000: episode: 483, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1331.000 [1331.000, 1331.000],  loss: 10670435.000000, mae: 182.719254, mean_q: 86.465797\n",
      "wrong_move\n",
      "   499/50000: episode: 484, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2939.000 [2939.000, 2939.000],  loss: 10287740.000000, mae: 179.719009, mean_q: 103.776459\n",
      "wrong_move\n",
      "   500/50000: episode: 485, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: 8984972.000000, mae: 177.364838, mean_q: 73.978241\n",
      "wrong_move\n",
      "   501/50000: episode: 486, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2562.000 [2562.000, 2562.000],  loss: 10413630.000000, mae: 179.264542, mean_q: 109.284454\n",
      "wrong_move\n",
      "   502/50000: episode: 487, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3083.000 [3083.000, 3083.000],  loss: 10466828.000000, mae: 181.127899, mean_q: 106.024323\n",
      "wrong_move\n",
      "   503/50000: episode: 488, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 848.000 [848.000, 848.000],  loss: 11104252.000000, mae: 179.321716, mean_q: 72.770226\n",
      "wrong_move\n",
      "   504/50000: episode: 489, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 886.000 [886.000, 886.000],  loss: 9190604.000000, mae: 184.822235, mean_q: 111.857544\n",
      "wrong_move\n",
      "   505/50000: episode: 490, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2758.000 [2758.000, 2758.000],  loss: 10333469.000000, mae: 177.205841, mean_q: 67.425339\n",
      "wrong_move\n",
      "   506/50000: episode: 491, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2415.000 [2415.000, 2415.000],  loss: 10503873.000000, mae: 178.526337, mean_q: 62.050949\n",
      "wrong_move\n",
      "   507/50000: episode: 492, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3957.000 [3957.000, 3957.000],  loss: 11113921.000000, mae: 178.536163, mean_q: 78.022491\n",
      "wrong_move\n",
      "   508/50000: episode: 493, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4056.000 [4056.000, 4056.000],  loss: 10590487.000000, mae: 180.887299, mean_q: 81.228996\n",
      "wrong_move\n",
      "   509/50000: episode: 494, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1361.000 [1361.000, 1361.000],  loss: 10577774.000000, mae: 185.110321, mean_q: 95.852982\n",
      "wrong_move\n",
      "   510/50000: episode: 495, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2562.000 [2562.000, 2562.000],  loss: 10569126.000000, mae: 179.299500, mean_q: 77.296196\n",
      "wrong_move\n",
      "   511/50000: episode: 496, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2914.000 [2914.000, 2914.000],  loss: 10992512.000000, mae: 183.469742, mean_q: 107.462830\n",
      "wrong_move\n",
      "   512/50000: episode: 497, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2310.000 [2310.000, 2310.000],  loss: 9878306.000000, mae: 180.399139, mean_q: 114.954803\n",
      "wrong_move\n",
      "   513/50000: episode: 498, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3248.000 [3248.000, 3248.000],  loss: 10934769.000000, mae: 179.868774, mean_q: 57.175125\n",
      "wrong_move\n",
      "   514/50000: episode: 499, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2126.000 [2126.000, 2126.000],  loss: 10581810.000000, mae: 180.770615, mean_q: 81.757614\n",
      "wrong_move\n",
      "   515/50000: episode: 500, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2914.000 [2914.000, 2914.000],  loss: 9563424.000000, mae: 179.484192, mean_q: 66.634659\n",
      "wrong_move\n",
      "   516/50000: episode: 501, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2366.000 [2366.000, 2366.000],  loss: 10762665.000000, mae: 181.100647, mean_q: 96.774605\n",
      "wrong_move\n",
      "   517/50000: episode: 502, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2120.000 [2120.000, 2120.000],  loss: 10784947.000000, mae: 181.172760, mean_q: 87.401146\n",
      "wrong_move\n",
      "   518/50000: episode: 503, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 43.000 [43.000, 43.000],  loss: 10827090.000000, mae: 179.759491, mean_q: 82.923325\n",
      "wrong_move\n",
      "   519/50000: episode: 504, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3316.000 [3316.000, 3316.000],  loss: 10525224.000000, mae: 179.598846, mean_q: 64.487381\n",
      "wrong_move\n",
      "   520/50000: episode: 505, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 10193096.000000, mae: 184.342239, mean_q: 107.519882\n",
      "wrong_move\n",
      "   521/50000: episode: 506, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3312.000 [3312.000, 3312.000],  loss: 9326633.000000, mae: 179.202911, mean_q: 101.206100\n",
      "wrong_move\n",
      "   523/50000: episode: 507, duration: 0.120s, episode steps:   2, steps per second:  17, episode reward: -4961.000, mean reward: -2480.500 [-5000.000, 39.000], mean action: 192.000 [192.000, 192.000],  loss: 10393336.000000, mae: 181.344208, mean_q: 89.032364\n",
      "wrong_move\n",
      "   524/50000: episode: 508, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: 9787632.000000, mae: 181.492188, mean_q: 79.669464\n",
      "wrong_move\n",
      "   525/50000: episode: 509, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 428.000 [428.000, 428.000],  loss: 10470603.000000, mae: 183.185120, mean_q: 95.162689\n",
      "wrong_move\n",
      "   526/50000: episode: 510, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 294.000 [294.000, 294.000],  loss: 10217166.000000, mae: 178.553329, mean_q: 53.350212\n",
      "wrong_move\n",
      "   527/50000: episode: 511, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3446.000 [3446.000, 3446.000],  loss: 9561916.000000, mae: 180.873505, mean_q: 71.989296\n",
      "wrong_move\n",
      "   528/50000: episode: 512, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2673.000 [2673.000, 2673.000],  loss: 10199380.000000, mae: 183.394180, mean_q: 82.921860\n",
      "wrong_move\n",
      "   529/50000: episode: 513, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3723.000 [3723.000, 3723.000],  loss: 10205033.000000, mae: 181.464386, mean_q: 90.445274\n",
      "wrong_move\n",
      "   530/50000: episode: 514, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3316.000 [3316.000, 3316.000],  loss: 10448139.000000, mae: 181.212585, mean_q: 70.240387\n",
      "wrong_move\n",
      "   531/50000: episode: 515, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1692.000 [1692.000, 1692.000],  loss: 10327775.000000, mae: 189.269501, mean_q: 137.596588\n",
      "wrong_move\n",
      "   532/50000: episode: 516, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 718.000 [718.000, 718.000],  loss: 10639664.000000, mae: 184.848175, mean_q: 114.098892\n",
      "wrong_move\n",
      "   533/50000: episode: 517, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1746.000 [1746.000, 1746.000],  loss: 10326610.000000, mae: 182.883270, mean_q: 76.650787\n",
      "wrong_move\n",
      "   534/50000: episode: 518, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3003.000 [3003.000, 3003.000],  loss: 10555730.000000, mae: 182.180908, mean_q: 76.875580\n",
      "wrong_move\n",
      "   535/50000: episode: 519, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 529.000 [529.000, 529.000],  loss: 10122542.000000, mae: 185.952179, mean_q: 91.953857\n",
      "wrong_move\n",
      "   536/50000: episode: 520, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2092.000 [2092.000, 2092.000],  loss: 10516760.000000, mae: 185.736115, mean_q: 94.886215\n",
      "wrong_move\n",
      "   537/50000: episode: 521, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 529.000 [529.000, 529.000],  loss: 10356304.000000, mae: 184.630630, mean_q: 92.741768\n",
      "wrong_move\n",
      "   538/50000: episode: 522, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1855.000 [1855.000, 1855.000],  loss: 10850101.000000, mae: 180.639572, mean_q: 74.501320\n",
      "wrong_move\n",
      "   539/50000: episode: 523, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2705.000 [2705.000, 2705.000],  loss: 10708581.000000, mae: 186.313019, mean_q: 116.665848\n",
      "wrong_move\n",
      "   540/50000: episode: 524, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2493.000 [2493.000, 2493.000],  loss: 10348326.000000, mae: 184.670273, mean_q: 78.562790\n",
      "wrong_move\n",
      "   541/50000: episode: 525, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2664.000 [2664.000, 2664.000],  loss: 10319742.000000, mae: 182.298462, mean_q: 54.928616\n",
      "wrong_move\n",
      "   542/50000: episode: 526, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1648.000 [1648.000, 1648.000],  loss: 10624431.000000, mae: 184.824158, mean_q: 81.589722\n",
      "wrong_move\n",
      "   543/50000: episode: 527, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 398.000 [398.000, 398.000],  loss: 10433288.000000, mae: 182.212097, mean_q: 67.016739\n",
      "wrong_move\n",
      "   544/50000: episode: 528, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1677.000 [1677.000, 1677.000],  loss: 10212634.000000, mae: 184.047760, mean_q: 80.095161\n",
      "wrong_move\n",
      "   545/50000: episode: 529, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3514.000 [3514.000, 3514.000],  loss: 9884136.000000, mae: 183.613281, mean_q: 70.184357\n",
      "wrong_move\n",
      "   546/50000: episode: 530, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1899.000 [1899.000, 1899.000],  loss: 10328843.000000, mae: 184.728775, mean_q: 83.722061\n",
      "wrong_move\n",
      "   547/50000: episode: 531, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 569.000 [569.000, 569.000],  loss: 10489457.000000, mae: 182.146881, mean_q: 50.090302\n",
      "wrong_move\n",
      "   548/50000: episode: 532, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 265.000 [265.000, 265.000],  loss: 10049690.000000, mae: 183.699768, mean_q: 71.671257\n",
      "wrong_move\n",
      "   549/50000: episode: 533, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3003.000 [3003.000, 3003.000],  loss: 10337783.000000, mae: 183.735229, mean_q: 104.039780\n",
      "wrong_move\n",
      "   550/50000: episode: 534, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3032.000 [3032.000, 3032.000],  loss: 10480132.000000, mae: 183.624374, mean_q: 75.813049\n",
      "wrong_move\n",
      "   551/50000: episode: 535, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 247.000 [247.000, 247.000],  loss: 10389210.000000, mae: 183.010101, mean_q: 81.683510\n",
      "wrong_move\n",
      "   552/50000: episode: 536, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 622.000 [622.000, 622.000],  loss: 10520336.000000, mae: 181.909821, mean_q: 87.205788\n",
      "wrong_move\n",
      "   553/50000: episode: 537, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2312.000 [2312.000, 2312.000],  loss: 10368310.000000, mae: 184.597900, mean_q: 83.972916\n",
      "wrong_move\n",
      "   554/50000: episode: 538, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1651.000 [1651.000, 1651.000],  loss: 10459880.000000, mae: 187.964279, mean_q: 99.406937\n",
      "wrong_move\n",
      "   555/50000: episode: 539, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 472.000 [472.000, 472.000],  loss: 10499081.000000, mae: 185.758041, mean_q: 80.664719\n",
      "wrong_move\n",
      "   556/50000: episode: 540, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1102.000 [1102.000, 1102.000],  loss: 10709600.000000, mae: 181.958588, mean_q: 41.523338\n",
      "wrong_move\n",
      "   557/50000: episode: 541, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3596.000 [3596.000, 3596.000],  loss: 10906411.000000, mae: 183.374313, mean_q: 67.162148\n",
      "wrong_move\n",
      "   558/50000: episode: 542, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3001.000 [3001.000, 3001.000],  loss: 10512852.000000, mae: 186.682892, mean_q: 88.920380\n",
      "wrong_move\n",
      "   559/50000: episode: 543, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2106.000 [2106.000, 2106.000],  loss: 10550408.000000, mae: 184.315048, mean_q: 72.500458\n",
      "wrong_move\n",
      "   560/50000: episode: 544, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2705.000 [2705.000, 2705.000],  loss: 9753991.000000, mae: 183.301117, mean_q: 86.341133\n",
      "wrong_move\n",
      "   561/50000: episode: 545, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3297.000 [3297.000, 3297.000],  loss: 9615813.000000, mae: 185.855042, mean_q: 87.049324\n",
      "wrong_move\n",
      "   562/50000: episode: 546, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1231.000 [1231.000, 1231.000],  loss: 10163343.000000, mae: 185.825928, mean_q: 54.849159\n",
      "wrong_move\n",
      "   563/50000: episode: 547, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1065.000 [1065.000, 1065.000],  loss: 10758535.000000, mae: 184.600037, mean_q: 86.514427\n",
      "wrong_move\n",
      "   564/50000: episode: 548, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2377.000 [2377.000, 2377.000],  loss: 10438311.000000, mae: 187.361740, mean_q: 70.002861\n",
      "wrong_move\n",
      "   565/50000: episode: 549, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 529.000 [529.000, 529.000],  loss: 10197386.000000, mae: 185.908783, mean_q: 94.527039\n",
      "wrong_move\n",
      "   566/50000: episode: 550, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 856.000 [856.000, 856.000],  loss: 9935287.000000, mae: 184.584808, mean_q: 88.727516\n",
      "wrong_move\n",
      "   567/50000: episode: 551, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3801.000 [3801.000, 3801.000],  loss: 10590770.000000, mae: 186.549118, mean_q: 85.435783\n",
      "wrong_move\n",
      "   568/50000: episode: 552, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 622.000 [622.000, 622.000],  loss: 10560894.000000, mae: 184.419983, mean_q: 64.819748\n",
      "wrong_move\n",
      "   569/50000: episode: 553, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3289.000 [3289.000, 3289.000],  loss: 9790741.000000, mae: 185.492599, mean_q: 92.682533\n",
      "wrong_move\n",
      "   570/50000: episode: 554, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 332.000 [332.000, 332.000],  loss: 10192982.000000, mae: 185.743073, mean_q: 71.016052\n",
      "wrong_move\n",
      "   571/50000: episode: 555, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 112.000 [112.000, 112.000],  loss: 9759860.000000, mae: 186.591187, mean_q: 100.982414\n",
      "wrong_move\n",
      "   572/50000: episode: 556, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2806.000 [2806.000, 2806.000],  loss: 10390908.000000, mae: 190.939102, mean_q: 115.194000\n",
      "wrong_move\n",
      "   573/50000: episode: 557, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2806.000 [2806.000, 2806.000],  loss: 10324180.000000, mae: 192.512604, mean_q: 125.819725\n",
      "wrong_move\n",
      "   574/50000: episode: 558, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1297.000 [1297.000, 1297.000],  loss: 10926266.000000, mae: 188.303558, mean_q: 89.528069\n",
      "wrong_move\n",
      "   575/50000: episode: 559, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2100.000 [2100.000, 2100.000],  loss: 10424047.000000, mae: 190.856812, mean_q: 104.574265\n",
      "wrong_move\n",
      "   576/50000: episode: 560, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1407.000 [1407.000, 1407.000],  loss: 10371212.000000, mae: 187.099640, mean_q: 95.584618\n",
      "wrong_move\n",
      "   577/50000: episode: 561, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 981.000 [981.000, 981.000],  loss: 10253669.000000, mae: 184.306168, mean_q: 47.103691\n",
      "wrong_move\n",
      "   578/50000: episode: 562, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3572.000 [3572.000, 3572.000],  loss: 10077363.000000, mae: 190.766174, mean_q: 88.603607\n",
      "wrong_move\n",
      "   579/50000: episode: 563, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2107.000 [2107.000, 2107.000],  loss: 9484394.000000, mae: 186.456970, mean_q: 65.978691\n",
      "wrong_move\n",
      "   580/50000: episode: 564, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 507.000 [507.000, 507.000],  loss: 10068824.000000, mae: 188.547058, mean_q: 85.931259\n",
      "wrong_move\n",
      "   581/50000: episode: 565, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 446.000 [446.000, 446.000],  loss: 9798285.000000, mae: 184.752121, mean_q: 44.766182\n",
      "wrong_move\n",
      "   582/50000: episode: 566, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1862.000 [1862.000, 1862.000],  loss: 10488916.000000, mae: 189.195908, mean_q: 94.062851\n",
      "wrong_move\n",
      "   583/50000: episode: 567, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2881.000 [2881.000, 2881.000],  loss: 9704230.000000, mae: 189.593994, mean_q: 102.469353\n",
      "wrong_move\n",
      "   584/50000: episode: 568, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2171.000 [2171.000, 2171.000],  loss: 9830764.000000, mae: 190.449646, mean_q: 125.944763\n",
      "wrong_move\n",
      "   585/50000: episode: 569, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 703.000 [703.000, 703.000],  loss: 10248654.000000, mae: 188.816864, mean_q: 89.622314\n",
      "wrong_move\n",
      "   586/50000: episode: 570, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 794.000 [794.000, 794.000],  loss: 10416730.000000, mae: 192.816132, mean_q: 114.068344\n",
      "wrong_move\n",
      "   587/50000: episode: 571, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3222.000 [3222.000, 3222.000],  loss: 11032592.000000, mae: 187.929321, mean_q: 85.980492\n",
      "wrong_move\n",
      "   588/50000: episode: 572, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1381.000 [1381.000, 1381.000],  loss: 10497100.000000, mae: 186.438751, mean_q: 73.691612\n",
      "wrong_move\n",
      "   589/50000: episode: 573, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1762.000 [1762.000, 1762.000],  loss: 10582408.000000, mae: 189.617493, mean_q: 90.401398\n",
      "wrong_move\n",
      "   590/50000: episode: 574, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3615.000 [3615.000, 3615.000],  loss: 9767154.000000, mae: 187.621841, mean_q: 71.943115\n",
      "wrong_move\n",
      "   591/50000: episode: 575, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3548.000 [3548.000, 3548.000],  loss: 10346889.000000, mae: 189.622833, mean_q: 92.234756\n",
      "wrong_move\n",
      "   592/50000: episode: 576, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1381.000 [1381.000, 1381.000],  loss: 10481833.000000, mae: 193.139160, mean_q: 105.248535\n",
      "wrong_move\n",
      "   593/50000: episode: 577, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3859.000 [3859.000, 3859.000],  loss: 10978624.000000, mae: 187.637878, mean_q: 60.859409\n",
      "wrong_move\n",
      "   594/50000: episode: 578, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 10101494.000000, mae: 194.254120, mean_q: 95.609413\n",
      "wrong_move\n",
      "   595/50000: episode: 579, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1297.000 [1297.000, 1297.000],  loss: 9086648.000000, mae: 195.380402, mean_q: 126.493759\n",
      "wrong_move\n",
      "   596/50000: episode: 580, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2406.000 [2406.000, 2406.000],  loss: 9848078.000000, mae: 188.162323, mean_q: 83.220634\n",
      "wrong_move\n",
      "   597/50000: episode: 581, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 507.000 [507.000, 507.000],  loss: 10205900.000000, mae: 190.314819, mean_q: 85.168533\n",
      "wrong_move\n",
      "   598/50000: episode: 582, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1311.000 [1311.000, 1311.000],  loss: 10550681.000000, mae: 189.077698, mean_q: 82.053246\n",
      "wrong_move\n",
      "   599/50000: episode: 583, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2635.000 [2635.000, 2635.000],  loss: 10600778.000000, mae: 188.373810, mean_q: 66.592636\n",
      "wrong_move\n",
      "   600/50000: episode: 584, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 507.000 [507.000, 507.000],  loss: 9139971.000000, mae: 186.972504, mean_q: 65.574097\n",
      "wrong_move\n",
      "   601/50000: episode: 585, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 587.000 [587.000, 587.000],  loss: 10448966.000000, mae: 190.181015, mean_q: 70.104767\n",
      "wrong_move\n",
      "   602/50000: episode: 586, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2451.000 [2451.000, 2451.000],  loss: 10902276.000000, mae: 189.980118, mean_q: 90.378372\n",
      "wrong_move\n",
      "   603/50000: episode: 587, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1395.000 [1395.000, 1395.000],  loss: 10112145.000000, mae: 193.992035, mean_q: 98.109146\n",
      "wrong_move\n",
      "   604/50000: episode: 588, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1983.000 [1983.000, 1983.000],  loss: 10534504.000000, mae: 187.186707, mean_q: 49.119984\n",
      "wrong_move\n",
      "   605/50000: episode: 589, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2193.000 [2193.000, 2193.000],  loss: 9674442.000000, mae: 191.056198, mean_q: 90.225189\n",
      "wrong_move\n",
      "   606/50000: episode: 590, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3976.000 [3976.000, 3976.000],  loss: 10080272.000000, mae: 188.478058, mean_q: 55.447838\n",
      "wrong_move\n",
      "   607/50000: episode: 591, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2370.000 [2370.000, 2370.000],  loss: 9840562.000000, mae: 190.195648, mean_q: 73.280426\n",
      "wrong_move\n",
      "   608/50000: episode: 592, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1065.000 [1065.000, 1065.000],  loss: 9995900.000000, mae: 193.102798, mean_q: 110.664612\n",
      "wrong_move\n",
      "   609/50000: episode: 593, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2873.000 [2873.000, 2873.000],  loss: 9669160.000000, mae: 191.849060, mean_q: 104.384338\n",
      "wrong_move\n",
      "   610/50000: episode: 594, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 426.000 [426.000, 426.000],  loss: 10645688.000000, mae: 189.229111, mean_q: 49.575073\n",
      "wrong_move\n",
      "   611/50000: episode: 595, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3362.000 [3362.000, 3362.000],  loss: 9678170.000000, mae: 193.457733, mean_q: 89.630226\n",
      "wrong_move\n",
      "   612/50000: episode: 596, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2370.000 [2370.000, 2370.000],  loss: 10095138.000000, mae: 191.477295, mean_q: 84.400192\n",
      "wrong_move\n",
      "   613/50000: episode: 597, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2431.000 [2431.000, 2431.000],  loss: 10754445.000000, mae: 191.599915, mean_q: 98.372215\n",
      "wrong_move\n",
      "   614/50000: episode: 598, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 791.000 [791.000, 791.000],  loss: 10556372.000000, mae: 193.699783, mean_q: 79.222054\n",
      "wrong_move\n",
      "   615/50000: episode: 599, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3811.000 [3811.000, 3811.000],  loss: 10089420.000000, mae: 189.228012, mean_q: 74.488159\n",
      "wrong_move\n",
      "   616/50000: episode: 600, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3982.000 [3982.000, 3982.000],  loss: 9787850.000000, mae: 195.345917, mean_q: 100.837708\n",
      "wrong_move\n",
      "   617/50000: episode: 601, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2681.000 [2681.000, 2681.000],  loss: 10859015.000000, mae: 193.796722, mean_q: 105.638199\n",
      "wrong_move\n",
      "   618/50000: episode: 602, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3646.000 [3646.000, 3646.000],  loss: 10248695.000000, mae: 192.921509, mean_q: 87.983780\n",
      "wrong_move\n",
      "   619/50000: episode: 603, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3822.000 [3822.000, 3822.000],  loss: 10765378.000000, mae: 190.496658, mean_q: 82.577393\n",
      "wrong_move\n",
      "   620/50000: episode: 604, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3344.000 [3344.000, 3344.000],  loss: 10020056.000000, mae: 195.316116, mean_q: 97.712555\n",
      "wrong_move\n",
      "   621/50000: episode: 605, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2922.000 [2922.000, 2922.000],  loss: 10137832.000000, mae: 195.659851, mean_q: 99.356293\n",
      "wrong_move\n",
      "   622/50000: episode: 606, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2233.000 [2233.000, 2233.000],  loss: 9941712.000000, mae: 195.901077, mean_q: 103.796539\n",
      "wrong_move\n",
      "   623/50000: episode: 607, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1771.000 [1771.000, 1771.000],  loss: 10431089.000000, mae: 192.793777, mean_q: 65.328407\n",
      "wrong_move\n",
      "   624/50000: episode: 608, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 611.000 [611.000, 611.000],  loss: 10984652.000000, mae: 198.004059, mean_q: 109.223404\n",
      "wrong_move\n",
      "   625/50000: episode: 609, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1631.000 [1631.000, 1631.000],  loss: 10804946.000000, mae: 195.647491, mean_q: 124.934776\n",
      "wrong_move\n",
      "   626/50000: episode: 610, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3334.000 [3334.000, 3334.000],  loss: 9607714.000000, mae: 193.854706, mean_q: 98.818787\n",
      "wrong_move\n",
      "   627/50000: episode: 611, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2860.000 [2860.000, 2860.000],  loss: 10441964.000000, mae: 190.966431, mean_q: 67.481430\n",
      "wrong_move\n",
      "   628/50000: episode: 612, duration: 0.043s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1811.000 [1811.000, 1811.000],  loss: 9689560.000000, mae: 191.235733, mean_q: 59.073761\n",
      "wrong_move\n",
      "   629/50000: episode: 613, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1377.000 [1377.000, 1377.000],  loss: 10086816.000000, mae: 196.604584, mean_q: 98.485291\n",
      "wrong_move\n",
      "   630/50000: episode: 614, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 679.000 [679.000, 679.000],  loss: 9824392.000000, mae: 197.197021, mean_q: 117.285759\n",
      "wrong_move\n",
      "   631/50000: episode: 615, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 685.000 [685.000, 685.000],  loss: 10339666.000000, mae: 191.187225, mean_q: 56.410122\n",
      "wrong_move\n",
      "   632/50000: episode: 616, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3645.000 [3645.000, 3645.000],  loss: 10538808.000000, mae: 192.584427, mean_q: 59.464279\n",
      "wrong_move\n",
      "   633/50000: episode: 617, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1128.000 [1128.000, 1128.000],  loss: 10404030.000000, mae: 192.286041, mean_q: 67.025330\n",
      "wrong_move\n",
      "   634/50000: episode: 618, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3411.000 [3411.000, 3411.000],  loss: 9653100.000000, mae: 195.714905, mean_q: 112.838036\n",
      "wrong_move\n",
      "   635/50000: episode: 619, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3895.000 [3895.000, 3895.000],  loss: 10523048.000000, mae: 194.487183, mean_q: 75.811356\n",
      "wrong_move\n",
      "   636/50000: episode: 620, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1474.000 [1474.000, 1474.000],  loss: 10113436.000000, mae: 194.269531, mean_q: 97.304375\n",
      "wrong_move\n",
      "   637/50000: episode: 621, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3696.000 [3696.000, 3696.000],  loss: 9813750.000000, mae: 192.875824, mean_q: 55.423012\n",
      "wrong_move\n",
      "   638/50000: episode: 622, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1023.000 [1023.000, 1023.000],  loss: 9765661.000000, mae: 191.396927, mean_q: 56.340534\n",
      "wrong_move\n",
      "   639/50000: episode: 623, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3411.000 [3411.000, 3411.000],  loss: 10612884.000000, mae: 195.986023, mean_q: 107.949554\n",
      "wrong_move\n",
      "   640/50000: episode: 624, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: 9926526.000000, mae: 192.944321, mean_q: 79.843719\n",
      "wrong_move\n",
      "   641/50000: episode: 625, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3515.000 [3515.000, 3515.000],  loss: 9142854.000000, mae: 195.730530, mean_q: 108.250229\n",
      "wrong_move\n",
      "   642/50000: episode: 626, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1987.000 [1987.000, 1987.000],  loss: 10016980.000000, mae: 191.331818, mean_q: 51.041611\n",
      "wrong_move\n",
      "   643/50000: episode: 627, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 517.000 [517.000, 517.000],  loss: 9971436.000000, mae: 202.828018, mean_q: 121.523155\n",
      "wrong_move\n",
      "   644/50000: episode: 628, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1608.000 [1608.000, 1608.000],  loss: 10586322.000000, mae: 195.785522, mean_q: 66.141762\n",
      "wrong_move\n",
      "   645/50000: episode: 629, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 893.000 [893.000, 893.000],  loss: 10535902.000000, mae: 192.971008, mean_q: 71.776581\n",
      "wrong_move\n",
      "   646/50000: episode: 630, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3346.000 [3346.000, 3346.000],  loss: 10618208.000000, mae: 194.568192, mean_q: 77.936508\n",
      "wrong_move\n",
      "   647/50000: episode: 631, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 498.000 [498.000, 498.000],  loss: 10528210.000000, mae: 194.814667, mean_q: 90.239944\n",
      "wrong_move\n",
      "   648/50000: episode: 632, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3852.000 [3852.000, 3852.000],  loss: 9883081.000000, mae: 197.014084, mean_q: 103.820251\n",
      "wrong_move\n",
      "   649/50000: episode: 633, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 466.000 [466.000, 466.000],  loss: 10165744.000000, mae: 192.858948, mean_q: 59.190926\n",
      "wrong_move\n",
      "   650/50000: episode: 634, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2433.000 [2433.000, 2433.000],  loss: 9680112.000000, mae: 193.030670, mean_q: 70.447075\n",
      "wrong_move\n",
      "   651/50000: episode: 635, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1719.000 [1719.000, 1719.000],  loss: 10551428.000000, mae: 202.280655, mean_q: 112.829292\n",
      "wrong_move\n",
      "   652/50000: episode: 636, duration: 0.043s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3652.000 [3652.000, 3652.000],  loss: 9750673.000000, mae: 193.401550, mean_q: 84.559792\n",
      "wrong_move\n",
      "   653/50000: episode: 637, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1623.000 [1623.000, 1623.000],  loss: 10386655.000000, mae: 195.482849, mean_q: 77.428764\n",
      "wrong_move\n",
      "   654/50000: episode: 638, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3850.000 [3850.000, 3850.000],  loss: 10085296.000000, mae: 193.907227, mean_q: 72.629929\n",
      "wrong_move\n",
      "   655/50000: episode: 639, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2710.000 [2710.000, 2710.000],  loss: 10328664.000000, mae: 196.804886, mean_q: 104.089912\n",
      "wrong_move\n",
      "   656/50000: episode: 640, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 968.000 [968.000, 968.000],  loss: 10437023.000000, mae: 196.814255, mean_q: 74.463280\n",
      "wrong_move\n",
      "   657/50000: episode: 641, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 657.000 [657.000, 657.000],  loss: 10665398.000000, mae: 197.571930, mean_q: 136.324158\n",
      "wrong_move\n",
      "   658/50000: episode: 642, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1808.000 [1808.000, 1808.000],  loss: 10001450.000000, mae: 193.742126, mean_q: 57.712166\n",
      "wrong_move\n",
      "   659/50000: episode: 643, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1828.000 [1828.000, 1828.000],  loss: 9953424.000000, mae: 197.211868, mean_q: 82.670341\n",
      "wrong_move\n",
      "   660/50000: episode: 644, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 477.000 [477.000, 477.000],  loss: 10848308.000000, mae: 195.675140, mean_q: 77.119064\n",
      "wrong_move\n",
      "   661/50000: episode: 645, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 634.000 [634.000, 634.000],  loss: 10357294.000000, mae: 197.841797, mean_q: 79.901459\n",
      "wrong_move\n",
      "   662/50000: episode: 646, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1962.000 [1962.000, 1962.000],  loss: 10655138.000000, mae: 196.901337, mean_q: 89.072479\n",
      "wrong_move\n",
      "   663/50000: episode: 647, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 676.000 [676.000, 676.000],  loss: 10167801.000000, mae: 196.005630, mean_q: 53.928631\n",
      "wrong_move\n",
      "   664/50000: episode: 648, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2942.000 [2942.000, 2942.000],  loss: 10196868.000000, mae: 196.889313, mean_q: 87.823486\n",
      "wrong_move\n",
      "   665/50000: episode: 649, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 998.000 [998.000, 998.000],  loss: 9718100.000000, mae: 200.271118, mean_q: 112.127197\n",
      "wrong_move\n",
      "   666/50000: episode: 650, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: 9671391.000000, mae: 196.355804, mean_q: 75.661446\n",
      "wrong_move\n",
      "   667/50000: episode: 651, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1151.000 [1151.000, 1151.000],  loss: 10226768.000000, mae: 196.200897, mean_q: 71.265221\n",
      "wrong_move\n",
      "   668/50000: episode: 652, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1077.000 [1077.000, 1077.000],  loss: 10685048.000000, mae: 197.018768, mean_q: 80.129616\n",
      "wrong_move\n",
      "   669/50000: episode: 653, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3346.000 [3346.000, 3346.000],  loss: 9910428.000000, mae: 198.434143, mean_q: 79.604141\n",
      "wrong_move\n",
      "   670/50000: episode: 654, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1658.000 [1658.000, 1658.000],  loss: 9950626.000000, mae: 194.712616, mean_q: 56.244213\n",
      "wrong_move\n",
      "   671/50000: episode: 655, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 10710983.000000, mae: 196.431183, mean_q: 78.295197\n",
      "wrong_move\n",
      "   672/50000: episode: 656, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 741.000 [741.000, 741.000],  loss: 10290494.000000, mae: 203.405869, mean_q: 108.688766\n",
      "wrong_move\n",
      "   673/50000: episode: 657, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 908.000 [908.000, 908.000],  loss: 10045129.000000, mae: 200.054962, mean_q: 103.792107\n",
      "wrong_move\n",
      "   674/50000: episode: 658, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3794.000 [3794.000, 3794.000],  loss: 10530528.000000, mae: 197.535278, mean_q: 73.820183\n",
      "wrong_move\n",
      "   675/50000: episode: 659, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 707.000 [707.000, 707.000],  loss: 9383645.000000, mae: 199.668777, mean_q: 91.168594\n",
      "wrong_move\n",
      "   676/50000: episode: 660, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 9949689.000000, mae: 196.244781, mean_q: 63.023949\n",
      "wrong_move\n",
      "   677/50000: episode: 661, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 726.000 [726.000, 726.000],  loss: 9219201.000000, mae: 201.072845, mean_q: 94.798233\n",
      "wrong_move\n",
      "   678/50000: episode: 662, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3627.000 [3627.000, 3627.000],  loss: 10136820.000000, mae: 199.784317, mean_q: 113.267471\n",
      "wrong_move\n",
      "   679/50000: episode: 663, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3807.000 [3807.000, 3807.000],  loss: 9914332.000000, mae: 197.351868, mean_q: 64.878326\n",
      "wrong_move\n",
      "   680/50000: episode: 664, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3257.000 [3257.000, 3257.000],  loss: 10078056.000000, mae: 196.443115, mean_q: 82.137535\n",
      "wrong_move\n",
      "   681/50000: episode: 665, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1790.000 [1790.000, 1790.000],  loss: 10679958.000000, mae: 197.714508, mean_q: 75.260796\n",
      "wrong_move\n",
      "   682/50000: episode: 666, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1629.000 [1629.000, 1629.000],  loss: 10173236.000000, mae: 197.910583, mean_q: 70.604889\n",
      "wrong_move\n",
      "   683/50000: episode: 667, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1619.000 [1619.000, 1619.000],  loss: 10204493.000000, mae: 196.268219, mean_q: 65.960693\n",
      "wrong_move\n",
      "   684/50000: episode: 668, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1945.000 [1945.000, 1945.000],  loss: 10029308.000000, mae: 199.133377, mean_q: 75.641464\n",
      "wrong_move\n",
      "   685/50000: episode: 669, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1945.000 [1945.000, 1945.000],  loss: 9876688.000000, mae: 197.264984, mean_q: 62.558815\n",
      "wrong_move\n",
      "   686/50000: episode: 670, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 9891798.000000, mae: 199.456909, mean_q: 70.208069\n",
      "wrong_move\n",
      "   687/50000: episode: 671, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 55.000 [55.000, 55.000],  loss: 9556101.000000, mae: 197.395126, mean_q: 68.364105\n",
      "wrong_move\n",
      "   688/50000: episode: 672, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: 9640243.000000, mae: 196.398972, mean_q: 43.977219\n",
      "wrong_move\n",
      "   689/50000: episode: 673, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: 10725683.000000, mae: 198.242294, mean_q: 57.156967\n",
      "wrong_move\n",
      "   690/50000: episode: 674, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 830.000 [830.000, 830.000],  loss: 9935942.000000, mae: 199.378967, mean_q: 71.415024\n",
      "wrong_move\n",
      "   691/50000: episode: 675, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3691.000 [3691.000, 3691.000],  loss: 9307132.000000, mae: 198.244080, mean_q: 55.535378\n",
      "wrong_move\n",
      "   692/50000: episode: 676, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1878.000 [1878.000, 1878.000],  loss: 10518408.000000, mae: 198.573624, mean_q: 62.076828\n",
      "wrong_move\n",
      "   693/50000: episode: 677, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: 10524081.000000, mae: 199.703934, mean_q: 81.380653\n",
      "wrong_move\n",
      "   694/50000: episode: 678, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1884.000 [1884.000, 1884.000],  loss: 10984466.000000, mae: 199.229004, mean_q: 61.740009\n",
      "wrong_move\n",
      "   695/50000: episode: 679, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 488.000 [488.000, 488.000],  loss: 10136828.000000, mae: 199.259918, mean_q: 74.573212\n",
      "wrong_move\n",
      "   696/50000: episode: 680, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2842.000 [2842.000, 2842.000],  loss: 9608597.000000, mae: 201.567871, mean_q: 100.746948\n",
      "wrong_move\n",
      "   697/50000: episode: 681, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1427.000 [1427.000, 1427.000],  loss: 10290478.000000, mae: 197.734924, mean_q: 60.057243\n",
      "wrong_move\n",
      "   698/50000: episode: 682, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2687.000 [2687.000, 2687.000],  loss: 9972191.000000, mae: 198.659744, mean_q: 44.872833\n",
      "wrong_move\n",
      "   699/50000: episode: 683, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 9914769.000000, mae: 199.029770, mean_q: 64.439377\n",
      "wrong_move\n",
      "   700/50000: episode: 684, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2904.000 [2904.000, 2904.000],  loss: 9312926.000000, mae: 197.661499, mean_q: 46.857113\n",
      "wrong_move\n",
      "   701/50000: episode: 685, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3408.000 [3408.000, 3408.000],  loss: 9624078.000000, mae: 202.260605, mean_q: 89.063316\n",
      "wrong_move\n",
      "   702/50000: episode: 686, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 951.000 [951.000, 951.000],  loss: 10068796.000000, mae: 200.650146, mean_q: 81.887276\n",
      "wrong_move\n",
      "   703/50000: episode: 687, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 929.000 [929.000, 929.000],  loss: 10439676.000000, mae: 198.409668, mean_q: 47.319313\n",
      "wrong_move\n",
      "   704/50000: episode: 688, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3796.000 [3796.000, 3796.000],  loss: 9824151.000000, mae: 201.004028, mean_q: 64.106636\n",
      "wrong_move\n",
      "   705/50000: episode: 689, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 552.000 [552.000, 552.000],  loss: 9784922.000000, mae: 204.722275, mean_q: 105.567680\n",
      "wrong_move\n",
      "   706/50000: episode: 690, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3267.000 [3267.000, 3267.000],  loss: 9919757.000000, mae: 200.663452, mean_q: 64.177795\n",
      "wrong_move\n",
      "   707/50000: episode: 691, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 9633476.000000, mae: 199.911224, mean_q: 47.546165\n",
      "wrong_move\n",
      "   708/50000: episode: 692, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1066.000 [1066.000, 1066.000],  loss: 9569905.000000, mae: 200.137039, mean_q: 84.621643\n",
      "wrong_move\n",
      "   709/50000: episode: 693, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1205.000 [1205.000, 1205.000],  loss: 9943386.000000, mae: 203.311172, mean_q: 100.055565\n",
      "wrong_move\n",
      "   710/50000: episode: 694, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2105.000 [2105.000, 2105.000],  loss: 10158992.000000, mae: 201.933197, mean_q: 107.448151\n",
      "wrong_move\n",
      "   711/50000: episode: 695, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2910.000 [2910.000, 2910.000],  loss: 10159515.000000, mae: 201.643127, mean_q: 62.201385\n",
      "wrong_move\n",
      "   712/50000: episode: 696, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 495.000 [495.000, 495.000],  loss: 9655540.000000, mae: 204.962646, mean_q: 93.441635\n",
      "wrong_move\n",
      "   713/50000: episode: 697, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4072.000 [4072.000, 4072.000],  loss: 8745634.000000, mae: 199.319870, mean_q: 56.923847\n",
      "wrong_move\n",
      "   714/50000: episode: 698, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 886.000 [886.000, 886.000],  loss: 9544206.000000, mae: 201.435928, mean_q: 80.874619\n",
      "wrong_move\n",
      "   715/50000: episode: 699, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1431.000 [1431.000, 1431.000],  loss: 9705876.000000, mae: 206.764099, mean_q: 112.531258\n",
      "wrong_move\n",
      "   716/50000: episode: 700, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1205.000 [1205.000, 1205.000],  loss: 10467978.000000, mae: 205.188293, mean_q: 109.099167\n",
      "wrong_move\n",
      "   717/50000: episode: 701, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3009.000 [3009.000, 3009.000],  loss: 9935633.000000, mae: 201.060104, mean_q: 61.726105\n",
      "wrong_move\n",
      "   718/50000: episode: 702, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3970.000 [3970.000, 3970.000],  loss: 10232404.000000, mae: 204.641205, mean_q: 73.748688\n",
      "wrong_move\n",
      "   719/50000: episode: 703, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2834.000 [2834.000, 2834.000],  loss: 9544126.000000, mae: 204.722565, mean_q: 84.775055\n",
      "wrong_move\n",
      "   720/50000: episode: 704, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2711.000 [2711.000, 2711.000],  loss: 9742758.000000, mae: 203.694351, mean_q: 80.977722\n",
      "wrong_move\n",
      "   721/50000: episode: 705, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2007.000 [2007.000, 2007.000],  loss: 9674056.000000, mae: 202.743454, mean_q: 83.217628\n",
      "wrong_move\n",
      "   722/50000: episode: 706, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3020.000 [3020.000, 3020.000],  loss: 10261531.000000, mae: 202.099030, mean_q: 62.444710\n",
      "wrong_move\n",
      "   723/50000: episode: 707, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3359.000 [3359.000, 3359.000],  loss: 10614804.000000, mae: 203.872864, mean_q: 72.294739\n",
      "wrong_move\n",
      "   724/50000: episode: 708, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2748.000 [2748.000, 2748.000],  loss: 10364376.000000, mae: 203.760574, mean_q: 74.688026\n",
      "wrong_move\n",
      "   725/50000: episode: 709, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1723.000 [1723.000, 1723.000],  loss: 10348708.000000, mae: 206.681198, mean_q: 107.884834\n",
      "wrong_move\n",
      "   726/50000: episode: 710, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4092.000 [4092.000, 4092.000],  loss: 10402735.000000, mae: 205.096863, mean_q: 93.129456\n",
      "wrong_move\n",
      "   727/50000: episode: 711, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2652.000 [2652.000, 2652.000],  loss: 10230304.000000, mae: 208.912643, mean_q: 110.691986\n",
      "wrong_move\n",
      "   728/50000: episode: 712, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1134.000 [1134.000, 1134.000],  loss: 10058068.000000, mae: 205.171539, mean_q: 89.607178\n",
      "wrong_move\n",
      "   729/50000: episode: 713, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3213.000 [3213.000, 3213.000],  loss: 10456196.000000, mae: 205.558289, mean_q: 93.523987\n",
      "wrong_move\n",
      "   730/50000: episode: 714, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2576.000 [2576.000, 2576.000],  loss: 10148328.000000, mae: 204.169449, mean_q: 87.827271\n",
      "wrong_move\n",
      "   731/50000: episode: 715, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 754.000 [754.000, 754.000],  loss: 10412788.000000, mae: 208.200272, mean_q: 95.892166\n",
      "wrong_move\n",
      "   732/50000: episode: 716, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 10514925.000000, mae: 204.441910, mean_q: 101.150887\n",
      "wrong_move\n",
      "   733/50000: episode: 717, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2146.000 [2146.000, 2146.000],  loss: 9774948.000000, mae: 204.582520, mean_q: 76.668503\n",
      "wrong_move\n",
      "   734/50000: episode: 718, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 10556951.000000, mae: 206.050690, mean_q: 75.453194\n",
      "wrong_move\n",
      "   735/50000: episode: 719, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 896.000 [896.000, 896.000],  loss: 10502860.000000, mae: 203.529449, mean_q: 70.987946\n",
      "wrong_move\n",
      "   736/50000: episode: 720, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2687.000 [2687.000, 2687.000],  loss: 9920177.000000, mae: 202.826218, mean_q: 56.919014\n",
      "wrong_move\n",
      "   737/50000: episode: 721, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2553.000 [2553.000, 2553.000],  loss: 10201368.000000, mae: 209.117401, mean_q: 120.478806\n",
      "wrong_move\n",
      "   738/50000: episode: 722, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3518.000 [3518.000, 3518.000],  loss: 10062124.000000, mae: 207.511017, mean_q: 115.119064\n",
      "wrong_move\n",
      "   739/50000: episode: 723, duration: 0.044s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1912.000 [1912.000, 1912.000],  loss: 9504794.000000, mae: 210.425674, mean_q: 118.462219\n",
      "wrong_move\n",
      "   740/50000: episode: 724, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2701.000 [2701.000, 2701.000],  loss: 9513097.000000, mae: 205.895264, mean_q: 85.099083\n",
      "wrong_move\n",
      "   741/50000: episode: 725, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2075.000 [2075.000, 2075.000],  loss: 9727643.000000, mae: 208.220383, mean_q: 112.566727\n",
      "wrong_move\n",
      "   742/50000: episode: 726, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 77.000 [77.000, 77.000],  loss: 9935100.000000, mae: 203.498932, mean_q: 64.699188\n",
      "wrong_move\n",
      "   743/50000: episode: 727, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 296.000 [296.000, 296.000],  loss: 10209692.000000, mae: 209.825974, mean_q: 109.256165\n",
      "wrong_move\n",
      "   744/50000: episode: 728, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1491.000 [1491.000, 1491.000],  loss: 9918250.000000, mae: 203.367172, mean_q: 50.989979\n",
      "wrong_move\n",
      "   745/50000: episode: 729, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3797.000 [3797.000, 3797.000],  loss: 10515474.000000, mae: 204.166428, mean_q: 65.268539\n",
      "wrong_move\n",
      "   746/50000: episode: 730, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3454.000 [3454.000, 3454.000],  loss: 10285030.000000, mae: 206.630707, mean_q: 129.496002\n",
      "wrong_move\n",
      "   747/50000: episode: 731, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3666.000 [3666.000, 3666.000],  loss: 10392988.000000, mae: 204.979401, mean_q: 67.907845\n",
      "wrong_move\n",
      "   748/50000: episode: 732, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3354.000 [3354.000, 3354.000],  loss: 9922285.000000, mae: 207.865875, mean_q: 96.005295\n",
      "wrong_move\n",
      "   749/50000: episode: 733, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3833.000 [3833.000, 3833.000],  loss: 9869438.000000, mae: 206.562210, mean_q: 78.565048\n",
      "wrong_move\n",
      "   750/50000: episode: 734, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2910.000 [2910.000, 2910.000],  loss: 10394478.000000, mae: 206.688110, mean_q: 93.418365\n",
      "wrong_move\n",
      "   751/50000: episode: 735, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1992.000 [1992.000, 1992.000],  loss: 9778724.000000, mae: 207.852814, mean_q: 109.440979\n",
      "wrong_move\n",
      "   752/50000: episode: 736, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1982.000 [1982.000, 1982.000],  loss: 10161738.000000, mae: 209.208282, mean_q: 104.324417\n",
      "wrong_move\n",
      "   753/50000: episode: 737, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1514.000 [1514.000, 1514.000],  loss: 9281992.000000, mae: 207.299713, mean_q: 83.924316\n",
      "wrong_move\n",
      "   754/50000: episode: 738, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3534.000 [3534.000, 3534.000],  loss: 10835957.000000, mae: 205.870132, mean_q: 85.957954\n",
      "wrong_move\n",
      "   755/50000: episode: 739, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2403.000 [2403.000, 2403.000],  loss: 9946356.000000, mae: 208.610596, mean_q: 88.441269\n",
      "wrong_move\n",
      "   756/50000: episode: 740, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2819.000 [2819.000, 2819.000],  loss: 9444396.000000, mae: 209.469070, mean_q: 93.325302\n",
      "wrong_move\n",
      "   757/50000: episode: 741, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1562.000 [1562.000, 1562.000],  loss: 10676748.000000, mae: 207.242569, mean_q: 64.240387\n",
      "wrong_move\n",
      "   758/50000: episode: 742, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2017.000 [2017.000, 2017.000],  loss: 9517874.000000, mae: 206.109192, mean_q: 51.184349\n",
      "wrong_move\n",
      "   759/50000: episode: 743, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 10599485.000000, mae: 209.699310, mean_q: 101.404442\n",
      "wrong_move\n",
      "   760/50000: episode: 744, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2075.000 [2075.000, 2075.000],  loss: 10307722.000000, mae: 206.821365, mean_q: 74.704788\n",
      "wrong_move\n",
      "   761/50000: episode: 745, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: 10371071.000000, mae: 210.907089, mean_q: 107.162956\n",
      "wrong_move\n",
      "   762/50000: episode: 746, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 620.000 [620.000, 620.000],  loss: 10592674.000000, mae: 205.390717, mean_q: 43.903557\n",
      "wrong_move\n",
      "   763/50000: episode: 747, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 220.000 [220.000, 220.000],  loss: 9683153.000000, mae: 211.055801, mean_q: 107.734634\n",
      "wrong_move\n",
      "   764/50000: episode: 748, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3048.000 [3048.000, 3048.000],  loss: 10350909.000000, mae: 206.857727, mean_q: 76.113129\n",
      "wrong_move\n",
      "   765/50000: episode: 749, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4.000 [4.000, 4.000],  loss: 9402480.000000, mae: 207.462036, mean_q: 88.951645\n",
      "wrong_move\n",
      "   766/50000: episode: 750, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 324.000 [324.000, 324.000],  loss: 10157226.000000, mae: 211.861969, mean_q: 100.255920\n",
      "wrong_move\n",
      "   767/50000: episode: 751, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3962.000 [3962.000, 3962.000],  loss: 10608596.000000, mae: 210.783325, mean_q: 86.243225\n",
      "wrong_move\n",
      "   768/50000: episode: 752, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4087.000 [4087.000, 4087.000],  loss: 10215566.000000, mae: 208.201477, mean_q: 82.647095\n",
      "wrong_move\n",
      "   769/50000: episode: 753, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3418.000 [3418.000, 3418.000],  loss: 10107110.000000, mae: 209.290344, mean_q: 80.145866\n",
      "wrong_move\n",
      "   770/50000: episode: 754, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 9618207.000000, mae: 207.381714, mean_q: 57.525230\n",
      "wrong_move\n",
      "   771/50000: episode: 755, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3153.000 [3153.000, 3153.000],  loss: 10411382.000000, mae: 208.611206, mean_q: 77.587433\n",
      "wrong_move\n",
      "   772/50000: episode: 756, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 815.000 [815.000, 815.000],  loss: 9841164.000000, mae: 208.435791, mean_q: 95.024330\n",
      "wrong_move\n",
      "   773/50000: episode: 757, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1204.000 [1204.000, 1204.000],  loss: 10321505.000000, mae: 214.188126, mean_q: 134.920609\n",
      "wrong_move\n",
      "   774/50000: episode: 758, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2898.000 [2898.000, 2898.000],  loss: 10275128.000000, mae: 210.045242, mean_q: 88.601768\n",
      "wrong_move\n",
      "   775/50000: episode: 759, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3198.000 [3198.000, 3198.000],  loss: 9973832.000000, mae: 214.961868, mean_q: 124.170303\n",
      "wrong_move\n",
      "   776/50000: episode: 760, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3200.000 [3200.000, 3200.000],  loss: 10270742.000000, mae: 208.264603, mean_q: 80.911240\n",
      "wrong_move\n",
      "   777/50000: episode: 761, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3117.000 [3117.000, 3117.000],  loss: 9883128.000000, mae: 209.873123, mean_q: 72.530251\n",
      "wrong_move\n",
      "   778/50000: episode: 762, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1477.000 [1477.000, 1477.000],  loss: 10150120.000000, mae: 212.514404, mean_q: 93.861847\n",
      "wrong_move\n",
      "   779/50000: episode: 763, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3198.000 [3198.000, 3198.000],  loss: 10327372.000000, mae: 213.133942, mean_q: 111.648834\n",
      "wrong_move\n",
      "   780/50000: episode: 764, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1751.000 [1751.000, 1751.000],  loss: 9618990.000000, mae: 209.151794, mean_q: 75.682213\n",
      "wrong_move\n",
      "   781/50000: episode: 765, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2818.000 [2818.000, 2818.000],  loss: 9665346.000000, mae: 213.390411, mean_q: 120.468613\n",
      "wrong_move\n",
      "   782/50000: episode: 766, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3926.000 [3926.000, 3926.000],  loss: 9881726.000000, mae: 212.020142, mean_q: 74.837173\n",
      "wrong_move\n",
      "   783/50000: episode: 767, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2317.000 [2317.000, 2317.000],  loss: 9853347.000000, mae: 210.316238, mean_q: 75.771721\n",
      "wrong_move\n",
      "   784/50000: episode: 768, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 992.000 [992.000, 992.000],  loss: 10813464.000000, mae: 207.487503, mean_q: 47.055748\n",
      "wrong_move\n",
      "   785/50000: episode: 769, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 938.000 [938.000, 938.000],  loss: 10486396.000000, mae: 213.076843, mean_q: 93.310654\n",
      "wrong_move\n",
      "   786/50000: episode: 770, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2820.000 [2820.000, 2820.000],  loss: 10370608.000000, mae: 209.361786, mean_q: 72.317383\n",
      "wrong_move\n",
      "   787/50000: episode: 771, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3733.000 [3733.000, 3733.000],  loss: 10246238.000000, mae: 212.387497, mean_q: 91.234512\n",
      "wrong_move\n",
      "   788/50000: episode: 772, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2896.000 [2896.000, 2896.000],  loss: 9849273.000000, mae: 212.528900, mean_q: 92.624893\n",
      "wrong_move\n",
      "   789/50000: episode: 773, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 988.000 [988.000, 988.000],  loss: 10000534.000000, mae: 216.360992, mean_q: 132.910156\n",
      "wrong_move\n",
      "   790/50000: episode: 774, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2283.000 [2283.000, 2283.000],  loss: 10297944.000000, mae: 211.698608, mean_q: 83.986130\n",
      "wrong_move\n",
      "   791/50000: episode: 775, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 623.000 [623.000, 623.000],  loss: 9375186.000000, mae: 209.069763, mean_q: 44.983932\n",
      "wrong_move\n",
      "   792/50000: episode: 776, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 988.000 [988.000, 988.000],  loss: 10492276.000000, mae: 211.453949, mean_q: 64.254448\n",
      "wrong_move\n",
      "   793/50000: episode: 777, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 475.000 [475.000, 475.000],  loss: 10108321.000000, mae: 209.157532, mean_q: 46.319523\n",
      "wrong_move\n",
      "   794/50000: episode: 778, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1869.000 [1869.000, 1869.000],  loss: 10547202.000000, mae: 209.840118, mean_q: 52.455349\n",
      "wrong_move\n",
      "   795/50000: episode: 779, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4083.000 [4083.000, 4083.000],  loss: 9930380.000000, mae: 214.407196, mean_q: 81.779236\n",
      "wrong_move\n",
      "   796/50000: episode: 780, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 228.000 [228.000, 228.000],  loss: 9174422.000000, mae: 210.191223, mean_q: 73.791443\n",
      "wrong_move\n",
      "   797/50000: episode: 781, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3535.000 [3535.000, 3535.000],  loss: 10207746.000000, mae: 210.297104, mean_q: 60.415787\n",
      "wrong_move\n",
      "   798/50000: episode: 782, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 109.000 [109.000, 109.000],  loss: 9408550.000000, mae: 214.326172, mean_q: 108.817757\n",
      "wrong_move\n",
      "   799/50000: episode: 783, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1630.000 [1630.000, 1630.000],  loss: 10290114.000000, mae: 210.378448, mean_q: 55.024345\n",
      "wrong_move\n",
      "   800/50000: episode: 784, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2376.000 [2376.000, 2376.000],  loss: 10331730.000000, mae: 211.649048, mean_q: 60.091049\n",
      "wrong_move\n",
      "   801/50000: episode: 785, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1554.000 [1554.000, 1554.000],  loss: 8914436.000000, mae: 213.563873, mean_q: 75.761047\n",
      "wrong_move\n",
      "   802/50000: episode: 786, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3553.000 [3553.000, 3553.000],  loss: 9231936.000000, mae: 216.561951, mean_q: 76.427147\n",
      "wrong_move\n",
      "   803/50000: episode: 787, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2852.000 [2852.000, 2852.000],  loss: 10177029.000000, mae: 211.673660, mean_q: 73.045258\n",
      "wrong_move\n",
      "   804/50000: episode: 788, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3113.000 [3113.000, 3113.000],  loss: 9732164.000000, mae: 213.706528, mean_q: 82.723892\n",
      "wrong_move\n",
      "   805/50000: episode: 789, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 529.000 [529.000, 529.000],  loss: 10277436.000000, mae: 212.294830, mean_q: 66.099220\n",
      "wrong_move\n",
      "   806/50000: episode: 790, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3111.000 [3111.000, 3111.000],  loss: 8828928.000000, mae: 216.399292, mean_q: 98.409966\n",
      "wrong_move\n",
      "   807/50000: episode: 791, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: 9765706.000000, mae: 210.308624, mean_q: 47.259583\n",
      "wrong_move\n",
      "   808/50000: episode: 792, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2445.000 [2445.000, 2445.000],  loss: 9954434.000000, mae: 212.260803, mean_q: 73.816170\n",
      "wrong_move\n",
      "   809/50000: episode: 793, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 31.000 [31.000, 31.000],  loss: 8800505.000000, mae: 211.252380, mean_q: 61.567192\n",
      "wrong_move\n",
      "   810/50000: episode: 794, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 719.000 [719.000, 719.000],  loss: 10097567.000000, mae: 213.287430, mean_q: 59.233665\n",
      "wrong_move\n",
      "   811/50000: episode: 795, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1348.000 [1348.000, 1348.000],  loss: 10055761.000000, mae: 215.625122, mean_q: 111.557823\n",
      "wrong_move\n",
      "   812/50000: episode: 796, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2993.000 [2993.000, 2993.000],  loss: 10286769.000000, mae: 212.699112, mean_q: 77.145805\n",
      "wrong_move\n",
      "   813/50000: episode: 797, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 719.000 [719.000, 719.000],  loss: 10596232.000000, mae: 213.315247, mean_q: 71.559410\n",
      "wrong_move\n",
      "   814/50000: episode: 798, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 31.000 [31.000, 31.000],  loss: 9796002.000000, mae: 215.098007, mean_q: 91.675064\n",
      "wrong_move\n",
      "   815/50000: episode: 799, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: 10381990.000000, mae: 219.202820, mean_q: 129.027954\n",
      "wrong_move\n",
      "   816/50000: episode: 800, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 642.000 [642.000, 642.000],  loss: 9740725.000000, mae: 212.844330, mean_q: 66.822601\n",
      "wrong_move\n",
      "   817/50000: episode: 801, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3939.000 [3939.000, 3939.000],  loss: 9637563.000000, mae: 213.521225, mean_q: 59.012676\n",
      "wrong_move\n",
      "   818/50000: episode: 802, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1542.000 [1542.000, 1542.000],  loss: 9657362.000000, mae: 216.434738, mean_q: 116.831444\n",
      "wrong_move\n",
      "   819/50000: episode: 803, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: 10033106.000000, mae: 221.631836, mean_q: 118.858978\n",
      "wrong_move\n",
      "   820/50000: episode: 804, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1265.000 [1265.000, 1265.000],  loss: 9900842.000000, mae: 218.223999, mean_q: 105.272430\n",
      "wrong_move\n",
      "   821/50000: episode: 805, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 327.000 [327.000, 327.000],  loss: 10229038.000000, mae: 215.163055, mean_q: 93.484924\n",
      "wrong_move\n",
      "   822/50000: episode: 806, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3697.000 [3697.000, 3697.000],  loss: 9791862.000000, mae: 212.444077, mean_q: 66.491089\n",
      "wrong_move\n",
      "   823/50000: episode: 807, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2236.000 [2236.000, 2236.000],  loss: 10200155.000000, mae: 215.768860, mean_q: 79.981804\n",
      "wrong_move\n",
      "   824/50000: episode: 808, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3866.000 [3866.000, 3866.000],  loss: 10234142.000000, mae: 217.622955, mean_q: 83.864655\n",
      "wrong_move\n",
      "   825/50000: episode: 809, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3188.000 [3188.000, 3188.000],  loss: 9537819.000000, mae: 219.401276, mean_q: 115.859444\n",
      "wrong_move\n",
      "   826/50000: episode: 810, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: 10578406.000000, mae: 219.515137, mean_q: 103.616226\n",
      "wrong_move\n",
      "   827/50000: episode: 811, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1440.000 [1440.000, 1440.000],  loss: 9502046.000000, mae: 216.542572, mean_q: 76.121429\n",
      "wrong_move\n",
      "   828/50000: episode: 812, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 483.000 [483.000, 483.000],  loss: 9544988.000000, mae: 220.574371, mean_q: 85.249313\n",
      "wrong_move\n",
      "   829/50000: episode: 813, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2987.000 [2987.000, 2987.000],  loss: 9012344.000000, mae: 218.041595, mean_q: 104.218781\n",
      "wrong_move\n",
      "   830/50000: episode: 814, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3595.000 [3595.000, 3595.000],  loss: 10129347.000000, mae: 215.609253, mean_q: 69.789925\n",
      "wrong_move\n",
      "   831/50000: episode: 815, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 108.000 [108.000, 108.000],  loss: 10322174.000000, mae: 214.857498, mean_q: 61.300739\n",
      "wrong_move\n",
      "   832/50000: episode: 816, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2799.000 [2799.000, 2799.000],  loss: 9743114.000000, mae: 216.263184, mean_q: 74.286301\n",
      "wrong_move\n",
      "   833/50000: episode: 817, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2038.000 [2038.000, 2038.000],  loss: 10251592.000000, mae: 222.279190, mean_q: 110.024002\n",
      "wrong_move\n",
      "   834/50000: episode: 818, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3156.000 [3156.000, 3156.000],  loss: 10316034.000000, mae: 220.717743, mean_q: 119.997528\n",
      "wrong_move\n",
      "   835/50000: episode: 819, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1971.000 [1971.000, 1971.000],  loss: 10041812.000000, mae: 217.896469, mean_q: 82.776283\n",
      "wrong_move\n",
      "   836/50000: episode: 820, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 681.000 [681.000, 681.000],  loss: 9904800.000000, mae: 221.060394, mean_q: 99.584869\n",
      "wrong_move\n",
      "   837/50000: episode: 821, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 681.000 [681.000, 681.000],  loss: 10060366.000000, mae: 219.814819, mean_q: 91.529549\n",
      "wrong_move\n",
      "   838/50000: episode: 822, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 697.000 [697.000, 697.000],  loss: 9805519.000000, mae: 227.625443, mean_q: 179.032837\n",
      "wrong_move\n",
      "   839/50000: episode: 823, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3172.000 [3172.000, 3172.000],  loss: 10013372.000000, mae: 215.028503, mean_q: 65.257996\n",
      "wrong_move\n",
      "   840/50000: episode: 824, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1508.000 [1508.000, 1508.000],  loss: 9489859.000000, mae: 217.220383, mean_q: 75.461685\n",
      "wrong_move\n",
      "   841/50000: episode: 825, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3483.000 [3483.000, 3483.000],  loss: 9551524.000000, mae: 216.177155, mean_q: 68.293045\n",
      "wrong_move\n",
      "   842/50000: episode: 826, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2381.000 [2381.000, 2381.000],  loss: 9136642.000000, mae: 217.070953, mean_q: 75.996895\n",
      "wrong_move\n",
      "   843/50000: episode: 827, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 779.000 [779.000, 779.000],  loss: 9831508.000000, mae: 217.994186, mean_q: 88.294815\n",
      "wrong_move\n",
      "   844/50000: episode: 828, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 697.000 [697.000, 697.000],  loss: 9776518.000000, mae: 219.726776, mean_q: 104.722374\n",
      "wrong_move\n",
      "   845/50000: episode: 829, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 29.000 [29.000, 29.000],  loss: 9905922.000000, mae: 218.495056, mean_q: 87.349365\n",
      "wrong_move\n",
      "   846/50000: episode: 830, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 52.000 [52.000, 52.000],  loss: 9999214.000000, mae: 215.895065, mean_q: 48.962399\n",
      "wrong_move\n",
      "   847/50000: episode: 831, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: 10398683.000000, mae: 222.558868, mean_q: 91.457924\n",
      "wrong_move\n",
      "   848/50000: episode: 832, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2694.000 [2694.000, 2694.000],  loss: 9726385.000000, mae: 220.201691, mean_q: 103.010971\n",
      "wrong_move\n",
      "   849/50000: episode: 833, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3210.000 [3210.000, 3210.000],  loss: 10090322.000000, mae: 226.802002, mean_q: 140.341339\n",
      "wrong_move\n",
      "   850/50000: episode: 834, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 697.000 [697.000, 697.000],  loss: 10079840.000000, mae: 227.113495, mean_q: 139.101776\n",
      "wrong_move\n",
      "   851/50000: episode: 835, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2545.000 [2545.000, 2545.000],  loss: 9612720.000000, mae: 223.961975, mean_q: 96.521072\n",
      "wrong_move\n",
      "   852/50000: episode: 836, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3169.000 [3169.000, 3169.000],  loss: 8745242.000000, mae: 223.076569, mean_q: 121.128662\n",
      "wrong_move\n",
      "   853/50000: episode: 837, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1298.000 [1298.000, 1298.000],  loss: 9684540.000000, mae: 220.575806, mean_q: 66.788742\n",
      "wrong_move\n",
      "   854/50000: episode: 838, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3932.000 [3932.000, 3932.000],  loss: 9699728.000000, mae: 221.851227, mean_q: 81.368912\n",
      "wrong_move\n",
      "   855/50000: episode: 839, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1397.000 [1397.000, 1397.000],  loss: 10189923.000000, mae: 223.701065, mean_q: 101.647591\n",
      "wrong_move\n",
      "   856/50000: episode: 840, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2399.000 [2399.000, 2399.000],  loss: 10057654.000000, mae: 220.804810, mean_q: 83.857681\n",
      "wrong_move\n",
      "   857/50000: episode: 841, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2532.000 [2532.000, 2532.000],  loss: 10082730.000000, mae: 217.180237, mean_q: 51.895988\n",
      "wrong_move\n",
      "   858/50000: episode: 842, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 9467065.000000, mae: 222.822205, mean_q: 94.576363\n",
      "wrong_move\n",
      "   859/50000: episode: 843, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 694.000 [694.000, 694.000],  loss: 9904202.000000, mae: 223.576248, mean_q: 113.053658\n",
      "wrong_move\n",
      "   860/50000: episode: 844, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3866.000 [3866.000, 3866.000],  loss: 9658671.000000, mae: 219.174667, mean_q: 95.424576\n",
      "wrong_move\n",
      "   861/50000: episode: 845, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1307.000 [1307.000, 1307.000],  loss: 9810033.000000, mae: 218.255325, mean_q: 53.889492\n",
      "wrong_move\n",
      "   862/50000: episode: 846, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3903.000 [3903.000, 3903.000],  loss: 10801358.000000, mae: 217.667038, mean_q: 49.717003\n",
      "wrong_move\n",
      "   864/50000: episode: 847, duration: 0.162s, episode steps:   2, steps per second:  12, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 3177.000 [3177.000, 3177.000],  loss: 10059965.000000, mae: 221.681213, mean_q: 82.035164\n",
      "wrong_move\n",
      "   865/50000: episode: 848, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2185.000 [2185.000, 2185.000],  loss: 9540388.000000, mae: 220.244659, mean_q: 97.967453\n",
      "wrong_move\n",
      "   866/50000: episode: 849, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 529.000 [529.000, 529.000],  loss: 10292243.000000, mae: 220.603348, mean_q: 95.880737\n",
      "wrong_move\n",
      "   867/50000: episode: 850, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2737.000 [2737.000, 2737.000],  loss: 9669607.000000, mae: 219.278427, mean_q: 72.940865\n",
      "wrong_move\n",
      "   868/50000: episode: 851, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 478.000 [478.000, 478.000],  loss: 10457236.000000, mae: 227.133743, mean_q: 95.971100\n",
      "wrong_move\n",
      "   869/50000: episode: 852, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 314.000 [314.000, 314.000],  loss: 10330159.000000, mae: 218.806580, mean_q: 51.103565\n",
      "wrong_move\n",
      "   870/50000: episode: 853, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 417.000 [417.000, 417.000],  loss: 9751552.000000, mae: 221.325089, mean_q: 69.569168\n",
      "wrong_move\n",
      "   871/50000: episode: 854, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1188.000 [1188.000, 1188.000],  loss: 8815441.000000, mae: 219.878830, mean_q: 63.469765\n",
      "wrong_move\n",
      "   872/50000: episode: 855, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3283.000 [3283.000, 3283.000],  loss: 10080174.000000, mae: 225.635956, mean_q: 101.315460\n",
      "wrong_move\n",
      "   873/50000: episode: 856, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1121.000 [1121.000, 1121.000],  loss: 10246371.000000, mae: 219.388153, mean_q: 51.136757\n",
      "wrong_move\n",
      "   874/50000: episode: 857, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1293.000 [1293.000, 1293.000],  loss: 10075419.000000, mae: 222.406860, mean_q: 80.551132\n",
      "wrong_move\n",
      "   875/50000: episode: 858, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 371.000 [371.000, 371.000],  loss: 9285498.000000, mae: 221.377472, mean_q: 62.026794\n",
      "wrong_move\n",
      "   876/50000: episode: 859, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1293.000 [1293.000, 1293.000],  loss: 10312720.000000, mae: 220.691864, mean_q: 70.479210\n",
      "wrong_move\n",
      "   877/50000: episode: 860, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2215.000 [2215.000, 2215.000],  loss: 10051595.000000, mae: 222.244125, mean_q: 90.030586\n",
      "wrong_move\n",
      "   878/50000: episode: 861, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1586.000 [1586.000, 1586.000],  loss: 9612014.000000, mae: 226.317719, mean_q: 105.032082\n",
      "wrong_move\n",
      "   879/50000: episode: 862, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1641.000 [1641.000, 1641.000],  loss: 9573140.000000, mae: 223.780243, mean_q: 53.106392\n",
      "wrong_move\n",
      "   880/50000: episode: 863, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2902.000 [2902.000, 2902.000],  loss: 9417566.000000, mae: 224.441727, mean_q: 92.248230\n",
      "wrong_move\n",
      "   881/50000: episode: 864, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 879.000 [879.000, 879.000],  loss: 9717857.000000, mae: 227.775513, mean_q: 111.184067\n",
      "wrong_move\n",
      "   882/50000: episode: 865, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2557.000 [2557.000, 2557.000],  loss: 10620054.000000, mae: 223.829041, mean_q: 75.211945\n",
      "wrong_move\n",
      "   883/50000: episode: 866, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3294.000 [3294.000, 3294.000],  loss: 9844652.000000, mae: 226.157013, mean_q: 100.582413\n",
      "wrong_move\n",
      "   884/50000: episode: 867, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2558.000 [2558.000, 2558.000],  loss: 9764915.000000, mae: 222.992523, mean_q: 89.520622\n",
      "wrong_move\n",
      "   885/50000: episode: 868, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 961.000 [961.000, 961.000],  loss: 9432254.000000, mae: 220.715591, mean_q: 45.850311\n",
      "wrong_move\n",
      "   886/50000: episode: 869, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 478.000 [478.000, 478.000],  loss: 10139786.000000, mae: 221.999680, mean_q: 55.186447\n",
      "done, took 56.171 seconds\n"
     ]
    }
   ],
   "source": [
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=500000, window_length=1)\n",
    "policy = EpsGreedyQPolicy(0.3)\n",
    "dqn = DQNAgent(model=model, nb_actions=NB_ACTIONS, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "his = dqn.fit(env, nb_steps=500000, visualize=False, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save dqn\n",
    "# dqn.save_weights('dqn_{}_weights.h5f'.format('chess'), overwrite=True)\n",
    "\n",
    "# # save model\n",
    "# model.save('chess_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# model = keras.models.load_model('chess_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -40.,  -20.,  -30.,  -50., -900.,  -30.,  -20.,  -40.],\n",
       "       [ -10.,  -10.,  -10.,  -10.,  -10.,  -10.,  -10.,  -10.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
       "       [  10.,   10.,   10.,   10.,   10.,   10.,   10.,   10.],\n",
       "       [  40.,   20.,   30.,   50.,  900.,   30.,   20.,   40.]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(env.state.reshape((1, 1) + STATE_SHAPE))\n",
    "idx_sort = np.argsort(pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
