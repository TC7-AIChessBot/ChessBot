{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install keras-rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten,\\\n",
    "     Input, Conv2D, BatchNormalization, MaxPool2D, Reshape, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# import gym_chess\n",
    "\n",
    "import chess\n",
    "# import sys\n",
    "# sys.path.insert(0, '../')\n",
    "# sys.path.insert(0, '../alpha_beta')\n",
    "# from MyChessBoard import MyChessBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6919682660233441301\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SHAPE = (65, )\n",
    "NB_ACTIONS = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessEnv:\n",
    "    '''\n",
    "    state - obser: ndarray - (65,): [:65] is flatten from int_board; [65] is color of bot; 1 is white and -1 is black\n",
    "    step: int. step_range = (0, 4096) , is encoded from square A to square B (64 x 64 val)\n",
    "    reward: int\n",
    "    '''\n",
    "\n",
    "    mapped = {\n",
    "            'P': 10,     # White Pawn\n",
    "            'p': -10,    # Black Pawn\n",
    "            'N': 20,     # White Knight\n",
    "            'n': -20,    # Black Knight\n",
    "            'B': 30,     # White Bishop\n",
    "            'b': -30,    # Black Bishop\n",
    "            'R': 40,     # White Rook\n",
    "            'r': -40,    # Black Rook\n",
    "            'Q': 50,     # White Queen\n",
    "            'q': -50,    # Black Queen\n",
    "            'K': 900,     # White King\n",
    "            'k': -900     # Black King\n",
    "    }\n",
    "    # state_shape = (8, 8)\n",
    "    # nb_actions = 4096\n",
    "    model = None\n",
    "    \n",
    "    def __init__(self, model: Sequential, neg_r_each_step = -1) -> None:\n",
    "        self.env = chess.Board()\n",
    "        self.state = self.reset()\n",
    "        # [-1] = 1 -> white, -1 -> black\n",
    "        self.bot_color = self.env.turn * 2 - 1\n",
    "        self.neg_r_each_step = neg_r_each_step\n",
    "        self.model = model\n",
    "\n",
    "    def is_draw(self):\n",
    "        if self.env.is_stalemate():\n",
    "            print(\"statlemate\")\n",
    "            return True\n",
    "        if self.env.is_fivefold_repetition():\n",
    "            print(\"fivefold repetition\")\n",
    "            return True\n",
    "        if self.env.is_seventyfive_moves():\n",
    "            print(\"75 moves\")\n",
    "            return True\n",
    "        if self.env.is_insufficient_material():\n",
    "            print(\"Insufficient Material\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_checkmate(self):\n",
    "        # If There is checkmate then it will be TRUE else FALSE.It will be a boolean value.\n",
    "        return self.env.is_checkmate()\n",
    "\n",
    "    def convert_board_to_int(self):\n",
    "        epd_string = self.env.epd()\n",
    "        list_int = np.empty((0, ))\n",
    "        for i in epd_string:\n",
    "            if i == \" \":\n",
    "                list_int = list_int.reshape((8, 8))\n",
    "                return list_int\n",
    "            elif i != \"/\":\n",
    "                if i in self.mapped:\n",
    "                    list_int = np.append(list_int, self.mapped[i])\n",
    "                else:\n",
    "                    for counter in range(0, int(i)):\n",
    "                        list_int = np.append(list_int, 0)\n",
    "        list_int = list_int.reshape((8, 8))\n",
    "        return list_int\n",
    "\n",
    "    def get_state(self) -> np.ndarray:\n",
    "        return np.append(self.convert_board_to_int().reshape(64,), self.env.turn * 2 - 1)\n",
    "\n",
    "    def legal_moves(self):\n",
    "        return list(self.env.legal_moves)\n",
    "\n",
    "    def encodeMove(self, move_uci:str):\n",
    "        if len(move_uci) != 4:\n",
    "            raise ValueError()\n",
    "        a, b = chess.parse_square(move_uci[:2]), chess.parse_square(move_uci[2:])\n",
    "        return a * 64 + b\n",
    "\n",
    "    def decodeMove(self, move_int:int):\n",
    "        a, b = move_int//64, move_int%64\n",
    "        # a, b = chess.square_name(a), chess.square_name(b)\n",
    "\n",
    "        move = self.env.find_move(from_square= a,to_square= b)\n",
    "        return move\n",
    "\n",
    "    def render(self):\n",
    "        print(self.env.unicode())\n",
    "\n",
    "    def reset(self):\n",
    "        # random state\n",
    "        redo = True\n",
    "        num_sample_steps = 0\n",
    "        while redo:\n",
    "            redo = False\n",
    "            self.env = chess.Board()\n",
    "            num_sample_steps = np.random.randint(0, 50)\n",
    "            for i in range (num_sample_steps):\n",
    "                lg_move = self.legal_moves()\n",
    "                if len(lg_move) != 0:\n",
    "                    move = np.random.choice(self.legal_moves())\n",
    "                    self.env.push(move)\n",
    "                else:\n",
    "                    redo = True\n",
    "                    break\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action: int):\n",
    "        reward = 0\n",
    "        done = True\n",
    "\n",
    "        try:\n",
    "            # move in legal move\n",
    "            move = self.decodeMove(action)\n",
    "\n",
    "            # neg reward each step\n",
    "            reward = self.neg_r_each_step\n",
    "\n",
    "            # location to_square\n",
    "            to_r, to_c = move.to_square//8, move.to_square%8\n",
    "            reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
    "\n",
    "            # action\n",
    "            self.env.push(move)\n",
    "            self.state = self.get_state()\n",
    "\n",
    "            # check end game\n",
    "            if self.is_checkmate():\n",
    "                reward += self.mapped['K']\n",
    "                done = True\n",
    "            elif self.is_draw():\n",
    "                reward += 300\n",
    "                done = True\n",
    "\n",
    "            # opponent's turn   \n",
    "            else:\n",
    "                done = False\n",
    "                Q_val = self.model.predict(self.state.reshape((1, 1) + STATE_SHAPE)).reshape(-1, )\n",
    "                idx_sorted = np.argsort(Q_val)\n",
    "\n",
    "                for act in idx_sorted:\n",
    "                    try:\n",
    "                        move = self.decodeMove(act)\n",
    "\n",
    "                        # location to_square\n",
    "                        to_r, to_c = move.to_square//8, move.to_square%8\n",
    "                        reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
    "\n",
    "                        # action\n",
    "                        self.env.push(move)\n",
    "                        self.state = self.get_state()\n",
    "\n",
    "                        # check end game\n",
    "                        if self.is_checkmate():\n",
    "                            reward -= self.mapped['K']\n",
    "                            done = True\n",
    "                        elif self.is_draw():\n",
    "                            reward += 300\n",
    "                            done = True\n",
    "                        \n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "        except:\n",
    "            # wrong move\n",
    "            reward = -5000\n",
    "            done = True\n",
    "            print('wrong_move')\n",
    "\n",
    "        return self.state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_11 (Flatten)         (None, 65)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 128)               8448      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 4096)              528384    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 4096)              0         \n",
      "=================================================================\n",
      "Total params: 554,368\n",
      "Trainable params: 553,856\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = Sequential()\n",
    "model.add(Input((1, ) + STATE_SHAPE))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(NB_ACTIONS))\n",
    "model.add(Activation('linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChessEnv(model, neg_r_each_step=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong_move\n",
      "     1/50000: episode: 1, duration: 1.147s, episode steps:   1, steps per second:   1, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1304.000 [1304.000, 1304.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     2/50000: episode: 2, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     3/50000: episode: 3, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     4/50000: episode: 4, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     6/50000: episode: 5, duration: 0.059s, episode steps:   2, steps per second:  34, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 862.000 [593.000, 1131.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     7/50000: episode: 6, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 818.000 [818.000, 818.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     8/50000: episode: 7, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     9/50000: episode: 8, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    10/50000: episode: 9, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 116.000 [116.000, 116.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    11/50000: episode: 10, duration: 3.724s, episode steps:   1, steps per second:   0, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    12/50000: episode: 11, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 11719283.000000, mae: 1.334973, mean_q: 0.831600\n",
      "wrong_move\n",
      "    13/50000: episode: 12, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3831.000 [3831.000, 3831.000],  loss: 11328168.000000, mae: 1.299542, mean_q: 0.847287\n",
      "wrong_move\n",
      "    14/50000: episode: 13, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3488.000 [3488.000, 3488.000],  loss: 11717225.000000, mae: 1.335592, mean_q: 0.850023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/home/kienanh/.local/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/home/kienanh/.local/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/home/kienanh/.local/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong_move\n",
      "    15/50000: episode: 14, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12498538.000000, mae: 1.400364, mean_q: 0.822411\n",
      "wrong_move\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/home/kienanh/.local/lib/python3.9/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    16/50000: episode: 15, duration: 0.158s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12107253.000000, mae: 1.370684, mean_q: 0.883663\n",
      "wrong_move\n",
      "    17/50000: episode: 16, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 11325270.000000, mae: 1.290806, mean_q: 0.849983\n",
      "wrong_move\n",
      "    18/50000: episode: 17, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 11715925.000000, mae: 1.329496, mean_q: 0.881052\n",
      "wrong_move\n",
      "    19/50000: episode: 18, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3110.000 [3110.000, 3110.000],  loss: 12106858.000000, mae: 1.369627, mean_q: 0.845237\n",
      "wrong_move\n",
      "    20/50000: episode: 19, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12106228.000000, mae: 1.371561, mean_q: 0.911488\n",
      "wrong_move\n",
      "    21/50000: episode: 20, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 11714420.000000, mae: 1.332497, mean_q: 0.925232\n",
      "wrong_move\n",
      "    22/50000: episode: 21, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 11715897.000000, mae: 1.332643, mean_q: 0.955096\n",
      "wrong_move\n",
      "    23/50000: episode: 22, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3488.000 [3488.000, 3488.000],  loss: 11324788.000000, mae: 1.294543, mean_q: 0.971787\n",
      "wrong_move\n",
      "    24/50000: episode: 23, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 10933751.000000, mae: 1.254725, mean_q: 1.044128\n",
      "wrong_move\n",
      "    25/50000: episode: 24, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12105380.000000, mae: 1.361518, mean_q: 1.046531\n",
      "wrong_move\n",
      "    26/50000: episode: 25, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 818.000 [818.000, 818.000],  loss: 12493551.000000, mae: 1.402462, mean_q: 0.996628\n",
      "wrong_move\n",
      "    27/50000: episode: 26, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 818.000 [818.000, 818.000],  loss: 12103591.000000, mae: 1.364663, mean_q: 1.086217\n",
      "wrong_move\n",
      "    28/50000: episode: 27, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 818.000 [818.000, 818.000],  loss: 12493830.000000, mae: 1.401799, mean_q: 1.136532\n",
      "wrong_move\n",
      "    29/50000: episode: 28, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1968.000 [1968.000, 1968.000],  loss: 12103541.000000, mae: 1.368737, mean_q: 1.079772\n",
      "wrong_move\n",
      "    30/50000: episode: 29, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3488.000 [3488.000, 3488.000],  loss: 12105164.000000, mae: 1.367000, mean_q: 1.125339\n",
      "wrong_move\n",
      "    31/50000: episode: 30, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12495044.000000, mae: 1.392244, mean_q: 1.157037\n",
      "wrong_move\n",
      "    32/50000: episode: 31, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3831.000 [3831.000, 3831.000],  loss: 11710164.000000, mae: 1.331236, mean_q: 1.232738\n",
      "wrong_move\n",
      "    33/50000: episode: 32, duration: 0.163s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12104131.000000, mae: 1.357801, mean_q: 1.273989\n",
      "wrong_move\n",
      "    34/50000: episode: 33, duration: 0.149s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12495055.000000, mae: 1.406757, mean_q: 1.208999\n",
      "wrong_move\n",
      "    35/50000: episode: 34, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12492134.000000, mae: 1.413487, mean_q: 1.229410\n",
      "wrong_move\n",
      "    36/50000: episode: 35, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 10929048.000000, mae: 1.256920, mean_q: 1.368922\n",
      "wrong_move\n",
      "    37/50000: episode: 36, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2173.000 [2173.000, 2173.000],  loss: 12495857.000000, mae: 1.402860, mean_q: 1.299496\n",
      "wrong_move\n",
      "    38/50000: episode: 37, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 116.000 [116.000, 116.000],  loss: 11713134.000000, mae: 1.330119, mean_q: 1.361137\n",
      "wrong_move\n",
      "    39/50000: episode: 38, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 12100953.000000, mae: 1.366676, mean_q: 1.438614\n",
      "wrong_move\n",
      "    40/50000: episode: 39, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2541.000 [2541.000, 2541.000],  loss: 12097914.000000, mae: 1.367081, mean_q: 1.567021\n",
      "wrong_move\n",
      "    41/50000: episode: 40, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 12103739.000000, mae: 1.373078, mean_q: 1.580800\n",
      "wrong_move\n",
      "    42/50000: episode: 41, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 116.000 [116.000, 116.000],  loss: 12495107.000000, mae: 1.400416, mean_q: 1.492867\n",
      "wrong_move\n",
      "    43/50000: episode: 42, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 116.000 [116.000, 116.000],  loss: 12490818.000000, mae: 1.409837, mean_q: 1.384876\n",
      "wrong_move\n",
      "    44/50000: episode: 43, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 12100188.000000, mae: 1.364249, mean_q: 1.475643\n",
      "wrong_move\n",
      "    45/50000: episode: 44, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 116.000 [116.000, 116.000],  loss: 12490920.000000, mae: 1.405355, mean_q: 1.609473\n",
      "wrong_move\n",
      "    46/50000: episode: 45, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 116.000 [116.000, 116.000],  loss: 12098952.000000, mae: 1.371174, mean_q: 1.728513\n",
      "wrong_move\n",
      "    47/50000: episode: 46, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12494186.000000, mae: 1.411064, mean_q: 1.692446\n",
      "wrong_move\n",
      "    48/50000: episode: 47, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12490542.000000, mae: 1.417830, mean_q: 1.689649\n",
      "wrong_move\n",
      "    49/50000: episode: 48, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3831.000 [3831.000, 3831.000],  loss: 11710330.000000, mae: 1.337126, mean_q: 1.805392\n",
      "wrong_move\n",
      "    50/50000: episode: 49, duration: 0.178s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 11711812.000000, mae: 1.342879, mean_q: 1.926914\n",
      "wrong_move\n",
      "    51/50000: episode: 50, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3831.000 [3831.000, 3831.000],  loss: 12101177.000000, mae: 1.379597, mean_q: 2.083442\n",
      "wrong_move\n",
      "    52/50000: episode: 51, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12488772.000000, mae: 1.412680, mean_q: 1.873182\n",
      "wrong_move\n",
      "    53/50000: episode: 52, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 11705801.000000, mae: 1.335141, mean_q: 2.011177\n",
      "wrong_move\n",
      "    54/50000: episode: 53, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12489306.000000, mae: 1.416656, mean_q: 1.796368\n",
      "wrong_move\n",
      "    55/50000: episode: 54, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3831.000 [3831.000, 3831.000],  loss: 12098123.000000, mae: 1.369783, mean_q: 1.982074\n",
      "wrong_move\n",
      "    56/50000: episode: 55, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3581.000 [3581.000, 3581.000],  loss: 11703318.000000, mae: 1.336488, mean_q: 1.965783\n",
      "wrong_move\n",
      "    57/50000: episode: 56, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 12095447.000000, mae: 1.375843, mean_q: 2.127439\n",
      "wrong_move\n",
      "    58/50000: episode: 57, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 12485116.000000, mae: 1.412600, mean_q: 2.335620\n",
      "wrong_move\n",
      "    59/50000: episode: 58, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12098208.000000, mae: 1.381377, mean_q: 2.001457\n",
      "wrong_move\n",
      "    60/50000: episode: 59, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12487469.000000, mae: 1.416769, mean_q: 2.116588\n",
      "wrong_move\n",
      "    61/50000: episode: 60, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 12487055.000000, mae: 1.416905, mean_q: 2.105362\n",
      "wrong_move\n",
      "    62/50000: episode: 61, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12483554.000000, mae: 1.411635, mean_q: 2.159855\n",
      "wrong_move\n",
      "    63/50000: episode: 62, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12488496.000000, mae: 1.421713, mean_q: 2.317328\n",
      "wrong_move\n",
      "    64/50000: episode: 63, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 12486164.000000, mae: 1.420867, mean_q: 2.471165\n",
      "wrong_move\n",
      "    65/50000: episode: 64, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12094400.000000, mae: 1.371623, mean_q: 2.409573\n",
      "wrong_move\n",
      "    66/50000: episode: 65, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 12092165.000000, mae: 1.389233, mean_q: 2.534265\n",
      "wrong_move\n",
      "    67/50000: episode: 66, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 12484188.000000, mae: 1.420463, mean_q: 2.476756\n",
      "wrong_move\n",
      "    68/50000: episode: 67, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 12482022.000000, mae: 1.426539, mean_q: 2.600068\n",
      "wrong_move\n",
      "    69/50000: episode: 68, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 939.000 [939.000, 939.000],  loss: 12095830.000000, mae: 1.386564, mean_q: 2.533064\n",
      "wrong_move\n",
      "    70/50000: episode: 69, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12488264.000000, mae: 1.425520, mean_q: 2.557065\n",
      "wrong_move\n",
      "    71/50000: episode: 70, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12092430.000000, mae: 1.382122, mean_q: 2.752959\n",
      "wrong_move\n",
      "    72/50000: episode: 71, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1327.000 [1327.000, 1327.000],  loss: 12484456.000000, mae: 1.421905, mean_q: 2.555745\n",
      "wrong_move\n",
      "    73/50000: episode: 72, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1327.000 [1327.000, 1327.000],  loss: 12483852.000000, mae: 1.422624, mean_q: 2.553630\n",
      "wrong_move\n",
      "    74/50000: episode: 73, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2632.000 [2632.000, 2632.000],  loss: 12482349.000000, mae: 1.430141, mean_q: 2.723729\n",
      "wrong_move\n",
      "    75/50000: episode: 74, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12090870.000000, mae: 1.391225, mean_q: 2.866905\n",
      "wrong_move\n",
      "    76/50000: episode: 75, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12486290.000000, mae: 1.424744, mean_q: 2.483092\n",
      "wrong_move\n",
      "    77/50000: episode: 76, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12091194.000000, mae: 1.388371, mean_q: 2.811405\n",
      "wrong_move\n",
      "    78/50000: episode: 77, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 116.000 [116.000, 116.000],  loss: 12480104.000000, mae: 1.430876, mean_q: 2.694644\n",
      "wrong_move\n",
      "    79/50000: episode: 78, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 116.000 [116.000, 116.000],  loss: 12094260.000000, mae: 1.398295, mean_q: 3.095163\n",
      "wrong_move\n",
      "    80/50000: episode: 79, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 12480264.000000, mae: 1.428879, mean_q: 2.829114\n",
      "wrong_move\n",
      "    81/50000: episode: 80, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12483850.000000, mae: 1.432907, mean_q: 2.981098\n",
      "wrong_move\n",
      "    82/50000: episode: 81, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 12475924.000000, mae: 1.431946, mean_q: 2.951693\n",
      "wrong_move\n",
      "    83/50000: episode: 82, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12481046.000000, mae: 1.442122, mean_q: 3.024541\n",
      "wrong_move\n",
      "    84/50000: episode: 83, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12475361.000000, mae: 1.434675, mean_q: 2.959285\n",
      "wrong_move\n",
      "    85/50000: episode: 84, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12474838.000000, mae: 1.436347, mean_q: 3.393018\n",
      "wrong_move\n",
      "    86/50000: episode: 85, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12475387.000000, mae: 1.435362, mean_q: 3.444628\n",
      "wrong_move\n",
      "    87/50000: episode: 86, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 11690074.000000, mae: 1.357184, mean_q: 3.551768\n",
      "wrong_move\n",
      "    88/50000: episode: 87, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12480172.000000, mae: 1.438537, mean_q: 3.538699\n",
      "wrong_move\n",
      "    89/50000: episode: 88, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 818.000 [818.000, 818.000],  loss: 12480126.000000, mae: 1.442891, mean_q: 3.504522\n",
      "wrong_move\n",
      "    90/50000: episode: 89, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12473613.000000, mae: 1.441610, mean_q: 3.554063\n",
      "wrong_move\n",
      "    91/50000: episode: 90, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12470262.000000, mae: 1.440313, mean_q: 3.371759\n",
      "wrong_move\n",
      "    92/50000: episode: 91, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 11695935.000000, mae: 1.359565, mean_q: 3.274715\n",
      "wrong_move\n",
      "    93/50000: episode: 92, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12081392.000000, mae: 1.405300, mean_q: 3.617723\n",
      "wrong_move\n",
      "    94/50000: episode: 93, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12466576.000000, mae: 1.447786, mean_q: 3.865404\n",
      "wrong_move\n",
      "    95/50000: episode: 94, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12075735.000000, mae: 1.407599, mean_q: 3.800267\n",
      "wrong_move\n",
      "    96/50000: episode: 95, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12470480.000000, mae: 1.446814, mean_q: 3.733740\n",
      "wrong_move\n",
      "    97/50000: episode: 96, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12081460.000000, mae: 1.407252, mean_q: 4.054996\n",
      "wrong_move\n",
      "    98/50000: episode: 97, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12471058.000000, mae: 1.438796, mean_q: 3.971258\n",
      "wrong_move\n",
      "    99/50000: episode: 98, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12467904.000000, mae: 1.440980, mean_q: 3.727648\n",
      "wrong_move\n",
      "   100/50000: episode: 99, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12473983.000000, mae: 1.453249, mean_q: 4.218616\n",
      "wrong_move\n",
      "   101/50000: episode: 100, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 558.000 [558.000, 558.000],  loss: 12462806.000000, mae: 1.449888, mean_q: 4.029790\n",
      "wrong_move\n",
      "   102/50000: episode: 101, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12463940.000000, mae: 1.446179, mean_q: 3.619839\n",
      "wrong_move\n",
      "   103/50000: episode: 102, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 818.000 [818.000, 818.000],  loss: 12468507.000000, mae: 1.449915, mean_q: 4.490971\n",
      "wrong_move\n",
      "   104/50000: episode: 103, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12078465.000000, mae: 1.417853, mean_q: 4.633979\n",
      "wrong_move\n",
      "   105/50000: episode: 104, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1648.000 [1648.000, 1648.000],  loss: 12477326.000000, mae: 1.451305, mean_q: 3.959520\n",
      "wrong_move\n",
      "   106/50000: episode: 105, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12076565.000000, mae: 1.411436, mean_q: 4.181035\n",
      "wrong_move\n",
      "   107/50000: episode: 106, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 116.000 [116.000, 116.000],  loss: 12083910.000000, mae: 1.416908, mean_q: 4.673115\n",
      "wrong_move\n",
      "   108/50000: episode: 107, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3436.000 [3436.000, 3436.000],  loss: 12461697.000000, mae: 1.449343, mean_q: 4.213596\n",
      "wrong_move\n",
      "   109/50000: episode: 108, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12464086.000000, mae: 1.454491, mean_q: 4.728661\n",
      "wrong_move\n",
      "   110/50000: episode: 109, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1836.000 [1836.000, 1836.000],  loss: 12065434.000000, mae: 1.416966, mean_q: 5.615750\n",
      "wrong_move\n",
      "   111/50000: episode: 110, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3110.000 [3110.000, 3110.000],  loss: 12072496.000000, mae: 1.411785, mean_q: 4.180501\n",
      "wrong_move\n",
      "   112/50000: episode: 111, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2350.000 [2350.000, 2350.000],  loss: 12468919.000000, mae: 1.451628, mean_q: 4.101693\n",
      "wrong_move\n",
      "   113/50000: episode: 112, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3581.000 [3581.000, 3581.000],  loss: 11672216.000000, mae: 1.378645, mean_q: 4.969339\n",
      "wrong_move\n",
      "   114/50000: episode: 113, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 468.000 [468.000, 468.000],  loss: 12458500.000000, mae: 1.456594, mean_q: 4.009551\n",
      "wrong_move\n",
      "   115/50000: episode: 114, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 12462968.000000, mae: 1.460500, mean_q: 4.377139\n",
      "wrong_move\n",
      "   116/50000: episode: 115, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 818.000 [818.000, 818.000],  loss: 12457514.000000, mae: 1.464995, mean_q: 4.802781\n",
      "wrong_move\n",
      "   117/50000: episode: 116, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3581.000 [3581.000, 3581.000],  loss: 12458230.000000, mae: 1.459796, mean_q: 5.131408\n",
      "wrong_move\n",
      "   118/50000: episode: 117, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12062850.000000, mae: 1.421764, mean_q: 5.299060\n",
      "wrong_move\n",
      "   119/50000: episode: 118, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12459366.000000, mae: 1.462550, mean_q: 4.571499\n",
      "wrong_move\n",
      "   120/50000: episode: 119, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12447056.000000, mae: 1.460852, mean_q: 5.142030\n",
      "wrong_move\n",
      "   121/50000: episode: 120, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3110.000 [3110.000, 3110.000],  loss: 12052268.000000, mae: 1.427943, mean_q: 5.293922\n",
      "wrong_move\n",
      "   122/50000: episode: 121, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3581.000 [3581.000, 3581.000],  loss: 12458073.000000, mae: 1.469218, mean_q: 5.509250\n",
      "wrong_move\n",
      "   123/50000: episode: 122, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12447376.000000, mae: 1.455564, mean_q: 4.814613\n",
      "wrong_move\n",
      "   124/50000: episode: 123, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12443880.000000, mae: 1.467132, mean_q: 5.573763\n",
      "wrong_move\n",
      "   125/50000: episode: 124, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12069075.000000, mae: 1.428063, mean_q: 5.823221\n",
      "wrong_move\n",
      "   126/50000: episode: 125, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3110.000 [3110.000, 3110.000],  loss: 12463704.000000, mae: 1.465834, mean_q: 5.143826\n",
      "wrong_move\n",
      "   127/50000: episode: 126, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3581.000 [3581.000, 3581.000],  loss: 12459484.000000, mae: 1.473548, mean_q: 5.753264\n",
      "wrong_move\n",
      "   128/50000: episode: 127, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1225.000 [1225.000, 1225.000],  loss: 12452597.000000, mae: 1.471372, mean_q: 5.302030\n",
      "wrong_move\n",
      "   129/50000: episode: 128, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1225.000 [1225.000, 1225.000],  loss: 12454875.000000, mae: 1.471932, mean_q: 5.738200\n",
      "wrong_move\n",
      "   130/50000: episode: 129, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12449842.000000, mae: 1.477268, mean_q: 5.515228\n",
      "wrong_move\n",
      "   131/50000: episode: 130, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2512.000 [2512.000, 2512.000],  loss: 12452949.000000, mae: 1.472030, mean_q: 5.671218\n",
      "wrong_move\n",
      "   132/50000: episode: 131, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12062766.000000, mae: 1.439823, mean_q: 5.994187\n",
      "wrong_move\n",
      "   133/50000: episode: 132, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3110.000 [3110.000, 3110.000],  loss: 12068712.000000, mae: 1.438748, mean_q: 6.001557\n",
      "wrong_move\n",
      "   134/50000: episode: 133, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 11669210.000000, mae: 1.398018, mean_q: 6.325736\n",
      "wrong_move\n",
      "   135/50000: episode: 134, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2720.000 [2720.000, 2720.000],  loss: 12054573.000000, mae: 1.436157, mean_q: 6.378442\n",
      "wrong_move\n",
      "   136/50000: episode: 135, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 12061644.000000, mae: 1.442812, mean_q: 6.167582\n",
      "wrong_move\n",
      "   137/50000: episode: 136, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3110.000 [3110.000, 3110.000],  loss: 12445201.000000, mae: 1.479677, mean_q: 5.954806\n",
      "wrong_move\n",
      "   138/50000: episode: 137, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3110.000 [3110.000, 3110.000],  loss: 12062532.000000, mae: 1.439451, mean_q: 6.276169\n",
      "wrong_move\n",
      "   139/50000: episode: 138, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12448332.000000, mae: 1.481510, mean_q: 5.571278\n",
      "wrong_move\n",
      "   140/50000: episode: 139, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 116.000 [116.000, 116.000],  loss: 12426966.000000, mae: 1.482143, mean_q: 6.543264\n",
      "wrong_move\n",
      "   141/50000: episode: 140, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12050325.000000, mae: 1.450692, mean_q: 6.867008\n",
      "wrong_move\n",
      "   142/50000: episode: 141, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4019.000 [4019.000, 4019.000],  loss: 12452923.000000, mae: 1.489372, mean_q: 5.829311\n",
      "wrong_move\n",
      "   143/50000: episode: 142, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12429818.000000, mae: 1.488061, mean_q: 6.627036\n",
      "wrong_move\n",
      "   144/50000: episode: 143, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 116.000 [116.000, 116.000],  loss: 12438367.000000, mae: 1.491520, mean_q: 6.863922\n",
      "wrong_move\n",
      "   145/50000: episode: 144, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 12440823.000000, mae: 1.484309, mean_q: 5.721604\n",
      "wrong_move\n",
      "   146/50000: episode: 145, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12428798.000000, mae: 1.493023, mean_q: 6.681717\n",
      "wrong_move\n",
      "   147/50000: episode: 146, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12044912.000000, mae: 1.451960, mean_q: 6.831388\n",
      "wrong_move\n",
      "   148/50000: episode: 147, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12430271.000000, mae: 1.487679, mean_q: 5.343951\n",
      "wrong_move\n",
      "   149/50000: episode: 148, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12428409.000000, mae: 1.494108, mean_q: 6.659928\n",
      "wrong_move\n",
      "   150/50000: episode: 149, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12062022.000000, mae: 1.460959, mean_q: 7.837450\n",
      "wrong_move\n",
      "   151/50000: episode: 150, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 986.000 [986.000, 986.000],  loss: 12054342.000000, mae: 1.460459, mean_q: 6.791122\n",
      "wrong_move\n",
      "   152/50000: episode: 151, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 12049533.000000, mae: 1.454509, mean_q: 8.069228\n",
      "wrong_move\n",
      "   153/50000: episode: 152, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 12440063.000000, mae: 1.497317, mean_q: 7.637911\n",
      "wrong_move\n",
      "   154/50000: episode: 153, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1547.000 [1547.000, 1547.000],  loss: 12442016.000000, mae: 1.501266, mean_q: 7.571700\n",
      "wrong_move\n",
      "   155/50000: episode: 154, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 12432012.000000, mae: 1.490978, mean_q: 7.414485\n",
      "wrong_move\n",
      "   156/50000: episode: 155, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 12431987.000000, mae: 1.499230, mean_q: 6.484217\n",
      "wrong_move\n",
      "   157/50000: episode: 156, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 12430308.000000, mae: 1.502678, mean_q: 7.204548\n",
      "wrong_move\n",
      "   158/50000: episode: 157, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 914.000 [914.000, 914.000],  loss: 12423148.000000, mae: 1.505838, mean_q: 8.583746\n",
      "wrong_move\n",
      "   159/50000: episode: 158, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 914.000 [914.000, 914.000],  loss: 12434934.000000, mae: 1.504735, mean_q: 8.015167\n",
      "wrong_move\n",
      "   160/50000: episode: 159, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1048.000 [1048.000, 1048.000],  loss: 12040918.000000, mae: 1.468454, mean_q: 7.360831\n",
      "wrong_move\n",
      "   161/50000: episode: 160, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3059.000 [3059.000, 3059.000],  loss: 12027294.000000, mae: 1.466126, mean_q: 7.753664\n",
      "wrong_move\n",
      "   162/50000: episode: 161, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2632.000 [2632.000, 2632.000],  loss: 12409690.000000, mae: 1.508834, mean_q: 8.889399\n",
      "wrong_move\n",
      "   163/50000: episode: 162, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12415458.000000, mae: 1.502624, mean_q: 7.659101\n",
      "wrong_move\n",
      "   164/50000: episode: 163, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12406047.000000, mae: 1.511123, mean_q: 8.108054\n",
      "wrong_move\n",
      "   165/50000: episode: 164, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 693.000 [693.000, 693.000],  loss: 11649096.000000, mae: 1.439564, mean_q: 9.561341\n",
      "wrong_move\n",
      "   166/50000: episode: 165, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2632.000 [2632.000, 2632.000],  loss: 12398867.000000, mae: 1.504787, mean_q: 7.566534\n",
      "wrong_move\n",
      "   167/50000: episode: 166, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12443629.000000, mae: 1.512148, mean_q: 7.274639\n",
      "wrong_move\n",
      "   169/50000: episode: 167, duration: 0.111s, episode steps:   2, steps per second:  18, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 3203.000 [3172.000, 3234.000],  loss: 12232455.000000, mae: 1.495596, mean_q: 7.874572\n",
      "wrong_move\n",
      "   171/50000: episode: 168, duration: 0.252s, episode steps:   2, steps per second:   8, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 1719.000 [375.000, 3063.000],  loss: 12411084.000000, mae: 1.516299, mean_q: 8.633635\n",
      "wrong_move\n",
      "   172/50000: episode: 169, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12395365.000000, mae: 1.520734, mean_q: 9.054406\n",
      "wrong_move\n",
      "   173/50000: episode: 170, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2839.000 [2839.000, 2839.000],  loss: 12400348.000000, mae: 1.520132, mean_q: 8.804884\n",
      "wrong_move\n",
      "   174/50000: episode: 171, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 282.000 [282.000, 282.000],  loss: 12026230.000000, mae: 1.480997, mean_q: 7.817226\n",
      "wrong_move\n",
      "   175/50000: episode: 172, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 27.000 [27.000, 27.000],  loss: 11214715.000000, mae: 1.406943, mean_q: 8.237614\n",
      "wrong_move\n",
      "   176/50000: episode: 173, duration: 0.148s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12011338.000000, mae: 1.484236, mean_q: 8.241538\n",
      "wrong_move\n",
      "   177/50000: episode: 174, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3079.000 [3079.000, 3079.000],  loss: 12391352.000000, mae: 1.513621, mean_q: 7.836293\n",
      "wrong_move\n",
      "   178/50000: episode: 175, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 12411714.000000, mae: 1.523470, mean_q: 7.975477\n",
      "wrong_move\n",
      "   179/50000: episode: 176, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3073.000 [3073.000, 3073.000],  loss: 12010784.000000, mae: 1.488502, mean_q: 9.402110\n",
      "wrong_move\n",
      "   180/50000: episode: 177, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 851.000 [851.000, 851.000],  loss: 12008120.000000, mae: 1.488240, mean_q: 9.189466\n",
      "wrong_move\n",
      "   181/50000: episode: 178, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 11635895.000000, mae: 1.453822, mean_q: 8.464294\n",
      "wrong_move\n",
      "   182/50000: episode: 179, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 593.000 [593.000, 593.000],  loss: 12393419.000000, mae: 1.527368, mean_q: 8.599886\n",
      "wrong_move\n",
      "   183/50000: episode: 180, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3063.000 [3063.000, 3063.000],  loss: 12394054.000000, mae: 1.535098, mean_q: 10.085161\n",
      "wrong_move\n",
      "   184/50000: episode: 181, duration: 0.147s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12013685.000000, mae: 1.499623, mean_q: 10.327463\n",
      "wrong_move\n",
      "   185/50000: episode: 182, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12412675.000000, mae: 1.537079, mean_q: 9.466166\n",
      "wrong_move\n",
      "   186/50000: episode: 183, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 818.000 [818.000, 818.000],  loss: 12388980.000000, mae: 1.540536, mean_q: 10.548999\n",
      "wrong_move\n",
      "   187/50000: episode: 184, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 11629683.000000, mae: 1.460802, mean_q: 8.829695\n",
      "wrong_move\n",
      "   188/50000: episode: 185, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12007451.000000, mae: 1.496427, mean_q: 9.805083\n",
      "wrong_move\n",
      "   189/50000: episode: 186, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 12402639.000000, mae: 1.538446, mean_q: 8.106813\n",
      "wrong_move\n",
      "   190/50000: episode: 187, duration: 0.235s, episode steps:   1, steps per second:   4, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12391470.000000, mae: 1.544305, mean_q: 11.255398\n",
      "wrong_move\n",
      "   191/50000: episode: 188, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 12386252.000000, mae: 1.544622, mean_q: 10.112970\n",
      "wrong_move\n",
      "   192/50000: episode: 189, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3213.000 [3213.000, 3213.000],  loss: 11624183.000000, mae: 1.465096, mean_q: 9.473242\n",
      "wrong_move\n",
      "   193/50000: episode: 190, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 11611913.000000, mae: 1.473254, mean_q: 10.908350\n",
      "wrong_move\n",
      "   194/50000: episode: 191, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3526.000 [3526.000, 3526.000],  loss: 12028291.000000, mae: 1.509989, mean_q: 9.749935\n",
      "wrong_move\n",
      "   195/50000: episode: 192, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 320.000 [320.000, 320.000],  loss: 12398792.000000, mae: 1.551564, mean_q: 10.445580\n",
      "wrong_move\n",
      "   196/50000: episode: 193, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1836.000 [1836.000, 1836.000],  loss: 12384233.000000, mae: 1.545791, mean_q: 9.132298\n",
      "wrong_move\n",
      "   197/50000: episode: 194, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1273.000 [1273.000, 1273.000],  loss: 12003128.000000, mae: 1.512267, mean_q: 8.373523\n",
      "wrong_move\n",
      "   198/50000: episode: 195, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2582.000 [2582.000, 2582.000],  loss: 12394139.000000, mae: 1.548862, mean_q: 9.933352\n",
      "wrong_move\n",
      "   199/50000: episode: 196, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12382742.000000, mae: 1.559596, mean_q: 11.116290\n",
      "wrong_move\n",
      "   200/50000: episode: 197, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 12010202.000000, mae: 1.519128, mean_q: 10.644242\n",
      "wrong_move\n",
      "   201/50000: episode: 198, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12373982.000000, mae: 1.554571, mean_q: 10.923001\n",
      "wrong_move\n",
      "   202/50000: episode: 199, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3809.000 [3809.000, 3809.000],  loss: 12394054.000000, mae: 1.554915, mean_q: 10.503810\n",
      "wrong_move\n",
      "   203/50000: episode: 200, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3261.000 [3261.000, 3261.000],  loss: 12373198.000000, mae: 1.550292, mean_q: 9.327422\n",
      "wrong_move\n",
      "   204/50000: episode: 201, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1953.000 [1953.000, 1953.000],  loss: 12006208.000000, mae: 1.522892, mean_q: 9.598904\n",
      "wrong_move\n",
      "   205/50000: episode: 202, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 12396646.000000, mae: 1.564354, mean_q: 11.481302\n",
      "wrong_move\n",
      "   206/50000: episode: 203, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 11985899.000000, mae: 1.525185, mean_q: 10.398554\n",
      "wrong_move\n",
      "   207/50000: episode: 204, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3973.000 [3973.000, 3973.000],  loss: 12400634.000000, mae: 1.560995, mean_q: 10.136774\n",
      "wrong_move\n",
      "   208/50000: episode: 205, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 660.000 [660.000, 660.000],  loss: 11962293.000000, mae: 1.529053, mean_q: 13.347548\n",
      "wrong_move\n",
      "   209/50000: episode: 206, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1109.000 [1109.000, 1109.000],  loss: 12384167.000000, mae: 1.576999, mean_q: 12.381803\n",
      "wrong_move\n",
      "   210/50000: episode: 207, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4020.000 [4020.000, 4020.000],  loss: 12364062.000000, mae: 1.569823, mean_q: 10.672605\n",
      "wrong_move\n",
      "   212/50000: episode: 208, duration: 0.108s, episode steps:   2, steps per second:  18, episode reward: -4101.000, mean reward: -2050.500 [-5000.000, 899.000], mean action: 3827.500 [3708.000, 3947.000],  loss: 12357202.000000, mae: 1.569964, mean_q: 11.830449\n",
      "wrong_move\n",
      "   213/50000: episode: 209, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12358180.000000, mae: 1.565888, mean_q: 9.836622\n",
      "wrong_move\n",
      "   214/50000: episode: 210, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 11980252.000000, mae: 1.540891, mean_q: 11.859032\n",
      "wrong_move\n",
      "   215/50000: episode: 211, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12029885.000000, mae: 1.545366, mean_q: 12.104942\n",
      "wrong_move\n",
      "   216/50000: episode: 212, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 11569572.000000, mae: 1.506315, mean_q: 10.956869\n",
      "wrong_move\n",
      "   217/50000: episode: 213, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12366232.000000, mae: 1.578250, mean_q: 11.423893\n",
      "wrong_move\n",
      "   218/50000: episode: 214, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3947.000 [3947.000, 3947.000],  loss: 11216355.000000, mae: 1.472494, mean_q: 10.946484\n",
      "wrong_move\n",
      "   219/50000: episode: 215, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1498.000 [1498.000, 1498.000],  loss: 12023212.000000, mae: 1.551998, mean_q: 10.584842\n",
      "wrong_move\n",
      "   220/50000: episode: 216, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1836.000 [1836.000, 1836.000],  loss: 12371183.000000, mae: 1.585540, mean_q: 11.496744\n",
      "wrong_move\n",
      "   221/50000: episode: 217, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 11957392.000000, mae: 1.548015, mean_q: 13.304191\n",
      "wrong_move\n",
      "   222/50000: episode: 218, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 11939377.000000, mae: 1.548920, mean_q: 11.571789\n",
      "wrong_move\n",
      "   223/50000: episode: 219, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3947.000 [3947.000, 3947.000],  loss: 12007414.000000, mae: 1.551590, mean_q: 9.917388\n",
      "wrong_move\n",
      "   224/50000: episode: 220, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12304850.000000, mae: 1.590136, mean_q: 11.606608\n",
      "wrong_move\n",
      "   225/50000: episode: 221, duration: 0.154s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1836.000 [1836.000, 1836.000],  loss: 12393126.000000, mae: 1.600580, mean_q: 13.120739\n",
      "wrong_move\n",
      "   226/50000: episode: 222, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 12338041.000000, mae: 1.595532, mean_q: 10.381961\n",
      "wrong_move\n",
      "   227/50000: episode: 223, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 12347024.000000, mae: 1.593351, mean_q: 11.243422\n",
      "wrong_move\n",
      "   228/50000: episode: 224, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3947.000 [3947.000, 3947.000],  loss: 12370915.000000, mae: 1.599317, mean_q: 12.320295\n",
      "wrong_move\n",
      "   229/50000: episode: 225, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3947.000 [3947.000, 3947.000],  loss: 11953431.000000, mae: 1.563163, mean_q: 11.244070\n",
      "wrong_move\n",
      "   230/50000: episode: 226, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12379134.000000, mae: 1.609985, mean_q: 12.409761\n",
      "wrong_move\n",
      "   231/50000: episode: 227, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 11610579.000000, mae: 1.538250, mean_q: 12.091734\n",
      "wrong_move\n",
      "   232/50000: episode: 228, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1836.000 [1836.000, 1836.000],  loss: 12394028.000000, mae: 1.610091, mean_q: 11.128148\n",
      "wrong_move\n",
      "   233/50000: episode: 229, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 348.000 [348.000, 348.000],  loss: 12308510.000000, mae: 1.600912, mean_q: 11.563347\n",
      "wrong_move\n",
      "   234/50000: episode: 230, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 11951591.000000, mae: 1.570787, mean_q: 11.794537\n",
      "wrong_move\n",
      "   235/50000: episode: 231, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 827.000 [827.000, 827.000],  loss: 12337073.000000, mae: 1.613580, mean_q: 13.580976\n",
      "wrong_move\n",
      "   236/50000: episode: 232, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 11589289.000000, mae: 1.534734, mean_q: 11.424208\n",
      "wrong_move\n",
      "   237/50000: episode: 233, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 12346791.000000, mae: 1.607846, mean_q: 9.594633\n",
      "wrong_move\n",
      "   238/50000: episode: 234, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3733.000 [3733.000, 3733.000],  loss: 12343952.000000, mae: 1.612635, mean_q: 10.740634\n",
      "wrong_move\n",
      "   239/50000: episode: 235, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1836.000 [1836.000, 1836.000],  loss: 12335934.000000, mae: 1.625265, mean_q: 14.237137\n",
      "wrong_move\n",
      "   240/50000: episode: 236, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 11955722.000000, mae: 1.585253, mean_q: 12.317985\n",
      "wrong_move\n",
      "   241/50000: episode: 237, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 11548350.000000, mae: 1.528857, mean_q: 8.330578\n",
      "wrong_move\n",
      "   242/50000: episode: 238, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12327474.000000, mae: 1.622735, mean_q: 11.439749\n",
      "wrong_move\n",
      "   243/50000: episode: 239, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2989.000 [2989.000, 2989.000],  loss: 12343361.000000, mae: 1.631199, mean_q: 12.746872\n",
      "wrong_move\n",
      "   244/50000: episode: 240, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3141.000 [3141.000, 3141.000],  loss: 12359357.000000, mae: 1.625832, mean_q: 11.288812\n",
      "wrong_move\n",
      "   245/50000: episode: 241, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12343070.000000, mae: 1.625646, mean_q: 10.616256\n",
      "wrong_move\n",
      "   246/50000: episode: 242, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2090.000 [2090.000, 2090.000],  loss: 12337264.000000, mae: 1.628788, mean_q: 11.780760\n",
      "wrong_move\n",
      "   247/50000: episode: 243, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12333612.000000, mae: 1.629533, mean_q: 11.981595\n",
      "wrong_move\n",
      "   248/50000: episode: 244, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 903.000 [903.000, 903.000],  loss: 11920532.000000, mae: 1.600527, mean_q: 12.618576\n",
      "wrong_move\n",
      "   249/50000: episode: 245, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 11990644.000000, mae: 1.601118, mean_q: 13.038504\n",
      "wrong_move\n",
      "   250/50000: episode: 246, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 11557604.000000, mae: 1.565262, mean_q: 11.488489\n",
      "wrong_move\n",
      "   251/50000: episode: 247, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1154.000 [1154.000, 1154.000],  loss: 12401172.000000, mae: 1.648581, mean_q: 13.359705\n",
      "wrong_move\n",
      "   252/50000: episode: 248, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12019720.000000, mae: 1.622232, mean_q: 14.343931\n",
      "wrong_move\n",
      "   253/50000: episode: 249, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3234.000 [3234.000, 3234.000],  loss: 11955780.000000, mae: 1.606412, mean_q: 11.264540\n",
      "wrong_move\n",
      "   254/50000: episode: 250, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3234.000 [3234.000, 3234.000],  loss: 12336019.000000, mae: 1.656520, mean_q: 13.867791\n",
      "wrong_move\n",
      "   255/50000: episode: 251, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3480.000 [3480.000, 3480.000],  loss: 11968878.000000, mae: 1.626464, mean_q: 12.696585\n",
      "wrong_move\n",
      "   256/50000: episode: 252, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2064.000 [2064.000, 2064.000],  loss: 12352391.000000, mae: 1.648439, mean_q: 12.485562\n",
      "wrong_move\n",
      "   257/50000: episode: 253, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 661.000 [661.000, 661.000],  loss: 12313751.000000, mae: 1.646380, mean_q: 10.903210\n",
      "wrong_move\n",
      "   258/50000: episode: 254, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 222.000 [222.000, 222.000],  loss: 12003906.000000, mae: 1.628290, mean_q: 12.053241\n",
      "wrong_move\n",
      "   259/50000: episode: 255, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12321086.000000, mae: 1.656658, mean_q: 13.146081\n",
      "wrong_move\n",
      "   260/50000: episode: 256, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1822.000 [1822.000, 1822.000],  loss: 12277107.000000, mae: 1.653447, mean_q: 14.297925\n",
      "wrong_move\n",
      "   261/50000: episode: 257, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2179.000 [2179.000, 2179.000],  loss: 12345667.000000, mae: 1.659639, mean_q: 12.061026\n",
      "wrong_move\n",
      "   262/50000: episode: 258, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 661.000 [661.000, 661.000],  loss: 12314558.000000, mae: 1.666827, mean_q: 12.965850\n",
      "wrong_move\n",
      "   263/50000: episode: 259, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 818.000 [818.000, 818.000],  loss: 12299854.000000, mae: 1.666313, mean_q: 12.196946\n",
      "wrong_move\n",
      "   264/50000: episode: 260, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2280.000 [2280.000, 2280.000],  loss: 12300356.000000, mae: 1.668392, mean_q: 13.843256\n",
      "wrong_move\n",
      "   265/50000: episode: 261, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1639.000 [1639.000, 1639.000],  loss: 12282656.000000, mae: 1.669019, mean_q: 12.753469\n",
      "wrong_move\n",
      "   266/50000: episode: 262, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12296135.000000, mae: 1.665074, mean_q: 11.034010\n",
      "wrong_move\n",
      "   267/50000: episode: 263, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3040.000 [3040.000, 3040.000],  loss: 12296499.000000, mae: 1.672707, mean_q: 12.256874\n",
      "wrong_move\n",
      "   268/50000: episode: 264, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3013.000 [3013.000, 3013.000],  loss: 12278918.000000, mae: 1.674816, mean_q: 14.073862\n",
      "wrong_move\n",
      "   269/50000: episode: 265, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2634.000 [2634.000, 2634.000],  loss: 12269183.000000, mae: 1.681044, mean_q: 14.032777\n",
      "wrong_move\n",
      "   270/50000: episode: 266, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 433.000 [433.000, 433.000],  loss: 12265798.000000, mae: 1.678394, mean_q: 11.613249\n",
      "wrong_move\n",
      "   271/50000: episode: 267, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 134.000 [134.000, 134.000],  loss: 12280695.000000, mae: 1.678195, mean_q: 12.128153\n",
      "wrong_move\n",
      "   272/50000: episode: 268, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 11901526.000000, mae: 1.651908, mean_q: 14.462097\n",
      "wrong_move\n",
      "   273/50000: episode: 269, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3787.000 [3787.000, 3787.000],  loss: 12321365.000000, mae: 1.694098, mean_q: 15.357092\n",
      "wrong_move\n",
      "   274/50000: episode: 270, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3835.000 [3835.000, 3835.000],  loss: 12332470.000000, mae: 1.690280, mean_q: 14.024377\n",
      "wrong_move\n",
      "   275/50000: episode: 271, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1596.000 [1596.000, 1596.000],  loss: 11894990.000000, mae: 1.647073, mean_q: 12.699127\n",
      "wrong_move\n",
      "   276/50000: episode: 272, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 638.000 [638.000, 638.000],  loss: 12315042.000000, mae: 1.689175, mean_q: 12.723499\n",
      "wrong_move\n",
      "   277/50000: episode: 273, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 11925075.000000, mae: 1.669216, mean_q: 14.495146\n",
      "wrong_move\n",
      "   278/50000: episode: 274, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3533.000 [3533.000, 3533.000],  loss: 12358418.000000, mae: 1.705697, mean_q: 13.124973\n",
      "wrong_move\n",
      "   279/50000: episode: 275, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2392.000 [2392.000, 2392.000],  loss: 12262577.000000, mae: 1.694519, mean_q: 11.825470\n",
      "wrong_move\n",
      "   280/50000: episode: 276, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 96.000 [96.000, 96.000],  loss: 11517791.000000, mae: 1.628519, mean_q: 12.503238\n",
      "wrong_move\n",
      "   281/50000: episode: 277, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 618.000 [618.000, 618.000],  loss: 12332215.000000, mae: 1.700724, mean_q: 11.598939\n",
      "wrong_move\n",
      "   282/50000: episode: 278, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3769.000 [3769.000, 3769.000],  loss: 11882444.000000, mae: 1.662140, mean_q: 11.274415\n",
      "wrong_move\n",
      "   283/50000: episode: 279, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12314186.000000, mae: 1.710582, mean_q: 11.567665\n",
      "wrong_move\n",
      "   284/50000: episode: 280, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12273696.000000, mae: 1.712525, mean_q: 12.105172\n",
      "wrong_move\n",
      "   285/50000: episode: 281, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 175.000 [175.000, 175.000],  loss: 12304128.000000, mae: 1.711444, mean_q: 10.558344\n",
      "wrong_move\n",
      "   286/50000: episode: 282, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12258230.000000, mae: 1.713744, mean_q: 13.114953\n",
      "wrong_move\n",
      "   287/50000: episode: 283, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 102.000 [102.000, 102.000],  loss: 12259954.000000, mae: 1.723114, mean_q: 13.058506\n",
      "wrong_move\n",
      "   288/50000: episode: 284, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 526.000 [526.000, 526.000],  loss: 12291060.000000, mae: 1.727682, mean_q: 14.617056\n",
      "wrong_move\n",
      "   289/50000: episode: 285, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2122.000 [2122.000, 2122.000],  loss: 12258136.000000, mae: 1.724124, mean_q: 12.051750\n",
      "wrong_move\n",
      "   290/50000: episode: 286, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 11554880.000000, mae: 1.659754, mean_q: 15.821392\n",
      "wrong_move\n",
      "   291/50000: episode: 287, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12248217.000000, mae: 1.738520, mean_q: 15.666056\n",
      "wrong_move\n",
      "   292/50000: episode: 288, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 872.000 [872.000, 872.000],  loss: 12272963.000000, mae: 1.739244, mean_q: 14.373016\n",
      "wrong_move\n",
      "   293/50000: episode: 289, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 456.000 [456.000, 456.000],  loss: 12223358.000000, mae: 1.738069, mean_q: 15.726451\n",
      "wrong_move\n",
      "   294/50000: episode: 290, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12196416.000000, mae: 1.737761, mean_q: 14.662047\n",
      "wrong_move\n",
      "   295/50000: episode: 291, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12318652.000000, mae: 1.746590, mean_q: 13.566999\n",
      "wrong_move\n",
      "   296/50000: episode: 292, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12285226.000000, mae: 1.748834, mean_q: 13.856230\n",
      "wrong_move\n",
      "   297/50000: episode: 293, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 11921161.000000, mae: 1.712660, mean_q: 13.298710\n",
      "wrong_move\n",
      "   298/50000: episode: 294, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1573.000 [1573.000, 1573.000],  loss: 12278459.000000, mae: 1.750313, mean_q: 12.390516\n",
      "wrong_move\n",
      "   299/50000: episode: 295, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 11835539.000000, mae: 1.724298, mean_q: 15.593451\n",
      "wrong_move\n",
      "   300/50000: episode: 296, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 11899180.000000, mae: 1.721820, mean_q: 13.965328\n",
      "wrong_move\n",
      "   301/50000: episode: 297, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2193.000 [2193.000, 2193.000],  loss: 11861255.000000, mae: 1.722770, mean_q: 14.777276\n",
      "wrong_move\n",
      "   302/50000: episode: 298, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12272188.000000, mae: 1.764086, mean_q: 14.348581\n",
      "wrong_move\n",
      "   303/50000: episode: 299, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3107.000 [3107.000, 3107.000],  loss: 12305400.000000, mae: 1.773110, mean_q: 13.756855\n",
      "wrong_move\n",
      "   304/50000: episode: 300, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12261073.000000, mae: 1.776685, mean_q: 15.435812\n",
      "wrong_move\n",
      "   305/50000: episode: 301, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 11862144.000000, mae: 1.734962, mean_q: 11.780275\n",
      "wrong_move\n",
      "   306/50000: episode: 302, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 11931444.000000, mae: 1.742295, mean_q: 12.280828\n",
      "wrong_move\n",
      "   307/50000: episode: 303, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2041.000 [2041.000, 2041.000],  loss: 11883514.000000, mae: 1.749469, mean_q: 14.992640\n",
      "wrong_move\n",
      "   308/50000: episode: 304, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12218834.000000, mae: 1.779973, mean_q: 15.428545\n",
      "wrong_move\n",
      "   309/50000: episode: 305, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4049.000 [4049.000, 4049.000],  loss: 12219776.000000, mae: 1.786792, mean_q: 14.617079\n",
      "wrong_move\n",
      "   310/50000: episode: 306, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12316688.000000, mae: 1.787598, mean_q: 16.089859\n",
      "wrong_move\n",
      "   311/50000: episode: 307, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1095.000 [1095.000, 1095.000],  loss: 12234676.000000, mae: 1.786826, mean_q: 13.670591\n",
      "wrong_move\n",
      "   312/50000: episode: 308, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 11894051.000000, mae: 1.756248, mean_q: 13.355350\n",
      "wrong_move\n",
      "   313/50000: episode: 309, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2220.000 [2220.000, 2220.000],  loss: 12200754.000000, mae: 1.787590, mean_q: 14.182753\n",
      "wrong_move\n",
      "   314/50000: episode: 310, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12311972.000000, mae: 1.786507, mean_q: 10.955044\n",
      "wrong_move\n",
      "   315/50000: episode: 311, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 11876201.000000, mae: 1.754340, mean_q: 11.754252\n",
      "wrong_move\n",
      "   316/50000: episode: 312, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 11838511.000000, mae: 1.763404, mean_q: 13.947248\n",
      "wrong_move\n",
      "   317/50000: episode: 313, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 34.000 [34.000, 34.000],  loss: 11411854.000000, mae: 1.735604, mean_q: 14.995889\n",
      "wrong_move\n",
      "   318/50000: episode: 314, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 693.000 [693.000, 693.000],  loss: 12201932.000000, mae: 1.810486, mean_q: 14.562413\n",
      "wrong_move\n",
      "   319/50000: episode: 315, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12267600.000000, mae: 1.797672, mean_q: 11.881456\n",
      "wrong_move\n",
      "   320/50000: episode: 316, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12211691.000000, mae: 1.807949, mean_q: 11.526899\n",
      "wrong_move\n",
      "   321/50000: episode: 317, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12244226.000000, mae: 1.813555, mean_q: 14.139223\n",
      "wrong_move\n",
      "   322/50000: episode: 318, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: 12215078.000000, mae: 1.813947, mean_q: 13.270597\n",
      "wrong_move\n",
      "   323/50000: episode: 319, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12307516.000000, mae: 1.823521, mean_q: 12.776749\n",
      "wrong_move\n",
      "   324/50000: episode: 320, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1468.000 [1468.000, 1468.000],  loss: 12107362.000000, mae: 1.824757, mean_q: 16.477757\n",
      "wrong_move\n",
      "   325/50000: episode: 321, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3550.000 [3550.000, 3550.000],  loss: 11929638.000000, mae: 1.796620, mean_q: 16.358910\n",
      "wrong_move\n",
      "   326/50000: episode: 322, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4058.000 [4058.000, 4058.000],  loss: 11859167.000000, mae: 1.823828, mean_q: 21.663019\n",
      "wrong_move\n",
      "   327/50000: episode: 323, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2401.000 [2401.000, 2401.000],  loss: 12235916.000000, mae: 1.846611, mean_q: 18.192459\n",
      "wrong_move\n",
      "   328/50000: episode: 324, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1273.000 [1273.000, 1273.000],  loss: 11442233.000000, mae: 1.771565, mean_q: 14.912208\n",
      "wrong_move\n",
      "   329/50000: episode: 325, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 11796678.000000, mae: 1.800121, mean_q: 15.092875\n",
      "wrong_move\n",
      "   330/50000: episode: 326, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1365.000 [1365.000, 1365.000],  loss: 12237416.000000, mae: 1.837758, mean_q: 12.875900\n",
      "wrong_move\n",
      "   331/50000: episode: 327, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 197.000 [197.000, 197.000],  loss: 12208184.000000, mae: 1.847980, mean_q: 14.548458\n",
      "wrong_move\n",
      "   332/50000: episode: 328, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12234001.000000, mae: 1.854289, mean_q: 16.850193\n",
      "wrong_move\n",
      "   333/50000: episode: 329, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12189286.000000, mae: 1.869789, mean_q: 18.883942\n",
      "wrong_move\n",
      "   334/50000: episode: 330, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 403.000 [403.000, 403.000],  loss: 12276741.000000, mae: 1.878023, mean_q: 18.330793\n",
      "wrong_move\n",
      "   335/50000: episode: 331, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 256.000 [256.000, 256.000],  loss: 12164407.000000, mae: 1.860248, mean_q: 15.195454\n",
      "wrong_move\n",
      "   336/50000: episode: 332, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12221566.000000, mae: 1.868178, mean_q: 13.560881\n",
      "wrong_move\n",
      "   337/50000: episode: 333, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3811.000 [3811.000, 3811.000],  loss: 12153758.000000, mae: 1.878329, mean_q: 19.163624\n",
      "wrong_move\n",
      "   338/50000: episode: 334, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1999.000 [1999.000, 1999.000],  loss: 12198340.000000, mae: 1.877176, mean_q: 15.367096\n",
      "wrong_move\n",
      "   339/50000: episode: 335, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1053.000 [1053.000, 1053.000],  loss: 12226086.000000, mae: 1.878280, mean_q: 14.043179\n",
      "wrong_move\n",
      "   340/50000: episode: 336, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12210180.000000, mae: 1.891087, mean_q: 18.006844\n",
      "wrong_move\n",
      "   341/50000: episode: 337, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12234751.000000, mae: 1.892022, mean_q: 14.520845\n",
      "wrong_move\n",
      "   342/50000: episode: 338, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1203.000 [1203.000, 1203.000],  loss: 12213576.000000, mae: 1.881013, mean_q: 13.655352\n",
      "wrong_move\n",
      "   343/50000: episode: 339, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1418.000 [1418.000, 1418.000],  loss: 12186802.000000, mae: 1.898488, mean_q: 16.767899\n",
      "wrong_move\n",
      "   344/50000: episode: 340, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3710.000 [3710.000, 3710.000],  loss: 12271471.000000, mae: 1.891542, mean_q: 14.701128\n",
      "wrong_move\n",
      "   345/50000: episode: 341, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4006.000 [4006.000, 4006.000],  loss: 12232364.000000, mae: 1.903175, mean_q: 15.518072\n",
      "wrong_move\n",
      "   346/50000: episode: 342, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3064.000 [3064.000, 3064.000],  loss: 12193851.000000, mae: 1.895509, mean_q: 13.563668\n",
      "wrong_move\n",
      "   347/50000: episode: 343, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1782.000 [1782.000, 1782.000],  loss: 12185007.000000, mae: 1.909240, mean_q: 16.603500\n",
      "wrong_move\n",
      "   348/50000: episode: 344, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12204133.000000, mae: 1.909208, mean_q: 16.718670\n",
      "wrong_move\n",
      "   349/50000: episode: 345, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1892.000 [1892.000, 1892.000],  loss: 11831354.000000, mae: 1.878034, mean_q: 17.686562\n",
      "wrong_move\n",
      "   350/50000: episode: 346, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4088.000 [4088.000, 4088.000],  loss: 12211617.000000, mae: 1.917749, mean_q: 14.351326\n",
      "wrong_move\n",
      "   351/50000: episode: 347, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12179448.000000, mae: 1.919676, mean_q: 15.282792\n",
      "wrong_move\n",
      "   352/50000: episode: 348, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3105.000 [3105.000, 3105.000],  loss: 12228004.000000, mae: 1.922989, mean_q: 16.619122\n",
      "wrong_move\n",
      "   353/50000: episode: 349, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2188.000 [2188.000, 2188.000],  loss: 12197954.000000, mae: 1.919386, mean_q: 15.138620\n",
      "wrong_move\n",
      "   354/50000: episode: 350, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3443.000 [3443.000, 3443.000],  loss: 12207326.000000, mae: 1.936243, mean_q: 17.363075\n",
      "wrong_move\n",
      "   355/50000: episode: 351, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3644.000 [3644.000, 3644.000],  loss: 12166270.000000, mae: 1.929808, mean_q: 13.542994\n",
      "wrong_move\n",
      "   356/50000: episode: 352, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 287.000 [287.000, 287.000],  loss: 11776147.000000, mae: 1.905834, mean_q: 14.596066\n",
      "wrong_move\n",
      "   357/50000: episode: 353, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1476.000 [1476.000, 1476.000],  loss: 12103439.000000, mae: 1.949067, mean_q: 16.997211\n",
      "wrong_move\n",
      "   358/50000: episode: 354, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12132775.000000, mae: 1.952550, mean_q: 18.462288\n",
      "wrong_move\n",
      "   359/50000: episode: 355, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1367.000 [1367.000, 1367.000],  loss: 12240408.000000, mae: 1.984059, mean_q: 23.254704\n",
      "wrong_move\n",
      "   360/50000: episode: 356, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 715.000 [715.000, 715.000],  loss: 12207274.000000, mae: 1.958062, mean_q: 17.698622\n",
      "wrong_move\n",
      "   361/50000: episode: 357, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2632.000 [2632.000, 2632.000],  loss: 12143106.000000, mae: 1.949684, mean_q: 16.222158\n",
      "wrong_move\n",
      "   362/50000: episode: 358, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 12178172.000000, mae: 1.980018, mean_q: 17.274059\n",
      "wrong_move\n",
      "   363/50000: episode: 359, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2948.000 [2948.000, 2948.000],  loss: 12157742.000000, mae: 1.976665, mean_q: 16.930649\n",
      "wrong_move\n",
      "   364/50000: episode: 360, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1590.000 [1590.000, 1590.000],  loss: 11763047.000000, mae: 1.945102, mean_q: 17.292982\n",
      "wrong_move\n",
      "   365/50000: episode: 361, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12169240.000000, mae: 1.974081, mean_q: 15.239564\n",
      "wrong_move\n",
      "   366/50000: episode: 362, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 11727133.000000, mae: 1.945038, mean_q: 15.559628\n",
      "wrong_move\n",
      "   367/50000: episode: 363, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 11753438.000000, mae: 1.934566, mean_q: 17.246838\n",
      "wrong_move\n",
      "   368/50000: episode: 364, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1418.000 [1418.000, 1418.000],  loss: 11762857.000000, mae: 1.946118, mean_q: 15.900543\n",
      "wrong_move\n",
      "   369/50000: episode: 365, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2906.000 [2906.000, 2906.000],  loss: 12183846.000000, mae: 1.992079, mean_q: 16.885708\n",
      "wrong_move\n",
      "   370/50000: episode: 366, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12243506.000000, mae: 2.001626, mean_q: 16.242294\n",
      "wrong_move\n",
      "   371/50000: episode: 367, duration: 0.141s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2267.000 [2267.000, 2267.000],  loss: 12050522.000000, mae: 1.994240, mean_q: 15.474329\n",
      "wrong_move\n",
      "   372/50000: episode: 368, duration: 0.142s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12087277.000000, mae: 1.999990, mean_q: 17.713907\n",
      "wrong_move\n",
      "   373/50000: episode: 369, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3456.000 [3456.000, 3456.000],  loss: 12104572.000000, mae: 2.021670, mean_q: 18.419403\n",
      "wrong_move\n",
      "   374/50000: episode: 370, duration: 0.172s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1813.000 [1813.000, 1813.000],  loss: 12257549.000000, mae: 2.051267, mean_q: 22.799940\n",
      "wrong_move\n",
      "   375/50000: episode: 371, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1978.000 [1978.000, 1978.000],  loss: 12143575.000000, mae: 2.031048, mean_q: 16.678398\n",
      "wrong_move\n",
      "   376/50000: episode: 372, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1363.000 [1363.000, 1363.000],  loss: 12151134.000000, mae: 2.020919, mean_q: 15.416156\n",
      "wrong_move\n",
      "   377/50000: episode: 373, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1397.000 [1397.000, 1397.000],  loss: 12149262.000000, mae: 2.014442, mean_q: 16.878649\n",
      "wrong_move\n",
      "   378/50000: episode: 374, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1397.000 [1397.000, 1397.000],  loss: 12197350.000000, mae: 2.034906, mean_q: 19.520191\n",
      "wrong_move\n",
      "   379/50000: episode: 375, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1822.000 [1822.000, 1822.000],  loss: 12161443.000000, mae: 2.039595, mean_q: 20.947086\n",
      "wrong_move\n",
      "   380/50000: episode: 376, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1836.000 [1836.000, 1836.000],  loss: 12209092.000000, mae: 2.048066, mean_q: 17.863550\n",
      "wrong_move\n",
      "   381/50000: episode: 377, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4065.000 [4065.000, 4065.000],  loss: 11828594.000000, mae: 2.013626, mean_q: 19.952366\n",
      "wrong_move\n",
      "   382/50000: episode: 378, duration: 0.178s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12203532.000000, mae: 2.055648, mean_q: 19.074800\n",
      "wrong_move\n",
      "   383/50000: episode: 379, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3088.000 [3088.000, 3088.000],  loss: 12075100.000000, mae: 2.041586, mean_q: 16.587700\n",
      "wrong_move\n",
      "   384/50000: episode: 380, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1679.000 [1679.000, 1679.000],  loss: 12069679.000000, mae: 2.051167, mean_q: 17.478046\n",
      "wrong_move\n",
      "   385/50000: episode: 381, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3181.000 [3181.000, 3181.000],  loss: 12192793.000000, mae: 2.068075, mean_q: 18.590485\n",
      "wrong_move\n",
      "   386/50000: episode: 382, duration: 0.168s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12058778.000000, mae: 2.063736, mean_q: 19.992317\n",
      "wrong_move\n",
      "   387/50000: episode: 383, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3623.000 [3623.000, 3623.000],  loss: 12071966.000000, mae: 2.074272, mean_q: 18.258282\n",
      "wrong_move\n",
      "   388/50000: episode: 384, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1679.000 [1679.000, 1679.000],  loss: 12270695.000000, mae: 2.069332, mean_q: 15.219240\n",
      "wrong_move\n",
      "   389/50000: episode: 385, duration: 0.172s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 12106888.000000, mae: 2.085328, mean_q: 19.425695\n",
      "wrong_move\n",
      "   390/50000: episode: 386, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 11695499.000000, mae: 2.036371, mean_q: 12.516477\n",
      "wrong_move\n",
      "   391/50000: episode: 387, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3434.000 [3434.000, 3434.000],  loss: 12092244.000000, mae: 2.095293, mean_q: 22.417040\n",
      "wrong_move\n",
      "   392/50000: episode: 388, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 889.000 [889.000, 889.000],  loss: 12067409.000000, mae: 2.099329, mean_q: 20.612616\n",
      "wrong_move\n",
      "   393/50000: episode: 389, duration: 0.171s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2989.000 [2989.000, 2989.000],  loss: 12116635.000000, mae: 2.099603, mean_q: 17.349659\n",
      "wrong_move\n",
      "   394/50000: episode: 390, duration: 0.152s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3965.000 [3965.000, 3965.000],  loss: 12120360.000000, mae: 2.111483, mean_q: 18.744663\n",
      "wrong_move\n",
      "   395/50000: episode: 391, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 11682930.000000, mae: 2.075180, mean_q: 20.559681\n",
      "wrong_move\n",
      "   396/50000: episode: 392, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3531.000 [3531.000, 3531.000],  loss: 12109840.000000, mae: 2.120919, mean_q: 19.251673\n",
      "wrong_move\n",
      "   397/50000: episode: 393, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3097.000 [3097.000, 3097.000],  loss: 12059423.000000, mae: 2.094917, mean_q: 14.913940\n",
      "wrong_move\n",
      "   398/50000: episode: 394, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2985.000 [2985.000, 2985.000],  loss: 11355656.000000, mae: 2.037974, mean_q: 17.253922\n",
      "wrong_move\n",
      "   399/50000: episode: 395, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2122.000 [2122.000, 2122.000],  loss: 12110668.000000, mae: 2.108180, mean_q: 18.288801\n",
      "wrong_move\n",
      "   400/50000: episode: 396, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3805.000 [3805.000, 3805.000],  loss: 12103120.000000, mae: 2.135342, mean_q: 17.302288\n",
      "wrong_move\n",
      "   401/50000: episode: 397, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 12087834.000000, mae: 2.152206, mean_q: 20.585850\n",
      "wrong_move\n",
      "   402/50000: episode: 398, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 275.000 [275.000, 275.000],  loss: 11747236.000000, mae: 2.086209, mean_q: 12.927069\n",
      "wrong_move\n",
      "   403/50000: episode: 399, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2122.000 [2122.000, 2122.000],  loss: 12077858.000000, mae: 2.136766, mean_q: 15.781019\n",
      "wrong_move\n",
      "   404/50000: episode: 400, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 12260809.000000, mae: 2.151412, mean_q: 16.294565\n",
      "done, took 39.052 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f78ec182160>"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=500000, window_length=1)\n",
    "policy = EpsGreedyQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=NB_ACTIONS, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "his = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save dqn\n",
    "# dqn.save_weights('dqn_{}_weights.h5f'.format('chess'), overwrite=True)\n",
    "\n",
    "# # save model\n",
    "# model.save('chess_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# model = keras.models.load_model('chess_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -40.,  -20.,  -30.,  -50., -900.,  -30.,  -20.,  -40.],\n",
       "       [ -10.,  -10.,  -10.,  -10.,  -10.,  -10.,  -10.,  -10.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
       "       [  10.,   10.,   10.,   10.,   10.,   10.,   10.,   10.],\n",
       "       [  40.,   20.,   30.,   50.,  900.,   30.,   20.,   40.]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(env.state.reshape((1, 1) + STATE_SHAPE))\n",
    "idx_sort = np.argsort(pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
