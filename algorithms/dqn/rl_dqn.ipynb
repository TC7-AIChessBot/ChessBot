{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install keras-rl2\n",
    "# ! pip install chess\n",
    "# ! pip install python-chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten,\\\n",
    "     Input, Conv2D, BatchNormalization, MaxPool2D, Reshape, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# import gym_chess\n",
    "\n",
    "import chess\n",
    "# import sys\n",
    "# sys.path.insert(0, '../')\n",
    "# sys.path.insert(0, '../alpha_beta')\n",
    "# from MyChessBoard import MyChessBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16939680808500570223\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SHAPE = (65, )\n",
    "NB_ACTIONS = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessEnv:\n",
    "    '''\n",
    "    state - obser: ndarray - (65,): [:65] is flatten from int_board; [65] is color of bot; 1 is white and -1 is black\n",
    "    step: int. step_range = (0, 4096) , is encoded from square A to square B (64 x 64 val)\n",
    "    reward: int\n",
    "    '''\n",
    "\n",
    "    mapped = {\n",
    "            'P': 10,     # White Pawn\n",
    "            'p': -10,    # Black Pawn\n",
    "            'N': 20,     # White Knight\n",
    "            'n': -20,    # Black Knight\n",
    "            'B': 30,     # White Bishop\n",
    "            'b': -30,    # Black Bishop\n",
    "            'R': 40,     # White Rook\n",
    "            'r': -40,    # Black Rook\n",
    "            'Q': 50,     # White Queen\n",
    "            'q': -50,    # Black Queen\n",
    "            'K': 900,     # White King\n",
    "            'k': -900     # Black King\n",
    "    }\n",
    "    # state_shape = (8, 8)\n",
    "    # nb_actions = 4096\n",
    "    model = None\n",
    "    \n",
    "    def __init__(self, model: Sequential, neg_r_each_step = -1) -> None:\n",
    "        self.env = chess.Board()\n",
    "        self.state = self.reset()\n",
    "        # [-1] = 1 -> white, -1 -> black\n",
    "        self.bot_color = self.env.turn * 2 - 1\n",
    "        self.neg_r_each_step = neg_r_each_step\n",
    "        self.model = model\n",
    "\n",
    "    def is_draw(self):\n",
    "        if self.env.is_stalemate():\n",
    "            print(\"statlemate\")\n",
    "            return True\n",
    "        if self.env.is_fivefold_repetition():\n",
    "            print(\"fivefold repetition\")\n",
    "            return True\n",
    "        if self.env.is_seventyfive_moves():\n",
    "            print(\"75 moves\")\n",
    "            return True\n",
    "        if self.env.is_insufficient_material():\n",
    "            print(\"Insufficient Material\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_checkmate(self):\n",
    "        # If There is checkmate then it will be TRUE else FALSE.It will be a boolean value.\n",
    "        return self.env.is_checkmate()\n",
    "\n",
    "    def convert_board_to_int(self):\n",
    "        epd_string = self.env.epd()\n",
    "        list_int = np.empty((0, ))\n",
    "        for i in epd_string:\n",
    "            if i == \" \":\n",
    "                list_int = list_int.reshape((8, 8))\n",
    "                return list_int\n",
    "            elif i != \"/\":\n",
    "                if i in self.mapped:\n",
    "                    list_int = np.append(list_int, self.mapped[i])\n",
    "                else:\n",
    "                    for counter in range(0, int(i)):\n",
    "                        list_int = np.append(list_int, 0)\n",
    "        list_int = list_int.reshape((8, 8))\n",
    "        return list_int\n",
    "\n",
    "    def get_state(self) -> np.ndarray:\n",
    "        return np.append(self.convert_board_to_int().reshape(64,), self.env.turn * 2 - 1)\n",
    "\n",
    "    def legal_moves(self):\n",
    "        return list(self.env.legal_moves)\n",
    "\n",
    "    def encodeMove(self, move_uci:str):\n",
    "        if len(move_uci) != 4:\n",
    "            raise ValueError()\n",
    "        a, b = chess.parse_square(move_uci[:2]), chess.parse_square(move_uci[2:])\n",
    "        return a * 64 + b\n",
    "\n",
    "    def decodeMove(self, move_int:int):\n",
    "        a, b = move_int//64, move_int%64\n",
    "        # a, b = chess.square_name(a), chess.square_name(b)\n",
    "\n",
    "        move = self.env.find_move(from_square= a,to_square= b)\n",
    "        return move\n",
    "\n",
    "    def render(self):\n",
    "        print(self.env.unicode())\n",
    "\n",
    "    def reset(self):\n",
    "        # random state\n",
    "        redo = True\n",
    "        num_sample_steps = 0\n",
    "        while redo:\n",
    "            redo = False\n",
    "            self.env = chess.Board()\n",
    "            num_sample_steps = np.random.randint(0, 50)\n",
    "            for i in range (num_sample_steps):\n",
    "                lg_move = self.legal_moves()\n",
    "                if len(lg_move) != 0:\n",
    "                    move = np.random.choice(self.legal_moves())\n",
    "                    self.env.push(move)\n",
    "                else:\n",
    "                    redo = True\n",
    "                    break\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action: int):\n",
    "        reward = 0\n",
    "        done = True\n",
    "\n",
    "        try:\n",
    "            # move in legal move\n",
    "            move = self.decodeMove(action)\n",
    "\n",
    "            # neg reward each step\n",
    "            reward = self.neg_r_each_step\n",
    "\n",
    "            # location to_square\n",
    "            to_r, to_c = move.to_square//8, move.to_square%8\n",
    "            reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
    "\n",
    "            # action\n",
    "            self.env.push(move)\n",
    "            self.state = self.get_state()\n",
    "\n",
    "            # check end game\n",
    "            if self.is_checkmate():\n",
    "                reward += self.mapped['K']\n",
    "                done = True\n",
    "            elif self.is_draw():\n",
    "                reward += 300\n",
    "                done = True\n",
    "\n",
    "            # opponent's turn   \n",
    "            else:\n",
    "                done = False\n",
    "                Q_val = self.model.predict(self.state.reshape((1, 1) + STATE_SHAPE)).reshape(-1, )\n",
    "                idx_sorted = np.argsort(Q_val)\n",
    "\n",
    "                for act in idx_sorted:\n",
    "                    try:\n",
    "                        move = self.decodeMove(act)\n",
    "\n",
    "                        # location to_square\n",
    "                        to_r, to_c = move.to_square//8, move.to_square%8\n",
    "                        reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
    "\n",
    "                        # action\n",
    "                        self.env.push(move)\n",
    "                        self.state = self.get_state()\n",
    "\n",
    "                        # check end game\n",
    "                        if self.is_checkmate():\n",
    "                            reward -= self.mapped['K']\n",
    "                            done = True\n",
    "                        elif self.is_draw():\n",
    "                            reward += 300\n",
    "                            done = True\n",
    "                        \n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "        except:\n",
    "            # wrong move\n",
    "            reward = -5000\n",
    "            done = True\n",
    "            print('wrong_move')\n",
    "\n",
    "        return self.state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 65)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               8448      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4096)              528384    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4096)              0         \n",
      "=================================================================\n",
      "Total params: 554,368\n",
      "Trainable params: 553,856\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = Sequential()\n",
    "model.add(Input((1, ) + STATE_SHAPE))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(NB_ACTIONS))\n",
    "model.add(Activation('linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChessEnv(model, neg_r_each_step=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('chess_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong_move\n",
      "      1/500000: episode: 1, duration: 0.415s, episode steps:   1, steps per second:   2, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "      2/500000: episode: 2, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "      3/500000: episode: 3, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "      4/500000: episode: 4, duration: 0.007s, episode steps:   1, steps per second: 151, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "      5/500000: episode: 5, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1582.000 [1582.000, 1582.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "      6/500000: episode: 6, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "      7/500000: episode: 7, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "      8/500000: episode: 8, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3653.000 [3653.000, 3653.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "      9/500000: episode: 9, duration: 0.006s, episode steps:   1, steps per second: 156, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     10/500000: episode: 10, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2931.000 [2931.000, 2931.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     11/500000: episode: 11, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     12/500000: episode: 12, duration: 0.006s, episode steps:   1, steps per second: 172, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     13/500000: episode: 13, duration: 0.006s, episode steps:   1, steps per second: 154, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     14/500000: episode: 14, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 249.000 [249.000, 249.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     15/500000: episode: 15, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3248.000 [3248.000, 3248.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     16/500000: episode: 16, duration: 0.004s, episode steps:   1, steps per second: 269, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     17/500000: episode: 17, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     18/500000: episode: 18, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 389.000 [389.000, 389.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     20/500000: episode: 19, duration: 0.077s, episode steps:   2, steps per second:  26, episode reward: -4981.000, mean reward: -2490.500 [-5000.000, 19.000], mean action: 2947.500 [1801.000, 4094.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     21/500000: episode: 20, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2015.000 [2015.000, 2015.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     22/500000: episode: 21, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     23/500000: episode: 22, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1004.000 [1004.000, 1004.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     24/500000: episode: 23, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 477.000 [477.000, 477.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     25/500000: episode: 24, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 717.000 [717.000, 717.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     26/500000: episode: 25, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1051.000 [1051.000, 1051.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     27/500000: episode: 26, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2253.000 [2253.000, 2253.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     28/500000: episode: 27, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     29/500000: episode: 28, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1842.000 [1842.000, 1842.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     30/500000: episode: 29, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     31/500000: episode: 30, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3131.000 [3131.000, 3131.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     32/500000: episode: 31, duration: 0.007s, episode steps:   1, steps per second: 138, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     33/500000: episode: 32, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2882.000 [2882.000, 2882.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     34/500000: episode: 33, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     35/500000: episode: 34, duration: 0.005s, episode steps:   1, steps per second: 201, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     36/500000: episode: 35, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 437.000 [437.000, 437.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     37/500000: episode: 36, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2251.000 [2251.000, 2251.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     38/500000: episode: 37, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2146.000 [2146.000, 2146.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     39/500000: episode: 38, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     40/500000: episode: 39, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 462.000 [462.000, 462.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     41/500000: episode: 40, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3131.000 [3131.000, 3131.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     42/500000: episode: 41, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1170.000 [1170.000, 1170.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     43/500000: episode: 42, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3459.000 [3459.000, 3459.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     44/500000: episode: 43, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 135.000 [135.000, 135.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     45/500000: episode: 44, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 549.000 [549.000, 549.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     46/500000: episode: 45, duration: 0.003s, episode steps:   1, steps per second: 327, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 49.000 [49.000, 49.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     47/500000: episode: 46, duration: 0.003s, episode steps:   1, steps per second: 288, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3418.000 [3418.000, 3418.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     48/500000: episode: 47, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 887.000 [887.000, 887.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     49/500000: episode: 48, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2183.000 [2183.000, 2183.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     50/500000: episode: 49, duration: 0.005s, episode steps:   1, steps per second: 195, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     51/500000: episode: 50, duration: 0.006s, episode steps:   1, steps per second: 164, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3984.000 [3984.000, 3984.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     52/500000: episode: 51, duration: 0.005s, episode steps:   1, steps per second: 207, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     53/500000: episode: 52, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 389.000 [389.000, 389.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     54/500000: episode: 53, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2668.000 [2668.000, 2668.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     55/500000: episode: 54, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     56/500000: episode: 55, duration: 0.004s, episode steps:   1, steps per second: 251, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 846.000 [846.000, 846.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     57/500000: episode: 56, duration: 0.004s, episode steps:   1, steps per second: 237, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3479.000 [3479.000, 3479.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     58/500000: episode: 57, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     59/500000: episode: 58, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     60/500000: episode: 59, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 549.000 [549.000, 549.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     61/500000: episode: 60, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 181.000 [181.000, 181.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     62/500000: episode: 61, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2376.000 [2376.000, 2376.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     63/500000: episode: 62, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1925.000 [1925.000, 1925.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     64/500000: episode: 63, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3419.000 [3419.000, 3419.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     65/500000: episode: 64, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     66/500000: episode: 65, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     67/500000: episode: 66, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2246.000 [2246.000, 2246.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     68/500000: episode: 67, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 852.000 [852.000, 852.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     69/500000: episode: 68, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1861.000 [1861.000, 1861.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     70/500000: episode: 69, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2764.000 [2764.000, 2764.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     71/500000: episode: 70, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     72/500000: episode: 71, duration: 0.006s, episode steps:   1, steps per second: 173, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     73/500000: episode: 72, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     74/500000: episode: 73, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     75/500000: episode: 74, duration: 0.005s, episode steps:   1, steps per second: 193, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     76/500000: episode: 75, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3763.000 [3763.000, 3763.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     77/500000: episode: 76, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1272.000 [1272.000, 1272.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     78/500000: episode: 77, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2730.000 [2730.000, 2730.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     79/500000: episode: 78, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1861.000 [1861.000, 1861.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     80/500000: episode: 79, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3298.000 [3298.000, 3298.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     81/500000: episode: 80, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 462.000 [462.000, 462.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     82/500000: episode: 81, duration: 0.006s, episode steps:   1, steps per second: 160, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     83/500000: episode: 82, duration: 0.007s, episode steps:   1, steps per second: 140, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2.000 [2.000, 2.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     84/500000: episode: 83, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 954.000 [954.000, 954.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     85/500000: episode: 84, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     86/500000: episode: 85, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     87/500000: episode: 86, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     88/500000: episode: 87, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3488.000 [3488.000, 3488.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     89/500000: episode: 88, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     90/500000: episode: 89, duration: 0.006s, episode steps:   1, steps per second: 158, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     91/500000: episode: 90, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     92/500000: episode: 91, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 667.000 [667.000, 667.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     93/500000: episode: 92, duration: 0.005s, episode steps:   1, steps per second: 202, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     94/500000: episode: 93, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1587.000 [1587.000, 1587.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     95/500000: episode: 94, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2518.000 [2518.000, 2518.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     96/500000: episode: 95, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1715.000 [1715.000, 1715.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     97/500000: episode: 96, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1167.000 [1167.000, 1167.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     98/500000: episode: 97, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1821.000 [1821.000, 1821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     99/500000: episode: 98, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    100/500000: episode: 99, duration: 0.005s, episode steps:   1, steps per second: 204, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    101/500000: episode: 100, duration: 0.003s, episode steps:   1, steps per second: 325, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    102/500000: episode: 101, duration: 0.006s, episode steps:   1, steps per second: 170, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    103/500000: episode: 102, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    104/500000: episode: 103, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3663.000 [3663.000, 3663.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    105/500000: episode: 104, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    106/500000: episode: 105, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    107/500000: episode: 106, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3661.000 [3661.000, 3661.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    108/500000: episode: 107, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    109/500000: episode: 108, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    110/500000: episode: 109, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    111/500000: episode: 110, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    112/500000: episode: 111, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    113/500000: episode: 112, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2251.000 [2251.000, 2251.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    114/500000: episode: 113, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1839.000 [1839.000, 1839.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    115/500000: episode: 114, duration: 0.004s, episode steps:   1, steps per second: 267, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    116/500000: episode: 115, duration: 0.005s, episode steps:   1, steps per second: 206, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    117/500000: episode: 116, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2916.000 [2916.000, 2916.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    118/500000: episode: 117, duration: 0.006s, episode steps:   1, steps per second: 171, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1228.000 [1228.000, 1228.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    119/500000: episode: 118, duration: 0.007s, episode steps:   1, steps per second: 138, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3259.000 [3259.000, 3259.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    120/500000: episode: 119, duration: 0.005s, episode steps:   1, steps per second: 187, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 477.000 [477.000, 477.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    121/500000: episode: 120, duration: 0.006s, episode steps:   1, steps per second: 177, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    122/500000: episode: 121, duration: 0.005s, episode steps:   1, steps per second: 185, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 269.000 [269.000, 269.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    123/500000: episode: 122, duration: 0.004s, episode steps:   1, steps per second: 245, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    124/500000: episode: 123, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3469.000 [3469.000, 3469.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    125/500000: episode: 124, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 549.000 [549.000, 549.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    126/500000: episode: 125, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    127/500000: episode: 126, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1407.000 [1407.000, 1407.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    128/500000: episode: 127, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2927.000 [2927.000, 2927.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    129/500000: episode: 128, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1752.000 [1752.000, 1752.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    130/500000: episode: 129, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3053.000 [3053.000, 3053.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    131/500000: episode: 130, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1931.000 [1931.000, 1931.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    132/500000: episode: 131, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    133/500000: episode: 132, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    134/500000: episode: 133, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2605.000 [2605.000, 2605.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    135/500000: episode: 134, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    136/500000: episode: 135, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3878.000 [3878.000, 3878.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    137/500000: episode: 136, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    138/500000: episode: 137, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1272.000 [1272.000, 1272.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    139/500000: episode: 138, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2518.000 [2518.000, 2518.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    140/500000: episode: 139, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    141/500000: episode: 140, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    142/500000: episode: 141, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2743.000 [2743.000, 2743.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    143/500000: episode: 142, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    144/500000: episode: 143, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    145/500000: episode: 144, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2668.000 [2668.000, 2668.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    146/500000: episode: 145, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    147/500000: episode: 146, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    148/500000: episode: 147, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    149/500000: episode: 148, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    150/500000: episode: 149, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2951.000 [2951.000, 2951.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    151/500000: episode: 150, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2017.000 [2017.000, 2017.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    152/500000: episode: 151, duration: 0.004s, episode steps:   1, steps per second: 286, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    153/500000: episode: 152, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1370.000 [1370.000, 1370.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    154/500000: episode: 153, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4060.000 [4060.000, 4060.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    155/500000: episode: 154, duration: 0.007s, episode steps:   1, steps per second: 147, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    156/500000: episode: 155, duration: 0.005s, episode steps:   1, steps per second: 196, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    157/500000: episode: 156, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 777.000 [777.000, 777.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    158/500000: episode: 157, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 904.000 [904.000, 904.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    159/500000: episode: 158, duration: 0.005s, episode steps:   1, steps per second: 198, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 836.000 [836.000, 836.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    160/500000: episode: 159, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4070.000 [4070.000, 4070.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    161/500000: episode: 160, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    162/500000: episode: 161, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    163/500000: episode: 162, duration: 0.004s, episode steps:   1, steps per second: 266, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    164/500000: episode: 163, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    165/500000: episode: 164, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    166/500000: episode: 165, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1272.000 [1272.000, 1272.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    167/500000: episode: 166, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 462.000 [462.000, 462.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    168/500000: episode: 167, duration: 0.006s, episode steps:   1, steps per second: 158, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 694.000 [694.000, 694.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    169/500000: episode: 168, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    170/500000: episode: 169, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 477.000 [477.000, 477.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    171/500000: episode: 170, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1602.000 [1602.000, 1602.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    172/500000: episode: 171, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2377.000 [2377.000, 2377.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    173/500000: episode: 172, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    174/500000: episode: 173, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2907.000 [2907.000, 2907.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    175/500000: episode: 174, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2991.000 [2991.000, 2991.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    176/500000: episode: 175, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1053.000 [1053.000, 1053.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    177/500000: episode: 176, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2836.000 [2836.000, 2836.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    178/500000: episode: 177, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1280.000 [1280.000, 1280.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    179/500000: episode: 178, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    180/500000: episode: 179, duration: 0.006s, episode steps:   1, steps per second: 171, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    181/500000: episode: 180, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3725.000 [3725.000, 3725.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    182/500000: episode: 181, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2251.000 [2251.000, 2251.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    183/500000: episode: 182, duration: 0.003s, episode steps:   1, steps per second: 365, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    184/500000: episode: 183, duration: 0.002s, episode steps:   1, steps per second: 408, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2333.000 [2333.000, 2333.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    185/500000: episode: 184, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1883.000 [1883.000, 1883.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    186/500000: episode: 185, duration: 0.006s, episode steps:   1, steps per second: 160, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1650.000 [1650.000, 1650.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    187/500000: episode: 186, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 136.000 [136.000, 136.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    188/500000: episode: 187, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1402.000 [1402.000, 1402.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    189/500000: episode: 188, duration: 0.004s, episode steps:   1, steps per second: 264, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1385.000 [1385.000, 1385.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    190/500000: episode: 189, duration: 0.006s, episode steps:   1, steps per second: 154, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 389.000 [389.000, 389.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    191/500000: episode: 190, duration: 0.003s, episode steps:   1, steps per second: 323, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1677.000 [1677.000, 1677.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    192/500000: episode: 191, duration: 0.005s, episode steps:   1, steps per second: 214, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    193/500000: episode: 192, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1658.000 [1658.000, 1658.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    194/500000: episode: 193, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3851.000 [3851.000, 3851.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    195/500000: episode: 194, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2139.000 [2139.000, 2139.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    196/500000: episode: 195, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 549.000 [549.000, 549.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    197/500000: episode: 196, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3661.000 [3661.000, 3661.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    198/500000: episode: 197, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2702.000 [2702.000, 2702.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    199/500000: episode: 198, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    200/500000: episode: 199, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    201/500000: episode: 200, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    202/500000: episode: 201, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3114.000 [3114.000, 3114.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    203/500000: episode: 202, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1804.000 [1804.000, 1804.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    204/500000: episode: 203, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    205/500000: episode: 204, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 260.000 [260.000, 260.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    206/500000: episode: 205, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 136.000 [136.000, 136.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    207/500000: episode: 206, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1584.000 [1584.000, 1584.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    208/500000: episode: 207, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    209/500000: episode: 208, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1280.000 [1280.000, 1280.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    210/500000: episode: 209, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3404.000 [3404.000, 3404.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    211/500000: episode: 210, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    212/500000: episode: 211, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2931.000 [2931.000, 2931.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    213/500000: episode: 212, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2798.000 [2798.000, 2798.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    214/500000: episode: 213, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3710.000 [3710.000, 3710.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    215/500000: episode: 214, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    216/500000: episode: 215, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2953.000 [2953.000, 2953.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    217/500000: episode: 216, duration: 0.003s, episode steps:   1, steps per second: 338, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1421.000 [1421.000, 1421.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    218/500000: episode: 217, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1952.000 [1952.000, 1952.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    219/500000: episode: 218, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1171.000 [1171.000, 1171.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    220/500000: episode: 219, duration: 0.005s, episode steps:   1, steps per second: 208, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    221/500000: episode: 220, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3049.000 [3049.000, 3049.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    222/500000: episode: 221, duration: 0.006s, episode steps:   1, steps per second: 170, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    223/500000: episode: 222, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    224/500000: episode: 223, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3618.000 [3618.000, 3618.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    225/500000: episode: 224, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1173.000 [1173.000, 1173.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    226/500000: episode: 225, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    227/500000: episode: 226, duration: 0.007s, episode steps:   1, steps per second: 140, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    228/500000: episode: 227, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1272.000 [1272.000, 1272.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    229/500000: episode: 228, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    230/500000: episode: 229, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1325.000 [1325.000, 1325.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    231/500000: episode: 230, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    232/500000: episode: 231, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2756.000 [2756.000, 2756.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    233/500000: episode: 232, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2925.000 [2925.000, 2925.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    234/500000: episode: 233, duration: 0.009s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 477.000 [477.000, 477.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    235/500000: episode: 234, duration: 0.003s, episode steps:   1, steps per second: 286, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 490.000 [490.000, 490.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    236/500000: episode: 235, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3517.000 [3517.000, 3517.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    237/500000: episode: 236, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2668.000 [2668.000, 2668.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    238/500000: episode: 237, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    239/500000: episode: 238, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    240/500000: episode: 239, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 708.000 [708.000, 708.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    241/500000: episode: 240, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2252.000 [2252.000, 2252.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    242/500000: episode: 241, duration: 0.007s, episode steps:   1, steps per second: 150, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    243/500000: episode: 242, duration: 0.005s, episode steps:   1, steps per second: 194, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3045.000 [3045.000, 3045.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    244/500000: episode: 243, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 389.000 [389.000, 389.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    245/500000: episode: 244, duration: 0.006s, episode steps:   1, steps per second: 168, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    246/500000: episode: 245, duration: 0.007s, episode steps:   1, steps per second: 150, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    247/500000: episode: 246, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 14.000 [14.000, 14.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    248/500000: episode: 247, duration: 0.005s, episode steps:   1, steps per second: 192, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    249/500000: episode: 248, duration: 0.004s, episode steps:   1, steps per second: 271, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    250/500000: episode: 249, duration: 0.008s, episode steps:   1, steps per second: 132, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3661.000 [3661.000, 3661.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    251/500000: episode: 250, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    252/500000: episode: 251, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1170.000 [1170.000, 1170.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    253/500000: episode: 252, duration: 0.004s, episode steps:   1, steps per second: 268, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 727.000 [727.000, 727.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    254/500000: episode: 253, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 549.000 [549.000, 549.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    255/500000: episode: 254, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3053.000 [3053.000, 3053.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    256/500000: episode: 255, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    257/500000: episode: 256, duration: 0.010s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2347.000 [2347.000, 2347.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    258/500000: episode: 257, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2046.000 [2046.000, 2046.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    259/500000: episode: 258, duration: 0.005s, episode steps:   1, steps per second: 183, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 496.000 [496.000, 496.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    260/500000: episode: 259, duration: 0.007s, episode steps:   1, steps per second: 146, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 842.000 [842.000, 842.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    261/500000: episode: 260, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    262/500000: episode: 261, duration: 0.005s, episode steps:   1, steps per second: 207, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3209.000 [3209.000, 3209.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    263/500000: episode: 262, duration: 0.005s, episode steps:   1, steps per second: 208, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    264/500000: episode: 263, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 136.000 [136.000, 136.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    265/500000: episode: 264, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    266/500000: episode: 265, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1796.000 [1796.000, 1796.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    267/500000: episode: 266, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    268/500000: episode: 267, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    269/500000: episode: 268, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 583.000 [583.000, 583.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    270/500000: episode: 269, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 543.000 [543.000, 543.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    271/500000: episode: 270, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    272/500000: episode: 271, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 389.000 [389.000, 389.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    273/500000: episode: 272, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    274/500000: episode: 273, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    275/500000: episode: 274, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3661.000 [3661.000, 3661.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    276/500000: episode: 275, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    277/500000: episode: 276, duration: 0.006s, episode steps:   1, steps per second: 160, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2878.000 [2878.000, 2878.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    278/500000: episode: 277, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1272.000 [1272.000, 1272.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    279/500000: episode: 278, duration: 0.003s, episode steps:   1, steps per second: 344, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2180.000 [2180.000, 2180.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    280/500000: episode: 279, duration: 0.005s, episode steps:   1, steps per second: 188, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2696.000 [2696.000, 2696.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    281/500000: episode: 280, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    282/500000: episode: 281, duration: 0.007s, episode steps:   1, steps per second: 146, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 549.000 [549.000, 549.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    283/500000: episode: 282, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 253.000 [253.000, 253.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    284/500000: episode: 283, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3667.000 [3667.000, 3667.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    285/500000: episode: 284, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 477.000 [477.000, 477.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    286/500000: episode: 285, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    287/500000: episode: 286, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1664.000 [1664.000, 1664.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    288/500000: episode: 287, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 817.000 [817.000, 817.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    289/500000: episode: 288, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    290/500000: episode: 289, duration: 0.006s, episode steps:   1, steps per second: 160, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    291/500000: episode: 290, duration: 0.005s, episode steps:   1, steps per second: 189, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    292/500000: episode: 291, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1096.000 [1096.000, 1096.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    293/500000: episode: 292, duration: 0.006s, episode steps:   1, steps per second: 170, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3812.000 [3812.000, 3812.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    294/500000: episode: 293, duration: 0.004s, episode steps:   1, steps per second: 239, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    295/500000: episode: 294, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1925.000 [1925.000, 1925.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    296/500000: episode: 295, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    297/500000: episode: 296, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    298/500000: episode: 297, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1747.000 [1747.000, 1747.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    299/500000: episode: 298, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 845.000 [845.000, 845.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    300/500000: episode: 299, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 477.000 [477.000, 477.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    301/500000: episode: 300, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    302/500000: episode: 301, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    303/500000: episode: 302, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    304/500000: episode: 303, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3278.000 [3278.000, 3278.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    305/500000: episode: 304, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    306/500000: episode: 305, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3036.000 [3036.000, 3036.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    307/500000: episode: 306, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    308/500000: episode: 307, duration: 0.004s, episode steps:   1, steps per second: 238, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    309/500000: episode: 308, duration: 0.005s, episode steps:   1, steps per second: 195, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3897.000 [3897.000, 3897.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    310/500000: episode: 309, duration: 0.003s, episode steps:   1, steps per second: 322, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    311/500000: episode: 310, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1272.000 [1272.000, 1272.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    313/500000: episode: 311, duration: 0.039s, episode steps:   2, steps per second:  51, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 992.500 [666.000, 1319.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    314/500000: episode: 312, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1986.000 [1986.000, 1986.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    315/500000: episode: 313, duration: 0.005s, episode steps:   1, steps per second: 200, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    316/500000: episode: 314, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3993.000 [3993.000, 3993.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    317/500000: episode: 315, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1170.000 [1170.000, 1170.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    318/500000: episode: 316, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3875.000 [3875.000, 3875.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    319/500000: episode: 317, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3972.000 [3972.000, 3972.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    320/500000: episode: 318, duration: 0.004s, episode steps:   1, steps per second: 257, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    321/500000: episode: 319, duration: 0.004s, episode steps:   1, steps per second: 272, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 791.000 [791.000, 791.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    322/500000: episode: 320, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2728.000 [2728.000, 2728.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    323/500000: episode: 321, duration: 0.007s, episode steps:   1, steps per second: 150, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    324/500000: episode: 322, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1134.000 [1134.000, 1134.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    325/500000: episode: 323, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 895.000 [895.000, 895.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    326/500000: episode: 324, duration: 0.007s, episode steps:   1, steps per second: 138, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 218.000 [218.000, 218.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    327/500000: episode: 325, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    328/500000: episode: 326, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    329/500000: episode: 327, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    330/500000: episode: 328, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    331/500000: episode: 329, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2518.000 [2518.000, 2518.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    332/500000: episode: 330, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4087.000 [4087.000, 4087.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    333/500000: episode: 331, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    334/500000: episode: 332, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    335/500000: episode: 333, duration: 0.004s, episode steps:   1, steps per second: 244, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    336/500000: episode: 334, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    337/500000: episode: 335, duration: 0.006s, episode steps:   1, steps per second: 156, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    338/500000: episode: 336, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 324.000 [324.000, 324.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    339/500000: episode: 337, duration: 0.005s, episode steps:   1, steps per second: 183, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    340/500000: episode: 338, duration: 0.006s, episode steps:   1, steps per second: 164, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1012.000 [1012.000, 1012.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    341/500000: episode: 339, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    342/500000: episode: 340, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3404.000 [3404.000, 3404.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    343/500000: episode: 341, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    344/500000: episode: 342, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1861.000 [1861.000, 1861.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    345/500000: episode: 343, duration: 0.006s, episode steps:   1, steps per second: 162, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    346/500000: episode: 344, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2058.000 [2058.000, 2058.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    347/500000: episode: 345, duration: 0.005s, episode steps:   1, steps per second: 219, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    348/500000: episode: 346, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1170.000 [1170.000, 1170.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    349/500000: episode: 347, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    350/500000: episode: 348, duration: 0.004s, episode steps:   1, steps per second: 224, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    351/500000: episode: 349, duration: 0.003s, episode steps:   1, steps per second: 391, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    352/500000: episode: 350, duration: 0.004s, episode steps:   1, steps per second: 232, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    353/500000: episode: 351, duration: 0.004s, episode steps:   1, steps per second: 262, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 676.000 [676.000, 676.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    354/500000: episode: 352, duration: 0.007s, episode steps:   1, steps per second: 154, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    355/500000: episode: 353, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3947.000 [3947.000, 3947.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    356/500000: episode: 354, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1926.000 [1926.000, 1926.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    357/500000: episode: 355, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    358/500000: episode: 356, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 984.000 [984.000, 984.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    359/500000: episode: 357, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1280.000 [1280.000, 1280.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    360/500000: episode: 358, duration: 0.005s, episode steps:   1, steps per second: 185, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    361/500000: episode: 359, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3772.000 [3772.000, 3772.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    362/500000: episode: 360, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 895.000 [895.000, 895.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    363/500000: episode: 361, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 852.000 [852.000, 852.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    364/500000: episode: 362, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1327.000 [1327.000, 1327.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    365/500000: episode: 363, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2015.000 [2015.000, 2015.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    366/500000: episode: 364, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 749.000 [749.000, 749.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    367/500000: episode: 365, duration: 0.005s, episode steps:   1, steps per second: 218, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2314.000 [2314.000, 2314.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    368/500000: episode: 366, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3404.000 [3404.000, 3404.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    369/500000: episode: 367, duration: 0.007s, episode steps:   1, steps per second: 134, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 135.000 [135.000, 135.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    370/500000: episode: 368, duration: 0.007s, episode steps:   1, steps per second: 147, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    371/500000: episode: 369, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1272.000 [1272.000, 1272.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    372/500000: episode: 370, duration: 0.003s, episode steps:   1, steps per second: 330, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    373/500000: episode: 371, duration: 0.004s, episode steps:   1, steps per second: 268, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2780.000 [2780.000, 2780.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    374/500000: episode: 372, duration: 0.004s, episode steps:   1, steps per second: 279, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    375/500000: episode: 373, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    376/500000: episode: 374, duration: 0.005s, episode steps:   1, steps per second: 202, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    377/500000: episode: 375, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 608.000 [608.000, 608.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    378/500000: episode: 376, duration: 0.004s, episode steps:   1, steps per second: 268, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4028.000 [4028.000, 4028.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    379/500000: episode: 377, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2930.000 [2930.000, 2930.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    380/500000: episode: 378, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    381/500000: episode: 379, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 514.000 [514.000, 514.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    382/500000: episode: 380, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2347.000 [2347.000, 2347.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    383/500000: episode: 381, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1494.000 [1494.000, 1494.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    384/500000: episode: 382, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1821.000 [1821.000, 1821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    385/500000: episode: 383, duration: 0.004s, episode steps:   1, steps per second: 270, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3975.000 [3975.000, 3975.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    386/500000: episode: 384, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 630.000 [630.000, 630.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    387/500000: episode: 385, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3296.000 [3296.000, 3296.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    388/500000: episode: 386, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4001.000 [4001.000, 4001.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    389/500000: episode: 387, duration: 0.006s, episode steps:   1, steps per second: 167, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1439.000 [1439.000, 1439.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    390/500000: episode: 388, duration: 0.007s, episode steps:   1, steps per second: 138, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    391/500000: episode: 389, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    392/500000: episode: 390, duration: 0.005s, episode steps:   1, steps per second: 197, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    393/500000: episode: 391, duration: 0.003s, episode steps:   1, steps per second: 362, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3202.000 [3202.000, 3202.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    394/500000: episode: 392, duration: 0.003s, episode steps:   1, steps per second: 344, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2161.000 [2161.000, 2161.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    395/500000: episode: 393, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    396/500000: episode: 394, duration: 0.004s, episode steps:   1, steps per second: 232, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1875.000 [1875.000, 1875.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    397/500000: episode: 395, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1158.000 [1158.000, 1158.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    398/500000: episode: 396, duration: 0.006s, episode steps:   1, steps per second: 160, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1931.000 [1931.000, 1931.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    399/500000: episode: 397, duration: 0.004s, episode steps:   1, steps per second: 231, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3908.000 [3908.000, 3908.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    400/500000: episode: 398, duration: 0.005s, episode steps:   1, steps per second: 189, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    401/500000: episode: 399, duration: 0.006s, episode steps:   1, steps per second: 155, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    402/500000: episode: 400, duration: 0.002s, episode steps:   1, steps per second: 402, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    403/500000: episode: 401, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 852.000 [852.000, 852.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    404/500000: episode: 402, duration: 0.005s, episode steps:   1, steps per second: 213, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3500.000 [3500.000, 3500.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    405/500000: episode: 403, duration: 0.006s, episode steps:   1, steps per second: 175, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2489.000 [2489.000, 2489.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    406/500000: episode: 404, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 253.000 [253.000, 253.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    407/500000: episode: 405, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3631.000 [3631.000, 3631.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    408/500000: episode: 406, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    409/500000: episode: 407, duration: 0.005s, episode steps:   1, steps per second: 193, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    410/500000: episode: 408, duration: 0.004s, episode steps:   1, steps per second: 263, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2556.000 [2556.000, 2556.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    411/500000: episode: 409, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2650.000 [2650.000, 2650.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    412/500000: episode: 410, duration: 0.004s, episode steps:   1, steps per second: 233, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    413/500000: episode: 411, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 603.000 [603.000, 603.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    414/500000: episode: 412, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1265.000 [1265.000, 1265.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    415/500000: episode: 413, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1278.000 [1278.000, 1278.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    416/500000: episode: 414, duration: 0.004s, episode steps:   1, steps per second: 231, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1731.000 [1731.000, 1731.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    417/500000: episode: 415, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2015.000 [2015.000, 2015.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    418/500000: episode: 416, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3409.000 [3409.000, 3409.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    419/500000: episode: 417, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2735.000 [2735.000, 2735.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    420/500000: episode: 418, duration: 0.003s, episode steps:   1, steps per second: 397, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    421/500000: episode: 419, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 617.000 [617.000, 617.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    422/500000: episode: 420, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 389.000 [389.000, 389.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    423/500000: episode: 421, duration: 0.004s, episode steps:   1, steps per second: 269, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    424/500000: episode: 422, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3155.000 [3155.000, 3155.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    425/500000: episode: 423, duration: 0.004s, episode steps:   1, steps per second: 223, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3696.000 [3696.000, 3696.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    426/500000: episode: 424, duration: 0.003s, episode steps:   1, steps per second: 289, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    427/500000: episode: 425, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1280.000 [1280.000, 1280.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    428/500000: episode: 426, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1124.000 [1124.000, 1124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    429/500000: episode: 427, duration: 0.003s, episode steps:   1, steps per second: 319, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    430/500000: episode: 428, duration: 0.003s, episode steps:   1, steps per second: 324, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1141.000 [1141.000, 1141.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    431/500000: episode: 429, duration: 0.003s, episode steps:   1, steps per second: 293, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    432/500000: episode: 430, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1278.000 [1278.000, 1278.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    433/500000: episode: 431, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 279.000 [279.000, 279.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    434/500000: episode: 432, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 437.000 [437.000, 437.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    435/500000: episode: 433, duration: 0.007s, episode steps:   1, steps per second: 145, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3251.000 [3251.000, 3251.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    436/500000: episode: 434, duration: 0.003s, episode steps:   1, steps per second: 292, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    437/500000: episode: 435, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2347.000 [2347.000, 2347.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    438/500000: episode: 436, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    439/500000: episode: 437, duration: 0.004s, episode steps:   1, steps per second: 243, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    440/500000: episode: 438, duration: 0.006s, episode steps:   1, steps per second: 159, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    441/500000: episode: 439, duration: 0.009s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3367.000 [3367.000, 3367.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    442/500000: episode: 440, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3670.000 [3670.000, 3670.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    443/500000: episode: 441, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3123.000 [3123.000, 3123.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    444/500000: episode: 442, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4001.000 [4001.000, 4001.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    445/500000: episode: 443, duration: 0.006s, episode steps:   1, steps per second: 170, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    446/500000: episode: 444, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3661.000 [3661.000, 3661.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    447/500000: episode: 445, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    448/500000: episode: 446, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1008.000 [1008.000, 1008.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    449/500000: episode: 447, duration: 0.003s, episode steps:   1, steps per second: 368, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 50.000 [50.000, 50.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    450/500000: episode: 448, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3563.000 [3563.000, 3563.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    451/500000: episode: 449, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    452/500000: episode: 450, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    453/500000: episode: 451, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2281.000 [2281.000, 2281.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    454/500000: episode: 452, duration: 0.007s, episode steps:   1, steps per second: 145, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    455/500000: episode: 453, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2907.000 [2907.000, 2907.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    456/500000: episode: 454, duration: 0.006s, episode steps:   1, steps per second: 169, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4063.000 [4063.000, 4063.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    457/500000: episode: 455, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    458/500000: episode: 456, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3064.000 [3064.000, 3064.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    459/500000: episode: 457, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1806.000 [1806.000, 1806.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    460/500000: episode: 458, duration: 0.005s, episode steps:   1, steps per second: 214, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 677.000 [677.000, 677.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    461/500000: episode: 459, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2991.000 [2991.000, 2991.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    462/500000: episode: 460, duration: 0.005s, episode steps:   1, steps per second: 211, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    463/500000: episode: 461, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    464/500000: episode: 462, duration: 0.006s, episode steps:   1, steps per second: 163, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    465/500000: episode: 463, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    467/500000: episode: 464, duration: 0.021s, episode steps:   2, steps per second:  95, episode reward: -5021.000, mean reward: -2510.500 [-5000.000, -21.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    468/500000: episode: 465, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 912.000 [912.000, 912.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    469/500000: episode: 466, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    470/500000: episode: 467, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    471/500000: episode: 468, duration: 0.003s, episode steps:   1, steps per second: 355, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3968.000 [3968.000, 3968.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    472/500000: episode: 469, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2643.000 [2643.000, 2643.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    473/500000: episode: 470, duration: 0.006s, episode steps:   1, steps per second: 157, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2452.000 [2452.000, 2452.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    474/500000: episode: 471, duration: 0.005s, episode steps:   1, steps per second: 188, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    475/500000: episode: 472, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    476/500000: episode: 473, duration: 0.008s, episode steps:   1, steps per second: 132, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    477/500000: episode: 474, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    478/500000: episode: 475, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    479/500000: episode: 476, duration: 0.005s, episode steps:   1, steps per second: 195, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    480/500000: episode: 477, duration: 0.003s, episode steps:   1, steps per second: 297, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    481/500000: episode: 478, duration: 0.003s, episode steps:   1, steps per second: 332, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    482/500000: episode: 479, duration: 0.005s, episode steps:   1, steps per second: 208, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 136.000 [136.000, 136.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    483/500000: episode: 480, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1280.000 [1280.000, 1280.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    484/500000: episode: 481, duration: 0.005s, episode steps:   1, steps per second: 216, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 477.000 [477.000, 477.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    485/500000: episode: 482, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3763.000 [3763.000, 3763.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    486/500000: episode: 483, duration: 0.005s, episode steps:   1, steps per second: 186, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2650.000 [2650.000, 2650.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    487/500000: episode: 484, duration: 0.006s, episode steps:   1, steps per second: 172, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 641.000 [641.000, 641.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    488/500000: episode: 485, duration: 0.004s, episode steps:   1, steps per second: 280, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    489/500000: episode: 486, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 48.000 [48.000, 48.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    490/500000: episode: 487, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 180.000 [180.000, 180.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    491/500000: episode: 488, duration: 0.005s, episode steps:   1, steps per second: 204, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3661.000 [3661.000, 3661.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    492/500000: episode: 489, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    493/500000: episode: 490, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2615.000 [2615.000, 2615.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    494/500000: episode: 491, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 135.000 [135.000, 135.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    495/500000: episode: 492, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    496/500000: episode: 493, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3253.000 [3253.000, 3253.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    497/500000: episode: 494, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2931.000 [2931.000, 2931.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    498/500000: episode: 495, duration: 0.003s, episode steps:   1, steps per second: 305, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    499/500000: episode: 496, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    500/500000: episode: 497, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3269.000 [3269.000, 3269.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    501/500000: episode: 498, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    502/500000: episode: 499, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    504/500000: episode: 500, duration: 0.022s, episode steps:   2, steps per second:  92, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2428.000 [2347.000, 2509.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    505/500000: episode: 501, duration: 0.005s, episode steps:   1, steps per second: 203, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    506/500000: episode: 502, duration: 0.004s, episode steps:   1, steps per second: 278, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    507/500000: episode: 503, duration: 0.006s, episode steps:   1, steps per second: 171, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 211.000 [211.000, 211.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    508/500000: episode: 504, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1931.000 [1931.000, 1931.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    509/500000: episode: 505, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3937.000 [3937.000, 3937.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    510/500000: episode: 506, duration: 0.007s, episode steps:   1, steps per second: 154, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    511/500000: episode: 507, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    512/500000: episode: 508, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 830.000 [830.000, 830.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    513/500000: episode: 509, duration: 0.004s, episode steps:   1, steps per second: 223, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    514/500000: episode: 510, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 136.000 [136.000, 136.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    515/500000: episode: 511, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 136.000 [136.000, 136.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    516/500000: episode: 512, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2022.000 [2022.000, 2022.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    517/500000: episode: 513, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1113.000 [1113.000, 1113.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    518/500000: episode: 514, duration: 0.010s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2668.000 [2668.000, 2668.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    519/500000: episode: 515, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    520/500000: episode: 516, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 136.000 [136.000, 136.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    521/500000: episode: 517, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2833.000 [2833.000, 2833.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    522/500000: episode: 518, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3711.000 [3711.000, 3711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    523/500000: episode: 519, duration: 0.004s, episode steps:   1, steps per second: 256, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    524/500000: episode: 520, duration: 0.005s, episode steps:   1, steps per second: 206, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    525/500000: episode: 521, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    526/500000: episode: 522, duration: 0.003s, episode steps:   1, steps per second: 293, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    527/500000: episode: 523, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2476.000 [2476.000, 2476.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    528/500000: episode: 524, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    529/500000: episode: 525, duration: 0.005s, episode steps:   1, steps per second: 213, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    530/500000: episode: 526, duration: 0.003s, episode steps:   1, steps per second: 293, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    531/500000: episode: 527, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    532/500000: episode: 528, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1146.000 [1146.000, 1146.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    533/500000: episode: 529, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    534/500000: episode: 530, duration: 0.006s, episode steps:   1, steps per second: 174, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    535/500000: episode: 531, duration: 0.004s, episode steps:   1, steps per second: 264, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    536/500000: episode: 532, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4001.000 [4001.000, 4001.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    537/500000: episode: 533, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1083.000 [1083.000, 1083.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    538/500000: episode: 534, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    539/500000: episode: 535, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1737.000 [1737.000, 1737.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    540/500000: episode: 536, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    541/500000: episode: 537, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    542/500000: episode: 538, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 690.000 [690.000, 690.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    543/500000: episode: 539, duration: 0.006s, episode steps:   1, steps per second: 163, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3157.000 [3157.000, 3157.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    544/500000: episode: 540, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    545/500000: episode: 541, duration: 0.005s, episode steps:   1, steps per second: 184, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    546/500000: episode: 542, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 136.000 [136.000, 136.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    547/500000: episode: 543, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 895.000 [895.000, 895.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    548/500000: episode: 544, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3803.000 [3803.000, 3803.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    549/500000: episode: 545, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    550/500000: episode: 546, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    551/500000: episode: 547, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 311.000 [311.000, 311.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    552/500000: episode: 548, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    553/500000: episode: 549, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2396.000 [2396.000, 2396.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    554/500000: episode: 550, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1879.000 [1879.000, 1879.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    555/500000: episode: 551, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    556/500000: episode: 552, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    557/500000: episode: 553, duration: 0.007s, episode steps:   1, steps per second: 138, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    558/500000: episode: 554, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 135.000 [135.000, 135.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    559/500000: episode: 555, duration: 0.004s, episode steps:   1, steps per second: 237, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    560/500000: episode: 556, duration: 0.007s, episode steps:   1, steps per second: 138, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    561/500000: episode: 557, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    562/500000: episode: 558, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1918.000 [1918.000, 1918.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    563/500000: episode: 559, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    564/500000: episode: 560, duration: 0.004s, episode steps:   1, steps per second: 246, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2026.000 [2026.000, 2026.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    565/500000: episode: 561, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1602.000 [1602.000, 1602.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    566/500000: episode: 562, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4001.000 [4001.000, 4001.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    567/500000: episode: 563, duration: 0.005s, episode steps:   1, steps per second: 206, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1136.000 [1136.000, 1136.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    568/500000: episode: 564, duration: 0.007s, episode steps:   1, steps per second: 147, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    569/500000: episode: 565, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 391.000 [391.000, 391.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    570/500000: episode: 566, duration: 0.005s, episode steps:   1, steps per second: 206, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    571/500000: episode: 567, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3251.000 [3251.000, 3251.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    572/500000: episode: 568, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1257.000 [1257.000, 1257.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    573/500000: episode: 569, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 964.000 [964.000, 964.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    574/500000: episode: 570, duration: 0.006s, episode steps:   1, steps per second: 176, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 477.000 [477.000, 477.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    575/500000: episode: 571, duration: 0.005s, episode steps:   1, steps per second: 207, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    576/500000: episode: 572, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3826.000 [3826.000, 3826.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    577/500000: episode: 573, duration: 0.004s, episode steps:   1, steps per second: 234, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    578/500000: episode: 574, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    579/500000: episode: 575, duration: 0.004s, episode steps:   1, steps per second: 222, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    580/500000: episode: 576, duration: 0.005s, episode steps:   1, steps per second: 187, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 186.000 [186.000, 186.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    581/500000: episode: 577, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2239.000 [2239.000, 2239.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    582/500000: episode: 578, duration: 0.006s, episode steps:   1, steps per second: 175, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    583/500000: episode: 579, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 274.000 [274.000, 274.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    584/500000: episode: 580, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 174.000 [174.000, 174.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    585/500000: episode: 581, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2668.000 [2668.000, 2668.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    586/500000: episode: 582, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1280.000 [1280.000, 1280.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    587/500000: episode: 583, duration: 0.003s, episode steps:   1, steps per second: 297, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    588/500000: episode: 584, duration: 0.004s, episode steps:   1, steps per second: 234, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    589/500000: episode: 585, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2347.000 [2347.000, 2347.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    590/500000: episode: 586, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    591/500000: episode: 587, duration: 0.005s, episode steps:   1, steps per second: 182, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    592/500000: episode: 588, duration: 0.006s, episode steps:   1, steps per second: 165, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3916.000 [3916.000, 3916.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    593/500000: episode: 589, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2139.000 [2139.000, 2139.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    594/500000: episode: 590, duration: 0.003s, episode steps:   1, steps per second: 313, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    595/500000: episode: 591, duration: 0.006s, episode steps:   1, steps per second: 163, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    596/500000: episode: 592, duration: 0.006s, episode steps:   1, steps per second: 157, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    597/500000: episode: 593, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    598/500000: episode: 594, duration: 0.006s, episode steps:   1, steps per second: 164, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    599/500000: episode: 595, duration: 0.003s, episode steps:   1, steps per second: 321, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    600/500000: episode: 596, duration: 0.005s, episode steps:   1, steps per second: 197, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    601/500000: episode: 597, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    602/500000: episode: 598, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2965.000 [2965.000, 2965.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    603/500000: episode: 599, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2210.000 [2210.000, 2210.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    604/500000: episode: 600, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2605.000 [2605.000, 2605.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    605/500000: episode: 601, duration: 0.005s, episode steps:   1, steps per second: 203, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    606/500000: episode: 602, duration: 0.006s, episode steps:   1, steps per second: 181, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3140.000 [3140.000, 3140.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    607/500000: episode: 603, duration: 0.003s, episode steps:   1, steps per second: 290, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    608/500000: episode: 604, duration: 0.007s, episode steps:   1, steps per second: 150, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 388.000 [388.000, 388.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    609/500000: episode: 605, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    610/500000: episode: 606, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    611/500000: episode: 607, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    612/500000: episode: 608, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3404.000 [3404.000, 3404.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    613/500000: episode: 609, duration: 0.005s, episode steps:   1, steps per second: 215, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    614/500000: episode: 610, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3404.000 [3404.000, 3404.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    615/500000: episode: 611, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    616/500000: episode: 612, duration: 0.004s, episode steps:   1, steps per second: 246, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    617/500000: episode: 613, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4022.000 [4022.000, 4022.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    618/500000: episode: 614, duration: 0.004s, episode steps:   1, steps per second: 276, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2811.000 [2811.000, 2811.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    619/500000: episode: 615, duration: 0.004s, episode steps:   1, steps per second: 266, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1841.000 [1841.000, 1841.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    620/500000: episode: 616, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    621/500000: episode: 617, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 219.000 [219.000, 219.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    622/500000: episode: 618, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    623/500000: episode: 619, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1918.000 [1918.000, 1918.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    624/500000: episode: 620, duration: 0.005s, episode steps:   1, steps per second: 219, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    625/500000: episode: 621, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2347.000 [2347.000, 2347.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    626/500000: episode: 622, duration: 0.003s, episode steps:   1, steps per second: 298, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2849.000 [2849.000, 2849.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    627/500000: episode: 623, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 389.000 [389.000, 389.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    628/500000: episode: 624, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2991.000 [2991.000, 2991.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    629/500000: episode: 625, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3404.000 [3404.000, 3404.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    630/500000: episode: 626, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2347.000 [2347.000, 2347.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    631/500000: episode: 627, duration: 0.004s, episode steps:   1, steps per second: 234, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    632/500000: episode: 628, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1134.000 [1134.000, 1134.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    633/500000: episode: 629, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    634/500000: episode: 630, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1683.000 [1683.000, 1683.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    635/500000: episode: 631, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2313.000 [2313.000, 2313.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    636/500000: episode: 632, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1821.000 [1821.000, 1821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    637/500000: episode: 633, duration: 0.005s, episode steps:   1, steps per second: 208, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    638/500000: episode: 634, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1230.000 [1230.000, 1230.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    639/500000: episode: 635, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1289.000 [1289.000, 1289.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    640/500000: episode: 636, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    641/500000: episode: 637, duration: 0.007s, episode steps:   1, steps per second: 138, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    642/500000: episode: 638, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 136.000 [136.000, 136.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    643/500000: episode: 639, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 136.000 [136.000, 136.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    644/500000: episode: 640, duration: 0.006s, episode steps:   1, steps per second: 182, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    645/500000: episode: 641, duration: 0.004s, episode steps:   1, steps per second: 242, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    646/500000: episode: 642, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    647/500000: episode: 643, duration: 0.005s, episode steps:   1, steps per second: 205, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 676.000 [676.000, 676.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    648/500000: episode: 644, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    649/500000: episode: 645, duration: 0.005s, episode steps:   1, steps per second: 192, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    650/500000: episode: 646, duration: 0.003s, episode steps:   1, steps per second: 296, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 916.000 [916.000, 916.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    651/500000: episode: 647, duration: 0.003s, episode steps:   1, steps per second: 324, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    652/500000: episode: 648, duration: 0.004s, episode steps:   1, steps per second: 231, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3251.000 [3251.000, 3251.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    653/500000: episode: 649, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2518.000 [2518.000, 2518.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    654/500000: episode: 650, duration: 0.005s, episode steps:   1, steps per second: 189, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2347.000 [2347.000, 2347.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    655/500000: episode: 651, duration: 0.002s, episode steps:   1, steps per second: 410, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    656/500000: episode: 652, duration: 0.007s, episode steps:   1, steps per second: 145, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    657/500000: episode: 653, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1212.000 [1212.000, 1212.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    658/500000: episode: 654, duration: 0.005s, episode steps:   1, steps per second: 205, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 228.000 [228.000, 228.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    659/500000: episode: 655, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    660/500000: episode: 656, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    661/500000: episode: 657, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3469.000 [3469.000, 3469.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    662/500000: episode: 658, duration: 0.005s, episode steps:   1, steps per second: 213, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1148.000 [1148.000, 1148.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    663/500000: episode: 659, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    664/500000: episode: 660, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    665/500000: episode: 661, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1134.000 [1134.000, 1134.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    666/500000: episode: 662, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1975.000 [1975.000, 1975.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    667/500000: episode: 663, duration: 0.008s, episode steps:   1, steps per second: 132, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3725.000 [3725.000, 3725.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    668/500000: episode: 664, duration: 0.004s, episode steps:   1, steps per second: 224, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    669/500000: episode: 665, duration: 0.003s, episode steps:   1, steps per second: 292, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    670/500000: episode: 666, duration: 0.006s, episode steps:   1, steps per second: 158, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    671/500000: episode: 667, duration: 0.006s, episode steps:   1, steps per second: 162, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    672/500000: episode: 668, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1925.000 [1925.000, 1925.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    673/500000: episode: 669, duration: 0.006s, episode steps:   1, steps per second: 158, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    674/500000: episode: 670, duration: 0.007s, episode steps:   1, steps per second: 153, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    675/500000: episode: 671, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2931.000 [2931.000, 2931.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    676/500000: episode: 672, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1837.000 [1837.000, 1837.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    677/500000: episode: 673, duration: 0.003s, episode steps:   1, steps per second: 363, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3450.000 [3450.000, 3450.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    678/500000: episode: 674, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3131.000 [3131.000, 3131.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    679/500000: episode: 675, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3131.000 [3131.000, 3131.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    680/500000: episode: 676, duration: 0.004s, episode steps:   1, steps per second: 224, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    681/500000: episode: 677, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    682/500000: episode: 678, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    683/500000: episode: 679, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2639.000 [2639.000, 2639.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    684/500000: episode: 680, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1280.000 [1280.000, 1280.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    685/500000: episode: 681, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    686/500000: episode: 682, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 106.000 [106.000, 106.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    687/500000: episode: 683, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4052.000 [4052.000, 4052.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    688/500000: episode: 684, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 173.000 [173.000, 173.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    689/500000: episode: 685, duration: 0.007s, episode steps:   1, steps per second: 151, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4078.000 [4078.000, 4078.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    690/500000: episode: 686, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    691/500000: episode: 687, duration: 0.005s, episode steps:   1, steps per second: 190, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    692/500000: episode: 688, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    693/500000: episode: 689, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2579.000 [2579.000, 2579.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    694/500000: episode: 690, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 389.000 [389.000, 389.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    695/500000: episode: 691, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 834.000 [834.000, 834.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    696/500000: episode: 692, duration: 0.008s, episode steps:   1, steps per second: 133, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    697/500000: episode: 693, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3137.000 [3137.000, 3137.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    698/500000: episode: 694, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2448.000 [2448.000, 2448.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    699/500000: episode: 695, duration: 0.003s, episode steps:   1, steps per second: 338, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    700/500000: episode: 696, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    701/500000: episode: 697, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    702/500000: episode: 698, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1272.000 [1272.000, 1272.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    703/500000: episode: 699, duration: 0.005s, episode steps:   1, steps per second: 199, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 135.000 [135.000, 135.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    704/500000: episode: 700, duration: 0.003s, episode steps:   1, steps per second: 311, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3713.000 [3713.000, 3713.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    705/500000: episode: 701, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    706/500000: episode: 702, duration: 0.004s, episode steps:   1, steps per second: 230, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    707/500000: episode: 703, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    708/500000: episode: 704, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    709/500000: episode: 705, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    710/500000: episode: 706, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3661.000 [3661.000, 3661.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    711/500000: episode: 707, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 227.000 [227.000, 227.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    712/500000: episode: 708, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2051.000 [2051.000, 2051.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    713/500000: episode: 709, duration: 0.004s, episode steps:   1, steps per second: 235, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 40.000 [40.000, 40.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    714/500000: episode: 710, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 549.000 [549.000, 549.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    715/500000: episode: 711, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 136.000 [136.000, 136.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    716/500000: episode: 712, duration: 0.004s, episode steps:   1, steps per second: 247, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    717/500000: episode: 713, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    718/500000: episode: 714, duration: 0.004s, episode steps:   1, steps per second: 224, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    719/500000: episode: 715, duration: 0.007s, episode steps:   1, steps per second: 150, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    720/500000: episode: 716, duration: 0.004s, episode steps:   1, steps per second: 235, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    721/500000: episode: 717, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    722/500000: episode: 718, duration: 0.005s, episode steps:   1, steps per second: 201, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    723/500000: episode: 719, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 14.000 [14.000, 14.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    724/500000: episode: 720, duration: 0.005s, episode steps:   1, steps per second: 222, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2499.000 [2499.000, 2499.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    725/500000: episode: 721, duration: 0.005s, episode steps:   1, steps per second: 183, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 130.000 [130.000, 130.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    726/500000: episode: 722, duration: 0.003s, episode steps:   1, steps per second: 289, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2909.000 [2909.000, 2909.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    727/500000: episode: 723, duration: 0.004s, episode steps:   1, steps per second: 229, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 989.000 [989.000, 989.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    728/500000: episode: 724, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    729/500000: episode: 725, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2814.000 [2814.000, 2814.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    730/500000: episode: 726, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    731/500000: episode: 727, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 477.000 [477.000, 477.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    732/500000: episode: 728, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3251.000 [3251.000, 3251.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    733/500000: episode: 729, duration: 0.005s, episode steps:   1, steps per second: 209, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    734/500000: episode: 730, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1159.000 [1159.000, 1159.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    735/500000: episode: 731, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3404.000 [3404.000, 3404.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    736/500000: episode: 732, duration: 0.006s, episode steps:   1, steps per second: 159, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    737/500000: episode: 733, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    738/500000: episode: 734, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    739/500000: episode: 735, duration: 0.004s, episode steps:   1, steps per second: 258, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    740/500000: episode: 736, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    741/500000: episode: 737, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    742/500000: episode: 738, duration: 0.006s, episode steps:   1, steps per second: 155, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 181.000 [181.000, 181.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    743/500000: episode: 739, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3404.000 [3404.000, 3404.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    744/500000: episode: 740, duration: 0.007s, episode steps:   1, steps per second: 150, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    745/500000: episode: 741, duration: 0.003s, episode steps:   1, steps per second: 295, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    746/500000: episode: 742, duration: 0.003s, episode steps:   1, steps per second: 286, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    747/500000: episode: 743, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    748/500000: episode: 744, duration: 0.005s, episode steps:   1, steps per second: 204, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    749/500000: episode: 745, duration: 0.004s, episode steps:   1, steps per second: 259, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2518.000 [2518.000, 2518.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    750/500000: episode: 746, duration: 0.006s, episode steps:   1, steps per second: 174, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1758.000 [1758.000, 1758.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    751/500000: episode: 747, duration: 0.005s, episode steps:   1, steps per second: 195, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    752/500000: episode: 748, duration: 0.005s, episode steps:   1, steps per second: 184, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2907.000 [2907.000, 2907.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    753/500000: episode: 749, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2251.000 [2251.000, 2251.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    754/500000: episode: 750, duration: 0.004s, episode steps:   1, steps per second: 236, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    755/500000: episode: 751, duration: 0.005s, episode steps:   1, steps per second: 220, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    756/500000: episode: 752, duration: 0.004s, episode steps:   1, steps per second: 235, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    757/500000: episode: 753, duration: 0.006s, episode steps:   1, steps per second: 167, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    758/500000: episode: 754, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    759/500000: episode: 755, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    760/500000: episode: 756, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 12.000 [12.000, 12.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    761/500000: episode: 757, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 830.000 [830.000, 830.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    762/500000: episode: 758, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3404.000 [3404.000, 3404.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    763/500000: episode: 759, duration: 0.006s, episode steps:   1, steps per second: 165, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2860.000 [2860.000, 2860.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    764/500000: episode: 760, duration: 0.006s, episode steps:   1, steps per second: 156, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2284.000 [2284.000, 2284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    765/500000: episode: 761, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3889.000 [3889.000, 3889.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    766/500000: episode: 762, duration: 0.004s, episode steps:   1, steps per second: 261, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    767/500000: episode: 763, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1397.000 [1397.000, 1397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    768/500000: episode: 764, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 136.000 [136.000, 136.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    769/500000: episode: 765, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    770/500000: episode: 766, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 136.000 [136.000, 136.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    771/500000: episode: 767, duration: 0.006s, episode steps:   1, steps per second: 159, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2328.000 [2328.000, 2328.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    772/500000: episode: 768, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    773/500000: episode: 769, duration: 0.006s, episode steps:   1, steps per second: 158, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    774/500000: episode: 770, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    775/500000: episode: 771, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1821.000 [1821.000, 1821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    776/500000: episode: 772, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    777/500000: episode: 773, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 530.000 [530.000, 530.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    778/500000: episode: 774, duration: 0.006s, episode steps:   1, steps per second: 170, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1866.000 [1866.000, 1866.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    779/500000: episode: 775, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1139.000 [1139.000, 1139.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    780/500000: episode: 776, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    781/500000: episode: 777, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    782/500000: episode: 778, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    783/500000: episode: 779, duration: 0.004s, episode steps:   1, steps per second: 249, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 692.000 [692.000, 692.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    784/500000: episode: 780, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 895.000 [895.000, 895.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    785/500000: episode: 781, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    786/500000: episode: 782, duration: 0.005s, episode steps:   1, steps per second: 205, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    787/500000: episode: 783, duration: 0.005s, episode steps:   1, steps per second: 214, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    788/500000: episode: 784, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 389.000 [389.000, 389.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    789/500000: episode: 785, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    790/500000: episode: 786, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    791/500000: episode: 787, duration: 0.006s, episode steps:   1, steps per second: 164, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3172.000 [3172.000, 3172.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    792/500000: episode: 788, duration: 0.004s, episode steps:   1, steps per second: 276, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    793/500000: episode: 789, duration: 0.005s, episode steps:   1, steps per second: 220, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    794/500000: episode: 790, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1280.000 [1280.000, 1280.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    795/500000: episode: 791, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2347.000 [2347.000, 2347.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    796/500000: episode: 792, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    797/500000: episode: 793, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    798/500000: episode: 794, duration: 0.010s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1681.000 [1681.000, 1681.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    799/500000: episode: 795, duration: 0.005s, episode steps:   1, steps per second: 183, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3488.000 [3488.000, 3488.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    800/500000: episode: 796, duration: 0.003s, episode steps:   1, steps per second: 313, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    801/500000: episode: 797, duration: 0.005s, episode steps:   1, steps per second: 184, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    802/500000: episode: 798, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 253.000 [253.000, 253.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    803/500000: episode: 799, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2844.000 [2844.000, 2844.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    804/500000: episode: 800, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2888.000 [2888.000, 2888.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    805/500000: episode: 801, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1170.000 [1170.000, 1170.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    806/500000: episode: 802, duration: 0.006s, episode steps:   1, steps per second: 165, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    807/500000: episode: 803, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 354.000 [354.000, 354.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    808/500000: episode: 804, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2237.000 [2237.000, 2237.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    809/500000: episode: 805, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3886.000 [3886.000, 3886.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    810/500000: episode: 806, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 389.000 [389.000, 389.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    811/500000: episode: 807, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 717.000 [717.000, 717.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    812/500000: episode: 808, duration: 0.005s, episode steps:   1, steps per second: 185, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1209.000 [1209.000, 1209.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    813/500000: episode: 809, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 977.000 [977.000, 977.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    814/500000: episode: 810, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    815/500000: episode: 811, duration: 0.005s, episode steps:   1, steps per second: 190, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    816/500000: episode: 812, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1009.000 [1009.000, 1009.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    817/500000: episode: 813, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2251.000 [2251.000, 2251.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    818/500000: episode: 814, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    819/500000: episode: 815, duration: 0.005s, episode steps:   1, steps per second: 189, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    820/500000: episode: 816, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    821/500000: episode: 817, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1715.000 [1715.000, 1715.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    822/500000: episode: 818, duration: 0.003s, episode steps:   1, steps per second: 375, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2492.000 [2492.000, 2492.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    823/500000: episode: 819, duration: 0.006s, episode steps:   1, steps per second: 170, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    824/500000: episode: 820, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    825/500000: episode: 821, duration: 0.005s, episode steps:   1, steps per second: 197, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    826/500000: episode: 822, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1272.000 [1272.000, 1272.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    827/500000: episode: 823, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    828/500000: episode: 824, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    829/500000: episode: 825, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 118.000 [118.000, 118.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    830/500000: episode: 826, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    831/500000: episode: 827, duration: 0.004s, episode steps:   1, steps per second: 247, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    832/500000: episode: 828, duration: 0.010s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2139.000 [2139.000, 2139.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    833/500000: episode: 829, duration: 0.005s, episode steps:   1, steps per second: 203, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    834/500000: episode: 830, duration: 0.004s, episode steps:   1, steps per second: 237, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    835/500000: episode: 831, duration: 0.003s, episode steps:   1, steps per second: 338, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 383.000 [383.000, 383.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    836/500000: episode: 832, duration: 0.003s, episode steps:   1, steps per second: 297, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    837/500000: episode: 833, duration: 0.006s, episode steps:   1, steps per second: 167, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    838/500000: episode: 834, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2347.000 [2347.000, 2347.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    839/500000: episode: 835, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    840/500000: episode: 836, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2019.000 [2019.000, 2019.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    841/500000: episode: 837, duration: 0.004s, episode steps:   1, steps per second: 226, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    842/500000: episode: 838, duration: 0.003s, episode steps:   1, steps per second: 306, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3918.000 [3918.000, 3918.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    843/500000: episode: 839, duration: 0.004s, episode steps:   1, steps per second: 280, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    844/500000: episode: 840, duration: 0.005s, episode steps:   1, steps per second: 216, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    845/500000: episode: 841, duration: 0.006s, episode steps:   1, steps per second: 173, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    846/500000: episode: 842, duration: 0.002s, episode steps:   1, steps per second: 445, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 448.000 [448.000, 448.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    847/500000: episode: 843, duration: 0.004s, episode steps:   1, steps per second: 249, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2799.000 [2799.000, 2799.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    848/500000: episode: 844, duration: 0.005s, episode steps:   1, steps per second: 211, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    849/500000: episode: 845, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 267.000 [267.000, 267.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    850/500000: episode: 846, duration: 0.004s, episode steps:   1, steps per second: 224, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1059.000 [1059.000, 1059.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    851/500000: episode: 847, duration: 0.007s, episode steps:   1, steps per second: 145, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1452.000 [1452.000, 1452.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    852/500000: episode: 848, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    853/500000: episode: 849, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1871.000 [1871.000, 1871.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    854/500000: episode: 850, duration: 0.007s, episode steps:   1, steps per second: 153, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    855/500000: episode: 851, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    856/500000: episode: 852, duration: 0.005s, episode steps:   1, steps per second: 208, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    857/500000: episode: 853, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 257.000 [257.000, 257.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    858/500000: episode: 854, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3661.000 [3661.000, 3661.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    859/500000: episode: 855, duration: 0.004s, episode steps:   1, steps per second: 276, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    860/500000: episode: 856, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1272.000 [1272.000, 1272.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    861/500000: episode: 857, duration: 0.004s, episode steps:   1, steps per second: 240, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    862/500000: episode: 858, duration: 0.006s, episode steps:   1, steps per second: 180, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3928.000 [3928.000, 3928.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    863/500000: episode: 859, duration: 0.003s, episode steps:   1, steps per second: 334, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    864/500000: episode: 860, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    865/500000: episode: 861, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 59.000 [59.000, 59.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    866/500000: episode: 862, duration: 0.003s, episode steps:   1, steps per second: 374, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    867/500000: episode: 863, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    868/500000: episode: 864, duration: 0.003s, episode steps:   1, steps per second: 289, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 111.000 [111.000, 111.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    869/500000: episode: 865, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2347.000 [2347.000, 2347.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    870/500000: episode: 866, duration: 0.006s, episode steps:   1, steps per second: 173, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    871/500000: episode: 867, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3155.000 [3155.000, 3155.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    872/500000: episode: 868, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 549.000 [549.000, 549.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    873/500000: episode: 869, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1526.000 [1526.000, 1526.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    874/500000: episode: 870, duration: 0.005s, episode steps:   1, steps per second: 197, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2699.000 [2699.000, 2699.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    875/500000: episode: 871, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    876/500000: episode: 872, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2215.000 [2215.000, 2215.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    877/500000: episode: 873, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    878/500000: episode: 874, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    879/500000: episode: 875, duration: 0.006s, episode steps:   1, steps per second: 178, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1523.000 [1523.000, 1523.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    880/500000: episode: 876, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 568.000 [568.000, 568.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    881/500000: episode: 877, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    882/500000: episode: 878, duration: 0.004s, episode steps:   1, steps per second: 253, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3192.000 [3192.000, 3192.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    883/500000: episode: 879, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    884/500000: episode: 880, duration: 0.008s, episode steps:   1, steps per second: 133, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 57.000 [57.000, 57.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    885/500000: episode: 881, duration: 0.004s, episode steps:   1, steps per second: 242, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    886/500000: episode: 882, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3725.000 [3725.000, 3725.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    887/500000: episode: 883, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1715.000 [1715.000, 1715.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    888/500000: episode: 884, duration: 0.004s, episode steps:   1, steps per second: 248, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    889/500000: episode: 885, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3191.000 [3191.000, 3191.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    890/500000: episode: 886, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3970.000 [3970.000, 3970.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    891/500000: episode: 887, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    892/500000: episode: 888, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    893/500000: episode: 889, duration: 0.008s, episode steps:   1, steps per second: 133, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2305.000 [2305.000, 2305.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    894/500000: episode: 890, duration: 0.004s, episode steps:   1, steps per second: 241, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    895/500000: episode: 891, duration: 0.003s, episode steps:   1, steps per second: 352, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    896/500000: episode: 892, duration: 0.006s, episode steps:   1, steps per second: 181, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    897/500000: episode: 893, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    898/500000: episode: 894, duration: 0.009s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2347.000 [2347.000, 2347.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    899/500000: episode: 895, duration: 0.009s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    900/500000: episode: 896, duration: 0.006s, episode steps:   1, steps per second: 176, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    901/500000: episode: 897, duration: 0.003s, episode steps:   1, steps per second: 328, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    902/500000: episode: 898, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 510.000 [510.000, 510.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    903/500000: episode: 899, duration: 0.005s, episode steps:   1, steps per second: 212, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    904/500000: episode: 900, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2251.000 [2251.000, 2251.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    905/500000: episode: 901, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2441.000 [2441.000, 2441.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    906/500000: episode: 902, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3404.000 [3404.000, 3404.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    907/500000: episode: 903, duration: 0.006s, episode steps:   1, steps per second: 177, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    908/500000: episode: 904, duration: 0.005s, episode steps:   1, steps per second: 212, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3698.000 [3698.000, 3698.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    909/500000: episode: 905, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2737.000 [2737.000, 2737.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    910/500000: episode: 906, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    911/500000: episode: 907, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    912/500000: episode: 908, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2696.000 [2696.000, 2696.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    913/500000: episode: 909, duration: 0.006s, episode steps:   1, steps per second: 180, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    914/500000: episode: 910, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    915/500000: episode: 911, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    916/500000: episode: 912, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1923.000 [1923.000, 1923.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    917/500000: episode: 913, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    918/500000: episode: 914, duration: 0.006s, episode steps:   1, steps per second: 178, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2438.000 [2438.000, 2438.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    919/500000: episode: 915, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    920/500000: episode: 916, duration: 0.004s, episode steps:   1, steps per second: 282, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    921/500000: episode: 917, duration: 0.005s, episode steps:   1, steps per second: 221, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    922/500000: episode: 918, duration: 0.003s, episode steps:   1, steps per second: 381, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1688.000 [1688.000, 1688.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    923/500000: episode: 919, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1435.000 [1435.000, 1435.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    924/500000: episode: 920, duration: 0.006s, episode steps:   1, steps per second: 158, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    925/500000: episode: 921, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2568.000 [2568.000, 2568.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    926/500000: episode: 922, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 549.000 [549.000, 549.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    927/500000: episode: 923, duration: 0.003s, episode steps:   1, steps per second: 361, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    928/500000: episode: 924, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    929/500000: episode: 925, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3620.000 [3620.000, 3620.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    930/500000: episode: 926, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1326.000 [1326.000, 1326.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    931/500000: episode: 927, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3173.000 [3173.000, 3173.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    932/500000: episode: 928, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3404.000 [3404.000, 3404.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    933/500000: episode: 929, duration: 0.005s, episode steps:   1, steps per second: 185, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1388.000 [1388.000, 1388.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    934/500000: episode: 930, duration: 0.003s, episode steps:   1, steps per second: 356, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4071.000 [4071.000, 4071.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    935/500000: episode: 931, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    936/500000: episode: 932, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2811.000 [2811.000, 2811.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    937/500000: episode: 933, duration: 0.004s, episode steps:   1, steps per second: 251, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    938/500000: episode: 934, duration: 0.005s, episode steps:   1, steps per second: 222, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    939/500000: episode: 935, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 595.000 [595.000, 595.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    940/500000: episode: 936, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 340.000 [340.000, 340.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    941/500000: episode: 937, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 51.000 [51.000, 51.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    942/500000: episode: 938, duration: 0.003s, episode steps:   1, steps per second: 355, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3345.000 [3345.000, 3345.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    943/500000: episode: 939, duration: 0.003s, episode steps:   1, steps per second: 317, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    944/500000: episode: 940, duration: 0.003s, episode steps:   1, steps per second: 339, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    945/500000: episode: 941, duration: 0.006s, episode steps:   1, steps per second: 157, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    946/500000: episode: 942, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    947/500000: episode: 943, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1871.000 [1871.000, 1871.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    948/500000: episode: 944, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    949/500000: episode: 945, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2470.000 [2470.000, 2470.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    950/500000: episode: 946, duration: 0.005s, episode steps:   1, steps per second: 210, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    951/500000: episode: 947, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    952/500000: episode: 948, duration: 0.006s, episode steps:   1, steps per second: 154, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2907.000 [2907.000, 2907.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    953/500000: episode: 949, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1544.000 [1544.000, 1544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    954/500000: episode: 950, duration: 0.006s, episode steps:   1, steps per second: 174, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2638.000 [2638.000, 2638.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    955/500000: episode: 951, duration: 0.007s, episode steps:   1, steps per second: 146, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    956/500000: episode: 952, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 589.000 [589.000, 589.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    957/500000: episode: 953, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2668.000 [2668.000, 2668.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    958/500000: episode: 954, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    959/500000: episode: 955, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2980.000 [2980.000, 2980.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    960/500000: episode: 956, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    961/500000: episode: 957, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1213.000 [1213.000, 1213.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    962/500000: episode: 958, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    963/500000: episode: 959, duration: 0.006s, episode steps:   1, steps per second: 169, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 212.000 [212.000, 212.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    964/500000: episode: 960, duration: 0.004s, episode steps:   1, steps per second: 255, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    965/500000: episode: 961, duration: 0.006s, episode steps:   1, steps per second: 170, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    966/500000: episode: 962, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 977.000 [977.000, 977.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    967/500000: episode: 963, duration: 0.007s, episode steps:   1, steps per second: 140, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3970.000 [3970.000, 3970.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    968/500000: episode: 964, duration: 0.004s, episode steps:   1, steps per second: 235, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    969/500000: episode: 965, duration: 0.007s, episode steps:   1, steps per second: 138, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1203.000 [1203.000, 1203.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    970/500000: episode: 966, duration: 0.005s, episode steps:   1, steps per second: 186, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    971/500000: episode: 967, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 477.000 [477.000, 477.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    972/500000: episode: 968, duration: 0.006s, episode steps:   1, steps per second: 154, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3661.000 [3661.000, 3661.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    973/500000: episode: 969, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2139.000 [2139.000, 2139.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    974/500000: episode: 970, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1715.000 [1715.000, 1715.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    975/500000: episode: 971, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    976/500000: episode: 972, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1397.000 [1397.000, 1397.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    977/500000: episode: 973, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3473.000 [3473.000, 3473.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    978/500000: episode: 974, duration: 0.005s, episode steps:   1, steps per second: 207, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1626.000 [1626.000, 1626.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    979/500000: episode: 975, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    980/500000: episode: 976, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1054.000 [1054.000, 1054.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    981/500000: episode: 977, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3508.000 [3508.000, 3508.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    982/500000: episode: 978, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2347.000 [2347.000, 2347.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    983/500000: episode: 979, duration: 0.006s, episode steps:   1, steps per second: 173, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1253.000 [1253.000, 1253.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    984/500000: episode: 980, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2347.000 [2347.000, 2347.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    985/500000: episode: 981, duration: 0.003s, episode steps:   1, steps per second: 296, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    986/500000: episode: 982, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4087.000 [4087.000, 4087.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    987/500000: episode: 983, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2029.000 [2029.000, 2029.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    988/500000: episode: 984, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1928.000 [1928.000, 1928.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    989/500000: episode: 985, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    990/500000: episode: 986, duration: 0.003s, episode steps:   1, steps per second: 394, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    991/500000: episode: 987, duration: 0.005s, episode steps:   1, steps per second: 188, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2347.000 [2347.000, 2347.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    992/500000: episode: 988, duration: 0.007s, episode steps:   1, steps per second: 151, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1976.000 [1976.000, 1976.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    993/500000: episode: 989, duration: 0.005s, episode steps:   1, steps per second: 217, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    994/500000: episode: 990, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    995/500000: episode: 991, duration: 0.005s, episode steps:   1, steps per second: 193, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3118.000 [3118.000, 3118.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    996/500000: episode: 992, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 437.000 [437.000, 437.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    997/500000: episode: 993, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4000.000 [4000.000, 4000.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    998/500000: episode: 994, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 849.000 [849.000, 849.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    999/500000: episode: 995, duration: 0.003s, episode steps:   1, steps per second: 376, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   1000/500000: episode: 996, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2269.000 [2269.000, 2269.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1001/500000: episode: 997, duration: 1.557s, episode steps:   1, steps per second:   1, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3690.000 [3690.000, 3690.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   1002/500000: episode: 998, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 690.000 [690.000, 690.000],  loss: 12495588.000000, mae: 1.411053, mean_q: 1.895743\n",
      "wrong_move\n",
      "   1003/500000: episode: 999, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 150.000 [150.000, 150.000],  loss: 12452832.000000, mae: 1.469424, mean_q: 7.927648\n",
      "wrong_move\n",
      "   1004/500000: episode: 1000, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3784.000 [3784.000, 3784.000],  loss: 12419388.000000, mae: 1.583174, mean_q: 16.098618\n",
      "wrong_move\n",
      "   1005/500000: episode: 1001, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2347.000 [2347.000, 2347.000],  loss: 12352200.000000, mae: 1.742869, mean_q: 29.314247\n",
      "wrong_move\n",
      "   1006/500000: episode: 1002, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2347.000 [2347.000, 2347.000],  loss: 11838906.000000, mae: 1.928532, mean_q: 44.209385\n",
      "wrong_move\n",
      "   1007/500000: episode: 1003, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: 12108592.000000, mae: 2.215002, mean_q: 54.270836\n",
      "wrong_move\n",
      "   1008/500000: episode: 1004, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2840.000 [2840.000, 2840.000],  loss: 12009352.000000, mae: 2.659424, mean_q: 82.573883\n",
      "wrong_move\n",
      "   1009/500000: episode: 1005, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 549.000 [549.000, 549.000],  loss: 11570282.000000, mae: 3.002840, mean_q: 96.128922\n",
      "wrong_move\n",
      "   1010/500000: episode: 1006, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: 11029379.000000, mae: 3.562831, mean_q: 127.583084\n",
      "wrong_move\n",
      "   1011/500000: episode: 1007, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: 11264477.000000, mae: 4.076981, mean_q: 136.708740\n",
      "wrong_move\n",
      "   1012/500000: episode: 1008, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1096.000 [1096.000, 1096.000],  loss: 10897122.000000, mae: 4.761417, mean_q: 163.851364\n",
      "wrong_move\n",
      "   1013/500000: episode: 1009, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2576.000 [2576.000, 2576.000],  loss: 10795253.000000, mae: 5.370872, mean_q: 168.889343\n",
      "wrong_move\n",
      "   1014/500000: episode: 1010, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: 8950557.000000, mae: 6.675178, mean_q: 260.799072\n",
      "wrong_move\n",
      "   1015/500000: episode: 1011, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: 9764981.000000, mae: 8.041967, mean_q: 310.567902\n",
      "wrong_move\n",
      "   1016/500000: episode: 1012, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: 10183480.000000, mae: 8.122401, mean_q: 235.450027\n",
      "wrong_move\n",
      "   1017/500000: episode: 1013, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: 8917766.000000, mae: 10.794643, mean_q: 407.872559\n",
      "wrong_move\n",
      "   1018/500000: episode: 1014, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: 7425042.000000, mae: 12.011901, mean_q: 431.834961\n",
      "wrong_move\n",
      "   1019/500000: episode: 1015, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: 8383160.000000, mae: 13.575529, mean_q: 461.739594\n",
      "wrong_move\n",
      "   1020/500000: episode: 1016, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: 7538905.500000, mae: 14.443743, mean_q: 428.025635\n",
      "wrong_move\n",
      "   1021/500000: episode: 1017, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2497.000 [2497.000, 2497.000],  loss: 7627105.000000, mae: 16.461905, mean_q: 485.715485\n",
      "wrong_move\n",
      "   1022/500000: episode: 1018, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: 7980403.000000, mae: 17.688038, mean_q: 478.228638\n",
      "wrong_move\n",
      "   1023/500000: episode: 1019, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: 6236847.000000, mae: 20.742743, mean_q: 618.407471\n",
      "wrong_move\n",
      "   1024/500000: episode: 1020, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: 7251872.500000, mae: 21.093569, mean_q: 573.657837\n",
      "wrong_move\n",
      "   1025/500000: episode: 1021, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1733.000 [1733.000, 1733.000],  loss: 7876266.000000, mae: 23.075783, mean_q: 603.286743\n",
      "wrong_move\n",
      "   1026/500000: episode: 1022, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: 6347981.000000, mae: 25.838959, mean_q: 683.752319\n",
      "wrong_move\n",
      "   1027/500000: episode: 1023, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: 6130332.500000, mae: 28.415098, mean_q: 760.239014\n",
      "wrong_move\n",
      "   1028/500000: episode: 1024, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 136.000 [136.000, 136.000],  loss: 6837744.500000, mae: 28.815624, mean_q: 686.915894\n",
      "wrong_move\n",
      "   1029/500000: episode: 1025, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 136.000 [136.000, 136.000],  loss: 7032224.000000, mae: 31.191462, mean_q: 728.364502\n",
      "wrong_move\n",
      "   1030/500000: episode: 1026, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: 5363229.000000, mae: 32.451439, mean_q: 719.183411\n",
      "wrong_move\n",
      "   1031/500000: episode: 1027, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: 5851704.500000, mae: 35.483337, mean_q: 761.471436\n",
      "wrong_move\n",
      "   1032/500000: episode: 1028, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: 7596617.000000, mae: 36.208817, mean_q: 771.822449\n",
      "wrong_move\n",
      "   1033/500000: episode: 1029, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: 6547475.500000, mae: 37.490814, mean_q: 787.796265\n",
      "wrong_move\n",
      "   1034/500000: episode: 1030, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: 5116628.000000, mae: 41.245148, mean_q: 905.494812\n",
      "wrong_move\n",
      "   1035/500000: episode: 1031, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: 4884074.000000, mae: 41.530548, mean_q: 768.280396\n",
      "wrong_move\n",
      "   1036/500000: episode: 1032, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: 5592425.500000, mae: 43.225250, mean_q: 865.929443\n",
      "wrong_move\n",
      "   1037/500000: episode: 1033, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: 4958735.500000, mae: 45.721947, mean_q: 797.249695\n",
      "wrong_move\n",
      "   1038/500000: episode: 1034, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: 5036728.500000, mae: 48.856304, mean_q: 912.285645\n",
      "wrong_move\n",
      "   1039/500000: episode: 1035, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: 5003533.000000, mae: 51.736946, mean_q: 925.742554\n",
      "wrong_move\n",
      "   1040/500000: episode: 1036, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: 4966418.000000, mae: 52.322609, mean_q: 820.327271\n",
      "wrong_move\n",
      "   1041/500000: episode: 1037, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: 4945271.500000, mae: 55.065903, mean_q: 939.250977\n",
      "wrong_move\n",
      "   1042/500000: episode: 1038, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: 4774438.500000, mae: 58.432537, mean_q: 821.305176\n",
      "wrong_move\n",
      "   1043/500000: episode: 1039, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1544.000 [1544.000, 1544.000],  loss: 3768839.000000, mae: 63.000732, mean_q: 877.392090\n",
      "wrong_move\n",
      "   1044/500000: episode: 1040, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3750.000 [3750.000, 3750.000],  loss: 3749836.500000, mae: 64.448730, mean_q: 942.970154\n",
      "wrong_move\n",
      "   1045/500000: episode: 1041, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: 2038724.750000, mae: 68.885529, mean_q: 1057.076904\n",
      "wrong_move\n",
      "   1046/500000: episode: 1042, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: 3904484.500000, mae: 68.689003, mean_q: 903.662598\n",
      "wrong_move\n",
      "   1047/500000: episode: 1043, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: 3030386.500000, mae: 74.179977, mean_q: 930.909912\n",
      "wrong_move\n",
      "   1048/500000: episode: 1044, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: 2919466.750000, mae: 77.691475, mean_q: 1018.027588\n",
      "wrong_move\n",
      "   1049/500000: episode: 1045, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 389.000 [389.000, 389.000],  loss: 3768449.250000, mae: 79.451477, mean_q: 972.858398\n",
      "wrong_move\n",
      "   1050/500000: episode: 1046, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 85.000 [85.000, 85.000],  loss: 3861223.000000, mae: 81.132080, mean_q: 930.554932\n",
      "wrong_move\n",
      "   1051/500000: episode: 1047, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 389.000 [389.000, 389.000],  loss: 3760703.000000, mae: 87.697830, mean_q: 1011.052307\n",
      "wrong_move\n",
      "   1052/500000: episode: 1048, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4007.000 [4007.000, 4007.000],  loss: 3749234.500000, mae: 89.742714, mean_q: 995.317749\n",
      "wrong_move\n",
      "   1053/500000: episode: 1049, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3631.000 [3631.000, 3631.000],  loss: 2680532.000000, mae: 91.813614, mean_q: 981.027893\n",
      "wrong_move\n",
      "   1054/500000: episode: 1050, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 389.000 [389.000, 389.000],  loss: 3055153.000000, mae: 95.028893, mean_q: 851.455017\n",
      "wrong_move\n",
      "   1055/500000: episode: 1051, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 389.000 [389.000, 389.000],  loss: 3956226.750000, mae: 98.798019, mean_q: 907.849243\n",
      "wrong_move\n",
      "   1056/500000: episode: 1052, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: 3476606.500000, mae: 103.169250, mean_q: 1063.771484\n",
      "wrong_move\n",
      "   1057/500000: episode: 1053, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1054.000 [1054.000, 1054.000],  loss: 2566463.750000, mae: 105.804642, mean_q: 906.993286\n",
      "wrong_move\n",
      "   1058/500000: episode: 1054, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1715.000 [1715.000, 1715.000],  loss: 1922799.500000, mae: 108.649689, mean_q: 910.986938\n",
      "wrong_move\n",
      "   1059/500000: episode: 1055, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1715.000 [1715.000, 1715.000],  loss: 1910233.875000, mae: 112.994553, mean_q: 903.716797\n",
      "wrong_move\n",
      "   1060/500000: episode: 1056, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1715.000 [1715.000, 1715.000],  loss: 2294856.750000, mae: 117.878204, mean_q: 922.127258\n",
      "wrong_move\n",
      "   1061/500000: episode: 1057, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1212.000 [1212.000, 1212.000],  loss: 2566321.000000, mae: 122.662712, mean_q: 925.329956\n",
      "wrong_move\n",
      "   1062/500000: episode: 1058, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: 1586393.000000, mae: 125.047531, mean_q: 984.677002\n",
      "wrong_move\n",
      "   1063/500000: episode: 1059, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1134.000 [1134.000, 1134.000],  loss: 2798530.000000, mae: 134.605042, mean_q: 1041.273315\n",
      "wrong_move\n",
      "   1064/500000: episode: 1060, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1134.000 [1134.000, 1134.000],  loss: 1455402.000000, mae: 135.623505, mean_q: 1024.968262\n",
      "wrong_move\n",
      "   1065/500000: episode: 1061, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1134.000 [1134.000, 1134.000],  loss: 1472379.250000, mae: 140.040680, mean_q: 989.479248\n",
      "wrong_move\n",
      "   1066/500000: episode: 1062, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 135.000 [135.000, 135.000],  loss: 2370238.500000, mae: 143.136322, mean_q: 947.080200\n",
      "wrong_move\n",
      "   1067/500000: episode: 1063, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1864.000 [1864.000, 1864.000],  loss: 1475975.500000, mae: 148.569153, mean_q: 991.248657\n",
      "wrong_move\n",
      "   1068/500000: episode: 1064, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: 2618589.000000, mae: 149.785736, mean_q: 925.846802\n",
      "wrong_move\n",
      "   1069/500000: episode: 1065, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4091.000 [4091.000, 4091.000],  loss: 964358.750000, mae: 153.891418, mean_q: 884.830261\n",
      "wrong_move\n",
      "   1070/500000: episode: 1066, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: 1623706.750000, mae: 159.633362, mean_q: 1015.101929\n",
      "wrong_move\n",
      "   1071/500000: episode: 1067, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2139.000 [2139.000, 2139.000],  loss: 1661065.375000, mae: 164.310028, mean_q: 932.333130\n",
      "wrong_move\n",
      "   1072/500000: episode: 1068, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3147.000 [3147.000, 3147.000],  loss: 1192417.500000, mae: 166.266052, mean_q: 930.850952\n",
      "wrong_move\n",
      "   1073/500000: episode: 1069, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1806.000 [1806.000, 1806.000],  loss: 1229167.875000, mae: 168.079071, mean_q: 916.858521\n",
      "wrong_move\n",
      "   1074/500000: episode: 1070, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 641.000 [641.000, 641.000],  loss: 1248315.250000, mae: 173.772278, mean_q: 959.579468\n",
      "wrong_move\n",
      "   1075/500000: episode: 1071, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2139.000 [2139.000, 2139.000],  loss: 1194867.750000, mae: 177.468292, mean_q: 1014.182800\n",
      "wrong_move\n",
      "   1076/500000: episode: 1072, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2139.000 [2139.000, 2139.000],  loss: 557669.750000, mae: 180.058578, mean_q: 947.301697\n",
      "wrong_move\n",
      "   1077/500000: episode: 1073, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4094.000 [4094.000, 4094.000],  loss: 808552.500000, mae: 185.806458, mean_q: 1036.518311\n",
      "wrong_move\n",
      "   1078/500000: episode: 1074, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1477.000 [1477.000, 1477.000],  loss: 1563010.375000, mae: 188.658585, mean_q: 1005.421509\n",
      "wrong_move\n",
      "   1079/500000: episode: 1075, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1774.000 [1774.000, 1774.000],  loss: 736200.437500, mae: 193.204132, mean_q: 1077.169067\n",
      "wrong_move\n",
      "   1080/500000: episode: 1076, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2139.000 [2139.000, 2139.000],  loss: 889318.125000, mae: 195.997879, mean_q: 988.603149\n",
      "wrong_move\n",
      "   1081/500000: episode: 1077, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 488.000 [488.000, 488.000],  loss: 987844.375000, mae: 200.547943, mean_q: 1035.772949\n",
      "wrong_move\n",
      "   1082/500000: episode: 1078, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1212.000 [1212.000, 1212.000],  loss: 822374.000000, mae: 203.129700, mean_q: 1005.689758\n",
      "wrong_move\n",
      "   1083/500000: episode: 1079, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1806.000 [1806.000, 1806.000],  loss: 506275.250000, mae: 207.482666, mean_q: 1075.468018\n",
      "wrong_move\n",
      "   1084/500000: episode: 1080, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3093.000 [3093.000, 3093.000],  loss: 1138871.000000, mae: 214.884369, mean_q: 1161.925659\n",
      "wrong_move\n",
      "   1085/500000: episode: 1081, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1806.000 [1806.000, 1806.000],  loss: 1130009.750000, mae: 215.021179, mean_q: 1119.127930\n",
      "wrong_move\n",
      "   1086/500000: episode: 1082, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1806.000 [1806.000, 1806.000],  loss: 567568.937500, mae: 216.195953, mean_q: 1090.199829\n",
      "wrong_move\n",
      "   1087/500000: episode: 1083, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1806.000 [1806.000, 1806.000],  loss: 846999.562500, mae: 220.632233, mean_q: 1019.244385\n",
      "wrong_move\n",
      "   1088/500000: episode: 1084, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4022.000 [4022.000, 4022.000],  loss: 543266.750000, mae: 229.854340, mean_q: 1135.695312\n",
      "wrong_move\n",
      "   1089/500000: episode: 1085, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: 1481487.500000, mae: 228.810440, mean_q: 1056.641846\n",
      "wrong_move\n",
      "   1090/500000: episode: 1086, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2213.000 [2213.000, 2213.000],  loss: 255642.046875, mae: 233.038666, mean_q: 1117.514282\n",
      "wrong_move\n",
      "   1091/500000: episode: 1087, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2376.000 [2376.000, 2376.000],  loss: 593660.375000, mae: 235.812973, mean_q: 1047.022949\n",
      "wrong_move\n",
      "   1092/500000: episode: 1088, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 717.000 [717.000, 717.000],  loss: 769683.687500, mae: 239.249695, mean_q: 998.664062\n",
      "wrong_move\n",
      "   1093/500000: episode: 1089, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: 668615.875000, mae: 252.040237, mean_q: 1166.200439\n",
      "wrong_move\n",
      "   1094/500000: episode: 1090, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4087.000 [4087.000, 4087.000],  loss: 1114768.250000, mae: 248.737076, mean_q: 1169.845703\n",
      "wrong_move\n",
      "   1095/500000: episode: 1091, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 666.000 [666.000, 666.000],  loss: 406316.562500, mae: 245.333038, mean_q: 1017.636719\n",
      "wrong_move\n",
      "   1096/500000: episode: 1092, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1129.000 [1129.000, 1129.000],  loss: 1042707.125000, mae: 248.889709, mean_q: 1053.770020\n",
      "wrong_move\n",
      "   1097/500000: episode: 1093, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: 750427.500000, mae: 259.642822, mean_q: 1243.953613\n",
      "wrong_move\n",
      "   1098/500000: episode: 1094, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 666.000 [666.000, 666.000],  loss: 1159135.000000, mae: 256.740723, mean_q: 1180.291992\n",
      "wrong_move\n",
      "   1100/500000: episode: 1095, duration: 0.146s, episode steps:   2, steps per second:  14, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 666.000 [666.000, 666.000],  loss: 556762.250000, mae: 261.814270, mean_q: 1232.557373\n",
      "wrong_move\n",
      "   1101/500000: episode: 1096, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 717.000 [717.000, 717.000],  loss: 641241.375000, mae: 263.000610, mean_q: 1226.187500\n",
      "wrong_move\n",
      "   1102/500000: episode: 1097, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3298.000 [3298.000, 3298.000],  loss: 783157.687500, mae: 269.670166, mean_q: 1316.352783\n",
      "wrong_move\n",
      "   1103/500000: episode: 1098, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1889.000 [1889.000, 1889.000],  loss: 1220730.375000, mae: 271.425201, mean_q: 1208.615723\n",
      "wrong_move\n",
      "   1104/500000: episode: 1099, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 456.000 [456.000, 456.000],  loss: 689387.312500, mae: 276.475098, mean_q: 1192.202881\n",
      "wrong_move\n",
      "   1105/500000: episode: 1100, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 852.000 [852.000, 852.000],  loss: 569328.437500, mae: 277.836090, mean_q: 911.700562\n",
      "wrong_move\n",
      "   1106/500000: episode: 1101, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2376.000 [2376.000, 2376.000],  loss: 222668.781250, mae: 283.984009, mean_q: 934.969116\n",
      "wrong_move\n",
      "   1107/500000: episode: 1102, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 842.000 [842.000, 842.000],  loss: 714724.625000, mae: 279.980164, mean_q: 749.717896\n",
      "wrong_move\n",
      "   1108/500000: episode: 1103, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2018.000 [2018.000, 2018.000],  loss: 362196.562500, mae: 280.518921, mean_q: 512.928223\n",
      "wrong_move\n",
      "   1109/500000: episode: 1104, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 173.000 [173.000, 173.000],  loss: 278261.000000, mae: 281.686005, mean_q: 474.276855\n",
      "wrong_move\n",
      "   1110/500000: episode: 1105, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3826.000 [3826.000, 3826.000],  loss: 340382.656250, mae: 284.208984, mean_q: 456.169861\n",
      "wrong_move\n",
      "   1111/500000: episode: 1106, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1943.000 [1943.000, 1943.000],  loss: 92668.585938, mae: 293.244507, mean_q: 587.078613\n",
      "wrong_move\n",
      "   1112/500000: episode: 1107, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1170.000 [1170.000, 1170.000],  loss: 1103520.750000, mae: 288.992615, mean_q: 440.314056\n",
      "wrong_move\n",
      "   1113/500000: episode: 1108, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2876.000 [2876.000, 2876.000],  loss: 648043.375000, mae: 291.462250, mean_q: 454.828186\n",
      "wrong_move\n",
      "   1114/500000: episode: 1109, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2388.000 [2388.000, 2388.000],  loss: 233796.531250, mae: 295.246277, mean_q: 463.794769\n",
      "wrong_move\n",
      "   1115/500000: episode: 1110, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1024.000 [1024.000, 1024.000],  loss: 1813958.375000, mae: 299.697479, mean_q: 494.101196\n",
      "wrong_move\n",
      "   1116/500000: episode: 1111, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3124.000 [3124.000, 3124.000],  loss: 507935.062500, mae: 297.394165, mean_q: 374.682190\n",
      "wrong_move\n",
      "   1117/500000: episode: 1112, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4094.000 [4094.000, 4094.000],  loss: 539908.125000, mae: 300.570251, mean_q: 463.621643\n",
      "wrong_move\n",
      "   1118/500000: episode: 1113, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1995.000 [1995.000, 1995.000],  loss: 241514.531250, mae: 301.455994, mean_q: 451.184326\n",
      "wrong_move\n",
      "   1119/500000: episode: 1114, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4028.000 [4028.000, 4028.000],  loss: 227830.390625, mae: 304.784302, mean_q: 441.179749\n",
      "wrong_move\n",
      "   1120/500000: episode: 1115, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 463.000 [463.000, 463.000],  loss: 344524.906250, mae: 305.857452, mean_q: 487.079865\n",
      "wrong_move\n",
      "   1121/500000: episode: 1116, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 468.000 [468.000, 468.000],  loss: 309499.687500, mae: 308.333679, mean_q: 432.690796\n",
      "wrong_move\n",
      "   1122/500000: episode: 1117, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 211.000 [211.000, 211.000],  loss: 204947.328125, mae: 309.882080, mean_q: 451.120422\n",
      "wrong_move\n",
      "   1123/500000: episode: 1118, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2708.000 [2708.000, 2708.000],  loss: 351743.937500, mae: 311.999939, mean_q: 415.485107\n",
      "wrong_move\n",
      "   1124/500000: episode: 1119, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3809.000 [3809.000, 3809.000],  loss: 122220.367188, mae: 313.045532, mean_q: 286.887146\n",
      "wrong_move\n",
      "   1125/500000: episode: 1120, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1808.000 [1808.000, 1808.000],  loss: 359021.625000, mae: 315.190735, mean_q: 269.910950\n",
      "wrong_move\n",
      "   1126/500000: episode: 1121, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1147.000 [1147.000, 1147.000],  loss: 136420.656250, mae: 316.982819, mean_q: 212.015594\n",
      "wrong_move\n",
      "   1127/500000: episode: 1122, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3987.000 [3987.000, 3987.000],  loss: 550443.500000, mae: 325.757782, mean_q: 243.719543\n",
      "wrong_move\n",
      "   1128/500000: episode: 1123, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2009.000 [2009.000, 2009.000],  loss: 475117.343750, mae: 319.513916, mean_q: 223.579605\n",
      "wrong_move\n",
      "   1129/500000: episode: 1124, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1526.000 [1526.000, 1526.000],  loss: 480753.406250, mae: 320.603851, mean_q: 179.339706\n",
      "wrong_move\n",
      "   1130/500000: episode: 1125, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2564.000 [2564.000, 2564.000],  loss: 624422.125000, mae: 321.915283, mean_q: 161.530701\n",
      "wrong_move\n",
      "   1131/500000: episode: 1126, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1526.000 [1526.000, 1526.000],  loss: 294431.875000, mae: 323.147705, mean_q: 152.588165\n",
      "wrong_move\n",
      "   1132/500000: episode: 1127, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1526.000 [1526.000, 1526.000],  loss: 522391.406250, mae: 326.573547, mean_q: 177.420410\n",
      "wrong_move\n",
      "   1133/500000: episode: 1128, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2497.000 [2497.000, 2497.000],  loss: 358120.687500, mae: 325.226929, mean_q: 151.866104\n",
      "wrong_move\n",
      "   1134/500000: episode: 1129, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2951.000 [2951.000, 2951.000],  loss: 184529.453125, mae: 327.313934, mean_q: 132.212891\n",
      "wrong_move\n",
      "   1136/500000: episode: 1130, duration: 0.094s, episode steps:   2, steps per second:  21, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 270.000 [270.000, 270.000],  loss: 855792.187500, mae: 328.890594, mean_q: 106.507156\n",
      "wrong_move\n",
      "   1137/500000: episode: 1131, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3690.000 [3690.000, 3690.000],  loss: 231014.750000, mae: 330.645294, mean_q: 77.729691\n",
      "wrong_move\n",
      "   1138/500000: episode: 1132, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 270.000 [270.000, 270.000],  loss: 508946.375000, mae: 335.232483, mean_q: 172.268219\n",
      "wrong_move\n",
      "   1139/500000: episode: 1133, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 270.000 [270.000, 270.000],  loss: 221299.671875, mae: 336.707977, mean_q: 100.228844\n",
      "wrong_move\n",
      "   1140/500000: episode: 1134, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1724.000 [1724.000, 1724.000],  loss: 137362.281250, mae: 336.895386, mean_q: 99.515533\n",
      "wrong_move\n",
      "   1141/500000: episode: 1135, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2660.000 [2660.000, 2660.000],  loss: 543296.000000, mae: 337.526672, mean_q: 119.799294\n",
      "wrong_move\n",
      "   1142/500000: episode: 1136, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3507.000 [3507.000, 3507.000],  loss: 73512.492188, mae: 339.057678, mean_q: 102.803665\n",
      "wrong_move\n",
      "   1143/500000: episode: 1137, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3654.000 [3654.000, 3654.000],  loss: 376741.562500, mae: 340.554993, mean_q: 95.708710\n",
      "wrong_move\n",
      "   1144/500000: episode: 1138, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3222.000 [3222.000, 3222.000],  loss: 427551.031250, mae: 343.567200, mean_q: 144.661743\n",
      "wrong_move\n",
      "   1145/500000: episode: 1139, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1724.000 [1724.000, 1724.000],  loss: 595716.375000, mae: 345.026215, mean_q: 125.078827\n",
      "wrong_move\n",
      "   1146/500000: episode: 1140, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3220.000 [3220.000, 3220.000],  loss: 290928.812500, mae: 346.367981, mean_q: 114.652206\n",
      "wrong_move\n",
      "   1147/500000: episode: 1141, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1724.000 [1724.000, 1724.000],  loss: 625117.625000, mae: 347.175446, mean_q: 80.052559\n",
      "wrong_move\n",
      "   1148/500000: episode: 1142, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 412.000 [412.000, 412.000],  loss: 213795.906250, mae: 348.673553, mean_q: 53.348198\n",
      "wrong_move\n",
      "   1149/500000: episode: 1143, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3064.000 [3064.000, 3064.000],  loss: 321866.375000, mae: 349.990295, mean_q: 66.945862\n",
      "wrong_move\n",
      "   1150/500000: episode: 1144, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3064.000 [3064.000, 3064.000],  loss: 520336.593750, mae: 351.738922, mean_q: 84.657295\n",
      "wrong_move\n",
      "   1151/500000: episode: 1145, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 727.000 [727.000, 727.000],  loss: 642023.500000, mae: 353.059448, mean_q: 73.271767\n",
      "wrong_move\n",
      "   1152/500000: episode: 1146, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2312.000 [2312.000, 2312.000],  loss: 112446.343750, mae: 354.286682, mean_q: 31.659071\n",
      "wrong_move\n",
      "   1153/500000: episode: 1147, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 412.000 [412.000, 412.000],  loss: 188143.562500, mae: 355.628540, mean_q: 52.400902\n",
      "wrong_move\n",
      "   1154/500000: episode: 1148, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 982.000 [982.000, 982.000],  loss: 103684.968750, mae: 356.885223, mean_q: 69.442131\n",
      "wrong_move\n",
      "   1155/500000: episode: 1149, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1744.000 [1744.000, 1744.000],  loss: 879352.687500, mae: 358.737000, mean_q: 96.427094\n",
      "wrong_move\n",
      "   1156/500000: episode: 1150, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2882.000 [2882.000, 2882.000],  loss: 56806.601562, mae: 360.304565, mean_q: 80.774124\n",
      "wrong_move\n",
      "   1157/500000: episode: 1151, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3435.000 [3435.000, 3435.000],  loss: 429824.906250, mae: 360.568420, mean_q: 68.007858\n",
      "wrong_move\n",
      "   1158/500000: episode: 1152, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1744.000 [1744.000, 1744.000],  loss: 40350.144531, mae: 361.249817, mean_q: 35.443359\n",
      "wrong_move\n",
      "   1159/500000: episode: 1153, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2415.000 [2415.000, 2415.000],  loss: 210875.140625, mae: 362.272522, mean_q: 53.795578\n",
      "wrong_move\n",
      "   1160/500000: episode: 1154, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1932.000 [1932.000, 1932.000],  loss: 625416.187500, mae: 363.508179, mean_q: 54.533646\n",
      "wrong_move\n",
      "   1161/500000: episode: 1155, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 253.000 [253.000, 253.000],  loss: 772383.125000, mae: 365.120850, mean_q: 67.573212\n",
      "wrong_move\n",
      "   1162/500000: episode: 1156, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 933.000 [933.000, 933.000],  loss: 341836.531250, mae: 366.152435, mean_q: 36.621639\n",
      "wrong_move\n",
      "   1163/500000: episode: 1157, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1744.000 [1744.000, 1744.000],  loss: 65488.187500, mae: 367.592896, mean_q: 69.890076\n",
      "wrong_move\n",
      "   1165/500000: episode: 1158, duration: 0.097s, episode steps:   2, steps per second:  21, episode reward: -4981.000, mean reward: -2490.500 [-5000.000, 19.000], mean action: 3827.500 [3817.000, 3838.000],  loss: 223543.250000, mae: 370.343414, mean_q: 105.088303\n",
      "wrong_move\n",
      "   1166/500000: episode: 1159, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1744.000 [1744.000, 1744.000],  loss: 178825.640625, mae: 370.105133, mean_q: 31.788265\n",
      "wrong_move\n",
      "   1167/500000: episode: 1160, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3435.000 [3435.000, 3435.000],  loss: 156558.031250, mae: 374.223633, mean_q: 104.641579\n",
      "wrong_move\n",
      "   1168/500000: episode: 1161, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3023.000 [3023.000, 3023.000],  loss: 28502.039062, mae: 371.988159, mean_q: 58.229366\n",
      "wrong_move\n",
      "   1169/500000: episode: 1162, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3370.000 [3370.000, 3370.000],  loss: 60205.046875, mae: 371.792480, mean_q: 75.456718\n",
      "wrong_move\n",
      "   1170/500000: episode: 1163, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 267.000 [267.000, 267.000],  loss: 812613.562500, mae: 372.335449, mean_q: 23.583881\n",
      "wrong_move\n",
      "   1171/500000: episode: 1164, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3395.000 [3395.000, 3395.000],  loss: 93395.945312, mae: 372.626373, mean_q: 68.740784\n",
      "wrong_move\n",
      "   1172/500000: episode: 1165, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2306.000 [2306.000, 2306.000],  loss: 280254.000000, mae: 372.980133, mean_q: 15.867977\n",
      "wrong_move\n",
      "   1173/500000: episode: 1166, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3764.000 [3764.000, 3764.000],  loss: 275388.062500, mae: 373.504730, mean_q: 219.978333\n",
      "wrong_move\n",
      "   1174/500000: episode: 1167, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 394674.625000, mae: 375.449707, mean_q: 585.119324\n",
      "wrong_move\n",
      "   1175/500000: episode: 1168, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 477954.250000, mae: 375.842773, mean_q: 854.251465\n",
      "wrong_move\n",
      "   1176/500000: episode: 1169, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 708454.125000, mae: 377.325562, mean_q: 1091.006958\n",
      "wrong_move\n",
      "   1177/500000: episode: 1170, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2093.000 [2093.000, 2093.000],  loss: 25370.050781, mae: 377.081543, mean_q: 1246.252930\n",
      "wrong_move\n",
      "   1178/500000: episode: 1171, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1160.000 [1160.000, 1160.000],  loss: 557983.187500, mae: 378.039001, mean_q: 1396.304199\n",
      "wrong_move\n",
      "   1179/500000: episode: 1172, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 771.000 [771.000, 771.000],  loss: 399035.750000, mae: 378.956848, mean_q: 1519.067139\n",
      "wrong_move\n",
      "   1180/500000: episode: 1173, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 77816.367188, mae: 384.485352, mean_q: 1689.123901\n",
      "wrong_move\n",
      "   1181/500000: episode: 1174, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 78754.687500, mae: 382.335083, mean_q: 1710.357422\n",
      "wrong_move\n",
      "   1182/500000: episode: 1175, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 182019.062500, mae: 382.040344, mean_q: 1767.624023\n",
      "wrong_move\n",
      "   1183/500000: episode: 1176, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 166138.156250, mae: 383.204773, mean_q: 1822.931152\n",
      "wrong_move\n",
      "   1184/500000: episode: 1177, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 621263.437500, mae: 384.136841, mean_q: 1867.398438\n",
      "wrong_move\n",
      "   1185/500000: episode: 1178, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 977122.500000, mae: 384.919250, mean_q: 1903.567871\n",
      "wrong_move\n",
      "   1186/500000: episode: 1179, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 84381.156250, mae: 385.766418, mean_q: 1435.962891\n",
      "wrong_move\n",
      "   1187/500000: episode: 1180, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2990.000 [2990.000, 2990.000],  loss: 976830.937500, mae: 386.494385, mean_q: 1033.682617\n",
      "wrong_move\n",
      "   1188/500000: episode: 1181, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 123883.023438, mae: 387.009766, mean_q: 499.263885\n",
      "wrong_move\n",
      "   1189/500000: episode: 1182, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3294.000 [3294.000, 3294.000],  loss: 343675.031250, mae: 387.815186, mean_q: 227.155807\n",
      "wrong_move\n",
      "   1190/500000: episode: 1183, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3294.000 [3294.000, 3294.000],  loss: 113798.281250, mae: 388.863159, mean_q: 167.277222\n",
      "wrong_move\n",
      "   1191/500000: episode: 1184, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3294.000 [3294.000, 3294.000],  loss: 210044.703125, mae: 389.448120, mean_q: 52.874481\n",
      "wrong_move\n",
      "   1192/500000: episode: 1185, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3294.000 [3294.000, 3294.000],  loss: 318351.375000, mae: 390.520416, mean_q: 59.529175\n",
      "wrong_move\n",
      "   1193/500000: episode: 1186, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2110.000 [2110.000, 2110.000],  loss: 401798.406250, mae: 391.256165, mean_q: 53.477299\n",
      "wrong_move\n",
      "   1194/500000: episode: 1187, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3316.000 [3316.000, 3316.000],  loss: 45927.894531, mae: 391.749390, mean_q: 18.131126\n",
      "wrong_move\n",
      "   1195/500000: episode: 1188, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2454.000 [2454.000, 2454.000],  loss: 54716.660156, mae: 392.603943, mean_q: 81.272438\n",
      "wrong_move\n",
      "   1196/500000: episode: 1189, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 275.000 [275.000, 275.000],  loss: 478847.843750, mae: 393.611023, mean_q: 55.758995\n",
      "wrong_move\n",
      "   1197/500000: episode: 1190, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1701.000 [1701.000, 1701.000],  loss: 136457.843750, mae: 394.730103, mean_q: 47.440269\n",
      "wrong_move\n",
      "   1198/500000: episode: 1191, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 199.000 [199.000, 199.000],  loss: 476842.437500, mae: 396.120026, mean_q: 18.362743\n",
      "wrong_move\n",
      "   1199/500000: episode: 1192, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1701.000 [1701.000, 1701.000],  loss: 106127.359375, mae: 397.685944, mean_q: 28.668993\n",
      "wrong_move\n",
      "   1200/500000: episode: 1193, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 277.000 [277.000, 277.000],  loss: 186434.531250, mae: 399.573669, mean_q: 45.682739\n",
      "wrong_move\n",
      "   1201/500000: episode: 1194, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3052.000 [3052.000, 3052.000],  loss: 384654.718750, mae: 401.328369, mean_q: 52.837173\n",
      "wrong_move\n",
      "   1202/500000: episode: 1195, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1701.000 [1701.000, 1701.000],  loss: 167016.234375, mae: 402.574524, mean_q: 13.819038\n",
      "wrong_move\n",
      "   1203/500000: episode: 1196, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2454.000 [2454.000, 2454.000],  loss: 51429.753906, mae: 404.155029, mean_q: 45.592831\n",
      "wrong_move\n",
      "   1204/500000: episode: 1197, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1701.000 [1701.000, 1701.000],  loss: 260329.765625, mae: 405.549011, mean_q: 27.830994\n",
      "wrong_move\n",
      "   1205/500000: episode: 1198, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 560.000 [560.000, 560.000],  loss: 30381.027344, mae: 406.629578, mean_q: 13.885925\n",
      "wrong_move\n",
      "   1206/500000: episode: 1199, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3406.000 [3406.000, 3406.000],  loss: 48918.773438, mae: 407.993835, mean_q: 37.390224\n",
      "wrong_move\n",
      "   1207/500000: episode: 1200, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1701.000 [1701.000, 1701.000],  loss: 392222.468750, mae: 409.777832, mean_q: 59.123386\n",
      "wrong_move\n",
      "   1208/500000: episode: 1201, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2171.000 [2171.000, 2171.000],  loss: 278444.593750, mae: 408.764618, mean_q: 13.759119\n",
      "wrong_move\n",
      "   1209/500000: episode: 1202, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2172.000 [2172.000, 2172.000],  loss: 38247.207031, mae: 409.411835, mean_q: 51.099327\n",
      "wrong_move\n",
      "   1210/500000: episode: 1203, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3218.000 [3218.000, 3218.000],  loss: 43382.980469, mae: 409.798340, mean_q: 63.223728\n",
      "wrong_move\n",
      "   1211/500000: episode: 1204, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2171.000 [2171.000, 2171.000],  loss: 364157.062500, mae: 409.830261, mean_q: 13.597107\n",
      "wrong_move\n",
      "   1212/500000: episode: 1205, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1785.000 [1785.000, 1785.000],  loss: 213490.078125, mae: 410.023376, mean_q: 39.897644\n",
      "wrong_move\n",
      "   1213/500000: episode: 1206, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1616.000 [1616.000, 1616.000],  loss: 66800.859375, mae: 410.168152, mean_q: 13.507131\n",
      "wrong_move\n",
      "   1214/500000: episode: 1207, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 225.000 [225.000, 225.000],  loss: 65925.906250, mae: 410.722504, mean_q: 44.026318\n",
      "wrong_move\n",
      "   1215/500000: episode: 1208, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3243.000 [3243.000, 3243.000],  loss: 135570.031250, mae: 411.112183, mean_q: 13.194259\n",
      "wrong_move\n",
      "   1216/500000: episode: 1209, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1689.000 [1689.000, 1689.000],  loss: 12480.838867, mae: 411.928406, mean_q: 13.237237\n",
      "wrong_move\n",
      "   1217/500000: episode: 1210, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1785.000 [1785.000, 1785.000],  loss: 546163.812500, mae: 412.806885, mean_q: 28.319929\n",
      "wrong_move\n",
      "   1218/500000: episode: 1211, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2164.000 [2164.000, 2164.000],  loss: 27693.484375, mae: 413.400360, mean_q: 13.934446\n",
      "wrong_move\n",
      "   1219/500000: episode: 1212, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1785.000 [1785.000, 1785.000],  loss: 98708.000000, mae: 414.142029, mean_q: 64.862900\n",
      "wrong_move\n",
      "   1220/500000: episode: 1213, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2688.000 [2688.000, 2688.000],  loss: 404658.531250, mae: 417.486694, mean_q: 67.995819\n",
      "wrong_move\n",
      "   1221/500000: episode: 1214, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1667.000 [1667.000, 1667.000],  loss: 160065.687500, mae: 415.188049, mean_q: 13.296499\n",
      "wrong_move\n",
      "   1222/500000: episode: 1215, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3600.000 [3600.000, 3600.000],  loss: 102553.671875, mae: 415.756104, mean_q: 14.577465\n",
      "wrong_move\n",
      "   1223/500000: episode: 1216, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3775.000 [3775.000, 3775.000],  loss: 33188.355469, mae: 416.349365, mean_q: 12.865688\n",
      "wrong_move\n",
      "   1225/500000: episode: 1217, duration: 0.082s, episode steps:   2, steps per second:  24, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 1927.500 [731.000, 3124.000],  loss: 117081.703125, mae: 417.240784, mean_q: 17.801594\n",
      "wrong_move\n",
      "   1226/500000: episode: 1218, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 778.000 [778.000, 778.000],  loss: 51623.335938, mae: 418.197632, mean_q: 13.152004\n",
      "wrong_move\n",
      "   1227/500000: episode: 1219, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2382.000 [2382.000, 2382.000],  loss: 107801.953125, mae: 421.147552, mean_q: 59.838665\n",
      "wrong_move\n",
      "   1228/500000: episode: 1220, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 718.000 [718.000, 718.000],  loss: 477903.000000, mae: 420.415100, mean_q: 30.713913\n",
      "wrong_move\n",
      "   1229/500000: episode: 1221, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3942.000 [3942.000, 3942.000],  loss: 91660.914062, mae: 421.038513, mean_q: 19.851778\n",
      "wrong_move\n",
      "   1230/500000: episode: 1222, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 721.000 [721.000, 721.000],  loss: 17194.882812, mae: 421.591614, mean_q: 13.000076\n",
      "wrong_move\n",
      "   1231/500000: episode: 1223, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3168.000 [3168.000, 3168.000],  loss: 899068.000000, mae: 422.248291, mean_q: 13.282990\n",
      "wrong_move\n",
      "   1232/500000: episode: 1224, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 71.000 [71.000, 71.000],  loss: 28331.904297, mae: 422.684113, mean_q: 13.013864\n",
      "wrong_move\n",
      "   1233/500000: episode: 1225, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2149.000 [2149.000, 2149.000],  loss: 41461.835938, mae: 423.327087, mean_q: 20.735420\n",
      "wrong_move\n",
      "   1234/500000: episode: 1226, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2608.000 [2608.000, 2608.000],  loss: 440258.187500, mae: 424.117981, mean_q: 13.196676\n",
      "wrong_move\n",
      "   1236/500000: episode: 1227, duration: 0.068s, episode steps:   2, steps per second:  29, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 731.000 [731.000, 731.000],  loss: 149008.000000, mae: 425.237549, mean_q: 13.433708\n",
      "wrong_move\n",
      "   1237/500000: episode: 1228, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 296.000 [296.000, 296.000],  loss: 29116.771484, mae: 426.387085, mean_q: 13.278214\n",
      "wrong_move\n",
      "   1238/500000: episode: 1229, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 101.000 [101.000, 101.000],  loss: 448202.375000, mae: 427.482941, mean_q: 28.357899\n",
      "wrong_move\n",
      "   1239/500000: episode: 1230, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 446060.250000, mae: 428.733612, mean_q: 37.320454\n",
      "wrong_move\n",
      "   1240/500000: episode: 1231, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 902.000 [902.000, 902.000],  loss: 90098.953125, mae: 429.735229, mean_q: 12.995747\n",
      "wrong_move\n",
      "   1241/500000: episode: 1232, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2608.000 [2608.000, 2608.000],  loss: 450880.875000, mae: 430.791473, mean_q: 14.414259\n",
      "wrong_move\n",
      "   1242/500000: episode: 1233, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 77.000 [77.000, 77.000],  loss: 52858.433594, mae: 431.811951, mean_q: 47.434021\n",
      "wrong_move\n",
      "   1244/500000: episode: 1234, duration: 0.108s, episode steps:   2, steps per second:  19, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 285694.843750, mae: 432.716614, mean_q: 37.855019\n",
      "wrong_move\n",
      "   1245/500000: episode: 1235, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2608.000 [2608.000, 2608.000],  loss: 762735.750000, mae: 433.641113, mean_q: 19.874514\n",
      "wrong_move\n",
      "   1246/500000: episode: 1236, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 338196.406250, mae: 434.260315, mean_q: 246.748489\n",
      "wrong_move\n",
      "   1247/500000: episode: 1237, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 152746.593750, mae: 435.546295, mean_q: 699.511841\n",
      "wrong_move\n",
      "   1248/500000: episode: 1238, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 44337.769531, mae: 435.874939, mean_q: 1017.139343\n",
      "wrong_move\n",
      "   1249/500000: episode: 1239, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 23909.695312, mae: 436.424622, mean_q: 1268.676270\n",
      "wrong_move\n",
      "   1250/500000: episode: 1240, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 1054047.375000, mae: 437.111786, mean_q: 1490.250244\n",
      "wrong_move\n",
      "   1251/500000: episode: 1241, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 562522.062500, mae: 437.449219, mean_q: 936.030334\n",
      "wrong_move\n",
      "   1252/500000: episode: 1242, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1095.000 [1095.000, 1095.000],  loss: 98553.484375, mae: 437.807983, mean_q: 521.570190\n",
      "wrong_move\n",
      "   1253/500000: episode: 1243, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2608.000 [2608.000, 2608.000],  loss: 66927.789062, mae: 438.049988, mean_q: 193.663879\n",
      "wrong_move\n",
      "   1254/500000: episode: 1244, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2608.000 [2608.000, 2608.000],  loss: 429517.031250, mae: 438.826172, mean_q: 136.540344\n",
      "wrong_move\n",
      "   1255/500000: episode: 1245, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 41702.257812, mae: 439.745300, mean_q: 86.896011\n",
      "wrong_move\n",
      "   1256/500000: episode: 1246, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2608.000 [2608.000, 2608.000],  loss: 104017.250000, mae: 440.702637, mean_q: 44.150665\n",
      "wrong_move\n",
      "   1257/500000: episode: 1247, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 588.000 [588.000, 588.000],  loss: 36797.117188, mae: 441.772095, mean_q: 28.664604\n",
      "wrong_move\n",
      "   1258/500000: episode: 1248, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: 91852.335938, mae: 442.925110, mean_q: 35.889030\n",
      "wrong_move\n",
      "   1259/500000: episode: 1249, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: 335485.343750, mae: 444.090149, mean_q: 26.873663\n",
      "wrong_move\n",
      "   1260/500000: episode: 1250, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: 596813.875000, mae: 445.370178, mean_q: 21.642855\n",
      "wrong_move\n",
      "   1261/500000: episode: 1251, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3962.000 [3962.000, 3962.000],  loss: 81061.468750, mae: 446.406799, mean_q: 42.633080\n",
      "wrong_move\n",
      "   1262/500000: episode: 1252, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2054.000 [2054.000, 2054.000],  loss: 60443.593750, mae: 447.400696, mean_q: 17.341682\n",
      "wrong_move\n",
      "   1263/500000: episode: 1253, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2375.000 [2375.000, 2375.000],  loss: 26189.445312, mae: 448.244934, mean_q: 21.118156\n",
      "wrong_move\n",
      "   1264/500000: episode: 1254, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3003.000 [3003.000, 3003.000],  loss: 439023.687500, mae: 449.411530, mean_q: 16.636288\n",
      "wrong_move\n",
      "   1265/500000: episode: 1255, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3034.000 [3034.000, 3034.000],  loss: 450429.437500, mae: 450.892334, mean_q: 19.672283\n",
      "wrong_move\n",
      "   1266/500000: episode: 1256, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 348.000 [348.000, 348.000],  loss: 127112.414062, mae: 452.401306, mean_q: 30.472824\n",
      "wrong_move\n",
      "   1267/500000: episode: 1257, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3905.000 [3905.000, 3905.000],  loss: 202456.875000, mae: 453.692719, mean_q: 17.437298\n",
      "wrong_move\n",
      "   1268/500000: episode: 1258, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 1220390.125000, mae: 454.911011, mean_q: 37.513878\n",
      "wrong_move\n",
      "   1269/500000: episode: 1259, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2157.000 [2157.000, 2157.000],  loss: 59171.054688, mae: 455.546783, mean_q: 13.058252\n",
      "wrong_move\n",
      "   1270/500000: episode: 1260, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2626.000 [2626.000, 2626.000],  loss: 462491.968750, mae: 456.460449, mean_q: 12.740101\n",
      "wrong_move\n",
      "   1271/500000: episode: 1261, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 29.000 [29.000, 29.000],  loss: 414753.187500, mae: 457.344360, mean_q: 12.967722\n",
      "wrong_move\n",
      "   1272/500000: episode: 1262, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2307.000 [2307.000, 2307.000],  loss: 47224.890625, mae: 458.277405, mean_q: 12.688182\n",
      "wrong_move\n",
      "   1273/500000: episode: 1263, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2626.000 [2626.000, 2626.000],  loss: 42342.246094, mae: 459.397552, mean_q: 12.974648\n",
      "wrong_move\n",
      "   1274/500000: episode: 1264, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2078.000 [2078.000, 2078.000],  loss: 439282.500000, mae: 460.366516, mean_q: 26.158987\n",
      "wrong_move\n",
      "   1275/500000: episode: 1265, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1638.000 [1638.000, 1638.000],  loss: 43943.851562, mae: 460.780029, mean_q: 12.644455\n",
      "wrong_move\n",
      "   1276/500000: episode: 1266, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2307.000 [2307.000, 2307.000],  loss: 422467.156250, mae: 461.269196, mean_q: 12.749298\n",
      "wrong_move\n",
      "   1277/500000: episode: 1267, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2302.000 [2302.000, 2302.000],  loss: 49822.242188, mae: 461.667786, mean_q: 13.088030\n",
      "wrong_move\n",
      "   1278/500000: episode: 1268, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1745.000 [1745.000, 1745.000],  loss: 121038.265625, mae: 462.324707, mean_q: 27.166002\n",
      "wrong_move\n",
      "   1279/500000: episode: 1269, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2307.000 [2307.000, 2307.000],  loss: 115628.546875, mae: 462.600891, mean_q: 12.771233\n",
      "wrong_move\n",
      "   1280/500000: episode: 1270, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2307.000 [2307.000, 2307.000],  loss: 48837.132812, mae: 462.648987, mean_q: 12.868038\n",
      "wrong_move\n",
      "   1281/500000: episode: 1271, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3968.000 [3968.000, 3968.000],  loss: 416060.906250, mae: 462.182007, mean_q: 12.889952\n",
      "wrong_move\n",
      "   1282/500000: episode: 1272, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1537.000 [1537.000, 1537.000],  loss: 36255.703125, mae: 461.541443, mean_q: 13.015212\n",
      "wrong_move\n",
      "   1283/500000: episode: 1273, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2312.000 [2312.000, 2312.000],  loss: 1010592.500000, mae: 461.482849, mean_q: 12.835154\n",
      "wrong_move\n",
      "   1284/500000: episode: 1274, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: 58372.187500, mae: 461.671509, mean_q: 15.248747\n",
      "wrong_move\n",
      "   1285/500000: episode: 1275, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 61.000 [61.000, 61.000],  loss: 50797.429688, mae: 462.178345, mean_q: 19.906425\n",
      "wrong_move\n",
      "   1286/500000: episode: 1276, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1537.000 [1537.000, 1537.000],  loss: 169464.515625, mae: 463.022583, mean_q: 12.900620\n",
      "wrong_move\n",
      "   1287/500000: episode: 1277, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1537.000 [1537.000, 1537.000],  loss: 409037.031250, mae: 464.140625, mean_q: 33.649582\n",
      "wrong_move\n",
      "   1288/500000: episode: 1278, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2522.000 [2522.000, 2522.000],  loss: 75067.453125, mae: 465.133728, mean_q: 12.665903\n",
      "wrong_move\n",
      "   1289/500000: episode: 1279, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3899.000 [3899.000, 3899.000],  loss: 448499.531250, mae: 466.540344, mean_q: 12.539701\n",
      "wrong_move\n",
      "   1290/500000: episode: 1280, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 774.000 [774.000, 774.000],  loss: 455223.687500, mae: 468.043304, mean_q: 12.543649\n",
      "wrong_move\n",
      "   1291/500000: episode: 1281, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3090.000 [3090.000, 3090.000],  loss: 43638.476562, mae: 469.618866, mean_q: 12.669012\n",
      "wrong_move\n",
      "   1292/500000: episode: 1282, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3175.000 [3175.000, 3175.000],  loss: 453083.156250, mae: 471.450806, mean_q: 12.481400\n",
      "wrong_move\n",
      "   1293/500000: episode: 1283, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3676.000 [3676.000, 3676.000],  loss: 61282.832031, mae: 473.492493, mean_q: 12.484181\n",
      "wrong_move\n",
      "   1294/500000: episode: 1284, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2443.000 [2443.000, 2443.000],  loss: 282765.406250, mae: 475.773987, mean_q: 12.674407\n",
      "wrong_move\n",
      "   1295/500000: episode: 1285, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 377.000 [377.000, 377.000],  loss: 392108.968750, mae: 477.731354, mean_q: 28.654568\n",
      "wrong_move\n",
      "   1296/500000: episode: 1286, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 377.000 [377.000, 377.000],  loss: 82417.671875, mae: 478.816437, mean_q: 12.588044\n",
      "wrong_move\n",
      "   1297/500000: episode: 1287, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2677.000 [2677.000, 2677.000],  loss: 131474.250000, mae: 479.683136, mean_q: 12.591951\n",
      "wrong_move\n",
      "   1298/500000: episode: 1288, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2981.000 [2981.000, 2981.000],  loss: 51483.753906, mae: 480.305023, mean_q: 12.959629\n",
      "wrong_move\n",
      "   1299/500000: episode: 1289, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1646.000 [1646.000, 1646.000],  loss: 80756.703125, mae: 480.863037, mean_q: 13.931808\n",
      "wrong_move\n",
      "   1300/500000: episode: 1290, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 377.000 [377.000, 377.000],  loss: 454571.687500, mae: 481.639038, mean_q: 12.864331\n",
      "wrong_move\n",
      "   1301/500000: episode: 1291, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1104.000 [1104.000, 1104.000],  loss: 125509.203125, mae: 482.155640, mean_q: 12.639307\n",
      "wrong_move\n",
      "   1302/500000: episode: 1292, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1104.000 [1104.000, 1104.000],  loss: 61262.617188, mae: 482.791443, mean_q: 12.396571\n",
      "wrong_move\n",
      "   1303/500000: episode: 1293, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3962.000 [3962.000, 3962.000],  loss: 587935.375000, mae: 483.572327, mean_q: 12.745207\n",
      "wrong_move\n",
      "   1304/500000: episode: 1294, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3962.000 [3962.000, 3962.000],  loss: 412675.562500, mae: 484.313293, mean_q: 12.509200\n",
      "wrong_move\n",
      "   1305/500000: episode: 1295, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2443.000 [2443.000, 2443.000],  loss: 807642.500000, mae: 485.430267, mean_q: 12.532006\n",
      "wrong_move\n",
      "   1306/500000: episode: 1296, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 939.000 [939.000, 939.000],  loss: 159845.171875, mae: 486.643311, mean_q: 12.884078\n",
      "wrong_move\n",
      "   1307/500000: episode: 1297, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2869.000 [2869.000, 2869.000],  loss: 938519.000000, mae: 487.808899, mean_q: 12.230374\n",
      "wrong_move\n",
      "   1308/500000: episode: 1298, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3310.000 [3310.000, 3310.000],  loss: 48724.964844, mae: 488.988464, mean_q: 12.380356\n",
      "wrong_move\n",
      "   1309/500000: episode: 1299, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2496.000 [2496.000, 2496.000],  loss: 450983.031250, mae: 490.517181, mean_q: 12.317268\n",
      "wrong_move\n",
      "   1310/500000: episode: 1300, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3770.000 [3770.000, 3770.000],  loss: 414004.281250, mae: 491.886414, mean_q: 25.318905\n",
      "wrong_move\n",
      "   1311/500000: episode: 1301, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3212.000 [3212.000, 3212.000],  loss: 378569.281250, mae: 493.191101, mean_q: 43.170166\n",
      "wrong_move\n",
      "   1312/500000: episode: 1302, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3260.000 [3260.000, 3260.000],  loss: 32367.117188, mae: 493.744141, mean_q: 12.274747\n",
      "wrong_move\n",
      "   1313/500000: episode: 1303, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 594.000 [594.000, 594.000],  loss: 81344.984375, mae: 494.879578, mean_q: 12.286322\n",
      "wrong_move\n",
      "   1314/500000: episode: 1304, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1235.000 [1235.000, 1235.000],  loss: 32875.640625, mae: 496.364838, mean_q: 12.601458\n",
      "wrong_move\n",
      "   1315/500000: episode: 1305, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3120.000 [3120.000, 3120.000],  loss: 80958.421875, mae: 497.968933, mean_q: 12.447701\n",
      "wrong_move\n",
      "   1316/500000: episode: 1306, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1602.000 [1602.000, 1602.000],  loss: 114848.500000, mae: 499.450195, mean_q: 12.455471\n",
      "wrong_move\n",
      "   1317/500000: episode: 1307, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 21.000 [21.000, 21.000],  loss: 81439.773438, mae: 500.523956, mean_q: 12.733232\n",
      "wrong_move\n",
      "   1318/500000: episode: 1308, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1756.000 [1756.000, 1756.000],  loss: 230046.250000, mae: 501.529297, mean_q: 12.624614\n",
      "wrong_move\n",
      "   1319/500000: episode: 1309, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 171.000 [171.000, 171.000],  loss: 76788.921875, mae: 502.669861, mean_q: 12.788955\n",
      "wrong_move\n",
      "   1320/500000: episode: 1310, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 171.000 [171.000, 171.000],  loss: 63163.992188, mae: 503.674591, mean_q: 12.869917\n",
      "wrong_move\n",
      "   1321/500000: episode: 1311, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 171.000 [171.000, 171.000],  loss: 77058.671875, mae: 504.531555, mean_q: 12.356354\n",
      "wrong_move\n",
      "   1322/500000: episode: 1312, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1200.000 [1200.000, 1200.000],  loss: 28048.441406, mae: 505.156372, mean_q: 12.363674\n",
      "wrong_move\n",
      "   1323/500000: episode: 1313, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 171.000 [171.000, 171.000],  loss: 451408.187500, mae: 505.667480, mean_q: 12.578665\n",
      "wrong_move\n",
      "   1324/500000: episode: 1314, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 171.000 [171.000, 171.000],  loss: 45178.687500, mae: 506.211304, mean_q: 12.467780\n",
      "wrong_move\n",
      "   1325/500000: episode: 1315, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2757.000 [2757.000, 2757.000],  loss: 421283.843750, mae: 507.032104, mean_q: 12.218121\n",
      "wrong_move\n",
      "   1326/500000: episode: 1316, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2892.000 [2892.000, 2892.000],  loss: 81543.437500, mae: 507.926483, mean_q: 12.096455\n",
      "wrong_move\n",
      "   1327/500000: episode: 1317, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 520.000 [520.000, 520.000],  loss: 73309.109375, mae: 508.820496, mean_q: 12.356360\n",
      "wrong_move\n",
      "   1328/500000: episode: 1318, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1235.000 [1235.000, 1235.000],  loss: 512984.812500, mae: 509.762634, mean_q: 12.255789\n",
      "wrong_move\n",
      "   1329/500000: episode: 1319, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3852.000 [3852.000, 3852.000],  loss: 34552.820312, mae: 510.710541, mean_q: 12.332417\n",
      "wrong_move\n",
      "   1330/500000: episode: 1320, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3069.000 [3069.000, 3069.000],  loss: 78032.593750, mae: 511.938782, mean_q: 12.252707\n",
      "wrong_move\n",
      "   1331/500000: episode: 1321, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1799.000 [1799.000, 1799.000],  loss: 504677.968750, mae: 513.292847, mean_q: 12.460161\n",
      "wrong_move\n",
      "   1332/500000: episode: 1322, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 746.000 [746.000, 746.000],  loss: 427639.437500, mae: 514.329590, mean_q: 12.211308\n",
      "wrong_move\n",
      "   1333/500000: episode: 1323, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2656.000 [2656.000, 2656.000],  loss: 41224.210938, mae: 515.278809, mean_q: 12.158937\n",
      "wrong_move\n",
      "   1334/500000: episode: 1324, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1235.000 [1235.000, 1235.000],  loss: 20816.355469, mae: 516.078003, mean_q: 12.219545\n",
      "wrong_move\n",
      "   1335/500000: episode: 1325, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3233.000 [3233.000, 3233.000],  loss: 137407.703125, mae: 516.447937, mean_q: 12.360693\n",
      "wrong_move\n",
      "   1336/500000: episode: 1326, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3228.000 [3228.000, 3228.000],  loss: 55710.566406, mae: 516.827148, mean_q: 12.308044\n",
      "wrong_move\n",
      "   1337/500000: episode: 1327, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1398.000 [1398.000, 1398.000],  loss: 793276.250000, mae: 517.435669, mean_q: 12.302504\n",
      "wrong_move\n",
      "   1338/500000: episode: 1328, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 507.000 [507.000, 507.000],  loss: 495186.062500, mae: 517.868347, mean_q: 12.172392\n",
      "wrong_move\n",
      "   1339/500000: episode: 1329, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1235.000 [1235.000, 1235.000],  loss: 1154001.250000, mae: 518.565796, mean_q: 12.268416\n",
      "wrong_move\n",
      "   1340/500000: episode: 1330, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1235.000 [1235.000, 1235.000],  loss: 457180.437500, mae: 518.812622, mean_q: 12.372586\n",
      "wrong_move\n",
      "   1341/500000: episode: 1331, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 252.000 [252.000, 252.000],  loss: 808698.062500, mae: 519.481323, mean_q: 12.160114\n",
      "wrong_move\n",
      "   1342/500000: episode: 1332, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 635.000 [635.000, 635.000],  loss: 105184.656250, mae: 520.585022, mean_q: 35.648567\n",
      "wrong_move\n",
      "   1343/500000: episode: 1333, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3581.000 [3581.000, 3581.000],  loss: 29239.445312, mae: 521.719604, mean_q: 12.215601\n",
      "wrong_move\n",
      "   1344/500000: episode: 1334, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 33.000 [33.000, 33.000],  loss: 110370.585938, mae: 523.094666, mean_q: 12.164186\n",
      "wrong_move\n",
      "   1345/500000: episode: 1335, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3416.000 [3416.000, 3416.000],  loss: 27996.218750, mae: 524.602539, mean_q: 11.942348\n",
      "wrong_move\n",
      "   1346/500000: episode: 1336, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3416.000 [3416.000, 3416.000],  loss: 428963.218750, mae: 526.025391, mean_q: 12.144625\n",
      "wrong_move\n",
      "   1347/500000: episode: 1337, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 591.000 [591.000, 591.000],  loss: 301913.562500, mae: 527.202637, mean_q: 12.148287\n",
      "wrong_move\n",
      "   1348/500000: episode: 1338, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3416.000 [3416.000, 3416.000],  loss: 441409.093750, mae: 528.217285, mean_q: 12.299313\n",
      "wrong_move\n",
      "   1349/500000: episode: 1339, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 824.000 [824.000, 824.000],  loss: 83935.726562, mae: 529.358459, mean_q: 12.242847\n",
      "wrong_move\n",
      "   1350/500000: episode: 1340, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3174.000 [3174.000, 3174.000],  loss: 41998.527344, mae: 530.501404, mean_q: 16.886784\n",
      "wrong_move\n",
      "   1351/500000: episode: 1341, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3174.000 [3174.000, 3174.000],  loss: 22582.423828, mae: 531.643860, mean_q: 24.590851\n",
      "wrong_move\n",
      "   1352/500000: episode: 1342, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3174.000 [3174.000, 3174.000],  loss: 28102.585938, mae: 532.785889, mean_q: 12.342524\n",
      "wrong_move\n",
      "   1353/500000: episode: 1343, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3375.000 [3375.000, 3375.000],  loss: 106654.843750, mae: 533.679810, mean_q: 12.249844\n",
      "wrong_move\n",
      "   1354/500000: episode: 1344, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3980.000 [3980.000, 3980.000],  loss: 52871.679688, mae: 534.180176, mean_q: 12.080757\n",
      "wrong_move\n",
      "   1355/500000: episode: 1345, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3174.000 [3174.000, 3174.000],  loss: 28766.724609, mae: 534.353271, mean_q: 11.956073\n",
      "wrong_move\n",
      "   1356/500000: episode: 1346, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 678.000 [678.000, 678.000],  loss: 49169.707031, mae: 534.177734, mean_q: 29.365788\n",
      "wrong_move\n",
      "   1357/500000: episode: 1347, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1790.000 [1790.000, 1790.000],  loss: 448171.218750, mae: 533.676636, mean_q: 12.032370\n",
      "wrong_move\n",
      "   1358/500000: episode: 1348, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3980.000 [3980.000, 3980.000],  loss: 262452.250000, mae: 533.050110, mean_q: 12.188828\n",
      "wrong_move\n",
      "   1359/500000: episode: 1349, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3174.000 [3174.000, 3174.000],  loss: 32713.714844, mae: 532.586243, mean_q: 11.785331\n",
      "wrong_move\n",
      "   1360/500000: episode: 1350, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 17.000 [17.000, 17.000],  loss: 51910.664062, mae: 532.727356, mean_q: 11.974692\n",
      "wrong_move\n",
      "   1361/500000: episode: 1351, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3174.000 [3174.000, 3174.000],  loss: 218707.578125, mae: 533.300659, mean_q: 12.099951\n",
      "wrong_move\n",
      "   1362/500000: episode: 1352, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3174.000 [3174.000, 3174.000],  loss: 136013.625000, mae: 534.158813, mean_q: 18.850845\n",
      "wrong_move\n",
      "   1363/500000: episode: 1353, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3174.000 [3174.000, 3174.000],  loss: 466911.750000, mae: 535.017822, mean_q: 19.152473\n",
      "wrong_move\n",
      "   1364/500000: episode: 1354, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1508.000 [1508.000, 1508.000],  loss: 29672.271484, mae: 535.790710, mean_q: 11.918102\n",
      "wrong_move\n",
      "   1365/500000: episode: 1355, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3773.000 [3773.000, 3773.000],  loss: 403304.593750, mae: 536.702271, mean_q: 11.970530\n",
      "wrong_move\n",
      "   1366/500000: episode: 1356, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2747.000 [2747.000, 2747.000],  loss: 442572.562500, mae: 537.010437, mean_q: 11.994780\n",
      "wrong_move\n",
      "   1367/500000: episode: 1357, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3213.000 [3213.000, 3213.000],  loss: 170798.984375, mae: 537.665710, mean_q: 12.181915\n",
      "wrong_move\n",
      "   1368/500000: episode: 1358, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2391.000 [2391.000, 2391.000],  loss: 27033.882812, mae: 538.523315, mean_q: 11.859270\n",
      "wrong_move\n",
      "   1369/500000: episode: 1359, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3541.000 [3541.000, 3541.000],  loss: 391387.968750, mae: 539.422180, mean_q: 11.972034\n",
      "wrong_move\n",
      "   1370/500000: episode: 1360, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2391.000 [2391.000, 2391.000],  loss: 753677.625000, mae: 540.337341, mean_q: 11.882812\n",
      "wrong_move\n",
      "   1371/500000: episode: 1361, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 586.000 [586.000, 586.000],  loss: 57063.179688, mae: 541.372498, mean_q: 11.861802\n",
      "wrong_move\n",
      "   1372/500000: episode: 1362, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1508.000 [1508.000, 1508.000],  loss: 38873.746094, mae: 542.716248, mean_q: 12.049770\n",
      "wrong_move\n",
      "   1373/500000: episode: 1363, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 644.000 [644.000, 644.000],  loss: 35544.175781, mae: 544.129517, mean_q: 11.819916\n",
      "wrong_move\n",
      "   1374/500000: episode: 1364, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2838.000 [2838.000, 2838.000],  loss: 413084.312500, mae: 545.238098, mean_q: 11.869226\n",
      "wrong_move\n",
      "   1375/500000: episode: 1365, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2083.000 [2083.000, 2083.000],  loss: 64570.960938, mae: 546.175903, mean_q: 11.722666\n",
      "wrong_move\n",
      "   1376/500000: episode: 1366, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2558.000 [2558.000, 2558.000],  loss: 77869.007812, mae: 547.283813, mean_q: 11.764010\n",
      "wrong_move\n",
      "   1377/500000: episode: 1367, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1782.000 [1782.000, 1782.000],  loss: 512167.500000, mae: 548.504578, mean_q: 11.766253\n",
      "wrong_move\n",
      "   1378/500000: episode: 1368, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4080.000 [4080.000, 4080.000],  loss: 72210.609375, mae: 549.524292, mean_q: 11.670799\n",
      "wrong_move\n",
      "   1379/500000: episode: 1369, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 747.000 [747.000, 747.000],  loss: 77462.750000, mae: 550.379761, mean_q: 11.697074\n",
      "wrong_move\n",
      "   1380/500000: episode: 1370, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3078.000 [3078.000, 3078.000],  loss: 55804.929688, mae: 551.032104, mean_q: 11.990780\n",
      "wrong_move\n",
      "   1381/500000: episode: 1371, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1889.000 [1889.000, 1889.000],  loss: 428737.187500, mae: 551.664673, mean_q: 11.963869\n",
      "wrong_move\n",
      "   1382/500000: episode: 1372, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2229.000 [2229.000, 2229.000],  loss: 38707.945312, mae: 552.371460, mean_q: 11.739918\n",
      "wrong_move\n",
      "   1383/500000: episode: 1373, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4080.000 [4080.000, 4080.000],  loss: 421890.656250, mae: 553.076965, mean_q: 11.625095\n",
      "wrong_move\n",
      "   1384/500000: episode: 1374, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 694.000 [694.000, 694.000],  loss: 126248.117188, mae: 553.499878, mean_q: 11.863937\n",
      "wrong_move\n",
      "   1385/500000: episode: 1375, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3482.000 [3482.000, 3482.000],  loss: 437692.500000, mae: 554.045288, mean_q: 11.739681\n",
      "wrong_move\n",
      "   1386/500000: episode: 1376, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2191.000 [2191.000, 2191.000],  loss: 23112.019531, mae: 554.750122, mean_q: 11.703865\n",
      "wrong_move\n",
      "   1387/500000: episode: 1377, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4079.000 [4079.000, 4079.000],  loss: 156807.343750, mae: 555.609741, mean_q: 18.453514\n",
      "wrong_move\n",
      "   1388/500000: episode: 1378, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3078.000 [3078.000, 3078.000],  loss: 89730.265625, mae: 556.254761, mean_q: 38.053436\n",
      "wrong_move\n",
      "   1389/500000: episode: 1379, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1576.000 [1576.000, 1576.000],  loss: 845143.375000, mae: 556.927124, mean_q: 11.825650\n",
      "wrong_move\n",
      "   1390/500000: episode: 1380, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2921.000 [2921.000, 2921.000],  loss: 423139.875000, mae: 557.175537, mean_q: 11.958096\n",
      "wrong_move\n",
      "   1391/500000: episode: 1381, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2351.000 [2351.000, 2351.000],  loss: 38160.285156, mae: 557.711853, mean_q: 11.700806\n",
      "wrong_move\n",
      "   1392/500000: episode: 1382, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1058.000 [1058.000, 1058.000],  loss: 782237.000000, mae: 558.574402, mean_q: 11.630919\n",
      "wrong_move\n",
      "   1393/500000: episode: 1383, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2819.000 [2819.000, 2819.000],  loss: 334727.562500, mae: 559.608887, mean_q: 11.509981\n",
      "wrong_move\n",
      "   1394/500000: episode: 1384, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3078.000 [3078.000, 3078.000],  loss: 66684.671875, mae: 560.688110, mean_q: 21.435507\n",
      "wrong_move\n",
      "   1395/500000: episode: 1385, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1693.000 [1693.000, 1693.000],  loss: 434435.718750, mae: 562.053162, mean_q: 11.418674\n",
      "wrong_move\n",
      "   1396/500000: episode: 1386, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3774.000 [3774.000, 3774.000],  loss: 54343.960938, mae: 563.415955, mean_q: 27.754120\n",
      "wrong_move\n",
      "   1397/500000: episode: 1387, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3933.000 [3933.000, 3933.000],  loss: 485953.625000, mae: 564.501221, mean_q: 11.456835\n",
      "wrong_move\n",
      "   1398/500000: episode: 1388, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3933.000 [3933.000, 3933.000],  loss: 504484.250000, mae: 565.752808, mean_q: 11.713120\n",
      "wrong_move\n",
      "   1399/500000: episode: 1389, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 744.000 [744.000, 744.000],  loss: 109425.359375, mae: 566.744019, mean_q: 33.410130\n",
      "wrong_move\n",
      "   1400/500000: episode: 1390, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 744.000 [744.000, 744.000],  loss: 58657.867188, mae: 567.963928, mean_q: 11.675573\n",
      "wrong_move\n",
      "   1401/500000: episode: 1391, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 596.000 [596.000, 596.000],  loss: 829151.187500, mae: 569.766724, mean_q: 11.567719\n",
      "wrong_move\n",
      "   1402/500000: episode: 1392, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2807.000 [2807.000, 2807.000],  loss: 77242.710938, mae: 571.679626, mean_q: 11.323080\n",
      "wrong_move\n",
      "   1403/500000: episode: 1393, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3113.000 [3113.000, 3113.000],  loss: 40451.351562, mae: 573.367981, mean_q: 17.393595\n",
      "wrong_move\n",
      "   1404/500000: episode: 1394, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2807.000 [2807.000, 2807.000],  loss: 418459.062500, mae: 574.413391, mean_q: 11.354358\n",
      "wrong_move\n",
      "   1405/500000: episode: 1395, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2016.000 [2016.000, 2016.000],  loss: 36081.191406, mae: 575.275879, mean_q: 11.608785\n",
      "wrong_move\n",
      "   1406/500000: episode: 1396, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 596.000 [596.000, 596.000],  loss: 157836.875000, mae: 575.817017, mean_q: 11.578626\n",
      "wrong_move\n",
      "   1407/500000: episode: 1397, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1867.000 [1867.000, 1867.000],  loss: 431239.062500, mae: 575.933472, mean_q: 11.621922\n",
      "wrong_move\n",
      "   1408/500000: episode: 1398, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 162.000 [162.000, 162.000],  loss: 856301.687500, mae: 576.327454, mean_q: 11.351983\n",
      "wrong_move\n",
      "   1409/500000: episode: 1399, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1867.000 [1867.000, 1867.000],  loss: 42594.601562, mae: 576.960876, mean_q: 52.261765\n",
      "wrong_move\n",
      "   1410/500000: episode: 1400, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 248.000 [248.000, 248.000],  loss: 23708.136719, mae: 577.714355, mean_q: 11.799282\n",
      "wrong_move\n",
      "   1411/500000: episode: 1401, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3052.000 [3052.000, 3052.000],  loss: 73250.015625, mae: 578.356140, mean_q: 11.356279\n",
      "wrong_move\n",
      "   1412/500000: episode: 1402, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2427.000 [2427.000, 2427.000],  loss: 45149.492188, mae: 578.999878, mean_q: 11.154970\n",
      "wrong_move\n",
      "   1413/500000: episode: 1403, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1551.000 [1551.000, 1551.000],  loss: 199522.546875, mae: 579.589172, mean_q: 11.331464\n",
      "wrong_move\n",
      "   1414/500000: episode: 1404, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2427.000 [2427.000, 2427.000],  loss: 206031.531250, mae: 580.205444, mean_q: 11.243010\n",
      "wrong_move\n",
      "   1415/500000: episode: 1405, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2413.000 [2413.000, 2413.000],  loss: 109652.054688, mae: 580.684814, mean_q: 40.187347\n",
      "wrong_move\n",
      "   1416/500000: episode: 1406, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1019.000 [1019.000, 1019.000],  loss: 453525.562500, mae: 581.496094, mean_q: 11.326979\n",
      "wrong_move\n",
      "   1417/500000: episode: 1407, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2423.000 [2423.000, 2423.000],  loss: 329514.843750, mae: 582.440063, mean_q: 11.347717\n",
      "wrong_move\n",
      "   1418/500000: episode: 1408, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: 186825.437500, mae: 583.741089, mean_q: 11.666405\n",
      "wrong_move\n",
      "   1420/500000: episode: 1409, duration: 0.111s, episode steps:   2, steps per second:  18, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 3030.000 [2308.000, 3752.000],  loss: 86907.718750, mae: 585.408203, mean_q: 11.536356\n",
      "wrong_move\n",
      "   1421/500000: episode: 1410, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3752.000 [3752.000, 3752.000],  loss: 452597.187500, mae: 586.835632, mean_q: 11.252249\n",
      "wrong_move\n",
      "   1422/500000: episode: 1411, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1639.000 [1639.000, 1639.000],  loss: 26806.937500, mae: 587.981812, mean_q: 14.404862\n",
      "wrong_move\n",
      "   1423/500000: episode: 1412, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3957.000 [3957.000, 3957.000],  loss: 33731.144531, mae: 589.043091, mean_q: 11.242330\n",
      "wrong_move\n",
      "   1424/500000: episode: 1413, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 236.000 [236.000, 236.000],  loss: 71633.726562, mae: 590.101074, mean_q: 11.376806\n",
      "wrong_move\n",
      "   1425/500000: episode: 1414, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 944.000 [944.000, 944.000],  loss: 66749.671875, mae: 590.995605, mean_q: 11.437143\n",
      "wrong_move\n",
      "   1426/500000: episode: 1415, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 342.000 [342.000, 342.000],  loss: 137770.921875, mae: 591.291077, mean_q: 51.550652\n",
      "wrong_move\n",
      "   1427/500000: episode: 1416, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3000.000 [3000.000, 3000.000],  loss: 69179.023438, mae: 590.908203, mean_q: 11.421514\n",
      "wrong_move\n",
      "   1428/500000: episode: 1417, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: 136177.640625, mae: 590.549377, mean_q: 11.500067\n",
      "wrong_move\n",
      "   1429/500000: episode: 1418, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2529.000 [2529.000, 2529.000],  loss: 43352.796875, mae: 590.370850, mean_q: 11.476265\n",
      "wrong_move\n",
      "   1430/500000: episode: 1419, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3957.000 [3957.000, 3957.000],  loss: 170237.093750, mae: 590.182434, mean_q: 11.487819\n",
      "wrong_move\n",
      "   1431/500000: episode: 1420, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3957.000 [3957.000, 3957.000],  loss: 45946.515625, mae: 590.460938, mean_q: 11.228931\n",
      "wrong_move\n",
      "   1432/500000: episode: 1421, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3029.000 [3029.000, 3029.000],  loss: 426166.218750, mae: 591.034363, mean_q: 11.340002\n",
      "wrong_move\n",
      "   1433/500000: episode: 1422, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: 412316.500000, mae: 591.491211, mean_q: 11.406528\n",
      "wrong_move\n",
      "   1434/500000: episode: 1423, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: 197156.140625, mae: 591.848572, mean_q: 11.271030\n",
      "wrong_move\n",
      "   1435/500000: episode: 1424, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2843.000 [2843.000, 2843.000],  loss: 141518.750000, mae: 591.795776, mean_q: 11.248305\n",
      "wrong_move\n",
      "   1436/500000: episode: 1425, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: 155899.687500, mae: 591.546387, mean_q: 11.235953\n",
      "wrong_move\n",
      "   1437/500000: episode: 1426, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1539.000 [1539.000, 1539.000],  loss: 24593.175781, mae: 591.233521, mean_q: 11.293669\n",
      "wrong_move\n",
      "   1438/500000: episode: 1427, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2233.000 [2233.000, 2233.000],  loss: 16837.003906, mae: 591.233887, mean_q: 11.306992\n",
      "wrong_move\n",
      "   1439/500000: episode: 1428, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1701.000 [1701.000, 1701.000],  loss: 46869.441406, mae: 591.789001, mean_q: 11.548199\n",
      "wrong_move\n",
      "   1440/500000: episode: 1429, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3957.000 [3957.000, 3957.000],  loss: 420383.968750, mae: 593.377563, mean_q: 11.000738\n",
      "wrong_move\n",
      "   1441/500000: episode: 1430, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2338.000 [2338.000, 2338.000],  loss: 241588.234375, mae: 595.017151, mean_q: 11.322693\n",
      "wrong_move\n",
      "   1442/500000: episode: 1431, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: 35219.445312, mae: 596.577087, mean_q: 25.223577\n",
      "wrong_move\n",
      "   1443/500000: episode: 1432, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 23311.625000, mae: 597.803955, mean_q: 11.325825\n",
      "wrong_move\n",
      "   1444/500000: episode: 1433, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1983.000 [1983.000, 1983.000],  loss: 505214.468750, mae: 599.104980, mean_q: 55.062401\n",
      "wrong_move\n",
      "   1445/500000: episode: 1434, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2911.000 [2911.000, 2911.000],  loss: 418682.937500, mae: 600.447510, mean_q: 11.452562\n",
      "wrong_move\n",
      "   1446/500000: episode: 1435, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2911.000 [2911.000, 2911.000],  loss: 19596.462891, mae: 601.683105, mean_q: 12.156273\n",
      "wrong_move\n",
      "   1447/500000: episode: 1436, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1029.000 [1029.000, 1029.000],  loss: 24072.042969, mae: 602.694580, mean_q: 11.463967\n",
      "wrong_move\n",
      "   1448/500000: episode: 1437, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3616.000 [3616.000, 3616.000],  loss: 65468.894531, mae: 603.571045, mean_q: 11.263397\n",
      "wrong_move\n",
      "   1449/500000: episode: 1438, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: 55853.964844, mae: 604.093750, mean_q: 11.211499\n",
      "wrong_move\n",
      "   1450/500000: episode: 1439, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2717.000 [2717.000, 2717.000],  loss: 23164.480469, mae: 605.009644, mean_q: 11.013600\n",
      "wrong_move\n",
      "   1451/500000: episode: 1440, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3312.000 [3312.000, 3312.000],  loss: 28865.250000, mae: 606.236450, mean_q: 11.044785\n",
      "wrong_move\n",
      "   1452/500000: episode: 1441, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3616.000 [3616.000, 3616.000],  loss: 361073.875000, mae: 607.776672, mean_q: 11.098563\n",
      "wrong_move\n",
      "   1453/500000: episode: 1442, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2646.000 [2646.000, 2646.000],  loss: 62822.851562, mae: 609.400146, mean_q: 10.945430\n",
      "wrong_move\n",
      "   1454/500000: episode: 1443, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 39.000 [39.000, 39.000],  loss: 221198.187500, mae: 610.994934, mean_q: 43.563683\n",
      "wrong_move\n",
      "   1455/500000: episode: 1444, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: 30106.556641, mae: 612.570374, mean_q: 11.208604\n",
      "wrong_move\n",
      "   1456/500000: episode: 1445, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3850.000 [3850.000, 3850.000],  loss: 567173.750000, mae: 614.192993, mean_q: 11.271006\n",
      "wrong_move\n",
      "   1457/500000: episode: 1446, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2646.000 [2646.000, 2646.000],  loss: 125017.500000, mae: 614.992004, mean_q: 11.162244\n",
      "wrong_move\n",
      "   1458/500000: episode: 1447, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1547.000 [1547.000, 1547.000],  loss: 124355.710938, mae: 616.153687, mean_q: 11.021741\n",
      "wrong_move\n",
      "   1459/500000: episode: 1448, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 39.000 [39.000, 39.000],  loss: 74071.796875, mae: 617.501709, mean_q: 11.134009\n",
      "wrong_move\n",
      "   1460/500000: episode: 1449, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2795.000 [2795.000, 2795.000],  loss: 17880.453125, mae: 618.951416, mean_q: 10.906626\n",
      "wrong_move\n",
      "   1461/500000: episode: 1450, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2947.000 [2947.000, 2947.000],  loss: 583545.000000, mae: 620.325684, mean_q: 11.214784\n",
      "wrong_move\n",
      "   1462/500000: episode: 1451, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2570.000 [2570.000, 2570.000],  loss: 465339.875000, mae: 621.590332, mean_q: 11.197277\n",
      "wrong_move\n",
      "   1463/500000: episode: 1452, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2962.000 [2962.000, 2962.000],  loss: 39116.640625, mae: 623.199646, mean_q: 21.119614\n",
      "wrong_move\n",
      "   1464/500000: episode: 1453, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2147.000 [2147.000, 2147.000],  loss: 49345.945312, mae: 624.907593, mean_q: 12.367111\n",
      "wrong_move\n",
      "   1465/500000: episode: 1454, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2549.000 [2549.000, 2549.000],  loss: 28601.429688, mae: 626.739441, mean_q: 11.135636\n",
      "wrong_move\n",
      "   1466/500000: episode: 1455, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3557.000 [3557.000, 3557.000],  loss: 190709.390625, mae: 628.500610, mean_q: 11.061945\n",
      "wrong_move\n",
      "   1467/500000: episode: 1456, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 39.000 [39.000, 39.000],  loss: 24789.792969, mae: 629.780029, mean_q: 10.982080\n",
      "wrong_move\n",
      "   1468/500000: episode: 1457, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 775.000 [775.000, 775.000],  loss: 41485.757812, mae: 630.620850, mean_q: 11.062478\n",
      "wrong_move\n",
      "   1469/500000: episode: 1458, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3308.000 [3308.000, 3308.000],  loss: 11788.707031, mae: 631.455200, mean_q: 11.125713\n",
      "wrong_move\n",
      "   1470/500000: episode: 1459, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1427.000 [1427.000, 1427.000],  loss: 379434.750000, mae: 632.441040, mean_q: 11.321942\n",
      "wrong_move\n",
      "   1471/500000: episode: 1460, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3182.000 [3182.000, 3182.000],  loss: 429149.500000, mae: 632.610901, mean_q: 10.839811\n",
      "wrong_move\n",
      "   1472/500000: episode: 1461, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1506.000 [1506.000, 1506.000],  loss: 415235.781250, mae: 632.427795, mean_q: 15.726260\n",
      "wrong_move\n",
      "   1473/500000: episode: 1462, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 802.000 [802.000, 802.000],  loss: 27330.068359, mae: 632.093018, mean_q: 11.026858\n",
      "wrong_move\n",
      "   1474/500000: episode: 1463, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1843.000 [1843.000, 1843.000],  loss: 80080.390625, mae: 631.942139, mean_q: 11.248269\n",
      "wrong_move\n",
      "   1475/500000: episode: 1464, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1078.000 [1078.000, 1078.000],  loss: 21890.292969, mae: 631.729492, mean_q: 11.184125\n",
      "wrong_move\n",
      "   1476/500000: episode: 1465, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 733731.750000, mae: 632.082581, mean_q: 11.318530\n",
      "wrong_move\n",
      "   1477/500000: episode: 1466, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2414.000 [2414.000, 2414.000],  loss: 27969.710938, mae: 632.131897, mean_q: 20.553925\n",
      "wrong_move\n",
      "   1478/500000: episode: 1467, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2414.000 [2414.000, 2414.000],  loss: 179116.171875, mae: 632.425293, mean_q: 11.078524\n",
      "wrong_move\n",
      "   1479/500000: episode: 1468, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1464.000 [1464.000, 1464.000],  loss: 409914.593750, mae: 632.397949, mean_q: 24.284584\n",
      "wrong_move\n",
      "   1480/500000: episode: 1469, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1999.000 [1999.000, 1999.000],  loss: 447509.906250, mae: 632.244568, mean_q: 10.817151\n",
      "wrong_move\n",
      "   1481/500000: episode: 1470, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 376.000 [376.000, 376.000],  loss: 139192.703125, mae: 632.254395, mean_q: 14.487070\n",
      "wrong_move\n",
      "   1482/500000: episode: 1471, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3787.000 [3787.000, 3787.000],  loss: 426107.656250, mae: 632.434692, mean_q: 62.369816\n",
      "wrong_move\n",
      "   1483/500000: episode: 1472, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3629.000 [3629.000, 3629.000],  loss: 49547.085938, mae: 633.704163, mean_q: 40.328121\n",
      "wrong_move\n",
      "   1484/500000: episode: 1473, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2866.000 [2866.000, 2866.000],  loss: 50690.429688, mae: 635.695679, mean_q: 11.055089\n",
      "wrong_move\n",
      "   1485/500000: episode: 1474, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 611.000 [611.000, 611.000],  loss: 163848.078125, mae: 638.105835, mean_q: 10.785515\n",
      "wrong_move\n",
      "   1486/500000: episode: 1475, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1232.000 [1232.000, 1232.000],  loss: 432925.406250, mae: 641.076233, mean_q: 10.900118\n",
      "wrong_move\n",
      "   1487/500000: episode: 1476, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2866.000 [2866.000, 2866.000],  loss: 31825.914062, mae: 643.873535, mean_q: 10.976435\n",
      "wrong_move\n",
      "   1488/500000: episode: 1477, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3205.000 [3205.000, 3205.000],  loss: 114906.078125, mae: 646.168579, mean_q: 10.910697\n",
      "wrong_move\n",
      "   1489/500000: episode: 1478, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1427.000 [1427.000, 1427.000],  loss: 902709.625000, mae: 648.081238, mean_q: 10.959305\n",
      "wrong_move\n",
      "   1490/500000: episode: 1479, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2369.000 [2369.000, 2369.000],  loss: 308454.031250, mae: 649.724243, mean_q: 10.965099\n",
      "wrong_move\n",
      "   1491/500000: episode: 1480, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2485.000 [2485.000, 2485.000],  loss: 72302.882812, mae: 650.527710, mean_q: 10.964443\n",
      "wrong_move\n",
      "   1492/500000: episode: 1481, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3286.000 [3286.000, 3286.000],  loss: 33928.519531, mae: 650.204651, mean_q: 11.048009\n",
      "wrong_move\n",
      "   1493/500000: episode: 1482, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 287.000 [287.000, 287.000],  loss: 46738.230469, mae: 650.332153, mean_q: 11.079014\n",
      "wrong_move\n",
      "   1494/500000: episode: 1483, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3596.000 [3596.000, 3596.000],  loss: 312103.125000, mae: 651.041992, mean_q: 10.827510\n",
      "wrong_move\n",
      "   1495/500000: episode: 1484, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2369.000 [2369.000, 2369.000],  loss: 420181.187500, mae: 652.026245, mean_q: 11.049410\n",
      "wrong_move\n",
      "   1496/500000: episode: 1485, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1634.000 [1634.000, 1634.000],  loss: 415351.093750, mae: 653.512451, mean_q: 10.755330\n",
      "wrong_move\n",
      "   1497/500000: episode: 1486, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1697.000 [1697.000, 1697.000],  loss: 495448.906250, mae: 654.856873, mean_q: 10.622061\n",
      "wrong_move\n",
      "   1498/500000: episode: 1487, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 356.000 [356.000, 356.000],  loss: 521385.531250, mae: 655.378296, mean_q: 10.866316\n",
      "wrong_move\n",
      "   1499/500000: episode: 1488, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1855.000 [1855.000, 1855.000],  loss: 226706.312500, mae: 655.276001, mean_q: 10.756762\n",
      "wrong_move\n",
      "   1500/500000: episode: 1489, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 356.000 [356.000, 356.000],  loss: 418916.250000, mae: 654.881348, mean_q: 10.658035\n",
      "wrong_move\n",
      "   1501/500000: episode: 1490, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 356.000 [356.000, 356.000],  loss: 457236.937500, mae: 655.570129, mean_q: 10.649152\n",
      "wrong_move\n",
      "   1502/500000: episode: 1491, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2369.000 [2369.000, 2369.000],  loss: 49507.347656, mae: 656.690430, mean_q: 10.595869\n",
      "wrong_move\n",
      "   1503/500000: episode: 1492, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 998.000 [998.000, 998.000],  loss: 33718.835938, mae: 658.313599, mean_q: 10.647854\n",
      "wrong_move\n",
      "   1504/500000: episode: 1493, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2147.000 [2147.000, 2147.000],  loss: 41310.617188, mae: 659.596313, mean_q: 10.756624\n",
      "wrong_move\n",
      "   1505/500000: episode: 1494, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2707.000 [2707.000, 2707.000],  loss: 546997.375000, mae: 660.929810, mean_q: 10.645950\n",
      "wrong_move\n",
      "   1506/500000: episode: 1495, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1373.000 [1373.000, 1373.000],  loss: 56465.441406, mae: 661.442749, mean_q: 10.877716\n",
      "wrong_move\n",
      "   1507/500000: episode: 1496, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1070.000 [1070.000, 1070.000],  loss: 804192.437500, mae: 661.674072, mean_q: 10.599154\n",
      "wrong_move\n",
      "   1508/500000: episode: 1497, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3677.000 [3677.000, 3677.000],  loss: 19337.597656, mae: 661.814392, mean_q: 10.626904\n",
      "wrong_move\n",
      "   1509/500000: episode: 1498, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3778.000 [3778.000, 3778.000],  loss: 73716.031250, mae: 662.175537, mean_q: 10.804384\n",
      "wrong_move\n",
      "   1510/500000: episode: 1499, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 278.000 [278.000, 278.000],  loss: 105773.218750, mae: 662.575256, mean_q: 10.495121\n",
      "wrong_move\n",
      "   1511/500000: episode: 1500, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 572.000 [572.000, 572.000],  loss: 35333.773438, mae: 663.188416, mean_q: 10.640234\n",
      "wrong_move\n",
      "   1512/500000: episode: 1501, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 393.000 [393.000, 393.000],  loss: 11744.357422, mae: 664.129028, mean_q: 10.627638\n",
      "wrong_move\n",
      "   1513/500000: episode: 1502, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3677.000 [3677.000, 3677.000],  loss: 139588.546875, mae: 665.062866, mean_q: 10.332541\n",
      "wrong_move\n",
      "   1514/500000: episode: 1503, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 81.000 [81.000, 81.000],  loss: 45912.156250, mae: 666.107422, mean_q: 10.460241\n",
      "wrong_move\n",
      "   1515/500000: episode: 1504, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 624.000 [624.000, 624.000],  loss: 989637.812500, mae: 667.332031, mean_q: 10.443860\n",
      "wrong_move\n",
      "   1516/500000: episode: 1505, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3596.000 [3596.000, 3596.000],  loss: 61984.687500, mae: 668.245605, mean_q: 10.584868\n",
      "wrong_move\n",
      "   1517/500000: episode: 1506, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 358.000 [358.000, 358.000],  loss: 252442.734375, mae: 669.939087, mean_q: 10.593216\n",
      "wrong_move\n",
      "   1518/500000: episode: 1507, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 95107.187500, mae: 671.961060, mean_q: 10.671070\n",
      "wrong_move\n",
      "   1519/500000: episode: 1508, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2136.000 [2136.000, 2136.000],  loss: 20850.400391, mae: 673.220032, mean_q: 10.498329\n",
      "wrong_move\n",
      "   1520/500000: episode: 1509, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 283.000 [283.000, 283.000],  loss: 30107.382812, mae: 674.283813, mean_q: 10.806164\n",
      "wrong_move\n",
      "   1521/500000: episode: 1510, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 948.000 [948.000, 948.000],  loss: 613846.375000, mae: 675.269714, mean_q: 10.594405\n",
      "wrong_move\n",
      "   1522/500000: episode: 1511, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 948.000 [948.000, 948.000],  loss: 67107.906250, mae: 676.316528, mean_q: 10.626179\n",
      "wrong_move\n",
      "   1523/500000: episode: 1512, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 948.000 [948.000, 948.000],  loss: 67673.062500, mae: 677.614746, mean_q: 10.532805\n",
      "wrong_move\n",
      "   1524/500000: episode: 1513, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1485.000 [1485.000, 1485.000],  loss: 17333.914062, mae: 678.305298, mean_q: 10.511840\n",
      "wrong_move\n",
      "   1525/500000: episode: 1514, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1135.000 [1135.000, 1135.000],  loss: 100901.843750, mae: 678.804138, mean_q: 10.368962\n",
      "wrong_move\n",
      "   1526/500000: episode: 1515, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3867.000 [3867.000, 3867.000],  loss: 126459.562500, mae: 679.628235, mean_q: 10.515086\n",
      "wrong_move\n",
      "   1527/500000: episode: 1516, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2156.000 [2156.000, 2156.000],  loss: 242455.640625, mae: 680.567444, mean_q: 10.546518\n",
      "wrong_move\n",
      "   1528/500000: episode: 1517, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 277.000 [277.000, 277.000],  loss: 190551.687500, mae: 681.825378, mean_q: 10.510500\n",
      "wrong_move\n",
      "   1529/500000: episode: 1518, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 490.000 [490.000, 490.000],  loss: 30244.128906, mae: 683.169678, mean_q: 10.580217\n",
      "wrong_move\n",
      "   1530/500000: episode: 1519, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3470.000 [3470.000, 3470.000],  loss: 460642.343750, mae: 684.940308, mean_q: 10.518169\n",
      "wrong_move\n",
      "   1531/500000: episode: 1520, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 624.000 [624.000, 624.000],  loss: 442289.468750, mae: 686.398865, mean_q: 10.513731\n",
      "wrong_move\n",
      "   1532/500000: episode: 1521, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3539.000 [3539.000, 3539.000],  loss: 26111.583984, mae: 687.248840, mean_q: 10.723234\n",
      "wrong_move\n",
      "   1533/500000: episode: 1522, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1586.000 [1586.000, 1586.000],  loss: 24476.230469, mae: 688.377502, mean_q: 10.422068\n",
      "wrong_move\n",
      "   1534/500000: episode: 1523, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3594.000 [3594.000, 3594.000],  loss: 805214.500000, mae: 689.421997, mean_q: 10.372252\n",
      "wrong_move\n",
      "   1535/500000: episode: 1524, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 759.000 [759.000, 759.000],  loss: 455835.125000, mae: 690.318970, mean_q: 10.480606\n",
      "wrong_move\n",
      "   1536/500000: episode: 1525, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1657.000 [1657.000, 1657.000],  loss: 21187.121094, mae: 691.467651, mean_q: 10.252821\n",
      "wrong_move\n",
      "   1537/500000: episode: 1526, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3043.000 [3043.000, 3043.000],  loss: 79467.906250, mae: 692.846558, mean_q: 10.446419\n",
      "wrong_move\n",
      "   1538/500000: episode: 1527, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3724.000 [3724.000, 3724.000],  loss: 492100.187500, mae: 693.690552, mean_q: 10.229910\n",
      "wrong_move\n",
      "   1539/500000: episode: 1528, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3955.000 [3955.000, 3955.000],  loss: 39829.777344, mae: 693.919373, mean_q: 10.476305\n",
      "wrong_move\n",
      "   1540/500000: episode: 1529, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2703.000 [2703.000, 2703.000],  loss: 28818.884766, mae: 694.094666, mean_q: 10.289408\n",
      "wrong_move\n",
      "   1541/500000: episode: 1530, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2703.000 [2703.000, 2703.000],  loss: 414038.000000, mae: 694.498535, mean_q: 10.416905\n",
      "wrong_move\n",
      "   1542/500000: episode: 1531, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1611.000 [1611.000, 1611.000],  loss: 192895.546875, mae: 694.596008, mean_q: 10.577028\n",
      "wrong_move\n",
      "   1543/500000: episode: 1532, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2935.000 [2935.000, 2935.000],  loss: 407669.125000, mae: 694.579346, mean_q: 10.532280\n",
      "wrong_move\n",
      "   1544/500000: episode: 1533, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 429.000 [429.000, 429.000],  loss: 36570.164062, mae: 694.959839, mean_q: 10.379752\n",
      "wrong_move\n",
      "   1545/500000: episode: 1534, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1875.000 [1875.000, 1875.000],  loss: 34326.765625, mae: 695.546082, mean_q: 10.210857\n",
      "wrong_move\n",
      "   1546/500000: episode: 1535, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3348.000 [3348.000, 3348.000],  loss: 415873.500000, mae: 695.958984, mean_q: 10.166336\n",
      "wrong_move\n",
      "   1547/500000: episode: 1536, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3264.000 [3264.000, 3264.000],  loss: 807889.187500, mae: 696.716614, mean_q: 10.434964\n",
      "wrong_move\n",
      "   1548/500000: episode: 1537, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1131.000 [1131.000, 1131.000],  loss: 341598.000000, mae: 697.820801, mean_q: 10.284351\n",
      "wrong_move\n",
      "   1549/500000: episode: 1538, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2878.000 [2878.000, 2878.000],  loss: 39937.171875, mae: 699.492676, mean_q: 10.246662\n",
      "wrong_move\n",
      "   1550/500000: episode: 1539, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1017.000 [1017.000, 1017.000],  loss: 581021.812500, mae: 700.790161, mean_q: 10.232989\n",
      "wrong_move\n",
      "   1551/500000: episode: 1540, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2113.000 [2113.000, 2113.000],  loss: 21395.363281, mae: 701.876648, mean_q: 10.249803\n",
      "wrong_move\n",
      "   1552/500000: episode: 1541, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3015.000 [3015.000, 3015.000],  loss: 178293.312500, mae: 703.294189, mean_q: 10.385693\n",
      "wrong_move\n",
      "   1553/500000: episode: 1542, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 848.000 [848.000, 848.000],  loss: 417320.906250, mae: 703.760132, mean_q: 10.313972\n",
      "wrong_move\n",
      "   1554/500000: episode: 1543, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1017.000 [1017.000, 1017.000],  loss: 419228.906250, mae: 704.843018, mean_q: 10.343292\n",
      "wrong_move\n",
      "   1555/500000: episode: 1544, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2916.000 [2916.000, 2916.000],  loss: 41170.191406, mae: 706.524536, mean_q: 10.197748\n",
      "wrong_move\n",
      "   1556/500000: episode: 1545, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 288.000 [288.000, 288.000],  loss: 450513.656250, mae: 708.284912, mean_q: 13.016848\n",
      "wrong_move\n",
      "   1557/500000: episode: 1546, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2096.000 [2096.000, 2096.000],  loss: 345012.656250, mae: 709.495117, mean_q: 15.938072\n",
      "wrong_move\n",
      "   1558/500000: episode: 1547, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 893.000 [893.000, 893.000],  loss: 159804.156250, mae: 709.946655, mean_q: 10.307326\n",
      "wrong_move\n",
      "   1559/500000: episode: 1548, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3379.000 [3379.000, 3379.000],  loss: 42329.242188, mae: 710.041077, mean_q: 21.568024\n",
      "wrong_move\n",
      "   1560/500000: episode: 1549, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2113.000 [2113.000, 2113.000],  loss: 34133.496094, mae: 710.327637, mean_q: 14.097358\n",
      "wrong_move\n",
      "   1561/500000: episode: 1550, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3379.000 [3379.000, 3379.000],  loss: 52945.054688, mae: 710.790527, mean_q: 33.604729\n",
      "wrong_move\n",
      "   1562/500000: episode: 1551, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3745.000 [3745.000, 3745.000],  loss: 464300.437500, mae: 711.455078, mean_q: 25.445761\n",
      "wrong_move\n",
      "   1563/500000: episode: 1552, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3379.000 [3379.000, 3379.000],  loss: 39619.406250, mae: 711.752258, mean_q: 33.549812\n",
      "wrong_move\n",
      "   1564/500000: episode: 1553, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3752.000 [3752.000, 3752.000],  loss: 50382.964844, mae: 712.457031, mean_q: 23.305538\n",
      "wrong_move\n",
      "   1565/500000: episode: 1554, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3752.000 [3752.000, 3752.000],  loss: 436108.312500, mae: 713.299194, mean_q: 11.502859\n",
      "wrong_move\n",
      "   1566/500000: episode: 1555, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3752.000 [3752.000, 3752.000],  loss: 13332.465820, mae: 713.783875, mean_q: 35.217514\n",
      "wrong_move\n",
      "   1567/500000: episode: 1556, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3594.000 [3594.000, 3594.000],  loss: 461594.937500, mae: 714.356567, mean_q: 17.029335\n",
      "wrong_move\n",
      "   1568/500000: episode: 1557, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 328.000 [328.000, 328.000],  loss: 31470.437500, mae: 714.975586, mean_q: 10.232126\n",
      "wrong_move\n",
      "   1569/500000: episode: 1558, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: 29846.550781, mae: 715.420593, mean_q: 10.303314\n",
      "wrong_move\n",
      "   1570/500000: episode: 1559, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 928.000 [928.000, 928.000],  loss: 409351.875000, mae: 715.811157, mean_q: 10.071945\n",
      "wrong_move\n",
      "   1571/500000: episode: 1560, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 713.000 [713.000, 713.000],  loss: 13652.500000, mae: 716.585693, mean_q: 10.217928\n",
      "wrong_move\n",
      "   1572/500000: episode: 1561, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2118.000 [2118.000, 2118.000],  loss: 474195.000000, mae: 717.713501, mean_q: 10.302519\n",
      "wrong_move\n",
      "   1573/500000: episode: 1562, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1047.000 [1047.000, 1047.000],  loss: 543053.562500, mae: 717.601196, mean_q: 10.074268\n",
      "wrong_move\n",
      "   1574/500000: episode: 1563, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1188.000 [1188.000, 1188.000],  loss: 21319.433594, mae: 717.804932, mean_q: 10.071108\n",
      "wrong_move\n",
      "   1575/500000: episode: 1564, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 928.000 [928.000, 928.000],  loss: 79933.406250, mae: 718.519531, mean_q: 10.222006\n",
      "wrong_move\n",
      "   1576/500000: episode: 1565, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2935.000 [2935.000, 2935.000],  loss: 458072.812500, mae: 719.773987, mean_q: 10.105252\n",
      "wrong_move\n",
      "   1577/500000: episode: 1566, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3362.000 [3362.000, 3362.000],  loss: 82756.757812, mae: 721.205933, mean_q: 31.675190\n",
      "wrong_move\n",
      "   1578/500000: episode: 1567, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2808.000 [2808.000, 2808.000],  loss: 616077.000000, mae: 723.682495, mean_q: 20.832899\n",
      "wrong_move\n",
      "   1579/500000: episode: 1568, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3007.000 [3007.000, 3007.000],  loss: 69838.125000, mae: 725.765747, mean_q: 10.407629\n",
      "wrong_move\n",
      "   1580/500000: episode: 1569, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 809.000 [809.000, 809.000],  loss: 67195.875000, mae: 728.001282, mean_q: 10.112130\n",
      "wrong_move\n",
      "   1581/500000: episode: 1570, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3360.000 [3360.000, 3360.000],  loss: 90999.875000, mae: 730.338745, mean_q: 10.053370\n",
      "wrong_move\n",
      "   1582/500000: episode: 1571, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 219.000 [219.000, 219.000],  loss: 19395.402344, mae: 733.139893, mean_q: 10.095070\n",
      "wrong_move\n",
      "   1583/500000: episode: 1572, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2309.000 [2309.000, 2309.000],  loss: 908201.500000, mae: 735.861450, mean_q: 10.071360\n",
      "wrong_move\n",
      "   1584/500000: episode: 1573, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3112.000 [3112.000, 3112.000],  loss: 102047.898438, mae: 735.794922, mean_q: 9.977548\n",
      "wrong_move\n",
      "   1585/500000: episode: 1574, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2309.000 [2309.000, 2309.000],  loss: 426371.750000, mae: 735.915894, mean_q: 10.157795\n",
      "wrong_move\n",
      "   1586/500000: episode: 1575, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2041.000 [2041.000, 2041.000],  loss: 863063.125000, mae: 736.356934, mean_q: 10.217474\n",
      "wrong_move\n",
      "   1587/500000: episode: 1576, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3094.000 [3094.000, 3094.000],  loss: 745359.500000, mae: 737.149536, mean_q: 10.096845\n",
      "wrong_move\n",
      "   1588/500000: episode: 1577, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1531.000 [1531.000, 1531.000],  loss: 553879.125000, mae: 738.349670, mean_q: 9.885072\n",
      "wrong_move\n",
      "   1589/500000: episode: 1578, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2309.000 [2309.000, 2309.000],  loss: 50046.984375, mae: 740.521606, mean_q: 10.174254\n",
      "wrong_move\n",
      "   1590/500000: episode: 1579, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3977.000 [3977.000, 3977.000],  loss: 35897.398438, mae: 742.517273, mean_q: 9.853893\n",
      "wrong_move\n",
      "   1591/500000: episode: 1580, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2857.000 [2857.000, 2857.000],  loss: 448812.250000, mae: 744.119019, mean_q: 9.813106\n",
      "wrong_move\n",
      "   1592/500000: episode: 1581, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3977.000 [3977.000, 3977.000],  loss: 36226.539062, mae: 744.988647, mean_q: 9.913334\n",
      "wrong_move\n",
      "   1593/500000: episode: 1582, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 418.000 [418.000, 418.000],  loss: 851953.687500, mae: 746.246948, mean_q: 9.885659\n",
      "wrong_move\n",
      "   1594/500000: episode: 1583, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1151.000 [1151.000, 1151.000],  loss: 429118.937500, mae: 748.690613, mean_q: 9.979681\n",
      "wrong_move\n",
      "   1595/500000: episode: 1584, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2140.000 [2140.000, 2140.000],  loss: 151946.125000, mae: 751.445068, mean_q: 10.046467\n",
      "wrong_move\n",
      "   1596/500000: episode: 1585, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3617.000 [3617.000, 3617.000],  loss: 184801.578125, mae: 752.715332, mean_q: 9.895519\n",
      "wrong_move\n",
      "   1597/500000: episode: 1586, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3977.000 [3977.000, 3977.000],  loss: 211946.593750, mae: 752.986511, mean_q: 9.828831\n",
      "wrong_move\n",
      "   1598/500000: episode: 1587, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2753.000 [2753.000, 2753.000],  loss: 339725.031250, mae: 753.987854, mean_q: 9.860216\n",
      "wrong_move\n",
      "   1599/500000: episode: 1588, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3977.000 [3977.000, 3977.000],  loss: 147078.171875, mae: 754.248474, mean_q: 10.095641\n",
      "wrong_move\n",
      "   1601/500000: episode: 1589, duration: 0.127s, episode steps:   2, steps per second:  16, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 202.000 [202.000, 202.000],  loss: 183913.484375, mae: 755.000366, mean_q: 9.889658\n",
      "wrong_move\n",
      "   1602/500000: episode: 1590, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2596.000 [2596.000, 2596.000],  loss: 418559.000000, mae: 755.327881, mean_q: 9.995857\n",
      "wrong_move\n",
      "   1603/500000: episode: 1591, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4019.000 [4019.000, 4019.000],  loss: 445890.656250, mae: 755.899780, mean_q: 9.790430\n",
      "wrong_move\n",
      "   1604/500000: episode: 1592, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1352.000 [1352.000, 1352.000],  loss: 51222.867188, mae: 757.508606, mean_q: 9.883801\n",
      "wrong_move\n",
      "   1605/500000: episode: 1593, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2309.000 [2309.000, 2309.000],  loss: 35090.191406, mae: 758.682861, mean_q: 9.889768\n",
      "wrong_move\n",
      "   1606/500000: episode: 1594, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1771.000 [1771.000, 1771.000],  loss: 63998.765625, mae: 759.389587, mean_q: 9.872507\n",
      "wrong_move\n",
      "   1607/500000: episode: 1595, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1771.000 [1771.000, 1771.000],  loss: 126372.000000, mae: 759.169739, mean_q: 9.698382\n",
      "wrong_move\n",
      "   1608/500000: episode: 1596, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2309.000 [2309.000, 2309.000],  loss: 77982.671875, mae: 758.891479, mean_q: 9.738948\n",
      "wrong_move\n",
      "   1609/500000: episode: 1597, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 909.000 [909.000, 909.000],  loss: 26313.806641, mae: 759.470337, mean_q: 9.772008\n",
      "wrong_move\n",
      "   1610/500000: episode: 1598, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1635.000 [1635.000, 1635.000],  loss: 126476.695312, mae: 759.794434, mean_q: 9.772499\n",
      "wrong_move\n",
      "   1611/500000: episode: 1599, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3227.000 [3227.000, 3227.000],  loss: 36543.726562, mae: 759.788086, mean_q: 9.902876\n",
      "wrong_move\n",
      "   1612/500000: episode: 1600, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 582.000 [582.000, 582.000],  loss: 118025.796875, mae: 760.137817, mean_q: 9.948729\n",
      "wrong_move\n",
      "   1613/500000: episode: 1601, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3553.000 [3553.000, 3553.000],  loss: 428449.218750, mae: 760.812500, mean_q: 9.803863\n",
      "wrong_move\n",
      "   1614/500000: episode: 1602, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3868.000 [3868.000, 3868.000],  loss: 896736.125000, mae: 761.580566, mean_q: 9.859005\n",
      "wrong_move\n",
      "   1615/500000: episode: 1603, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 959.000 [959.000, 959.000],  loss: 25217.945312, mae: 760.353638, mean_q: 9.798056\n",
      "wrong_move\n",
      "   1616/500000: episode: 1604, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1475.000 [1475.000, 1475.000],  loss: 210601.203125, mae: 760.161987, mean_q: 9.772344\n",
      "wrong_move\n",
      "   1617/500000: episode: 1605, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 315.000 [315.000, 315.000],  loss: 160922.500000, mae: 760.721252, mean_q: 9.674065\n",
      "wrong_move\n",
      "   1618/500000: episode: 1606, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 972.000 [972.000, 972.000],  loss: 288525.531250, mae: 760.644104, mean_q: 9.732006\n",
      "wrong_move\n",
      "   1619/500000: episode: 1607, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3718.000 [3718.000, 3718.000],  loss: 22347.792969, mae: 760.141846, mean_q: 9.846563\n",
      "wrong_move\n",
      "   1620/500000: episode: 1608, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1919.000 [1919.000, 1919.000],  loss: 608189.250000, mae: 760.531006, mean_q: 9.690949\n",
      "wrong_move\n",
      "   1621/500000: episode: 1609, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 125.000 [125.000, 125.000],  loss: 144712.562500, mae: 761.136108, mean_q: 9.715485\n",
      "wrong_move\n",
      "   1622/500000: episode: 1610, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 212.000 [212.000, 212.000],  loss: 110936.476562, mae: 762.826172, mean_q: 15.549185\n",
      "wrong_move\n",
      "   1623/500000: episode: 1611, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3388.000 [3388.000, 3388.000],  loss: 56876.484375, mae: 764.786987, mean_q: 9.707258\n",
      "wrong_move\n",
      "   1624/500000: episode: 1612, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 307.000 [307.000, 307.000],  loss: 225088.671875, mae: 767.597778, mean_q: 9.716585\n",
      "wrong_move\n",
      "   1625/500000: episode: 1613, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1035.000 [1035.000, 1035.000],  loss: 830673.250000, mae: 769.667847, mean_q: 9.535719\n",
      "wrong_move\n",
      "   1626/500000: episode: 1614, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3532.000 [3532.000, 3532.000],  loss: 33305.832031, mae: 771.299316, mean_q: 9.914139\n",
      "wrong_move\n",
      "   1627/500000: episode: 1615, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3260.000 [3260.000, 3260.000],  loss: 102359.687500, mae: 772.902832, mean_q: 9.740414\n",
      "wrong_move\n",
      "   1628/500000: episode: 1616, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2569.000 [2569.000, 2569.000],  loss: 34445.535156, mae: 774.267090, mean_q: 9.657534\n",
      "wrong_move\n",
      "   1629/500000: episode: 1617, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 218.000 [218.000, 218.000],  loss: 405759.187500, mae: 775.377441, mean_q: 9.563421\n",
      "wrong_move\n",
      "   1630/500000: episode: 1618, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2569.000 [2569.000, 2569.000],  loss: 27278.597656, mae: 776.226440, mean_q: 9.610417\n",
      "wrong_move\n",
      "   1631/500000: episode: 1619, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 829.000 [829.000, 829.000],  loss: 283803.468750, mae: 776.376831, mean_q: 9.864029\n",
      "wrong_move\n",
      "   1632/500000: episode: 1620, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 492.000 [492.000, 492.000],  loss: 521679.375000, mae: 775.977722, mean_q: 9.841315\n",
      "wrong_move\n",
      "   1633/500000: episode: 1621, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3868.000 [3868.000, 3868.000],  loss: 16186.469727, mae: 775.376709, mean_q: 9.751574\n",
      "wrong_move\n",
      "   1634/500000: episode: 1622, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3532.000 [3532.000, 3532.000],  loss: 463957.031250, mae: 774.645996, mean_q: 9.520986\n",
      "wrong_move\n",
      "   1635/500000: episode: 1623, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1705.000 [1705.000, 1705.000],  loss: 415802.093750, mae: 774.143066, mean_q: 9.570482\n",
      "wrong_move\n",
      "   1636/500000: episode: 1624, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2582.000 [2582.000, 2582.000],  loss: 291712.625000, mae: 774.216309, mean_q: 9.541775\n",
      "wrong_move\n",
      "   1637/500000: episode: 1625, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 363.000 [363.000, 363.000],  loss: 607911.125000, mae: 774.151123, mean_q: 9.427768\n",
      "wrong_move\n",
      "   1638/500000: episode: 1626, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1519.000 [1519.000, 1519.000],  loss: 801143.562500, mae: 774.182983, mean_q: 9.498670\n",
      "wrong_move\n",
      "   1639/500000: episode: 1627, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 363.000 [363.000, 363.000],  loss: 467028.312500, mae: 774.850830, mean_q: 12.070435\n",
      "wrong_move\n",
      "   1640/500000: episode: 1628, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1345.000 [1345.000, 1345.000],  loss: 432889.406250, mae: 776.177429, mean_q: 9.800346\n",
      "wrong_move\n",
      "   1641/500000: episode: 1629, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3303.000 [3303.000, 3303.000],  loss: 96837.414062, mae: 777.668274, mean_q: 9.510822\n",
      "wrong_move\n",
      "   1642/500000: episode: 1630, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3998.000 [3998.000, 3998.000],  loss: 109716.828125, mae: 779.399292, mean_q: 9.412182\n",
      "wrong_move\n",
      "   1643/500000: episode: 1631, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 363.000 [363.000, 363.000],  loss: 511176.843750, mae: 781.739563, mean_q: 9.719620\n",
      "wrong_move\n",
      "   1644/500000: episode: 1632, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 363.000 [363.000, 363.000],  loss: 62532.550781, mae: 783.540222, mean_q: 9.482859\n",
      "wrong_move\n",
      "   1645/500000: episode: 1633, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 687.000 [687.000, 687.000],  loss: 424456.531250, mae: 785.731140, mean_q: 9.397999\n",
      "wrong_move\n",
      "   1646/500000: episode: 1634, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1654.000 [1654.000, 1654.000],  loss: 537208.937500, mae: 787.660950, mean_q: 9.501858\n",
      "wrong_move\n",
      "   1647/500000: episode: 1635, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 220.000 [220.000, 220.000],  loss: 30419.144531, mae: 789.350098, mean_q: 9.553467\n",
      "wrong_move\n",
      "   1648/500000: episode: 1636, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3384.000 [3384.000, 3384.000],  loss: 225764.937500, mae: 791.269165, mean_q: 19.459217\n",
      "wrong_move\n",
      "   1649/500000: episode: 1637, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3021.000 [3021.000, 3021.000],  loss: 161010.390625, mae: 794.267944, mean_q: 9.468241\n",
      "wrong_move\n",
      "   1650/500000: episode: 1638, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1578.000 [1578.000, 1578.000],  loss: 67661.812500, mae: 797.042358, mean_q: 9.511036\n",
      "wrong_move\n",
      "   1651/500000: episode: 1639, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3166.000 [3166.000, 3166.000],  loss: 146159.171875, mae: 799.279419, mean_q: 9.792080\n",
      "wrong_move\n",
      "   1652/500000: episode: 1640, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2496.000 [2496.000, 2496.000],  loss: 100748.507812, mae: 801.322266, mean_q: 9.681001\n",
      "wrong_move\n",
      "   1653/500000: episode: 1641, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1380.000 [1380.000, 1380.000],  loss: 97690.312500, mae: 802.506165, mean_q: 22.845577\n",
      "wrong_move\n",
      "   1654/500000: episode: 1642, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1380.000 [1380.000, 1380.000],  loss: 57256.226562, mae: 802.343811, mean_q: 9.923473\n",
      "wrong_move\n",
      "   1655/500000: episode: 1643, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3745.000 [3745.000, 3745.000],  loss: 501570.062500, mae: 801.630981, mean_q: 9.394821\n",
      "wrong_move\n",
      "   1656/500000: episode: 1644, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4091.000 [4091.000, 4091.000],  loss: 99697.281250, mae: 801.639709, mean_q: 9.457445\n",
      "wrong_move\n",
      "   1657/500000: episode: 1645, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 715.000 [715.000, 715.000],  loss: 34290.742188, mae: 801.814575, mean_q: 9.477304\n",
      "wrong_move\n",
      "   1658/500000: episode: 1646, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2632.000 [2632.000, 2632.000],  loss: 114679.015625, mae: 801.783691, mean_q: 9.538506\n",
      "wrong_move\n",
      "   1659/500000: episode: 1647, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 60.000 [60.000, 60.000],  loss: 186641.640625, mae: 801.894348, mean_q: 9.527041\n",
      "wrong_move\n",
      "   1660/500000: episode: 1648, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 407.000 [407.000, 407.000],  loss: 18104.835938, mae: 802.065674, mean_q: 9.531257\n",
      "wrong_move\n",
      "   1661/500000: episode: 1649, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1735.000 [1735.000, 1735.000],  loss: 15926.633789, mae: 802.256348, mean_q: 9.390263\n",
      "wrong_move\n",
      "   1662/500000: episode: 1650, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3035.000 [3035.000, 3035.000],  loss: 67715.304688, mae: 802.285339, mean_q: 9.515215\n",
      "wrong_move\n",
      "   1663/500000: episode: 1651, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 727.000 [727.000, 727.000],  loss: 118123.437500, mae: 803.125610, mean_q: 33.309284\n",
      "wrong_move\n",
      "   1664/500000: episode: 1652, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3384.000 [3384.000, 3384.000],  loss: 20437.769531, mae: 803.875854, mean_q: 9.435987\n",
      "wrong_move\n",
      "   1665/500000: episode: 1653, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3106.000 [3106.000, 3106.000],  loss: 113151.281250, mae: 804.874146, mean_q: 9.384003\n",
      "wrong_move\n",
      "   1666/500000: episode: 1654, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1349.000 [1349.000, 1349.000],  loss: 25034.833984, mae: 805.701843, mean_q: 9.415462\n",
      "wrong_move\n",
      "   1667/500000: episode: 1655, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1484.000 [1484.000, 1484.000],  loss: 359967.906250, mae: 806.977905, mean_q: 25.446508\n",
      "wrong_move\n",
      "   1668/500000: episode: 1656, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3046.000 [3046.000, 3046.000],  loss: 235504.828125, mae: 808.846375, mean_q: 9.429365\n",
      "wrong_move\n",
      "   1669/500000: episode: 1657, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2442.000 [2442.000, 2442.000],  loss: 50355.324219, mae: 809.379700, mean_q: 9.523763\n",
      "wrong_move\n",
      "   1670/500000: episode: 1658, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3913.000 [3913.000, 3913.000],  loss: 27891.998047, mae: 809.521118, mean_q: 9.566102\n",
      "wrong_move\n",
      "   1671/500000: episode: 1659, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3046.000 [3046.000, 3046.000],  loss: 52220.863281, mae: 809.657837, mean_q: 9.430292\n",
      "wrong_move\n",
      "   1672/500000: episode: 1660, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3487.000 [3487.000, 3487.000],  loss: 453854.593750, mae: 809.859863, mean_q: 9.461823\n",
      "wrong_move\n",
      "   1673/500000: episode: 1661, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1503.000 [1503.000, 1503.000],  loss: 409027.781250, mae: 810.734619, mean_q: 18.882551\n",
      "wrong_move\n",
      "   1674/500000: episode: 1662, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3166.000 [3166.000, 3166.000],  loss: 672631.875000, mae: 811.502808, mean_q: 9.261000\n",
      "wrong_move\n",
      "   1675/500000: episode: 1663, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3196.000 [3196.000, 3196.000],  loss: 658607.500000, mae: 812.406616, mean_q: 9.307305\n",
      "wrong_move\n",
      "   1676/500000: episode: 1664, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 784.000 [784.000, 784.000],  loss: 117218.742188, mae: 813.946045, mean_q: 9.470454\n",
      "wrong_move\n",
      "   1677/500000: episode: 1665, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3523.000 [3523.000, 3523.000],  loss: 198613.515625, mae: 816.170776, mean_q: 9.236458\n",
      "wrong_move\n",
      "   1678/500000: episode: 1666, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1484.000 [1484.000, 1484.000],  loss: 966140.125000, mae: 818.432800, mean_q: 9.350762\n",
      "wrong_move\n",
      "   1679/500000: episode: 1667, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3166.000 [3166.000, 3166.000],  loss: 89494.421875, mae: 819.385376, mean_q: 9.291023\n",
      "wrong_move\n",
      "   1680/500000: episode: 1668, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3145.000 [3145.000, 3145.000],  loss: 42973.875000, mae: 819.680542, mean_q: 9.385547\n",
      "wrong_move\n",
      "   1681/500000: episode: 1669, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3575.000 [3575.000, 3575.000],  loss: 42276.867188, mae: 819.882935, mean_q: 9.232716\n",
      "wrong_move\n",
      "   1682/500000: episode: 1670, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3166.000 [3166.000, 3166.000],  loss: 138392.843750, mae: 820.158020, mean_q: 9.114598\n",
      "wrong_move\n",
      "   1683/500000: episode: 1671, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3166.000 [3166.000, 3166.000],  loss: 434650.968750, mae: 819.979370, mean_q: 9.201572\n",
      "wrong_move\n",
      "   1684/500000: episode: 1672, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 151.000 [151.000, 151.000],  loss: 805482.625000, mae: 819.832947, mean_q: 9.330538\n",
      "wrong_move\n",
      "   1685/500000: episode: 1673, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3166.000 [3166.000, 3166.000],  loss: 486642.250000, mae: 820.466187, mean_q: 9.196366\n",
      "wrong_move\n",
      "   1686/500000: episode: 1674, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2360.000 [2360.000, 2360.000],  loss: 64322.406250, mae: 821.854370, mean_q: 9.150885\n",
      "wrong_move\n",
      "   1687/500000: episode: 1675, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1432.000 [1432.000, 1432.000],  loss: 32110.000000, mae: 823.220154, mean_q: 9.152689\n",
      "wrong_move\n",
      "   1688/500000: episode: 1676, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3166.000 [3166.000, 3166.000],  loss: 18822.074219, mae: 824.238403, mean_q: 9.138084\n",
      "wrong_move\n",
      "   1689/500000: episode: 1677, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3209.000 [3209.000, 3209.000],  loss: 21174.609375, mae: 825.161255, mean_q: 9.143299\n",
      "wrong_move\n",
      "   1690/500000: episode: 1678, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3491.000 [3491.000, 3491.000],  loss: 451486.843750, mae: 826.240234, mean_q: 9.182425\n",
      "wrong_move\n",
      "   1691/500000: episode: 1679, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2455.000 [2455.000, 2455.000],  loss: 403399.625000, mae: 827.677612, mean_q: 9.249420\n",
      "wrong_move\n",
      "   1692/500000: episode: 1680, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2455.000 [2455.000, 2455.000],  loss: 429863.812500, mae: 828.047852, mean_q: 9.064986\n",
      "wrong_move\n",
      "   1693/500000: episode: 1681, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3159.000 [3159.000, 3159.000],  loss: 58235.480469, mae: 828.429138, mean_q: 9.199104\n",
      "wrong_move\n",
      "   1694/500000: episode: 1682, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3929.000 [3929.000, 3929.000],  loss: 495763.375000, mae: 829.123779, mean_q: 8.960388\n",
      "wrong_move\n",
      "   1695/500000: episode: 1683, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2897.000 [2897.000, 2897.000],  loss: 662151.562500, mae: 829.903931, mean_q: 9.139778\n",
      "wrong_move\n",
      "   1696/500000: episode: 1684, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1205.000 [1205.000, 1205.000],  loss: 138370.765625, mae: 829.924866, mean_q: 8.950149\n",
      "wrong_move\n",
      "   1697/500000: episode: 1685, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3395.000 [3395.000, 3395.000],  loss: 40664.660156, mae: 830.261353, mean_q: 9.086343\n",
      "wrong_move\n",
      "   1698/500000: episode: 1686, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 623.000 [623.000, 623.000],  loss: 27809.341797, mae: 830.784546, mean_q: 9.065483\n",
      "wrong_move\n",
      "   1699/500000: episode: 1687, duration: 0.065s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2090.000 [2090.000, 2090.000],  loss: 29198.914062, mae: 831.573364, mean_q: 9.161165\n",
      "wrong_move\n",
      "   1700/500000: episode: 1688, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 623.000 [623.000, 623.000],  loss: 18877.085938, mae: 833.055542, mean_q: 9.236540\n",
      "wrong_move\n",
      "   1701/500000: episode: 1689, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3003.000 [3003.000, 3003.000],  loss: 36864.078125, mae: 834.862244, mean_q: 9.400488\n",
      "wrong_move\n",
      "   1702/500000: episode: 1690, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 623.000 [623.000, 623.000],  loss: 28174.226562, mae: 836.356262, mean_q: 9.308669\n",
      "wrong_move\n",
      "   1703/500000: episode: 1691, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 937.000 [937.000, 937.000],  loss: 49425.718750, mae: 837.608337, mean_q: 9.075491\n",
      "wrong_move\n",
      "   1704/500000: episode: 1692, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3270.000 [3270.000, 3270.000],  loss: 29959.759766, mae: 838.694153, mean_q: 9.044995\n",
      "wrong_move\n",
      "   1705/500000: episode: 1693, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1307.000 [1307.000, 1307.000],  loss: 40934.000000, mae: 839.632019, mean_q: 9.027850\n",
      "wrong_move\n",
      "   1706/500000: episode: 1694, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1205.000 [1205.000, 1205.000],  loss: 238193.515625, mae: 841.770142, mean_q: 8.916666\n",
      "wrong_move\n",
      "   1707/500000: episode: 1695, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1307.000 [1307.000, 1307.000],  loss: 20110.660156, mae: 842.807617, mean_q: 9.030360\n",
      "wrong_move\n",
      "   1708/500000: episode: 1696, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1432.000 [1432.000, 1432.000],  loss: 803142.375000, mae: 844.302856, mean_q: 9.101544\n",
      "wrong_move\n",
      "   1709/500000: episode: 1697, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 424.000 [424.000, 424.000],  loss: 37081.019531, mae: 846.457397, mean_q: 8.851344\n",
      "wrong_move\n",
      "   1710/500000: episode: 1698, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2593.000 [2593.000, 2593.000],  loss: 30962.478516, mae: 848.081421, mean_q: 8.941979\n",
      "wrong_move\n",
      "   1711/500000: episode: 1699, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1337.000 [1337.000, 1337.000],  loss: 52164.500000, mae: 848.801514, mean_q: 8.912313\n",
      "wrong_move\n",
      "   1712/500000: episode: 1700, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1307.000 [1307.000, 1307.000],  loss: 27384.375000, mae: 849.546387, mean_q: 8.990936\n",
      "wrong_move\n",
      "   1713/500000: episode: 1701, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1876.000 [1876.000, 1876.000],  loss: 33182.316406, mae: 850.230835, mean_q: 8.963617\n",
      "wrong_move\n",
      "   1714/500000: episode: 1702, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1307.000 [1307.000, 1307.000],  loss: 935333.625000, mae: 850.476196, mean_q: 8.953495\n",
      "wrong_move\n",
      "   1715/500000: episode: 1703, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 369.000 [369.000, 369.000],  loss: 421655.968750, mae: 850.170532, mean_q: 8.763258\n",
      "wrong_move\n",
      "   1716/500000: episode: 1704, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1255.000 [1255.000, 1255.000],  loss: 255476.046875, mae: 850.213745, mean_q: 8.870146\n",
      "wrong_move\n",
      "   1717/500000: episode: 1705, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2222.000 [2222.000, 2222.000],  loss: 120359.781250, mae: 850.106934, mean_q: 8.882995\n",
      "wrong_move\n",
      "   1718/500000: episode: 1706, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3506.000 [3506.000, 3506.000],  loss: 168185.453125, mae: 850.955750, mean_q: 8.790593\n",
      "wrong_move\n",
      "   1719/500000: episode: 1707, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 561.000 [561.000, 561.000],  loss: 448045.406250, mae: 852.450928, mean_q: 9.101424\n",
      "wrong_move\n",
      "   1720/500000: episode: 1708, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 767.000 [767.000, 767.000],  loss: 28282.398438, mae: 853.535217, mean_q: 8.932322\n",
      "wrong_move\n",
      "   1721/500000: episode: 1709, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 452.000 [452.000, 452.000],  loss: 81574.820312, mae: 854.921448, mean_q: 8.684145\n",
      "wrong_move\n",
      "   1722/500000: episode: 1710, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 374.000 [374.000, 374.000],  loss: 177351.625000, mae: 857.079773, mean_q: 8.867901\n",
      "wrong_move\n",
      "   1723/500000: episode: 1711, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4039.000 [4039.000, 4039.000],  loss: 19501.796875, mae: 859.921509, mean_q: 8.963233\n",
      "wrong_move\n",
      "   1724/500000: episode: 1712, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2222.000 [2222.000, 2222.000],  loss: 18539.083984, mae: 862.671997, mean_q: 8.831269\n",
      "wrong_move\n",
      "   1725/500000: episode: 1713, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2432.000 [2432.000, 2432.000],  loss: 22459.425781, mae: 864.792847, mean_q: 8.852072\n",
      "wrong_move\n",
      "   1726/500000: episode: 1714, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1778.000 [1778.000, 1778.000],  loss: 55772.710938, mae: 866.209045, mean_q: 8.798445\n",
      "wrong_move\n",
      "   1727/500000: episode: 1715, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1778.000 [1778.000, 1778.000],  loss: 275560.906250, mae: 867.362244, mean_q: 8.764114\n",
      "wrong_move\n",
      "   1728/500000: episode: 1716, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1778.000 [1778.000, 1778.000],  loss: 39654.839844, mae: 868.072205, mean_q: 8.839994\n",
      "wrong_move\n",
      "   1729/500000: episode: 1717, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2593.000 [2593.000, 2593.000],  loss: 486147.750000, mae: 868.958618, mean_q: 8.737960\n",
      "wrong_move\n",
      "   1730/500000: episode: 1718, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2035.000 [2035.000, 2035.000],  loss: 863491.312500, mae: 869.719360, mean_q: 8.767307\n",
      "wrong_move\n",
      "   1731/500000: episode: 1719, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2035.000 [2035.000, 2035.000],  loss: 66200.921875, mae: 869.421753, mean_q: 8.702435\n",
      "wrong_move\n",
      "   1732/500000: episode: 1720, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3399.000 [3399.000, 3399.000],  loss: 29004.437500, mae: 869.035278, mean_q: 8.524469\n",
      "wrong_move\n",
      "   1733/500000: episode: 1721, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1877.000 [1877.000, 1877.000],  loss: 542073.375000, mae: 869.289795, mean_q: 8.606015\n",
      "wrong_move\n",
      "   1734/500000: episode: 1722, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 970.000 [970.000, 970.000],  loss: 64128.332031, mae: 869.936951, mean_q: 8.605236\n",
      "wrong_move\n",
      "   1735/500000: episode: 1723, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3807.000 [3807.000, 3807.000],  loss: 315933.093750, mae: 870.475220, mean_q: 8.727107\n",
      "wrong_move\n",
      "   1736/500000: episode: 1724, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3389.000 [3389.000, 3389.000],  loss: 461494.656250, mae: 869.236145, mean_q: 8.576105\n",
      "wrong_move\n",
      "   1737/500000: episode: 1725, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2248.000 [2248.000, 2248.000],  loss: 398906.437500, mae: 868.770508, mean_q: 8.603706\n",
      "wrong_move\n",
      "   1738/500000: episode: 1726, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 431.000 [431.000, 431.000],  loss: 29965.285156, mae: 868.989380, mean_q: 8.704562\n",
      "wrong_move\n",
      "   1739/500000: episode: 1727, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2248.000 [2248.000, 2248.000],  loss: 430448.812500, mae: 870.031372, mean_q: 8.525736\n",
      "wrong_move\n",
      "   1740/500000: episode: 1728, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2248.000 [2248.000, 2248.000],  loss: 29160.041016, mae: 872.205872, mean_q: 8.491983\n",
      "wrong_move\n",
      "   1741/500000: episode: 1729, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3887.000 [3887.000, 3887.000],  loss: 518804.937500, mae: 874.557495, mean_q: 8.548935\n",
      "wrong_move\n",
      "   1742/500000: episode: 1730, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2773.000 [2773.000, 2773.000],  loss: 58705.281250, mae: 875.711548, mean_q: 8.568008\n",
      "wrong_move\n",
      "   1743/500000: episode: 1731, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2773.000 [2773.000, 2773.000],  loss: 12965.218750, mae: 877.217102, mean_q: 8.488888\n",
      "wrong_move\n",
      "   1744/500000: episode: 1732, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1466.000 [1466.000, 1466.000],  loss: 50099.742188, mae: 878.729126, mean_q: 8.558915\n",
      "wrong_move\n",
      "   1745/500000: episode: 1733, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1636.000 [1636.000, 1636.000],  loss: 409467.156250, mae: 880.663513, mean_q: 8.650020\n",
      "wrong_move\n",
      "   1746/500000: episode: 1734, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 135.000 [135.000, 135.000],  loss: 40483.691406, mae: 882.384521, mean_q: 8.720139\n",
      "wrong_move\n",
      "   1747/500000: episode: 1735, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1455.000 [1455.000, 1455.000],  loss: 124015.656250, mae: 884.086670, mean_q: 8.667498\n",
      "wrong_move\n",
      "   1748/500000: episode: 1736, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2773.000 [2773.000, 2773.000],  loss: 152752.546875, mae: 886.358398, mean_q: 8.669638\n",
      "wrong_move\n",
      "   1749/500000: episode: 1737, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3662.000 [3662.000, 3662.000],  loss: 50219.121094, mae: 889.113892, mean_q: 8.565895\n",
      "wrong_move\n",
      "   1750/500000: episode: 1738, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2773.000 [2773.000, 2773.000],  loss: 41616.085938, mae: 891.569397, mean_q: 8.608874\n",
      "wrong_move\n",
      "   1751/500000: episode: 1739, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3332.000 [3332.000, 3332.000],  loss: 380527.187500, mae: 894.153320, mean_q: 8.502718\n",
      "wrong_move\n",
      "   1752/500000: episode: 1740, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2773.000 [2773.000, 2773.000],  loss: 42196.914062, mae: 895.567505, mean_q: 8.714470\n",
      "wrong_move\n",
      "   1753/500000: episode: 1741, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1636.000 [1636.000, 1636.000],  loss: 425008.656250, mae: 897.342102, mean_q: 8.661435\n",
      "wrong_move\n",
      "   1754/500000: episode: 1742, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2012.000 [2012.000, 2012.000],  loss: 218517.625000, mae: 898.476196, mean_q: 8.618073\n",
      "wrong_move\n",
      "   1755/500000: episode: 1743, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1766.000 [1766.000, 1766.000],  loss: 792671.312500, mae: 898.106201, mean_q: 8.593569\n",
      "wrong_move\n",
      "   1756/500000: episode: 1744, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2773.000 [2773.000, 2773.000],  loss: 117075.898438, mae: 898.240967, mean_q: 8.501129\n",
      "wrong_move\n",
      "   1757/500000: episode: 1745, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3654.000 [3654.000, 3654.000],  loss: 780826.625000, mae: 897.940674, mean_q: 8.731510\n",
      "wrong_move\n",
      "   1758/500000: episode: 1746, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1896.000 [1896.000, 1896.000],  loss: 33174.070312, mae: 897.967285, mean_q: 8.662125\n",
      "wrong_move\n",
      "   1759/500000: episode: 1747, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2248.000 [2248.000, 2248.000],  loss: 804619.750000, mae: 898.607056, mean_q: 13.284610\n",
      "wrong_move\n",
      "   1760/500000: episode: 1748, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2248.000 [2248.000, 2248.000],  loss: 22645.945312, mae: 899.828857, mean_q: 8.533214\n",
      "wrong_move\n",
      "   1761/500000: episode: 1749, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2248.000 [2248.000, 2248.000],  loss: 27937.619141, mae: 901.454041, mean_q: 8.411684\n",
      "wrong_move\n",
      "   1762/500000: episode: 1750, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3219.000 [3219.000, 3219.000],  loss: 17927.476562, mae: 903.392090, mean_q: 8.640539\n",
      "wrong_move\n",
      "   1763/500000: episode: 1751, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2248.000 [2248.000, 2248.000],  loss: 12223.009766, mae: 904.818726, mean_q: 8.581767\n",
      "wrong_move\n",
      "   1764/500000: episode: 1752, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2134.000 [2134.000, 2134.000],  loss: 428181.625000, mae: 906.693481, mean_q: 8.526946\n",
      "wrong_move\n",
      "   1765/500000: episode: 1753, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 964.000 [964.000, 964.000],  loss: 197712.484375, mae: 908.031250, mean_q: 8.453714\n",
      "wrong_move\n",
      "   1766/500000: episode: 1754, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 251.000 [251.000, 251.000],  loss: 21991.394531, mae: 909.351074, mean_q: 8.596929\n",
      "wrong_move\n",
      "   1767/500000: episode: 1755, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2777.000 [2777.000, 2777.000],  loss: 67003.421875, mae: 910.368286, mean_q: 8.488701\n",
      "wrong_move\n",
      "   1768/500000: episode: 1756, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1543.000 [1543.000, 1543.000],  loss: 468595.375000, mae: 911.275696, mean_q: 8.613722\n",
      "wrong_move\n",
      "   1769/500000: episode: 1757, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2184.000 [2184.000, 2184.000],  loss: 513322.250000, mae: 912.662231, mean_q: 8.375462\n",
      "wrong_move\n",
      "   1770/500000: episode: 1758, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1255.000 [1255.000, 1255.000],  loss: 27068.691406, mae: 913.366821, mean_q: 8.292616\n",
      "wrong_move\n",
      "   1771/500000: episode: 1759, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1255.000 [1255.000, 1255.000],  loss: 36271.800781, mae: 914.693237, mean_q: 8.435973\n",
      "wrong_move\n",
      "   1772/500000: episode: 1760, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3376.000 [3376.000, 3376.000],  loss: 886717.687500, mae: 916.224182, mean_q: 8.467489\n",
      "wrong_move\n",
      "   1773/500000: episode: 1761, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4055.000 [4055.000, 4055.000],  loss: 25388.148438, mae: 918.034302, mean_q: 8.410753\n",
      "wrong_move\n",
      "   1774/500000: episode: 1762, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 826.000 [826.000, 826.000],  loss: 700770.875000, mae: 919.588440, mean_q: 8.483906\n",
      "wrong_move\n",
      "   1775/500000: episode: 1763, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1877.000 [1877.000, 1877.000],  loss: 512524.875000, mae: 917.773926, mean_q: 8.271540\n",
      "wrong_move\n",
      "   1776/500000: episode: 1764, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1543.000 [1543.000, 1543.000],  loss: 37256.464844, mae: 916.262451, mean_q: 8.290991\n",
      "wrong_move\n",
      "   1777/500000: episode: 1765, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1783.000 [1783.000, 1783.000],  loss: 34221.320312, mae: 915.225464, mean_q: 8.299625\n",
      "wrong_move\n",
      "   1778/500000: episode: 1766, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 826.000 [826.000, 826.000],  loss: 119701.070312, mae: 914.626709, mean_q: 8.333519\n",
      "wrong_move\n",
      "   1779/500000: episode: 1767, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3739.000 [3739.000, 3739.000],  loss: 77838.890625, mae: 914.102783, mean_q: 8.200106\n",
      "wrong_move\n",
      "   1780/500000: episode: 1768, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 297.000 [297.000, 297.000],  loss: 74793.507812, mae: 914.284729, mean_q: 8.340671\n",
      "wrong_move\n",
      "   1781/500000: episode: 1769, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3739.000 [3739.000, 3739.000],  loss: 22220.166016, mae: 915.826538, mean_q: 8.499266\n",
      "wrong_move\n",
      "   1782/500000: episode: 1770, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4036.000 [4036.000, 4036.000],  loss: 30170.128906, mae: 918.144165, mean_q: 8.397251\n",
      "wrong_move\n",
      "   1783/500000: episode: 1771, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3007.000 [3007.000, 3007.000],  loss: 536242.125000, mae: 920.978455, mean_q: 8.274357\n",
      "wrong_move\n",
      "   1784/500000: episode: 1772, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 826.000 [826.000, 826.000],  loss: 453810.625000, mae: 923.350464, mean_q: 8.329018\n",
      "wrong_move\n",
      "   1785/500000: episode: 1773, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1633.000 [1633.000, 1633.000],  loss: 121449.546875, mae: 923.775696, mean_q: 8.277934\n",
      "wrong_move\n",
      "   1786/500000: episode: 1774, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3739.000 [3739.000, 3739.000],  loss: 406951.843750, mae: 923.847412, mean_q: 8.452885\n",
      "wrong_move\n",
      "   1787/500000: episode: 1775, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 605.000 [605.000, 605.000],  loss: 121414.296875, mae: 923.997192, mean_q: 8.376107\n",
      "wrong_move\n",
      "   1788/500000: episode: 1776, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 405331.187500, mae: 923.695312, mean_q: 8.402862\n",
      "wrong_move\n",
      "   1789/500000: episode: 1777, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3358.000 [3358.000, 3358.000],  loss: 821494.250000, mae: 924.170044, mean_q: 12.979631\n",
      "wrong_move\n",
      "   1790/500000: episode: 1778, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1989.000 [1989.000, 1989.000],  loss: 849581.375000, mae: 924.430908, mean_q: 8.150137\n",
      "wrong_move\n",
      "   1791/500000: episode: 1779, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 423.000 [423.000, 423.000],  loss: 458671.781250, mae: 925.222107, mean_q: 8.333921\n",
      "wrong_move\n",
      "   1792/500000: episode: 1780, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3580.000 [3580.000, 3580.000],  loss: 30024.947266, mae: 926.322327, mean_q: 8.157699\n",
      "wrong_move\n",
      "   1793/500000: episode: 1781, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3394.000 [3394.000, 3394.000],  loss: 32832.261719, mae: 926.672302, mean_q: 8.230503\n",
      "wrong_move\n",
      "   1794/500000: episode: 1782, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3580.000 [3580.000, 3580.000],  loss: 88585.992188, mae: 927.300659, mean_q: 8.213762\n",
      "wrong_move\n",
      "   1795/500000: episode: 1783, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 638.000 [638.000, 638.000],  loss: 75125.945312, mae: 928.094177, mean_q: 8.177865\n",
      "wrong_move\n",
      "   1796/500000: episode: 1784, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2296.000 [2296.000, 2296.000],  loss: 262540.062500, mae: 929.219604, mean_q: 8.233479\n",
      "wrong_move\n",
      "   1797/500000: episode: 1785, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1068.000 [1068.000, 1068.000],  loss: 111455.171875, mae: 931.242920, mean_q: 8.159125\n",
      "wrong_move\n",
      "   1798/500000: episode: 1786, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3580.000 [3580.000, 3580.000],  loss: 81737.921875, mae: 933.611694, mean_q: 8.144866\n",
      "wrong_move\n",
      "   1799/500000: episode: 1787, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2671.000 [2671.000, 2671.000],  loss: 384681.125000, mae: 936.825989, mean_q: 8.226284\n",
      "wrong_move\n",
      "   1800/500000: episode: 1788, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2671.000 [2671.000, 2671.000],  loss: 209112.828125, mae: 938.024048, mean_q: 8.085380\n",
      "wrong_move\n",
      "   1801/500000: episode: 1789, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 638.000 [638.000, 638.000],  loss: 146962.421875, mae: 937.088745, mean_q: 8.051125\n",
      "wrong_move\n",
      "   1802/500000: episode: 1790, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3580.000 [3580.000, 3580.000],  loss: 34003.195312, mae: 935.811768, mean_q: 8.257126\n",
      "wrong_move\n",
      "   1803/500000: episode: 1791, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3580.000 [3580.000, 3580.000],  loss: 408245.906250, mae: 934.318237, mean_q: 8.130983\n",
      "wrong_move\n",
      "   1804/500000: episode: 1792, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1810.000 [1810.000, 1810.000],  loss: 29505.570312, mae: 932.689392, mean_q: 8.122526\n",
      "wrong_move\n",
      "   1805/500000: episode: 1793, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 354506.937500, mae: 931.951294, mean_q: 8.118421\n",
      "wrong_move\n",
      "   1806/500000: episode: 1794, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1092.000 [1092.000, 1092.000],  loss: 25867.054688, mae: 931.891357, mean_q: 8.305719\n",
      "wrong_move\n",
      "   1807/500000: episode: 1795, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1092.000 [1092.000, 1092.000],  loss: 11092.092773, mae: 932.603394, mean_q: 8.070778\n",
      "wrong_move\n",
      "   1808/500000: episode: 1796, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1092.000 [1092.000, 1092.000],  loss: 24467.710938, mae: 933.859985, mean_q: 8.189736\n",
      "wrong_move\n",
      "   1809/500000: episode: 1797, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3325.000 [3325.000, 3325.000],  loss: 126289.156250, mae: 935.511780, mean_q: 7.896502\n",
      "wrong_move\n",
      "   1810/500000: episode: 1798, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1324.000 [1324.000, 1324.000],  loss: 98518.742188, mae: 935.999756, mean_q: 8.160534\n",
      "wrong_move\n",
      "   1811/500000: episode: 1799, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3325.000 [3325.000, 3325.000],  loss: 338388.437500, mae: 936.560669, mean_q: 8.083673\n",
      "wrong_move\n",
      "   1812/500000: episode: 1800, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 70.000 [70.000, 70.000],  loss: 498091.281250, mae: 936.915405, mean_q: 8.079723\n",
      "wrong_move\n",
      "   1813/500000: episode: 1801, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2629.000 [2629.000, 2629.000],  loss: 24269.007812, mae: 938.622314, mean_q: 7.863479\n",
      "wrong_move\n",
      "   1814/500000: episode: 1802, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3325.000 [3325.000, 3325.000],  loss: 30013.001953, mae: 939.981445, mean_q: 8.077939\n",
      "wrong_move\n",
      "   1815/500000: episode: 1803, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2071.000 [2071.000, 2071.000],  loss: 28154.625000, mae: 941.299194, mean_q: 7.955012\n",
      "wrong_move\n",
      "   1816/500000: episode: 1804, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3325.000 [3325.000, 3325.000],  loss: 410632.468750, mae: 942.733093, mean_q: 8.118172\n",
      "wrong_move\n",
      "   1817/500000: episode: 1805, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2994.000 [2994.000, 2994.000],  loss: 172205.109375, mae: 943.552368, mean_q: 8.119785\n",
      "wrong_move\n",
      "   1818/500000: episode: 1806, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2161.000 [2161.000, 2161.000],  loss: 18955.275391, mae: 943.662598, mean_q: 7.962134\n",
      "wrong_move\n",
      "   1819/500000: episode: 1807, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1365.000 [1365.000, 1365.000],  loss: 486362.906250, mae: 943.863159, mean_q: 8.010033\n",
      "wrong_move\n",
      "   1820/500000: episode: 1808, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3325.000 [3325.000, 3325.000],  loss: 44515.507812, mae: 945.264771, mean_q: 8.070090\n",
      "wrong_move\n",
      "   1821/500000: episode: 1809, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3325.000 [3325.000, 3325.000],  loss: 404649.218750, mae: 947.285767, mean_q: 8.178431\n",
      "wrong_move\n",
      "   1822/500000: episode: 1810, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3325.000 [3325.000, 3325.000],  loss: 483493.843750, mae: 949.379517, mean_q: 7.971608\n",
      "wrong_move\n",
      "   1823/500000: episode: 1811, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1429.000 [1429.000, 1429.000],  loss: 297896.468750, mae: 952.231079, mean_q: 7.959282\n",
      "wrong_move\n",
      "   1824/500000: episode: 1812, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1429.000 [1429.000, 1429.000],  loss: 53253.210938, mae: 954.377747, mean_q: 8.079014\n",
      "wrong_move\n",
      "   1825/500000: episode: 1813, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3376.000 [3376.000, 3376.000],  loss: 65059.687500, mae: 957.497559, mean_q: 7.947510\n",
      "wrong_move\n",
      "   1826/500000: episode: 1814, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1429.000 [1429.000, 1429.000],  loss: 38132.417969, mae: 959.951782, mean_q: 8.121202\n",
      "wrong_move\n",
      "   1827/500000: episode: 1815, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 78.000 [78.000, 78.000],  loss: 798316.000000, mae: 962.622864, mean_q: 7.928918\n",
      "wrong_move\n",
      "   1828/500000: episode: 1816, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3771.000 [3771.000, 3771.000],  loss: 247773.000000, mae: 965.054077, mean_q: 7.912634\n",
      "wrong_move\n",
      "   1829/500000: episode: 1817, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 664.000 [664.000, 664.000],  loss: 613190.500000, mae: 966.819336, mean_q: 7.921026\n",
      "wrong_move\n",
      "   1830/500000: episode: 1818, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 497.000 [497.000, 497.000],  loss: 146018.281250, mae: 968.051636, mean_q: 7.933777\n",
      "wrong_move\n",
      "   1831/500000: episode: 1819, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: 416952.218750, mae: 968.410889, mean_q: 8.070875\n",
      "wrong_move\n",
      "   1832/500000: episode: 1820, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 688.000 [688.000, 688.000],  loss: 312417.875000, mae: 968.471130, mean_q: 7.833728\n",
      "wrong_move\n",
      "   1833/500000: episode: 1821, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2013.000 [2013.000, 2013.000],  loss: 45567.214844, mae: 969.917908, mean_q: 7.871396\n",
      "wrong_move\n",
      "   1834/500000: episode: 1822, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: 24260.927734, mae: 971.126465, mean_q: 7.844090\n",
      "wrong_move\n",
      "   1835/500000: episode: 1823, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 104.000 [104.000, 104.000],  loss: 471763.687500, mae: 972.752441, mean_q: 7.891501\n",
      "wrong_move\n",
      "   1836/500000: episode: 1824, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2256.000 [2256.000, 2256.000],  loss: 44340.558594, mae: 973.476135, mean_q: 7.790358\n",
      "wrong_move\n",
      "   1837/500000: episode: 1825, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: 11943.820312, mae: 974.006592, mean_q: 7.847070\n",
      "wrong_move\n",
      "   1838/500000: episode: 1826, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: 64741.359375, mae: 975.078491, mean_q: 7.932627\n",
      "wrong_move\n",
      "   1839/500000: episode: 1827, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1633.000 [1633.000, 1633.000],  loss: 20063.128906, mae: 975.849365, mean_q: 7.791560\n",
      "wrong_move\n",
      "   1840/500000: episode: 1828, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3242.000 [3242.000, 3242.000],  loss: 251992.859375, mae: 976.306946, mean_q: 7.784166\n",
      "wrong_move\n",
      "   1841/500000: episode: 1829, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2879.000 [2879.000, 2879.000],  loss: 52075.460938, mae: 975.667358, mean_q: 7.611643\n",
      "wrong_move\n",
      "   1842/500000: episode: 1830, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 666.000 [666.000, 666.000],  loss: 415751.531250, mae: 974.833252, mean_q: 7.797774\n",
      "wrong_move\n",
      "   1843/500000: episode: 1831, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 320.000 [320.000, 320.000],  loss: 195953.234375, mae: 974.493652, mean_q: 7.730505\n",
      "wrong_move\n",
      "   1844/500000: episode: 1832, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1411.000 [1411.000, 1411.000],  loss: 384809.812500, mae: 973.112244, mean_q: 7.668622\n",
      "wrong_move\n",
      "   1845/500000: episode: 1833, duration: 0.154s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2734.000 [2734.000, 2734.000],  loss: 19184.464844, mae: 972.393921, mean_q: 7.623668\n",
      "wrong_move\n",
      "   1846/500000: episode: 1834, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2779.000 [2779.000, 2779.000],  loss: 76554.140625, mae: 971.787842, mean_q: 7.704377\n",
      "wrong_move\n",
      "   1847/500000: episode: 1835, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1027.000 [1027.000, 1027.000],  loss: 445672.218750, mae: 971.711060, mean_q: 7.596664\n",
      "wrong_move\n",
      "   1848/500000: episode: 1836, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3591.000 [3591.000, 3591.000],  loss: 172340.453125, mae: 973.305908, mean_q: 7.773384\n",
      "wrong_move\n",
      "   1849/500000: episode: 1837, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 476.000 [476.000, 476.000],  loss: 29086.996094, mae: 976.106506, mean_q: 7.859617\n",
      "wrong_move\n",
      "   1850/500000: episode: 1838, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3965.000 [3965.000, 3965.000],  loss: 11638.576172, mae: 978.962280, mean_q: 7.769990\n",
      "wrong_move\n",
      "   1851/500000: episode: 1839, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3818.000 [3818.000, 3818.000],  loss: 40533.343750, mae: 981.193115, mean_q: 7.739068\n",
      "wrong_move\n",
      "   1852/500000: episode: 1840, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 78.000 [78.000, 78.000],  loss: 175561.593750, mae: 982.799561, mean_q: 7.677420\n",
      "wrong_move\n",
      "   1853/500000: episode: 1841, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3073.000 [3073.000, 3073.000],  loss: 247187.062500, mae: 982.472290, mean_q: 7.701464\n",
      "wrong_move\n",
      "   1854/500000: episode: 1842, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3073.000 [3073.000, 3073.000],  loss: 214137.343750, mae: 983.966187, mean_q: 7.689275\n",
      "wrong_move\n",
      "   1855/500000: episode: 1843, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3953.000 [3953.000, 3953.000],  loss: 421222.375000, mae: 986.683044, mean_q: 7.791129\n",
      "wrong_move\n",
      "   1856/500000: episode: 1844, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 224.000 [224.000, 224.000],  loss: 50357.289062, mae: 989.503723, mean_q: 7.681201\n",
      "wrong_move\n",
      "   1857/500000: episode: 1845, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 313.000 [313.000, 313.000],  loss: 32263.796875, mae: 991.517700, mean_q: 7.694960\n",
      "wrong_move\n",
      "   1858/500000: episode: 1846, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2080.000 [2080.000, 2080.000],  loss: 11977.075195, mae: 992.926636, mean_q: 7.857699\n",
      "wrong_move\n",
      "   1859/500000: episode: 1847, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 126.000 [126.000, 126.000],  loss: 799772.250000, mae: 993.331665, mean_q: 7.856853\n",
      "wrong_move\n",
      "   1860/500000: episode: 1848, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 313.000 [313.000, 313.000],  loss: 66889.742188, mae: 992.990234, mean_q: 7.606308\n",
      "wrong_move\n",
      "   1861/500000: episode: 1849, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3591.000 [3591.000, 3591.000],  loss: 892691.812500, mae: 993.195984, mean_q: 7.733345\n",
      "wrong_move\n",
      "   1862/500000: episode: 1850, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1732.000 [1732.000, 1732.000],  loss: 428869.343750, mae: 992.941772, mean_q: 7.695612\n",
      "wrong_move\n",
      "   1863/500000: episode: 1851, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2468.000 [2468.000, 2468.000],  loss: 18079.560547, mae: 992.599121, mean_q: 7.671647\n",
      "wrong_move\n",
      "   1864/500000: episode: 1852, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 631.000 [631.000, 631.000],  loss: 50576.109375, mae: 992.708862, mean_q: 7.596237\n",
      "wrong_move\n",
      "   1865/500000: episode: 1853, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 301.000 [301.000, 301.000],  loss: 47430.503906, mae: 993.238159, mean_q: 7.707726\n",
      "wrong_move\n",
      "   1866/500000: episode: 1854, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1292.000 [1292.000, 1292.000],  loss: 400707.531250, mae: 994.325684, mean_q: 7.616596\n",
      "wrong_move\n",
      "   1867/500000: episode: 1855, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3032.000 [3032.000, 3032.000],  loss: 429138.531250, mae: 995.297913, mean_q: 7.588214\n",
      "wrong_move\n",
      "   1868/500000: episode: 1856, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 301.000 [301.000, 301.000],  loss: 445918.468750, mae: 995.471313, mean_q: 7.682249\n",
      "wrong_move\n",
      "   1869/500000: episode: 1857, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3591.000 [3591.000, 3591.000],  loss: 21530.111328, mae: 995.677246, mean_q: 7.537615\n",
      "wrong_move\n",
      "   1870/500000: episode: 1858, duration: 0.043s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2954.000 [2954.000, 2954.000],  loss: 406121.968750, mae: 996.486694, mean_q: 7.562253\n",
      "wrong_move\n",
      "   1871/500000: episode: 1859, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3400.000 [3400.000, 3400.000],  loss: 83209.601562, mae: 997.930664, mean_q: 7.537974\n",
      "wrong_move\n",
      "   1872/500000: episode: 1860, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1688.000 [1688.000, 1688.000],  loss: 76573.781250, mae: 998.960083, mean_q: 7.503838\n",
      "wrong_move\n",
      "   1873/500000: episode: 1861, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3400.000 [3400.000, 3400.000],  loss: 26007.390625, mae: 1000.475159, mean_q: 7.506429\n",
      "wrong_move\n",
      "   1874/500000: episode: 1862, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3524.000 [3524.000, 3524.000],  loss: 19899.894531, mae: 1002.042725, mean_q: 7.636546\n",
      "wrong_move\n",
      "   1875/500000: episode: 1863, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3471.000 [3471.000, 3471.000],  loss: 469240.500000, mae: 1004.345886, mean_q: 7.453075\n",
      "wrong_move\n",
      "   1876/500000: episode: 1864, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 313.000 [313.000, 313.000],  loss: 41314.625000, mae: 1007.268799, mean_q: 7.387792\n",
      "wrong_move\n",
      "   1877/500000: episode: 1865, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2346.000 [2346.000, 2346.000],  loss: 14130.898438, mae: 1010.068359, mean_q: 7.650269\n",
      "wrong_move\n",
      "   1878/500000: episode: 1866, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3038.000 [3038.000, 3038.000],  loss: 31508.250000, mae: 1011.889404, mean_q: 7.525290\n",
      "wrong_move\n",
      "   1879/500000: episode: 1867, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 603.000 [603.000, 603.000],  loss: 25753.253906, mae: 1013.456543, mean_q: 7.558937\n",
      "wrong_move\n",
      "   1880/500000: episode: 1868, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2779.000 [2779.000, 2779.000],  loss: 12483.455078, mae: 1014.540771, mean_q: 7.624804\n",
      "wrong_move\n",
      "   1881/500000: episode: 1869, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 479.000 [479.000, 479.000],  loss: 793358.500000, mae: 1015.773132, mean_q: 7.498066\n",
      "wrong_move\n",
      "   1882/500000: episode: 1870, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3524.000 [3524.000, 3524.000],  loss: 19491.734375, mae: 1015.720947, mean_q: 7.549241\n",
      "wrong_move\n",
      "   1883/500000: episode: 1871, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 525.000 [525.000, 525.000],  loss: 343337.781250, mae: 1016.239868, mean_q: 7.502483\n",
      "wrong_move\n",
      "   1884/500000: episode: 1872, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 403.000 [403.000, 403.000],  loss: 411991.156250, mae: 1016.669312, mean_q: 7.507703\n",
      "wrong_move\n",
      "   1885/500000: episode: 1873, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 403.000 [403.000, 403.000],  loss: 22526.140625, mae: 1017.941284, mean_q: 7.545520\n",
      "wrong_move\n",
      "   1886/500000: episode: 1874, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 403.000 [403.000, 403.000],  loss: 33743.398438, mae: 1019.451538, mean_q: 7.471334\n",
      "wrong_move\n",
      "   1887/500000: episode: 1875, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 403.000 [403.000, 403.000],  loss: 159294.765625, mae: 1019.830383, mean_q: 7.506493\n",
      "wrong_move\n",
      "   1888/500000: episode: 1876, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 301.000 [301.000, 301.000],  loss: 704149.125000, mae: 1018.893250, mean_q: 7.580686\n",
      "wrong_move\n",
      "   1889/500000: episode: 1877, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2044.000 [2044.000, 2044.000],  loss: 29918.857422, mae: 1015.817017, mean_q: 7.567308\n",
      "wrong_move\n",
      "   1890/500000: episode: 1878, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2548.000 [2548.000, 2548.000],  loss: 409686.781250, mae: 1013.263184, mean_q: 7.437787\n",
      "wrong_move\n",
      "   1891/500000: episode: 1879, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3487.000 [3487.000, 3487.000],  loss: 290483.187500, mae: 1011.654907, mean_q: 7.361375\n",
      "wrong_move\n",
      "   1892/500000: episode: 1880, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 201.000 [201.000, 201.000],  loss: 75537.500000, mae: 1012.498657, mean_q: 7.440308\n",
      "wrong_move\n",
      "   1893/500000: episode: 1881, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 211.000 [211.000, 211.000],  loss: 30854.207031, mae: 1014.182617, mean_q: 7.520739\n",
      "wrong_move\n",
      "   1894/500000: episode: 1882, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3052.000 [3052.000, 3052.000],  loss: 117332.765625, mae: 1015.430542, mean_q: 7.503307\n",
      "wrong_move\n",
      "   1895/500000: episode: 1883, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3841.000 [3841.000, 3841.000],  loss: 51466.726562, mae: 1015.385742, mean_q: 7.292487\n",
      "wrong_move\n",
      "   1896/500000: episode: 1884, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1014.000 [1014.000, 1014.000],  loss: 454006.875000, mae: 1015.816162, mean_q: 7.376584\n",
      "wrong_move\n",
      "   1897/500000: episode: 1885, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2642.000 [2642.000, 2642.000],  loss: 93241.039062, mae: 1016.846497, mean_q: 7.503504\n",
      "wrong_move\n",
      "   1898/500000: episode: 1886, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3841.000 [3841.000, 3841.000],  loss: 111733.835938, mae: 1017.530029, mean_q: 7.375796\n",
      "wrong_move\n",
      "   1899/500000: episode: 1887, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 620.000 [620.000, 620.000],  loss: 21196.980469, mae: 1019.055420, mean_q: 7.294185\n",
      "wrong_move\n",
      "   1900/500000: episode: 1888, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3343.000 [3343.000, 3343.000],  loss: 81324.867188, mae: 1020.554932, mean_q: 7.327412\n",
      "wrong_move\n",
      "   1901/500000: episode: 1889, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2386.000 [2386.000, 2386.000],  loss: 840937.625000, mae: 1022.956238, mean_q: 7.429546\n",
      "wrong_move\n",
      "   1902/500000: episode: 1890, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1755.000 [1755.000, 1755.000],  loss: 794815.625000, mae: 1022.023682, mean_q: 7.487186\n",
      "wrong_move\n",
      "   1903/500000: episode: 1891, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1755.000 [1755.000, 1755.000],  loss: 98139.226562, mae: 1020.689087, mean_q: 7.507022\n",
      "wrong_move\n",
      "   1904/500000: episode: 1892, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 736.000 [736.000, 736.000],  loss: 32118.187500, mae: 1019.953369, mean_q: 7.373786\n",
      "wrong_move\n",
      "   1905/500000: episode: 1893, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1786.000 [1786.000, 1786.000],  loss: 663385.562500, mae: 1020.034607, mean_q: 7.618358\n",
      "wrong_move\n",
      "   1906/500000: episode: 1894, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 728.000 [728.000, 728.000],  loss: 24877.916016, mae: 1019.757080, mean_q: 7.368804\n",
      "wrong_move\n",
      "   1907/500000: episode: 1895, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1605.000 [1605.000, 1605.000],  loss: 80916.375000, mae: 1020.043091, mean_q: 7.619714\n",
      "wrong_move\n",
      "   1908/500000: episode: 1896, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 736.000 [736.000, 736.000],  loss: 454495.031250, mae: 1020.631226, mean_q: 7.503255\n",
      "wrong_move\n",
      "   1909/500000: episode: 1897, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1637.000 [1637.000, 1637.000],  loss: 532092.000000, mae: 1021.736389, mean_q: 7.298074\n",
      "wrong_move\n",
      "   1910/500000: episode: 1898, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1924.000 [1924.000, 1924.000],  loss: 103179.710938, mae: 1023.760986, mean_q: 7.413749\n",
      "wrong_move\n",
      "   1911/500000: episode: 1899, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2215.000 [2215.000, 2215.000],  loss: 81239.273438, mae: 1025.242432, mean_q: 7.352023\n",
      "wrong_move\n",
      "   1912/500000: episode: 1900, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3741.000 [3741.000, 3741.000],  loss: 33049.136719, mae: 1024.113525, mean_q: 7.417371\n",
      "wrong_move\n",
      "   1913/500000: episode: 1901, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 716.000 [716.000, 716.000],  loss: 24592.835938, mae: 1023.522095, mean_q: 7.283459\n",
      "wrong_move\n",
      "   1914/500000: episode: 1902, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 736.000 [736.000, 736.000],  loss: 215499.937500, mae: 1024.096436, mean_q: 7.483772\n",
      "wrong_move\n",
      "   1915/500000: episode: 1903, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1120.000 [1120.000, 1120.000],  loss: 804478.875000, mae: 1023.099609, mean_q: 7.363068\n",
      "wrong_move\n",
      "   1916/500000: episode: 1904, duration: 0.148s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 892.000 [892.000, 892.000],  loss: 47384.503906, mae: 1021.863892, mean_q: 7.335355\n",
      "wrong_move\n",
      "   1917/500000: episode: 1905, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3321.000 [3321.000, 3321.000],  loss: 69729.687500, mae: 1022.090698, mean_q: 7.462747\n",
      "wrong_move\n",
      "   1918/500000: episode: 1906, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2770.000 [2770.000, 2770.000],  loss: 92052.468750, mae: 1022.849792, mean_q: 7.436471\n",
      "wrong_move\n",
      "   1919/500000: episode: 1907, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3746.000 [3746.000, 3746.000],  loss: 27418.980469, mae: 1023.037720, mean_q: 7.286750\n",
      "wrong_move\n",
      "   1920/500000: episode: 1908, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: 533448.625000, mae: 1023.730347, mean_q: 7.324466\n",
      "wrong_move\n",
      "   1921/500000: episode: 1909, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3321.000 [3321.000, 3321.000],  loss: 303667.312500, mae: 1025.983032, mean_q: 7.305934\n",
      "wrong_move\n",
      "   1922/500000: episode: 1910, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1548.000 [1548.000, 1548.000],  loss: 43769.984375, mae: 1029.636719, mean_q: 7.486619\n",
      "wrong_move\n",
      "   1923/500000: episode: 1911, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3321.000 [3321.000, 3321.000],  loss: 15982.115234, mae: 1032.782349, mean_q: 7.283818\n",
      "wrong_move\n",
      "   1924/500000: episode: 1912, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3321.000 [3321.000, 3321.000],  loss: 19220.343750, mae: 1034.905029, mean_q: 7.364346\n",
      "wrong_move\n",
      "   1925/500000: episode: 1913, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3321.000 [3321.000, 3321.000],  loss: 22399.466797, mae: 1037.026123, mean_q: 7.337060\n",
      "wrong_move\n",
      "   1926/500000: episode: 1914, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3321.000 [3321.000, 3321.000],  loss: 819755.562500, mae: 1039.984009, mean_q: 7.338786\n",
      "wrong_move\n",
      "   1927/500000: episode: 1915, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4038.000 [4038.000, 4038.000],  loss: 45929.710938, mae: 1043.391602, mean_q: 7.374880\n",
      "wrong_move\n",
      "   1928/500000: episode: 1916, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3133.000 [3133.000, 3133.000],  loss: 191799.000000, mae: 1045.227417, mean_q: 7.392943\n",
      "wrong_move\n",
      "   1930/500000: episode: 1917, duration: 0.235s, episode steps:   2, steps per second:   9, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1339.000 [736.000, 1942.000],  loss: 232112.953125, mae: 1047.831909, mean_q: 7.290689\n",
      "wrong_move\n",
      "   1931/500000: episode: 1918, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: 44713.453125, mae: 1049.898071, mean_q: 7.342885\n",
      "wrong_move\n",
      "   1932/500000: episode: 1919, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2539.000 [2539.000, 2539.000],  loss: 104702.937500, mae: 1050.006714, mean_q: 7.334964\n",
      "wrong_move\n",
      "   1933/500000: episode: 1920, duration: 0.146s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3052.000 [3052.000, 3052.000],  loss: 27235.750000, mae: 1048.414795, mean_q: 7.325815\n",
      "wrong_move\n",
      "   1934/500000: episode: 1921, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 997.000 [997.000, 997.000],  loss: 405377.968750, mae: 1047.521729, mean_q: 7.314560\n",
      "wrong_move\n",
      "   1935/500000: episode: 1922, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3642.000 [3642.000, 3642.000],  loss: 34723.601562, mae: 1047.244995, mean_q: 7.252723\n",
      "wrong_move\n",
      "   1936/500000: episode: 1923, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3092.000 [3092.000, 3092.000],  loss: 72950.593750, mae: 1046.898682, mean_q: 7.297875\n",
      "wrong_move\n",
      "   1937/500000: episode: 1924, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1441.000 [1441.000, 1441.000],  loss: 16755.330078, mae: 1046.971680, mean_q: 7.226965\n",
      "wrong_move\n",
      "   1938/500000: episode: 1925, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 997.000 [997.000, 997.000],  loss: 470272.625000, mae: 1047.786865, mean_q: 7.207113\n",
      "wrong_move\n",
      "   1939/500000: episode: 1926, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 177.000 [177.000, 177.000],  loss: 410617.718750, mae: 1047.099609, mean_q: 7.366589\n",
      "wrong_move\n",
      "   1940/500000: episode: 1927, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4009.000 [4009.000, 4009.000],  loss: 144637.468750, mae: 1047.117188, mean_q: 7.230342\n",
      "wrong_move\n",
      "   1941/500000: episode: 1928, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 997.000 [997.000, 997.000],  loss: 31741.216797, mae: 1048.094482, mean_q: 7.339502\n",
      "wrong_move\n",
      "   1942/500000: episode: 1929, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2032.000 [2032.000, 2032.000],  loss: 29602.654297, mae: 1048.644531, mean_q: 7.259354\n",
      "wrong_move\n",
      "   1943/500000: episode: 1930, duration: 0.172s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 997.000 [997.000, 997.000],  loss: 17486.595703, mae: 1048.190308, mean_q: 7.143864\n",
      "wrong_move\n",
      "   1944/500000: episode: 1931, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3642.000 [3642.000, 3642.000],  loss: 399338.031250, mae: 1047.553467, mean_q: 7.375420\n",
      "wrong_move\n",
      "   1945/500000: episode: 1932, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2600.000 [2600.000, 2600.000],  loss: 795921.062500, mae: 1047.468872, mean_q: 7.151737\n",
      "wrong_move\n",
      "   1946/500000: episode: 1933, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2868.000 [2868.000, 2868.000],  loss: 11309.090820, mae: 1048.240234, mean_q: 7.214767\n",
      "wrong_move\n",
      "   1947/500000: episode: 1934, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2568.000 [2568.000, 2568.000],  loss: 15488.236328, mae: 1049.886230, mean_q: 7.157093\n",
      "wrong_move\n",
      "   1948/500000: episode: 1935, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2032.000 [2032.000, 2032.000],  loss: 410466.500000, mae: 1051.828613, mean_q: 7.203496\n",
      "wrong_move\n",
      "   1949/500000: episode: 1936, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2032.000 [2032.000, 2032.000],  loss: 35354.066406, mae: 1053.268677, mean_q: 7.166824\n",
      "wrong_move\n",
      "   1950/500000: episode: 1937, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2032.000 [2032.000, 2032.000],  loss: 34823.273438, mae: 1054.894775, mean_q: 7.132700\n",
      "wrong_move\n",
      "   1951/500000: episode: 1938, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2032.000 [2032.000, 2032.000],  loss: 415247.031250, mae: 1057.989746, mean_q: 7.116799\n",
      "wrong_move\n",
      "   1952/500000: episode: 1939, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1691.000 [1691.000, 1691.000],  loss: 224581.515625, mae: 1062.025146, mean_q: 7.254801\n",
      "wrong_move\n",
      "   1953/500000: episode: 1940, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2546.000 [2546.000, 2546.000],  loss: 14285.680664, mae: 1064.408203, mean_q: 7.145862\n",
      "wrong_move\n",
      "   1954/500000: episode: 1941, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1862.000 [1862.000, 1862.000],  loss: 156328.109375, mae: 1065.844238, mean_q: 7.081957\n",
      "wrong_move\n",
      "   1955/500000: episode: 1942, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 169.000 [169.000, 169.000],  loss: 634457.312500, mae: 1066.614258, mean_q: 7.070521\n",
      "wrong_move\n",
      "   1956/500000: episode: 1943, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2927.000 [2927.000, 2927.000],  loss: 18543.052734, mae: 1066.695068, mean_q: 7.207011\n",
      "wrong_move\n",
      "   1957/500000: episode: 1944, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1691.000 [1691.000, 1691.000],  loss: 65268.351562, mae: 1066.354492, mean_q: 6.879518\n",
      "wrong_move\n",
      "   1958/500000: episode: 1945, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 95.000 [95.000, 95.000],  loss: 100866.726562, mae: 1065.979736, mean_q: 7.075309\n",
      "wrong_move\n",
      "   1959/500000: episode: 1946, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 95.000 [95.000, 95.000],  loss: 136304.031250, mae: 1063.978516, mean_q: 7.163388\n",
      "wrong_move\n",
      "   1960/500000: episode: 1947, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 95.000 [95.000, 95.000],  loss: 603408.437500, mae: 1063.479248, mean_q: 7.023170\n",
      "wrong_move\n",
      "   1961/500000: episode: 1948, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2925.000 [2925.000, 2925.000],  loss: 878068.875000, mae: 1064.372681, mean_q: 6.958004\n",
      "wrong_move\n",
      "   1962/500000: episode: 1949, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1388.000 [1388.000, 1388.000],  loss: 124085.484375, mae: 1065.244019, mean_q: 7.037837\n",
      "wrong_move\n",
      "   1963/500000: episode: 1950, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 95.000 [95.000, 95.000],  loss: 293858.218750, mae: 1066.034424, mean_q: 7.036142\n",
      "wrong_move\n",
      "   1964/500000: episode: 1951, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 95.000 [95.000, 95.000],  loss: 403544.125000, mae: 1065.565674, mean_q: 6.930764\n",
      "wrong_move\n",
      "   1966/500000: episode: 1952, duration: 0.145s, episode steps:   2, steps per second:  14, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 1242.500 [80.000, 2405.000],  loss: 48372.855469, mae: 1065.526367, mean_q: 7.005365\n",
      "wrong_move\n",
      "   1967/500000: episode: 1953, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 95.000 [95.000, 95.000],  loss: 313704.562500, mae: 1066.685425, mean_q: 7.038608\n",
      "wrong_move\n",
      "   1968/500000: episode: 1954, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4038.000 [4038.000, 4038.000],  loss: 47042.375000, mae: 1068.956787, mean_q: 6.873971\n",
      "wrong_move\n",
      "   1969/500000: episode: 1955, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3263.000 [3263.000, 3263.000],  loss: 33140.167969, mae: 1070.451172, mean_q: 7.047158\n",
      "wrong_move\n",
      "   1970/500000: episode: 1956, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4038.000 [4038.000, 4038.000],  loss: 199311.765625, mae: 1072.414062, mean_q: 6.976818\n",
      "wrong_move\n",
      "   1971/500000: episode: 1957, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4038.000 [4038.000, 4038.000],  loss: 32348.890625, mae: 1073.408325, mean_q: 6.990080\n",
      "wrong_move\n",
      "   1972/500000: episode: 1958, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 539.000 [539.000, 539.000],  loss: 145703.968750, mae: 1074.502075, mean_q: 6.924953\n",
      "wrong_move\n",
      "   1973/500000: episode: 1959, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 184.000 [184.000, 184.000],  loss: 423509.593750, mae: 1074.581177, mean_q: 6.952288\n",
      "wrong_move\n",
      "   1974/500000: episode: 1960, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 253.000 [253.000, 253.000],  loss: 13644.344727, mae: 1075.154053, mean_q: 6.888795\n",
      "wrong_move\n",
      "   1975/500000: episode: 1961, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1522.000 [1522.000, 1522.000],  loss: 836572.875000, mae: 1076.648193, mean_q: 7.298813\n",
      "wrong_move\n",
      "   1976/500000: episode: 1962, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 184.000 [184.000, 184.000],  loss: 138432.625000, mae: 1078.666260, mean_q: 6.923972\n",
      "wrong_move\n",
      "   1977/500000: episode: 1963, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 184.000 [184.000, 184.000],  loss: 20470.507812, mae: 1081.169189, mean_q: 7.005852\n",
      "wrong_move\n",
      "   1978/500000: episode: 1964, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 184.000 [184.000, 184.000],  loss: 1167738.125000, mae: 1083.532104, mean_q: 7.071699\n",
      "wrong_move\n",
      "   1979/500000: episode: 1965, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2211.000 [2211.000, 2211.000],  loss: 29744.730469, mae: 1081.502319, mean_q: 7.080263\n",
      "wrong_move\n",
      "   1980/500000: episode: 1966, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 714.000 [714.000, 714.000],  loss: 19551.621094, mae: 1079.309570, mean_q: 7.032868\n",
      "wrong_move\n",
      "   1981/500000: episode: 1967, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 184.000 [184.000, 184.000],  loss: 35507.875000, mae: 1077.757812, mean_q: 7.001936\n",
      "wrong_move\n",
      "   1982/500000: episode: 1968, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 133.000 [133.000, 133.000],  loss: 18568.605469, mae: 1077.081543, mean_q: 7.075471\n",
      "wrong_move\n",
      "   1983/500000: episode: 1969, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 652.000 [652.000, 652.000],  loss: 24676.437500, mae: 1077.815918, mean_q: 7.021266\n",
      "wrong_move\n",
      "   1984/500000: episode: 1970, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2034.000 [2034.000, 2034.000],  loss: 44859.277344, mae: 1079.179443, mean_q: 6.998253\n",
      "wrong_move\n",
      "   1985/500000: episode: 1971, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2776.000 [2776.000, 2776.000],  loss: 23229.070312, mae: 1080.186768, mean_q: 6.948900\n",
      "wrong_move\n",
      "   1986/500000: episode: 1972, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3060.000 [3060.000, 3060.000],  loss: 23514.171875, mae: 1081.946289, mean_q: 7.089258\n",
      "wrong_move\n",
      "   1987/500000: episode: 1973, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2211.000 [2211.000, 2211.000],  loss: 49766.257812, mae: 1083.964111, mean_q: 6.874473\n",
      "wrong_move\n",
      "   1988/500000: episode: 1974, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3816.000 [3816.000, 3816.000],  loss: 207110.734375, mae: 1084.546387, mean_q: 7.056220\n",
      "wrong_move\n",
      "   1989/500000: episode: 1975, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2609.000 [2609.000, 2609.000],  loss: 135211.671875, mae: 1083.131104, mean_q: 7.034769\n",
      "wrong_move\n",
      "   1990/500000: episode: 1976, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2952.000 [2952.000, 2952.000],  loss: 409037.843750, mae: 1082.289062, mean_q: 6.962668\n",
      "wrong_move\n",
      "   1991/500000: episode: 1977, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3816.000 [3816.000, 3816.000],  loss: 327187.062500, mae: 1083.128906, mean_q: 6.946683\n",
      "wrong_move\n",
      "   1992/500000: episode: 1978, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2943.000 [2943.000, 2943.000],  loss: 46316.406250, mae: 1082.556030, mean_q: 7.012086\n",
      "wrong_move\n",
      "   1993/500000: episode: 1979, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2952.000 [2952.000, 2952.000],  loss: 258419.093750, mae: 1082.859009, mean_q: 7.005945\n",
      "wrong_move\n",
      "   1994/500000: episode: 1980, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2609.000 [2609.000, 2609.000],  loss: 483929.156250, mae: 1081.788086, mean_q: 6.896914\n",
      "wrong_move\n",
      "   1995/500000: episode: 1981, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1825.000 [1825.000, 1825.000],  loss: 30997.130859, mae: 1080.060425, mean_q: 7.115557\n",
      "wrong_move\n",
      "   1996/500000: episode: 1982, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2609.000 [2609.000, 2609.000],  loss: 177166.984375, mae: 1078.579834, mean_q: 6.974029\n",
      "wrong_move\n",
      "   1997/500000: episode: 1983, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3816.000 [3816.000, 3816.000],  loss: 49266.488281, mae: 1077.264648, mean_q: 6.984521\n",
      "wrong_move\n",
      "   1998/500000: episode: 1984, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 457.000 [457.000, 457.000],  loss: 18458.554688, mae: 1078.001099, mean_q: 6.956697\n",
      "wrong_move\n",
      "   1999/500000: episode: 1985, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2609.000 [2609.000, 2609.000],  loss: 15785.386719, mae: 1080.058594, mean_q: 6.917717\n",
      "wrong_move\n",
      "   2000/500000: episode: 1986, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1911.000 [1911.000, 1911.000],  loss: 23124.824219, mae: 1082.314331, mean_q: 6.959849\n",
      "wrong_move\n",
      "   2001/500000: episode: 1987, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 378.000 [378.000, 378.000],  loss: 457525.781250, mae: 1084.923706, mean_q: 7.111686\n",
      "wrong_move\n",
      "   2002/500000: episode: 1988, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2533.000 [2533.000, 2533.000],  loss: 29983.933594, mae: 1087.800049, mean_q: 7.020855\n",
      "wrong_move\n",
      "   2003/500000: episode: 1989, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2533.000 [2533.000, 2533.000],  loss: 47916.695312, mae: 1090.466187, mean_q: 6.974839\n",
      "wrong_move\n",
      "   2004/500000: episode: 1990, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3621.000 [3621.000, 3621.000],  loss: 25933.933594, mae: 1093.280029, mean_q: 12.694750\n",
      "wrong_move\n",
      "   2005/500000: episode: 1991, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2533.000 [2533.000, 2533.000],  loss: 55837.429688, mae: 1094.706299, mean_q: 11.649542\n",
      "wrong_move\n",
      "   2006/500000: episode: 1992, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 84.000 [84.000, 84.000],  loss: 426562.281250, mae: 1094.529541, mean_q: 6.938095\n",
      "wrong_move\n",
      "   2007/500000: episode: 1993, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 457.000 [457.000, 457.000],  loss: 802483.375000, mae: 1095.833740, mean_q: 6.793471\n",
      "wrong_move\n",
      "   2008/500000: episode: 1994, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3125.000 [3125.000, 3125.000],  loss: 30352.185547, mae: 1096.856201, mean_q: 6.771085\n",
      "wrong_move\n",
      "   2009/500000: episode: 1995, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1287.000 [1287.000, 1287.000],  loss: 804413.375000, mae: 1097.097778, mean_q: 6.772477\n",
      "wrong_move\n",
      "   2010/500000: episode: 1996, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 706.000 [706.000, 706.000],  loss: 471767.843750, mae: 1096.181396, mean_q: 6.755625\n",
      "wrong_move\n",
      "   2011/500000: episode: 1997, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 198.000 [198.000, 198.000],  loss: 185689.968750, mae: 1093.761963, mean_q: 6.884187\n",
      "wrong_move\n",
      "   2012/500000: episode: 1998, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2416.000 [2416.000, 2416.000],  loss: 428961.468750, mae: 1092.473389, mean_q: 6.911133\n",
      "wrong_move\n",
      "   2013/500000: episode: 1999, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 951.000 [951.000, 951.000],  loss: 51913.171875, mae: 1093.258057, mean_q: 6.780892\n",
      "wrong_move\n",
      "   2014/500000: episode: 2000, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2209.000 [2209.000, 2209.000],  loss: 18806.925781, mae: 1093.978516, mean_q: 6.728454\n",
      "wrong_move\n",
      "   2015/500000: episode: 2001, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3295.000 [3295.000, 3295.000],  loss: 421718.281250, mae: 1095.077759, mean_q: 6.776417\n",
      "wrong_move\n",
      "   2016/500000: episode: 2002, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 951.000 [951.000, 951.000],  loss: 65682.812500, mae: 1094.708008, mean_q: 6.786596\n",
      "wrong_move\n",
      "   2017/500000: episode: 2003, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2209.000 [2209.000, 2209.000],  loss: 21450.507812, mae: 1096.269287, mean_q: 6.853127\n",
      "wrong_move\n",
      "   2018/500000: episode: 2004, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 84.000 [84.000, 84.000],  loss: 944405.375000, mae: 1098.377197, mean_q: 6.950408\n",
      "wrong_move\n",
      "   2019/500000: episode: 2005, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3935.000 [3935.000, 3935.000],  loss: 66332.976562, mae: 1098.933960, mean_q: 6.861469\n",
      "wrong_move\n",
      "   2020/500000: episode: 2006, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3048.000 [3048.000, 3048.000],  loss: 42167.750000, mae: 1098.803589, mean_q: 6.722782\n",
      "wrong_move\n",
      "   2021/500000: episode: 2007, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3048.000 [3048.000, 3048.000],  loss: 518032.000000, mae: 1097.928955, mean_q: 6.843654\n",
      "wrong_move\n",
      "   2022/500000: episode: 2008, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3504.000 [3504.000, 3504.000],  loss: 30807.068359, mae: 1097.395020, mean_q: 6.883264\n",
      "wrong_move\n",
      "   2023/500000: episode: 2009, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 102.000 [102.000, 102.000],  loss: 27544.568359, mae: 1098.269531, mean_q: 6.837327\n",
      "wrong_move\n",
      "   2024/500000: episode: 2010, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3048.000 [3048.000, 3048.000],  loss: 169334.656250, mae: 1099.976562, mean_q: 6.873031\n",
      "wrong_move\n",
      "   2025/500000: episode: 2011, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3216.000 [3216.000, 3216.000],  loss: 311351.406250, mae: 1100.174805, mean_q: 6.723207\n",
      "wrong_move\n",
      "   2026/500000: episode: 2012, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 168.000 [168.000, 168.000],  loss: 401877.781250, mae: 1099.616943, mean_q: 6.851210\n",
      "wrong_move\n",
      "   2027/500000: episode: 2013, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3101.000 [3101.000, 3101.000],  loss: 29964.277344, mae: 1098.832520, mean_q: 6.752933\n",
      "wrong_move\n",
      "   2028/500000: episode: 2014, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2907.000 [2907.000, 2907.000],  loss: 89592.468750, mae: 1099.241943, mean_q: 6.819204\n",
      "wrong_move\n",
      "   2029/500000: episode: 2015, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2177.000 [2177.000, 2177.000],  loss: 192049.281250, mae: 1099.789795, mean_q: 6.738822\n",
      "wrong_move\n",
      "   2030/500000: episode: 2016, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3167.000 [3167.000, 3167.000],  loss: 433033.781250, mae: 1100.401123, mean_q: 6.733378\n",
      "wrong_move\n",
      "   2031/500000: episode: 2017, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 538.000 [538.000, 538.000],  loss: 26387.179688, mae: 1101.604492, mean_q: 6.873584\n",
      "wrong_move\n",
      "   2032/500000: episode: 2018, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3076.000 [3076.000, 3076.000],  loss: 848841.500000, mae: 1104.051025, mean_q: 6.926486\n",
      "wrong_move\n",
      "   2033/500000: episode: 2019, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 461.000 [461.000, 461.000],  loss: 31055.148438, mae: 1107.009277, mean_q: 6.693532\n",
      "wrong_move\n",
      "   2034/500000: episode: 2020, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 473.000 [473.000, 473.000],  loss: 62398.199219, mae: 1108.968262, mean_q: 6.872583\n",
      "wrong_move\n",
      "   2035/500000: episode: 2021, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2902.000 [2902.000, 2902.000],  loss: 532546.937500, mae: 1111.644043, mean_q: 24.839527\n",
      "wrong_move\n",
      "   2036/500000: episode: 2022, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3076.000 [3076.000, 3076.000],  loss: 59633.976562, mae: 1116.472412, mean_q: 6.721674\n",
      "wrong_move\n",
      "   2037/500000: episode: 2023, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 899.000 [899.000, 899.000],  loss: 144642.250000, mae: 1120.596191, mean_q: 6.681768\n",
      "wrong_move\n",
      "   2038/500000: episode: 2024, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3167.000 [3167.000, 3167.000],  loss: 20999.970703, mae: 1124.362793, mean_q: 6.882789\n",
      "wrong_move\n",
      "   2039/500000: episode: 2025, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 36.000 [36.000, 36.000],  loss: 27979.501953, mae: 1126.233154, mean_q: 6.800204\n",
      "wrong_move\n",
      "   2040/500000: episode: 2026, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1846.000 [1846.000, 1846.000],  loss: 233846.687500, mae: 1126.662354, mean_q: 6.786228\n",
      "wrong_move\n",
      "   2041/500000: episode: 2027, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3104.000 [3104.000, 3104.000],  loss: 25536.375000, mae: 1125.755859, mean_q: 6.787219\n",
      "wrong_move\n",
      "   2042/500000: episode: 2028, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 887.000 [887.000, 887.000],  loss: 443490.593750, mae: 1125.221436, mean_q: 6.844735\n",
      "wrong_move\n",
      "   2043/500000: episode: 2029, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3621.000 [3621.000, 3621.000],  loss: 451144.281250, mae: 1126.114502, mean_q: 6.842170\n",
      "wrong_move\n",
      "   2044/500000: episode: 2030, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3216.000 [3216.000, 3216.000],  loss: 19402.667969, mae: 1125.320557, mean_q: 6.754493\n",
      "wrong_move\n",
      "   2045/500000: episode: 2031, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2599.000 [2599.000, 2599.000],  loss: 459310.875000, mae: 1124.838501, mean_q: 6.693812\n",
      "wrong_move\n",
      "   2046/500000: episode: 2032, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1329.000 [1329.000, 1329.000],  loss: 29842.429688, mae: 1123.186768, mean_q: 6.671450\n",
      "wrong_move\n",
      "   2047/500000: episode: 2033, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3767.000 [3767.000, 3767.000],  loss: 32496.628906, mae: 1119.289307, mean_q: 6.650673\n",
      "wrong_move\n",
      "   2048/500000: episode: 2034, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3251.000 [3251.000, 3251.000],  loss: 264741.656250, mae: 1114.151245, mean_q: 6.717128\n",
      "wrong_move\n",
      "   2049/500000: episode: 2035, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3621.000 [3621.000, 3621.000],  loss: 75540.125000, mae: 1110.039062, mean_q: 6.691314\n",
      "wrong_move\n",
      "   2050/500000: episode: 2036, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3534.000 [3534.000, 3534.000],  loss: 429377.250000, mae: 1109.765137, mean_q: 6.639883\n",
      "wrong_move\n",
      "   2051/500000: episode: 2037, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3216.000 [3216.000, 3216.000],  loss: 23427.462891, mae: 1112.024170, mean_q: 6.474841\n",
      "wrong_move\n",
      "   2052/500000: episode: 2038, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2065.000 [2065.000, 2065.000],  loss: 34749.457031, mae: 1115.065186, mean_q: 6.669879\n",
      "wrong_move\n",
      "   2053/500000: episode: 2039, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2065.000 [2065.000, 2065.000],  loss: 136100.390625, mae: 1117.747314, mean_q: 6.695845\n",
      "wrong_move\n",
      "   2054/500000: episode: 2040, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2719.000 [2719.000, 2719.000],  loss: 25624.000000, mae: 1121.550781, mean_q: 6.836273\n",
      "wrong_move\n",
      "   2055/500000: episode: 2041, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 948.000 [948.000, 948.000],  loss: 54334.554688, mae: 1125.357544, mean_q: 6.828851\n",
      "wrong_move\n",
      "   2056/500000: episode: 2042, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1535.000 [1535.000, 1535.000],  loss: 30856.357422, mae: 1129.790283, mean_q: 198.174652\n",
      "wrong_move\n",
      "   2057/500000: episode: 2043, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1450.000 [1450.000, 1450.000],  loss: 40898.273438, mae: 1135.239014, mean_q: 398.561584\n",
      "wrong_move\n",
      "   2058/500000: episode: 2044, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 80.000 [80.000, 80.000],  loss: 425160.625000, mae: 1139.595703, mean_q: 529.685303\n",
      "wrong_move\n",
      "   2059/500000: episode: 2045, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 80.000 [80.000, 80.000],  loss: 18516.636719, mae: 1143.033813, mean_q: 636.426514\n",
      "wrong_move\n",
      "   2061/500000: episode: 2046, duration: 0.106s, episode steps:   2, steps per second:  19, episode reward: -5021.000, mean reward: -2510.500 [-5000.000, -21.000], mean action: 80.000 [80.000, 80.000],  loss: 64368.300781, mae: 1146.722900, mean_q: 659.028503\n",
      "wrong_move\n",
      "   2062/500000: episode: 2047, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 80.000 [80.000, 80.000],  loss: 123232.500000, mae: 1147.593140, mean_q: 626.143555\n",
      "wrong_move\n",
      "   2063/500000: episode: 2048, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 80.000 [80.000, 80.000],  loss: 1159499.250000, mae: 1146.730591, mean_q: 604.869873\n",
      "wrong_move\n",
      "   2064/500000: episode: 2049, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4031.000 [4031.000, 4031.000],  loss: 27374.023438, mae: 1142.583984, mean_q: 7.130349\n",
      "wrong_move\n",
      "   2065/500000: episode: 2050, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4010.000 [4010.000, 4010.000],  loss: 192103.562500, mae: 1140.903809, mean_q: 6.491410\n",
      "wrong_move\n",
      "   2066/500000: episode: 2051, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2692.000 [2692.000, 2692.000],  loss: 509674.406250, mae: 1140.963379, mean_q: 6.581672\n",
      "wrong_move\n",
      "   2067/500000: episode: 2052, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3478.000 [3478.000, 3478.000],  loss: 215721.031250, mae: 1139.990479, mean_q: 6.529486\n",
      "wrong_move\n",
      "   2068/500000: episode: 2053, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1510.000 [1510.000, 1510.000],  loss: 124776.625000, mae: 1140.322510, mean_q: 6.675566\n",
      "wrong_move\n",
      "   2069/500000: episode: 2054, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1398.000 [1398.000, 1398.000],  loss: 58119.656250, mae: 1139.696289, mean_q: 6.627339\n",
      "wrong_move\n",
      "   2070/500000: episode: 2055, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4088.000 [4088.000, 4088.000],  loss: 17250.816406, mae: 1139.836426, mean_q: 6.625405\n",
      "wrong_move\n",
      "   2071/500000: episode: 2056, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1345.000 [1345.000, 1345.000],  loss: 110147.000000, mae: 1140.840332, mean_q: 6.636376\n",
      "wrong_move\n",
      "   2072/500000: episode: 2057, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4088.000 [4088.000, 4088.000],  loss: 53526.320312, mae: 1143.316650, mean_q: 6.777580\n",
      "wrong_move\n",
      "   2073/500000: episode: 2058, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4088.000 [4088.000, 4088.000],  loss: 30369.484375, mae: 1145.572876, mean_q: 6.618175\n",
      "wrong_move\n",
      "   2074/500000: episode: 2059, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2810.000 [2810.000, 2810.000],  loss: 581907.875000, mae: 1147.419922, mean_q: 6.740748\n",
      "wrong_move\n",
      "   2075/500000: episode: 2060, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2356.000 [2356.000, 2356.000],  loss: 41244.019531, mae: 1151.251221, mean_q: 6.624032\n",
      "wrong_move\n",
      "   2076/500000: episode: 2061, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 686.000 [686.000, 686.000],  loss: 465208.500000, mae: 1154.078857, mean_q: 6.678608\n",
      "wrong_move\n",
      "   2077/500000: episode: 2062, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 925.000 [925.000, 925.000],  loss: 30649.892578, mae: 1154.877686, mean_q: 6.610420\n",
      "wrong_move\n",
      "   2078/500000: episode: 2063, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 686.000 [686.000, 686.000],  loss: 24069.232422, mae: 1155.102295, mean_q: 6.785536\n",
      "wrong_move\n",
      "   2079/500000: episode: 2064, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 686.000 [686.000, 686.000],  loss: 124842.375000, mae: 1154.242065, mean_q: 6.683209\n",
      "wrong_move\n",
      "   2080/500000: episode: 2065, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1493.000 [1493.000, 1493.000],  loss: 15648.912109, mae: 1150.211914, mean_q: 6.744855\n",
      "wrong_move\n",
      "   2081/500000: episode: 2066, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2810.000 [2810.000, 2810.000],  loss: 42024.824219, mae: 1146.931030, mean_q: 6.539892\n",
      "wrong_move\n",
      "   2082/500000: episode: 2067, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 686.000 [686.000, 686.000],  loss: 598968.750000, mae: 1145.292725, mean_q: 6.577132\n",
      "wrong_move\n",
      "   2083/500000: episode: 2068, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 686.000 [686.000, 686.000],  loss: 23022.259766, mae: 1143.193481, mean_q: 6.581838\n",
      "wrong_move\n",
      "   2084/500000: episode: 2069, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1756.000 [1756.000, 1756.000],  loss: 424143.656250, mae: 1142.747437, mean_q: 6.518324\n",
      "wrong_move\n",
      "   2085/500000: episode: 2070, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 141.000 [141.000, 141.000],  loss: 21749.708984, mae: 1143.238159, mean_q: 6.536293\n",
      "wrong_move\n",
      "   2086/500000: episode: 2071, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2096.000 [2096.000, 2096.000],  loss: 21824.599609, mae: 1144.689697, mean_q: 6.586807\n",
      "wrong_move\n",
      "   2087/500000: episode: 2072, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3051.000 [3051.000, 3051.000],  loss: 20154.097656, mae: 1145.068115, mean_q: 6.474507\n",
      "wrong_move\n",
      "   2088/500000: episode: 2073, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 686.000 [686.000, 686.000],  loss: 101541.882812, mae: 1147.186035, mean_q: 6.624777\n",
      "wrong_move\n",
      "   2089/500000: episode: 2074, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2574.000 [2574.000, 2574.000],  loss: 13618.670898, mae: 1149.423218, mean_q: 6.425536\n",
      "wrong_move\n",
      "   2090/500000: episode: 2075, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 432315.937500, mae: 1151.973755, mean_q: 6.689218\n",
      "wrong_move\n",
      "   2091/500000: episode: 2076, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2771.000 [2771.000, 2771.000],  loss: 419557.156250, mae: 1154.382690, mean_q: 6.557957\n",
      "wrong_move\n",
      "   2092/500000: episode: 2077, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4002.000 [4002.000, 4002.000],  loss: 26961.644531, mae: 1156.828369, mean_q: 6.535553\n",
      "wrong_move\n",
      "   2093/500000: episode: 2078, duration: 0.154s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 839.000 [839.000, 839.000],  loss: 229421.093750, mae: 1159.206787, mean_q: 6.569180\n",
      "wrong_move\n",
      "   2094/500000: episode: 2079, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2771.000 [2771.000, 2771.000],  loss: 29806.564453, mae: 1162.632812, mean_q: 6.620275\n",
      "wrong_move\n",
      "   2095/500000: episode: 2080, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2606.000 [2606.000, 2606.000],  loss: 447920.125000, mae: 1166.944946, mean_q: 6.726800\n",
      "wrong_move\n",
      "   2096/500000: episode: 2081, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2968.000 [2968.000, 2968.000],  loss: 207035.296875, mae: 1170.190430, mean_q: 6.483815\n",
      "wrong_move\n",
      "   2097/500000: episode: 2082, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2968.000 [2968.000, 2968.000],  loss: 348948.000000, mae: 1169.780762, mean_q: 6.546887\n",
      "wrong_move\n",
      "   2098/500000: episode: 2083, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2968.000 [2968.000, 2968.000],  loss: 26202.460938, mae: 1171.018677, mean_q: 6.448244\n",
      "wrong_move\n",
      "   2099/500000: episode: 2084, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 195.000 [195.000, 195.000],  loss: 412051.218750, mae: 1171.451782, mean_q: 6.448916\n",
      "wrong_move\n",
      "   2100/500000: episode: 2085, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1035.000 [1035.000, 1035.000],  loss: 123032.289062, mae: 1171.527710, mean_q: 6.569316\n",
      "wrong_move\n",
      "   2101/500000: episode: 2086, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2500.000 [2500.000, 2500.000],  loss: 62859.933594, mae: 1172.942871, mean_q: 6.555612\n",
      "wrong_move\n",
      "   2102/500000: episode: 2087, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 138.000 [138.000, 138.000],  loss: 605816.000000, mae: 1175.108643, mean_q: 6.823424\n",
      "wrong_move\n",
      "   2103/500000: episode: 2088, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2968.000 [2968.000, 2968.000],  loss: 29041.982422, mae: 1175.714233, mean_q: 6.678907\n",
      "wrong_move\n",
      "   2104/500000: episode: 2089, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1941.000 [1941.000, 1941.000],  loss: 442211.750000, mae: 1176.280640, mean_q: 6.584595\n",
      "wrong_move\n",
      "   2105/500000: episode: 2090, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1046.000 [1046.000, 1046.000],  loss: 115086.765625, mae: 1175.985352, mean_q: 6.513540\n",
      "wrong_move\n",
      "   2106/500000: episode: 2091, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1510.000 [1510.000, 1510.000],  loss: 496236.343750, mae: 1174.898071, mean_q: 6.505251\n",
      "wrong_move\n",
      "   2107/500000: episode: 2092, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 539.000 [539.000, 539.000],  loss: 333376.843750, mae: 1172.898926, mean_q: 6.548156\n",
      "wrong_move\n",
      "   2108/500000: episode: 2093, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2876.000 [2876.000, 2876.000],  loss: 944769.375000, mae: 1171.371582, mean_q: 6.486094\n",
      "wrong_move\n",
      "   2109/500000: episode: 2094, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2107.000 [2107.000, 2107.000],  loss: 184200.718750, mae: 1167.542969, mean_q: 6.503087\n",
      "wrong_move\n",
      "   2110/500000: episode: 2095, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2968.000 [2968.000, 2968.000],  loss: 42964.449219, mae: 1165.045776, mean_q: 6.540246\n",
      "wrong_move\n",
      "   2111/500000: episode: 2096, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1510.000 [1510.000, 1510.000],  loss: 24096.402344, mae: 1164.663086, mean_q: 6.654031\n",
      "wrong_move\n",
      "   2112/500000: episode: 2097, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 652.000 [652.000, 652.000],  loss: 425898.781250, mae: 1164.142334, mean_q: 6.487432\n",
      "wrong_move\n",
      "   2113/500000: episode: 2098, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 522.000 [522.000, 522.000],  loss: 27204.708984, mae: 1165.004150, mean_q: 6.526466\n",
      "wrong_move\n",
      "   2114/500000: episode: 2099, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1963.000 [1963.000, 1963.000],  loss: 900591.375000, mae: 1167.207764, mean_q: 6.520751\n",
      "wrong_move\n",
      "   2115/500000: episode: 2100, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1963.000 [1963.000, 1963.000],  loss: 98820.343750, mae: 1169.614990, mean_q: 6.638449\n",
      "wrong_move\n",
      "   2116/500000: episode: 2101, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1963.000 [1963.000, 1963.000],  loss: 32512.041016, mae: 1172.572510, mean_q: 6.408159\n",
      "wrong_move\n",
      "   2117/500000: episode: 2102, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2074.000 [2074.000, 2074.000],  loss: 165079.812500, mae: 1174.168457, mean_q: 6.666306\n",
      "wrong_move\n",
      "   2118/500000: episode: 2103, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2453.000 [2453.000, 2453.000],  loss: 18580.917969, mae: 1176.053711, mean_q: 6.567080\n",
      "wrong_move\n",
      "   2119/500000: episode: 2104, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2074.000 [2074.000, 2074.000],  loss: 26426.039062, mae: 1178.258545, mean_q: 6.502973\n",
      "wrong_move\n",
      "   2120/500000: episode: 2105, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2290.000 [2290.000, 2290.000],  loss: 448362.093750, mae: 1180.348633, mean_q: 6.597739\n",
      "wrong_move\n",
      "   2121/500000: episode: 2106, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 593.000 [593.000, 593.000],  loss: 14468.847656, mae: 1179.556885, mean_q: 6.551167\n",
      "wrong_move\n",
      "   2122/500000: episode: 2107, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1963.000 [1963.000, 1963.000],  loss: 19950.794922, mae: 1177.058105, mean_q: 6.512785\n",
      "wrong_move\n",
      "   2123/500000: episode: 2108, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 25.000 [25.000, 25.000],  loss: 423357.062500, mae: 1174.760132, mean_q: 6.437920\n",
      "wrong_move\n",
      "   2124/500000: episode: 2109, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3180.000 [3180.000, 3180.000],  loss: 472212.218750, mae: 1173.936157, mean_q: 6.498908\n",
      "wrong_move\n",
      "   2125/500000: episode: 2110, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1562.000 [1562.000, 1562.000],  loss: 645465.250000, mae: 1174.010376, mean_q: 6.470837\n",
      "wrong_move\n",
      "   2126/500000: episode: 2111, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1149.000 [1149.000, 1149.000],  loss: 502930.531250, mae: 1176.893188, mean_q: 6.495512\n",
      "wrong_move\n",
      "   2127/500000: episode: 2112, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1772.000 [1772.000, 1772.000],  loss: 78029.125000, mae: 1178.344116, mean_q: 6.501137\n",
      "wrong_move\n",
      "   2128/500000: episode: 2113, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3767.000 [3767.000, 3767.000],  loss: 183755.828125, mae: 1177.637207, mean_q: 6.403126\n",
      "wrong_move\n",
      "   2129/500000: episode: 2114, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1108.000 [1108.000, 1108.000],  loss: 61101.921875, mae: 1177.195190, mean_q: 6.434066\n",
      "wrong_move\n",
      "   2130/500000: episode: 2115, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3647.000 [3647.000, 3647.000],  loss: 215092.906250, mae: 1178.294922, mean_q: 6.638631\n",
      "wrong_move\n",
      "   2131/500000: episode: 2116, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2524.000 [2524.000, 2524.000],  loss: 37324.703125, mae: 1181.563599, mean_q: 6.535093\n",
      "wrong_move\n",
      "   2132/500000: episode: 2117, duration: 0.142s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3632.000 [3632.000, 3632.000],  loss: 57478.515625, mae: 1184.639160, mean_q: 6.555280\n",
      "wrong_move\n",
      "   2133/500000: episode: 2118, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2595.000 [2595.000, 2595.000],  loss: 22675.421875, mae: 1188.485352, mean_q: 6.471741\n",
      "wrong_move\n",
      "   2134/500000: episode: 2119, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3566.000 [3566.000, 3566.000],  loss: 438129.937500, mae: 1192.098999, mean_q: 6.513301\n",
      "wrong_move\n",
      "   2135/500000: episode: 2120, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2006.000 [2006.000, 2006.000],  loss: 328920.468750, mae: 1194.374756, mean_q: 6.670342\n",
      "wrong_move\n",
      "   2136/500000: episode: 2121, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4021.000 [4021.000, 4021.000],  loss: 182223.187500, mae: 1193.305176, mean_q: 6.699179\n",
      "wrong_move\n",
      "   2137/500000: episode: 2122, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1108.000 [1108.000, 1108.000],  loss: 85052.601562, mae: 1191.282715, mean_q: 6.507890\n",
      "wrong_move\n",
      "   2138/500000: episode: 2123, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2806.000 [2806.000, 2806.000],  loss: 65756.414062, mae: 1188.863770, mean_q: 6.494595\n",
      "wrong_move\n",
      "   2139/500000: episode: 2124, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2806.000 [2806.000, 2806.000],  loss: 8844.807617, mae: 1187.271851, mean_q: 6.414958\n",
      "wrong_move\n",
      "   2140/500000: episode: 2125, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1190.000 [1190.000, 1190.000],  loss: 140873.953125, mae: 1187.339600, mean_q: 6.490357\n",
      "wrong_move\n",
      "   2141/500000: episode: 2126, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 139.000 [139.000, 139.000],  loss: 87614.601562, mae: 1186.707642, mean_q: 6.422223\n",
      "wrong_move\n",
      "   2142/500000: episode: 2127, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 875.000 [875.000, 875.000],  loss: 112622.125000, mae: 1184.567139, mean_q: 6.346984\n",
      "wrong_move\n",
      "   2143/500000: episode: 2128, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1082.000 [1082.000, 1082.000],  loss: 107499.664062, mae: 1184.658569, mean_q: 6.406178\n",
      "wrong_move\n",
      "   2144/500000: episode: 2129, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1314.000 [1314.000, 1314.000],  loss: 524497.437500, mae: 1185.418945, mean_q: 6.581494\n",
      "wrong_move\n",
      "   2145/500000: episode: 2130, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1781.000 [1781.000, 1781.000],  loss: 486801.906250, mae: 1187.939209, mean_q: 6.539164\n",
      "wrong_move\n",
      "   2146/500000: episode: 2131, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3518.000 [3518.000, 3518.000],  loss: 504036.031250, mae: 1189.436646, mean_q: 6.473427\n",
      "wrong_move\n",
      "   2147/500000: episode: 2132, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 875.000 [875.000, 875.000],  loss: 9204.969727, mae: 1191.564209, mean_q: 6.386504\n",
      "wrong_move\n",
      "   2148/500000: episode: 2133, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3537.000 [3537.000, 3537.000],  loss: 19526.062500, mae: 1193.698120, mean_q: 6.371400\n",
      "wrong_move\n",
      "   2149/500000: episode: 2134, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 237.000 [237.000, 237.000],  loss: 416126.875000, mae: 1194.930664, mean_q: 6.463354\n",
      "wrong_move\n",
      "   2150/500000: episode: 2135, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1565.000 [1565.000, 1565.000],  loss: 369950.906250, mae: 1193.816895, mean_q: 6.400949\n",
      "wrong_move\n",
      "   2151/500000: episode: 2136, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1430.000 [1430.000, 1430.000],  loss: 385012.250000, mae: 1194.066162, mean_q: 6.303871\n",
      "wrong_move\n",
      "   2152/500000: episode: 2137, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1430.000 [1430.000, 1430.000],  loss: 67512.656250, mae: 1193.167236, mean_q: 6.353006\n",
      "wrong_move\n",
      "   2153/500000: episode: 2138, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1430.000 [1430.000, 1430.000],  loss: 447952.750000, mae: 1195.216187, mean_q: 6.501663\n",
      "wrong_move\n",
      "   2154/500000: episode: 2139, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1430.000 [1430.000, 1430.000],  loss: 47469.847656, mae: 1199.399170, mean_q: 6.388441\n",
      "wrong_move\n",
      "   2155/500000: episode: 2140, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 139.000 [139.000, 139.000],  loss: 39609.546875, mae: 1202.925537, mean_q: 6.419843\n",
      "wrong_move\n",
      "   2156/500000: episode: 2141, duration: 0.044s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1430.000 [1430.000, 1430.000],  loss: 68293.265625, mae: 1205.543335, mean_q: 6.404563\n",
      "wrong_move\n",
      "   2157/500000: episode: 2142, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1430.000 [1430.000, 1430.000],  loss: 219735.781250, mae: 1206.697754, mean_q: 6.452468\n",
      "wrong_move\n",
      "   2158/500000: episode: 2143, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1430.000 [1430.000, 1430.000],  loss: 32465.949219, mae: 1205.634521, mean_q: 6.409469\n",
      "wrong_move\n",
      "   2159/500000: episode: 2144, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2480.000 [2480.000, 2480.000],  loss: 25599.271484, mae: 1206.070557, mean_q: 6.416808\n",
      "wrong_move\n",
      "   2160/500000: episode: 2145, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3921.000 [3921.000, 3921.000],  loss: 94508.984375, mae: 1207.126221, mean_q: 6.327984\n",
      "wrong_move\n",
      "   2161/500000: episode: 2146, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1670.000 [1670.000, 1670.000],  loss: 541192.187500, mae: 1206.523682, mean_q: 6.377141\n",
      "wrong_move\n",
      "   2162/500000: episode: 2147, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1430.000 [1430.000, 1430.000],  loss: 816476.500000, mae: 1205.603271, mean_q: 6.485678\n",
      "wrong_move\n",
      "   2163/500000: episode: 2148, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 292.000 [292.000, 292.000],  loss: 38861.542969, mae: 1204.090332, mean_q: 6.345543\n",
      "wrong_move\n",
      "   2164/500000: episode: 2149, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2006.000 [2006.000, 2006.000],  loss: 481648.875000, mae: 1201.714966, mean_q: 6.278121\n",
      "wrong_move\n",
      "   2165/500000: episode: 2150, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 882.000 [882.000, 882.000],  loss: 425171.187500, mae: 1199.695801, mean_q: 6.230696\n",
      "wrong_move\n",
      "   2166/500000: episode: 2151, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2480.000 [2480.000, 2480.000],  loss: 50225.742188, mae: 1200.625610, mean_q: 6.323793\n",
      "wrong_move\n",
      "   2167/500000: episode: 2152, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2288.000 [2288.000, 2288.000],  loss: 32197.949219, mae: 1203.048706, mean_q: 6.224432\n",
      "wrong_move\n",
      "   2168/500000: episode: 2153, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2792.000 [2792.000, 2792.000],  loss: 68480.445312, mae: 1206.723145, mean_q: 6.258532\n",
      "wrong_move\n",
      "   2169/500000: episode: 2154, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 882.000 [882.000, 882.000],  loss: 25970.632812, mae: 1208.775879, mean_q: 6.351612\n",
      "wrong_move\n",
      "   2170/500000: episode: 2155, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 882.000 [882.000, 882.000],  loss: 17801.742188, mae: 1209.171753, mean_q: 6.267640\n",
      "wrong_move\n",
      "   2171/500000: episode: 2156, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 882.000 [882.000, 882.000],  loss: 32543.222656, mae: 1208.515137, mean_q: 6.276260\n",
      "wrong_move\n",
      "   2172/500000: episode: 2157, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3521.000 [3521.000, 3521.000],  loss: 79077.093750, mae: 1209.192383, mean_q: 6.384475\n",
      "wrong_move\n",
      "   2173/500000: episode: 2158, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 139.000 [139.000, 139.000],  loss: 804092.000000, mae: 1210.500732, mean_q: 6.237310\n",
      "wrong_move\n",
      "   2174/500000: episode: 2159, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3459.000 [3459.000, 3459.000],  loss: 423546.750000, mae: 1212.438110, mean_q: 6.300400\n",
      "wrong_move\n",
      "   2175/500000: episode: 2160, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 139.000 [139.000, 139.000],  loss: 34027.726562, mae: 1214.183716, mean_q: 6.372886\n",
      "wrong_move\n",
      "   2176/500000: episode: 2161, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 581.000 [581.000, 581.000],  loss: 355751.718750, mae: 1217.961426, mean_q: 6.243824\n",
      "wrong_move\n",
      "   2177/500000: episode: 2162, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2774.000 [2774.000, 2774.000],  loss: 56105.714844, mae: 1219.658203, mean_q: 6.420190\n",
      "wrong_move\n",
      "   2178/500000: episode: 2163, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 433.000 [433.000, 433.000],  loss: 40479.250000, mae: 1219.569092, mean_q: 6.398341\n",
      "wrong_move\n",
      "   2179/500000: episode: 2164, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 381.000 [381.000, 381.000],  loss: 43771.281250, mae: 1217.572266, mean_q: 6.144147\n",
      "wrong_move\n",
      "   2180/500000: episode: 2165, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2792.000 [2792.000, 2792.000],  loss: 497946.093750, mae: 1215.216553, mean_q: 6.177839\n",
      "wrong_move\n",
      "   2181/500000: episode: 2166, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 924.000 [924.000, 924.000],  loss: 242953.062500, mae: 1213.891113, mean_q: 6.106374\n",
      "wrong_move\n",
      "   2182/500000: episode: 2167, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1449.000 [1449.000, 1449.000],  loss: 183113.296875, mae: 1209.773315, mean_q: 6.483481\n",
      "wrong_move\n",
      "   2183/500000: episode: 2168, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 581.000 [581.000, 581.000],  loss: 433563.812500, mae: 1205.182129, mean_q: 6.104692\n",
      "wrong_move\n",
      "   2184/500000: episode: 2169, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1910.000 [1910.000, 1910.000],  loss: 171933.671875, mae: 1203.076294, mean_q: 6.267295\n",
      "wrong_move\n",
      "   2185/500000: episode: 2170, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2774.000 [2774.000, 2774.000],  loss: 649365.687500, mae: 1203.954834, mean_q: 6.193053\n",
      "wrong_move\n",
      "   2186/500000: episode: 2171, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3814.000 [3814.000, 3814.000],  loss: 116882.718750, mae: 1205.484619, mean_q: 6.156518\n",
      "wrong_move\n",
      "   2187/500000: episode: 2172, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 203.000 [203.000, 203.000],  loss: 341788.937500, mae: 1206.829346, mean_q: 6.245439\n",
      "wrong_move\n",
      "   2188/500000: episode: 2173, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 167.000 [167.000, 167.000],  loss: 301260.406250, mae: 1211.058594, mean_q: 43.899544\n",
      "wrong_move\n",
      "   2189/500000: episode: 2174, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1302.000 [1302.000, 1302.000],  loss: 440157.687500, mae: 1217.737305, mean_q: 6.215563\n",
      "wrong_move\n",
      "   2190/500000: episode: 2175, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2285.000 [2285.000, 2285.000],  loss: 61258.937500, mae: 1222.047119, mean_q: 6.245548\n",
      "wrong_move\n",
      "   2191/500000: episode: 2176, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 799.000 [799.000, 799.000],  loss: 424199.687500, mae: 1227.022095, mean_q: 6.352435\n",
      "wrong_move\n",
      "   2192/500000: episode: 2177, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3682.000 [3682.000, 3682.000],  loss: 411097.593750, mae: 1233.796143, mean_q: 6.237179\n",
      "wrong_move\n",
      "   2193/500000: episode: 2178, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 139.000 [139.000, 139.000],  loss: 60832.992188, mae: 1239.598267, mean_q: 6.301273\n",
      "wrong_move\n",
      "   2194/500000: episode: 2179, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 167.000 [167.000, 167.000],  loss: 803375.375000, mae: 1245.380493, mean_q: 6.321935\n",
      "wrong_move\n",
      "   2195/500000: episode: 2180, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1509.000 [1509.000, 1509.000],  loss: 143545.703125, mae: 1251.074219, mean_q: 6.340292\n",
      "wrong_move\n",
      "   2196/500000: episode: 2181, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1645.000 [1645.000, 1645.000],  loss: 46037.820312, mae: 1256.537354, mean_q: 6.339172\n",
      "wrong_move\n",
      "   2197/500000: episode: 2182, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1509.000 [1509.000, 1509.000],  loss: 128458.843750, mae: 1258.475586, mean_q: 6.392373\n",
      "wrong_move\n",
      "   2198/500000: episode: 2183, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3267.000 [3267.000, 3267.000],  loss: 418511.906250, mae: 1258.020020, mean_q: 6.351124\n",
      "wrong_move\n",
      "   2199/500000: episode: 2184, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 177.000 [177.000, 177.000],  loss: 34478.015625, mae: 1256.317627, mean_q: 6.337288\n",
      "wrong_move\n",
      "   2200/500000: episode: 2185, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2884.000 [2884.000, 2884.000],  loss: 103659.617188, mae: 1253.556274, mean_q: 6.382515\n",
      "wrong_move\n",
      "   2201/500000: episode: 2186, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3642.000 [3642.000, 3642.000],  loss: 190809.453125, mae: 1249.036377, mean_q: 8.739945\n",
      "wrong_move\n",
      "   2202/500000: episode: 2187, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 167.000 [167.000, 167.000],  loss: 1026854.062500, mae: 1246.856445, mean_q: 6.367352\n",
      "wrong_move\n",
      "   2203/500000: episode: 2188, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1436.000 [1436.000, 1436.000],  loss: 421635.812500, mae: 1247.683960, mean_q: 6.153145\n",
      "wrong_move\n",
      "   2204/500000: episode: 2189, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3521.000 [3521.000, 3521.000],  loss: 163517.781250, mae: 1246.629761, mean_q: 6.214216\n",
      "wrong_move\n",
      "   2205/500000: episode: 2190, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 978.000 [978.000, 978.000],  loss: 49853.871094, mae: 1243.466675, mean_q: 6.258045\n",
      "wrong_move\n",
      "   2206/500000: episode: 2191, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1937.000 [1937.000, 1937.000],  loss: 324951.031250, mae: 1242.446655, mean_q: 6.249594\n",
      "wrong_move\n",
      "   2207/500000: episode: 2192, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 978.000 [978.000, 978.000],  loss: 230194.703125, mae: 1241.777222, mean_q: 6.324556\n",
      "wrong_move\n",
      "   2208/500000: episode: 2193, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2855.000 [2855.000, 2855.000],  loss: 33075.898438, mae: 1242.637085, mean_q: 6.314238\n",
      "wrong_move\n",
      "   2209/500000: episode: 2194, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2227.000 [2227.000, 2227.000],  loss: 411589.406250, mae: 1244.552002, mean_q: 6.363552\n",
      "wrong_move\n",
      "   2210/500000: episode: 2195, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3521.000 [3521.000, 3521.000],  loss: 289931.218750, mae: 1244.588135, mean_q: 6.262205\n",
      "wrong_move\n",
      "   2211/500000: episode: 2196, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2855.000 [2855.000, 2855.000],  loss: 37804.359375, mae: 1244.137695, mean_q: 6.541464\n",
      "wrong_move\n",
      "   2212/500000: episode: 2197, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3822.000 [3822.000, 3822.000],  loss: 63964.625000, mae: 1244.601929, mean_q: 6.504303\n",
      "wrong_move\n",
      "   2213/500000: episode: 2198, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 491.000 [491.000, 491.000],  loss: 209005.875000, mae: 1244.140625, mean_q: 6.427873\n",
      "wrong_move\n",
      "   2214/500000: episode: 2199, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1155.000 [1155.000, 1155.000],  loss: 33316.070312, mae: 1242.419922, mean_q: 6.390338\n",
      "wrong_move\n",
      "   2215/500000: episode: 2200, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3674.000 [3674.000, 3674.000],  loss: 251481.656250, mae: 1241.195679, mean_q: 6.375345\n",
      "wrong_move\n",
      "   2216/500000: episode: 2201, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 277.000 [277.000, 277.000],  loss: 498000.375000, mae: 1239.483154, mean_q: 6.312685\n",
      "wrong_move\n",
      "   2217/500000: episode: 2202, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1458.000 [1458.000, 1458.000],  loss: 167485.984375, mae: 1236.976562, mean_q: 6.566618\n",
      "wrong_move\n",
      "   2218/500000: episode: 2203, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3674.000 [3674.000, 3674.000],  loss: 481530.531250, mae: 1235.173340, mean_q: 18.050646\n",
      "wrong_move\n",
      "   2219/500000: episode: 2204, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3674.000 [3674.000, 3674.000],  loss: 128655.945312, mae: 1237.031128, mean_q: 6.451530\n",
      "wrong_move\n",
      "   2220/500000: episode: 2205, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3710.000 [3710.000, 3710.000],  loss: 24276.734375, mae: 1240.155640, mean_q: 6.198114\n",
      "wrong_move\n",
      "   2221/500000: episode: 2206, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1656.000 [1656.000, 1656.000],  loss: 81317.531250, mae: 1244.028320, mean_q: 6.410105\n",
      "wrong_move\n",
      "   2222/500000: episode: 2207, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1229.000 [1229.000, 1229.000],  loss: 46550.492188, mae: 1248.592041, mean_q: 6.253506\n",
      "wrong_move\n",
      "   2223/500000: episode: 2208, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1916.000 [1916.000, 1916.000],  loss: 414301.375000, mae: 1252.860840, mean_q: 6.160984\n",
      "wrong_move\n",
      "   2224/500000: episode: 2209, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3163.000 [3163.000, 3163.000],  loss: 35122.171875, mae: 1256.997192, mean_q: 6.481550\n",
      "wrong_move\n",
      "   2225/500000: episode: 2210, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3674.000 [3674.000, 3674.000],  loss: 107656.140625, mae: 1262.065674, mean_q: 6.243424\n",
      "wrong_move\n",
      "   2226/500000: episode: 2211, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2112.000 [2112.000, 2112.000],  loss: 507293.312500, mae: 1268.941895, mean_q: 6.380320\n",
      "wrong_move\n",
      "   2227/500000: episode: 2212, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2729.000 [2729.000, 2729.000],  loss: 128780.109375, mae: 1273.648071, mean_q: 6.286445\n",
      "wrong_move\n",
      "   2228/500000: episode: 2213, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1088.000 [1088.000, 1088.000],  loss: 166878.890625, mae: 1276.298218, mean_q: 6.444521\n",
      "wrong_move\n",
      "   2229/500000: episode: 2214, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1088.000 [1088.000, 1088.000],  loss: 468815.468750, mae: 1273.741211, mean_q: 6.306089\n",
      "wrong_move\n",
      "   2230/500000: episode: 2215, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2729.000 [2729.000, 2729.000],  loss: 19674.048828, mae: 1267.666260, mean_q: 6.394588\n",
      "wrong_move\n",
      "   2231/500000: episode: 2216, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2027.000 [2027.000, 2027.000],  loss: 68083.875000, mae: 1261.860229, mean_q: 6.408381\n",
      "wrong_move\n",
      "   2232/500000: episode: 2217, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 638.000 [638.000, 638.000],  loss: 101962.992188, mae: 1257.556885, mean_q: 6.388609\n",
      "wrong_move\n",
      "   2233/500000: episode: 2218, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3863.000 [3863.000, 3863.000],  loss: 21212.085938, mae: 1254.790039, mean_q: 6.322140\n",
      "wrong_move\n",
      "   2234/500000: episode: 2219, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2027.000 [2027.000, 2027.000],  loss: 52414.832031, mae: 1253.919434, mean_q: 6.434584\n",
      "wrong_move\n",
      "   2235/500000: episode: 2220, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3637.000 [3637.000, 3637.000],  loss: 90733.437500, mae: 1253.839844, mean_q: 6.260961\n",
      "wrong_move\n",
      "   2236/500000: episode: 2221, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 556.000 [556.000, 556.000],  loss: 37443.011719, mae: 1254.805542, mean_q: 6.284973\n",
      "wrong_move\n",
      "   2237/500000: episode: 2222, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 144.000 [144.000, 144.000],  loss: 22432.238281, mae: 1255.944580, mean_q: 6.227400\n",
      "wrong_move\n",
      "   2238/500000: episode: 2223, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2027.000 [2027.000, 2027.000],  loss: 101245.773438, mae: 1258.772705, mean_q: 6.297659\n",
      "wrong_move\n",
      "   2239/500000: episode: 2224, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2027.000 [2027.000, 2027.000],  loss: 425837.156250, mae: 1261.636475, mean_q: 6.303205\n",
      "wrong_move\n",
      "   2240/500000: episode: 2225, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3230.000 [3230.000, 3230.000],  loss: 46582.101562, mae: 1263.506470, mean_q: 6.273033\n",
      "wrong_move\n",
      "   2241/500000: episode: 2226, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1585.000 [1585.000, 1585.000],  loss: 97942.523438, mae: 1266.434082, mean_q: 6.385877\n",
      "wrong_move\n",
      "   2242/500000: episode: 2227, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3230.000 [3230.000, 3230.000],  loss: 68297.351562, mae: 1267.518799, mean_q: 6.352626\n",
      "wrong_move\n",
      "   2243/500000: episode: 2228, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3924.000 [3924.000, 3924.000],  loss: 127482.882812, mae: 1268.651123, mean_q: 6.348474\n",
      "wrong_move\n",
      "   2244/500000: episode: 2229, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1606.000 [1606.000, 1606.000],  loss: 416168.250000, mae: 1268.171509, mean_q: 6.146202\n",
      "wrong_move\n",
      "   2245/500000: episode: 2230, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3230.000 [3230.000, 3230.000],  loss: 459307.500000, mae: 1267.850098, mean_q: 6.349903\n",
      "wrong_move\n",
      "   2246/500000: episode: 2231, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2936.000 [2936.000, 2936.000],  loss: 79209.843750, mae: 1268.463623, mean_q: 6.168113\n",
      "wrong_move\n",
      "   2247/500000: episode: 2232, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3230.000 [3230.000, 3230.000],  loss: 429234.343750, mae: 1269.998657, mean_q: 6.218663\n",
      "wrong_move\n",
      "   2248/500000: episode: 2233, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1571.000 [1571.000, 1571.000],  loss: 73020.156250, mae: 1271.539307, mean_q: 6.197024\n",
      "wrong_move\n",
      "   2249/500000: episode: 2234, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2234.000 [2234.000, 2234.000],  loss: 105849.625000, mae: 1273.360107, mean_q: 6.397955\n",
      "wrong_move\n",
      "   2250/500000: episode: 2235, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3814.000 [3814.000, 3814.000],  loss: 447493.187500, mae: 1272.835815, mean_q: 6.253474\n",
      "wrong_move\n",
      "   2251/500000: episode: 2236, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 813.000 [813.000, 813.000],  loss: 423733.593750, mae: 1271.802979, mean_q: 6.210647\n",
      "wrong_move\n",
      "   2252/500000: episode: 2237, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1171.000 [1171.000, 1171.000],  loss: 91372.796875, mae: 1273.069336, mean_q: 6.219252\n",
      "wrong_move\n",
      "   2253/500000: episode: 2238, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1615.000 [1615.000, 1615.000],  loss: 192664.156250, mae: 1273.654053, mean_q: 6.183121\n",
      "wrong_move\n",
      "   2254/500000: episode: 2239, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1456.000 [1456.000, 1456.000],  loss: 481437.000000, mae: 1274.881348, mean_q: 6.365212\n",
      "wrong_move\n",
      "   2255/500000: episode: 2240, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1615.000 [1615.000, 1615.000],  loss: 24407.746094, mae: 1278.169434, mean_q: 6.122996\n",
      "wrong_move\n",
      "   2256/500000: episode: 2241, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1615.000 [1615.000, 1615.000],  loss: 414234.593750, mae: 1280.290039, mean_q: 6.241996\n",
      "wrong_move\n",
      "   2257/500000: episode: 2242, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4023.000 [4023.000, 4023.000],  loss: 47812.609375, mae: 1279.766602, mean_q: 6.202389\n",
      "wrong_move\n",
      "   2258/500000: episode: 2243, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 169.000 [169.000, 169.000],  loss: 47290.960938, mae: 1281.336304, mean_q: 6.180049\n",
      "wrong_move\n",
      "   2259/500000: episode: 2244, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3684.000 [3684.000, 3684.000],  loss: 408214.093750, mae: 1282.896729, mean_q: 6.031411\n",
      "wrong_move\n",
      "   2260/500000: episode: 2245, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3329.000 [3329.000, 3329.000],  loss: 918908.750000, mae: 1284.692871, mean_q: 6.085507\n",
      "wrong_move\n",
      "   2261/500000: episode: 2246, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1834.000 [1834.000, 1834.000],  loss: 50585.007812, mae: 1282.834351, mean_q: 6.139984\n",
      "wrong_move\n",
      "   2262/500000: episode: 2247, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1340.000 [1340.000, 1340.000],  loss: 47181.367188, mae: 1280.615234, mean_q: 6.080112\n",
      "wrong_move\n",
      "   2263/500000: episode: 2248, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1615.000 [1615.000, 1615.000],  loss: 27255.953125, mae: 1280.922974, mean_q: 6.066354\n",
      "wrong_move\n",
      "   2264/500000: episode: 2249, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1294.000 [1294.000, 1294.000],  loss: 35245.312500, mae: 1281.571533, mean_q: 6.157905\n",
      "wrong_move\n",
      "   2265/500000: episode: 2250, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1615.000 [1615.000, 1615.000],  loss: 420566.031250, mae: 1282.667236, mean_q: 6.131610\n",
      "wrong_move\n",
      "   2266/500000: episode: 2251, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1615.000 [1615.000, 1615.000],  loss: 226694.000000, mae: 1284.998291, mean_q: 6.292712\n",
      "wrong_move\n",
      "   2267/500000: episode: 2252, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 956.000 [956.000, 956.000],  loss: 310388.312500, mae: 1285.006836, mean_q: 6.029534\n",
      "wrong_move\n",
      "   2268/500000: episode: 2253, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 16.000 [16.000, 16.000],  loss: 101713.046875, mae: 1287.340698, mean_q: 6.128538\n",
      "wrong_move\n",
      "   2269/500000: episode: 2254, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1063.000 [1063.000, 1063.000],  loss: 1389316.500000, mae: 1289.839722, mean_q: 6.082051\n",
      "wrong_move\n",
      "   2270/500000: episode: 2255, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1615.000 [1615.000, 1615.000],  loss: 185255.640625, mae: 1290.667725, mean_q: 6.072949\n",
      "wrong_move\n",
      "   2271/500000: episode: 2256, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1950.000 [1950.000, 1950.000],  loss: 423547.187500, mae: 1291.275024, mean_q: 5.980186\n",
      "wrong_move\n",
      "   2272/500000: episode: 2257, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1698.000 [1698.000, 1698.000],  loss: 653745.000000, mae: 1290.286743, mean_q: 6.050978\n",
      "wrong_move\n",
      "   2273/500000: episode: 2258, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1698.000 [1698.000, 1698.000],  loss: 33070.421875, mae: 1290.280273, mean_q: 6.066443\n",
      "wrong_move\n",
      "   2274/500000: episode: 2259, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1698.000 [1698.000, 1698.000],  loss: 69840.375000, mae: 1289.931763, mean_q: 6.088309\n",
      "wrong_move\n",
      "   2275/500000: episode: 2260, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1698.000 [1698.000, 1698.000],  loss: 21974.884766, mae: 1290.861206, mean_q: 6.019694\n",
      "wrong_move\n",
      "   2276/500000: episode: 2261, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 755.000 [755.000, 755.000],  loss: 367756.062500, mae: 1291.850342, mean_q: 5.992331\n",
      "wrong_move\n",
      "   2277/500000: episode: 2262, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 627.000 [627.000, 627.000],  loss: 472356.218750, mae: 1292.530273, mean_q: 5.924878\n",
      "wrong_move\n",
      "   2278/500000: episode: 2263, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2463.000 [2463.000, 2463.000],  loss: 22139.000000, mae: 1292.442017, mean_q: 5.931827\n",
      "wrong_move\n",
      "   2279/500000: episode: 2264, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 114.000 [114.000, 114.000],  loss: 202082.656250, mae: 1293.737305, mean_q: 5.970280\n",
      "wrong_move\n",
      "   2280/500000: episode: 2265, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3100.000 [3100.000, 3100.000],  loss: 19004.392578, mae: 1294.558716, mean_q: 5.905709\n",
      "wrong_move\n",
      "   2281/500000: episode: 2266, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3693.000 [3693.000, 3693.000],  loss: 378801.656250, mae: 1295.787109, mean_q: 5.887743\n",
      "wrong_move\n",
      "   2282/500000: episode: 2267, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 627.000 [627.000, 627.000],  loss: 423150.187500, mae: 1294.690430, mean_q: 5.832647\n",
      "wrong_move\n",
      "   2283/500000: episode: 2268, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1585.000 [1585.000, 1585.000],  loss: 203656.531250, mae: 1296.310547, mean_q: 6.009518\n",
      "wrong_move\n",
      "   2284/500000: episode: 2269, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1126.000 [1126.000, 1126.000],  loss: 50222.437500, mae: 1295.536865, mean_q: 5.941451\n",
      "wrong_move\n",
      "   2285/500000: episode: 2270, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1126.000 [1126.000, 1126.000],  loss: 115973.203125, mae: 1297.898804, mean_q: 5.873048\n",
      "wrong_move\n",
      "   2286/500000: episode: 2271, duration: 0.065s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3007.000 [3007.000, 3007.000],  loss: 32958.347656, mae: 1302.769287, mean_q: 5.951800\n",
      "wrong_move\n",
      "   2287/500000: episode: 2272, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1126.000 [1126.000, 1126.000],  loss: 325554.156250, mae: 1305.336182, mean_q: 5.937009\n",
      "wrong_move\n",
      "   2288/500000: episode: 2273, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1126.000 [1126.000, 1126.000],  loss: 438423.625000, mae: 1304.124023, mean_q: 5.879761\n",
      "wrong_move\n",
      "   2289/500000: episode: 2274, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1126.000 [1126.000, 1126.000],  loss: 60971.046875, mae: 1303.254883, mean_q: 5.894320\n",
      "wrong_move\n",
      "   2290/500000: episode: 2275, duration: 0.118s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 627.000 [627.000, 627.000],  loss: 115887.664062, mae: 1303.646973, mean_q: 5.810510\n",
      "wrong_move\n",
      "   2291/500000: episode: 2276, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1126.000 [1126.000, 1126.000],  loss: 442392.187500, mae: 1304.575195, mean_q: 5.775725\n",
      "wrong_move\n",
      "   2292/500000: episode: 2277, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3242.000 [3242.000, 3242.000],  loss: 409770.343750, mae: 1307.403076, mean_q: 5.948822\n",
      "wrong_move\n",
      "   2293/500000: episode: 2278, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1831.000 [1831.000, 1831.000],  loss: 77481.710938, mae: 1310.212891, mean_q: 6.018507\n",
      "wrong_move\n",
      "   2294/500000: episode: 2279, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1132.000 [1132.000, 1132.000],  loss: 308344.562500, mae: 1309.513672, mean_q: 5.775753\n",
      "wrong_move\n",
      "   2295/500000: episode: 2280, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2176.000 [2176.000, 2176.000],  loss: 42740.601562, mae: 1308.483154, mean_q: 5.816610\n",
      "wrong_move\n",
      "   2296/500000: episode: 2281, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3684.000 [3684.000, 3684.000],  loss: 408768.187500, mae: 1307.775635, mean_q: 5.808834\n",
      "wrong_move\n",
      "   2297/500000: episode: 2282, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 709.000 [709.000, 709.000],  loss: 123595.015625, mae: 1306.613770, mean_q: 5.813584\n",
      "wrong_move\n",
      "   2298/500000: episode: 2283, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 943.000 [943.000, 943.000],  loss: 1061451.000000, mae: 1307.784424, mean_q: 5.819686\n",
      "wrong_move\n",
      "   2299/500000: episode: 2284, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 986.000 [986.000, 986.000],  loss: 468969.000000, mae: 1304.277588, mean_q: 5.883244\n",
      "wrong_move\n",
      "   2300/500000: episode: 2285, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 986.000 [986.000, 986.000],  loss: 319497.937500, mae: 1303.345825, mean_q: 5.940134\n",
      "wrong_move\n",
      "   2301/500000: episode: 2286, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 943.000 [943.000, 943.000],  loss: 44481.109375, mae: 1304.014038, mean_q: 5.914224\n",
      "wrong_move\n",
      "   2302/500000: episode: 2287, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 65.000 [65.000, 65.000],  loss: 77495.468750, mae: 1303.637695, mean_q: 5.798997\n",
      "wrong_move\n",
      "   2303/500000: episode: 2288, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2211.000 [2211.000, 2211.000],  loss: 92433.359375, mae: 1303.947998, mean_q: 5.913295\n",
      "wrong_move\n",
      "   2304/500000: episode: 2289, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 986.000 [986.000, 986.000],  loss: 554993.437500, mae: 1306.390747, mean_q: 5.781919\n",
      "wrong_move\n",
      "   2305/500000: episode: 2290, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1905.000 [1905.000, 1905.000],  loss: 450257.312500, mae: 1308.725830, mean_q: 5.831563\n",
      "wrong_move\n",
      "   2306/500000: episode: 2291, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4028.000 [4028.000, 4028.000],  loss: 26093.925781, mae: 1311.839355, mean_q: 5.896106\n",
      "wrong_move\n",
      "   2307/500000: episode: 2292, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3730.000 [3730.000, 3730.000],  loss: 62601.835938, mae: 1314.224243, mean_q: 5.901463\n",
      "wrong_move\n",
      "   2308/500000: episode: 2293, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1898.000 [1898.000, 1898.000],  loss: 58103.179688, mae: 1317.571045, mean_q: 5.827299\n",
      "wrong_move\n",
      "   2309/500000: episode: 2294, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1850.000 [1850.000, 1850.000],  loss: 246189.875000, mae: 1320.246460, mean_q: 5.782272\n",
      "wrong_move\n",
      "   2310/500000: episode: 2295, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3684.000 [3684.000, 3684.000],  loss: 63163.296875, mae: 1324.528442, mean_q: 5.755133\n",
      "wrong_move\n",
      "   2311/500000: episode: 2296, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1870.000 [1870.000, 1870.000],  loss: 99395.406250, mae: 1326.345947, mean_q: 5.862171\n",
      "wrong_move\n",
      "   2312/500000: episode: 2297, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 395.000 [395.000, 395.000],  loss: 207546.046875, mae: 1327.351807, mean_q: 5.918324\n",
      "wrong_move\n",
      "   2313/500000: episode: 2298, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1271.000 [1271.000, 1271.000],  loss: 364210.875000, mae: 1328.141357, mean_q: 5.853140\n",
      "wrong_move\n",
      "   2314/500000: episode: 2299, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 795.000 [795.000, 795.000],  loss: 353737.062500, mae: 1325.830933, mean_q: 5.835911\n",
      "wrong_move\n",
      "   2315/500000: episode: 2300, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3684.000 [3684.000, 3684.000],  loss: 465926.125000, mae: 1323.111206, mean_q: 5.855899\n",
      "wrong_move\n",
      "   2316/500000: episode: 2301, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2400.000 [2400.000, 2400.000],  loss: 94019.726562, mae: 1320.638428, mean_q: 5.768034\n",
      "wrong_move\n",
      "   2317/500000: episode: 2302, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1630.000 [1630.000, 1630.000],  loss: 374959.531250, mae: 1319.462769, mean_q: 11.438520\n",
      "wrong_move\n",
      "   2318/500000: episode: 2303, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1795.000 [1795.000, 1795.000],  loss: 35279.378906, mae: 1321.236328, mean_q: 5.799934\n",
      "wrong_move\n",
      "   2319/500000: episode: 2304, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2031.000 [2031.000, 2031.000],  loss: 718425.125000, mae: 1324.942261, mean_q: 5.753163\n",
      "wrong_move\n",
      "   2320/500000: episode: 2305, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2031.000 [2031.000, 2031.000],  loss: 560579.750000, mae: 1326.151367, mean_q: 5.709536\n",
      "wrong_move\n",
      "   2321/500000: episode: 2306, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 349.000 [349.000, 349.000],  loss: 214195.765625, mae: 1326.883057, mean_q: 5.839846\n",
      "wrong_move\n",
      "   2322/500000: episode: 2307, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2031.000 [2031.000, 2031.000],  loss: 785089.000000, mae: 1330.915894, mean_q: 5.760290\n",
      "wrong_move\n",
      "   2323/500000: episode: 2308, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1174.000 [1174.000, 1174.000],  loss: 584359.250000, mae: 1333.191040, mean_q: 5.757194\n",
      "wrong_move\n",
      "   2324/500000: episode: 2309, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 386.000 [386.000, 386.000],  loss: 54815.078125, mae: 1334.874634, mean_q: 5.784804\n",
      "wrong_move\n",
      "   2325/500000: episode: 2310, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2295.000 [2295.000, 2295.000],  loss: 641715.875000, mae: 1335.530396, mean_q: 5.708928\n",
      "wrong_move\n",
      "   2326/500000: episode: 2311, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2943.000 [2943.000, 2943.000],  loss: 14424.340820, mae: 1336.575195, mean_q: 5.915039\n",
      "wrong_move\n",
      "   2327/500000: episode: 2312, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 795.000 [795.000, 795.000],  loss: 86126.515625, mae: 1336.735352, mean_q: 5.842915\n",
      "wrong_move\n",
      "   2328/500000: episode: 2313, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 565.000 [565.000, 565.000],  loss: 432859.312500, mae: 1334.566162, mean_q: 5.826638\n",
      "wrong_move\n",
      "   2329/500000: episode: 2314, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3068.000 [3068.000, 3068.000],  loss: 88102.601562, mae: 1330.378540, mean_q: 5.783287\n",
      "wrong_move\n",
      "   2330/500000: episode: 2315, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1630.000 [1630.000, 1630.000],  loss: 31625.220703, mae: 1325.795166, mean_q: 5.754100\n",
      "wrong_move\n",
      "   2331/500000: episode: 2316, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3157.000 [3157.000, 3157.000],  loss: 29741.972656, mae: 1322.750000, mean_q: 5.902300\n",
      "wrong_move\n",
      "   2332/500000: episode: 2317, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3539.000 [3539.000, 3539.000],  loss: 144972.390625, mae: 1321.028320, mean_q: 5.742859\n",
      "wrong_move\n",
      "   2333/500000: episode: 2318, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2031.000 [2031.000, 2031.000],  loss: 33576.910156, mae: 1319.676636, mean_q: 5.819228\n",
      "wrong_move\n",
      "   2334/500000: episode: 2319, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3694.000 [3694.000, 3694.000],  loss: 162919.078125, mae: 1321.213867, mean_q: 5.695961\n",
      "wrong_move\n",
      "   2335/500000: episode: 2320, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 565.000 [565.000, 565.000],  loss: 38771.750000, mae: 1324.687500, mean_q: 5.836773\n",
      "wrong_move\n",
      "   2336/500000: episode: 2321, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3715.000 [3715.000, 3715.000],  loss: 188434.609375, mae: 1330.214722, mean_q: 5.766728\n",
      "wrong_move\n",
      "   2337/500000: episode: 2322, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 799.000 [799.000, 799.000],  loss: 52481.664062, mae: 1337.393921, mean_q: 5.762602\n",
      "wrong_move\n",
      "   2338/500000: episode: 2323, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1473.000 [1473.000, 1473.000],  loss: 67567.312500, mae: 1345.428223, mean_q: 5.813433\n",
      "wrong_move\n",
      "   2339/500000: episode: 2324, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3715.000 [3715.000, 3715.000],  loss: 37251.988281, mae: 1350.858887, mean_q: 5.749992\n",
      "wrong_move\n",
      "   2340/500000: episode: 2325, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2553.000 [2553.000, 2553.000],  loss: 11977.496094, mae: 1354.269287, mean_q: 5.903578\n",
      "wrong_move\n",
      "   2341/500000: episode: 2326, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 565.000 [565.000, 565.000],  loss: 438451.906250, mae: 1354.833984, mean_q: 5.756949\n",
      "wrong_move\n",
      "   2342/500000: episode: 2327, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1414.000 [1414.000, 1414.000],  loss: 26798.222656, mae: 1352.835327, mean_q: 5.937167\n",
      "wrong_move\n",
      "   2343/500000: episode: 2328, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 453.000 [453.000, 453.000],  loss: 513856.093750, mae: 1348.716309, mean_q: 5.882113\n",
      "wrong_move\n",
      "   2344/500000: episode: 2329, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 565.000 [565.000, 565.000],  loss: 63398.335938, mae: 1342.932739, mean_q: 5.756813\n",
      "wrong_move\n",
      "   2345/500000: episode: 2330, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 565.000 [565.000, 565.000],  loss: 18886.314453, mae: 1338.715210, mean_q: 5.759664\n",
      "wrong_move\n",
      "   2346/500000: episode: 2331, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3732.000 [3732.000, 3732.000],  loss: 21007.753906, mae: 1336.650879, mean_q: 5.700829\n",
      "wrong_move\n",
      "   2347/500000: episode: 2332, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3338.000 [3338.000, 3338.000],  loss: 241050.562500, mae: 1334.399902, mean_q: 5.780024\n",
      "wrong_move\n",
      "   2348/500000: episode: 2333, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3081.000 [3081.000, 3081.000],  loss: 483189.875000, mae: 1332.040649, mean_q: 5.582934\n",
      "wrong_move\n",
      "   2349/500000: episode: 2334, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3068.000 [3068.000, 3068.000],  loss: 340787.406250, mae: 1327.368286, mean_q: 5.824644\n",
      "wrong_move\n",
      "   2350/500000: episode: 2335, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 487.000 [487.000, 487.000],  loss: 760091.000000, mae: 1322.135864, mean_q: 5.679763\n",
      "wrong_move\n",
      "   2351/500000: episode: 2336, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2553.000 [2553.000, 2553.000],  loss: 39520.519531, mae: 1319.779175, mean_q: 5.706315\n",
      "wrong_move\n",
      "   2352/500000: episode: 2337, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1582.000 [1582.000, 1582.000],  loss: 88299.250000, mae: 1322.599243, mean_q: 5.708736\n",
      "wrong_move\n",
      "   2353/500000: episode: 2338, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3791.000 [3791.000, 3791.000],  loss: 27336.601562, mae: 1330.429932, mean_q: 5.634685\n",
      "wrong_move\n",
      "   2354/500000: episode: 2339, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 487.000 [487.000, 487.000],  loss: 36278.605469, mae: 1340.226318, mean_q: 5.788647\n",
      "wrong_move\n",
      "   2355/500000: episode: 2340, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 929.000 [929.000, 929.000],  loss: 16928.892578, mae: 1349.896484, mean_q: 5.617565\n",
      "wrong_move\n",
      "   2356/500000: episode: 2341, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1161.000 [1161.000, 1161.000],  loss: 33462.890625, mae: 1358.120361, mean_q: 5.757078\n",
      "wrong_move\n",
      "   2357/500000: episode: 2342, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2501.000 [2501.000, 2501.000],  loss: 18142.066406, mae: 1361.924561, mean_q: 5.806643\n",
      "wrong_move\n",
      "   2358/500000: episode: 2343, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2347.000 [2347.000, 2347.000],  loss: 44900.585938, mae: 1362.487549, mean_q: 5.750476\n",
      "wrong_move\n",
      "   2359/500000: episode: 2344, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1620.000 [1620.000, 1620.000],  loss: 862955.250000, mae: 1360.868652, mean_q: 5.788731\n",
      "wrong_move\n",
      "   2360/500000: episode: 2345, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2621.000 [2621.000, 2621.000],  loss: 18080.130859, mae: 1358.138184, mean_q: 5.719704\n",
      "wrong_move\n",
      "   2361/500000: episode: 2346, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1662.000 [1662.000, 1662.000],  loss: 305927.687500, mae: 1357.670166, mean_q: 5.766508\n",
      "wrong_move\n",
      "   2362/500000: episode: 2347, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2553.000 [2553.000, 2553.000],  loss: 291890.250000, mae: 1355.878174, mean_q: 5.745032\n",
      "wrong_move\n",
      "   2363/500000: episode: 2348, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1662.000 [1662.000, 1662.000],  loss: 25731.355469, mae: 1352.100342, mean_q: 5.639582\n",
      "wrong_move\n",
      "   2364/500000: episode: 2349, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2031.000 [2031.000, 2031.000],  loss: 39740.570312, mae: 1349.634155, mean_q: 5.824588\n",
      "wrong_move\n",
      "   2365/500000: episode: 2350, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3804.000 [3804.000, 3804.000],  loss: 40640.128906, mae: 1345.625244, mean_q: 5.682711\n",
      "wrong_move\n",
      "   2366/500000: episode: 2351, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 182.000 [182.000, 182.000],  loss: 43668.421875, mae: 1342.100586, mean_q: 5.768782\n",
      "wrong_move\n",
      "   2367/500000: episode: 2352, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 182.000 [182.000, 182.000],  loss: 272905.812500, mae: 1341.452515, mean_q: 5.696406\n",
      "wrong_move\n",
      "   2368/500000: episode: 2353, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3950.000 [3950.000, 3950.000],  loss: 26797.296875, mae: 1343.482666, mean_q: 5.563708\n",
      "wrong_move\n",
      "   2369/500000: episode: 2354, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3644.000 [3644.000, 3644.000],  loss: 418947.968750, mae: 1348.479126, mean_q: 5.704489\n",
      "wrong_move\n",
      "   2370/500000: episode: 2355, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2376.000 [2376.000, 2376.000],  loss: 22566.736328, mae: 1352.675781, mean_q: 5.694805\n",
      "wrong_move\n",
      "   2371/500000: episode: 2356, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 290.000 [290.000, 290.000],  loss: 68787.953125, mae: 1356.292725, mean_q: 5.604607\n",
      "wrong_move\n",
      "   2372/500000: episode: 2357, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 77.000 [77.000, 77.000],  loss: 428933.593750, mae: 1359.671143, mean_q: 5.624670\n",
      "wrong_move\n",
      "   2373/500000: episode: 2358, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2993.000 [2993.000, 2993.000],  loss: 43691.246094, mae: 1360.026123, mean_q: 5.652631\n",
      "wrong_move\n",
      "   2374/500000: episode: 2359, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1534.000 [1534.000, 1534.000],  loss: 92144.664062, mae: 1359.500122, mean_q: 5.625078\n",
      "wrong_move\n",
      "   2375/500000: episode: 2360, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3948.000 [3948.000, 3948.000],  loss: 26739.849609, mae: 1357.819946, mean_q: 5.657325\n",
      "wrong_move\n",
      "   2376/500000: episode: 2361, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2779.000 [2779.000, 2779.000],  loss: 30453.175781, mae: 1358.135376, mean_q: 5.683842\n",
      "wrong_move\n",
      "   2377/500000: episode: 2362, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1662.000 [1662.000, 1662.000],  loss: 422775.312500, mae: 1358.873047, mean_q: 5.579202\n",
      "wrong_move\n",
      "   2378/500000: episode: 2363, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 891.000 [891.000, 891.000],  loss: 456430.781250, mae: 1361.679321, mean_q: 5.706284\n",
      "wrong_move\n",
      "   2379/500000: episode: 2364, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3948.000 [3948.000, 3948.000],  loss: 38216.667969, mae: 1366.309326, mean_q: 5.658413\n",
      "wrong_move\n",
      "   2380/500000: episode: 2365, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3948.000 [3948.000, 3948.000],  loss: 11373.902344, mae: 1370.548584, mean_q: 5.598678\n",
      "wrong_move\n",
      "   2381/500000: episode: 2366, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3948.000 [3948.000, 3948.000],  loss: 497198.500000, mae: 1374.230957, mean_q: 5.651002\n",
      "wrong_move\n",
      "   2382/500000: episode: 2367, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2472.000 [2472.000, 2472.000],  loss: 78407.875000, mae: 1373.929688, mean_q: 5.641130\n",
      "wrong_move\n",
      "   2383/500000: episode: 2368, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3465.000 [3465.000, 3465.000],  loss: 451943.031250, mae: 1370.489014, mean_q: 5.589862\n",
      "wrong_move\n",
      "   2384/500000: episode: 2369, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3791.000 [3791.000, 3791.000],  loss: 32480.921875, mae: 1368.060547, mean_q: 5.521700\n",
      "wrong_move\n",
      "   2385/500000: episode: 2370, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2436.000 [2436.000, 2436.000],  loss: 416939.593750, mae: 1366.798218, mean_q: 5.490697\n",
      "wrong_move\n",
      "   2386/500000: episode: 2371, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 750.000 [750.000, 750.000],  loss: 27303.945312, mae: 1367.308350, mean_q: 5.584900\n",
      "wrong_move\n",
      "   2387/500000: episode: 2372, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2214.000 [2214.000, 2214.000],  loss: 55154.746094, mae: 1367.216797, mean_q: 5.501691\n",
      "wrong_move\n",
      "   2388/500000: episode: 2373, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2620.000 [2620.000, 2620.000],  loss: 395727.125000, mae: 1366.651855, mean_q: 5.450951\n",
      "wrong_move\n",
      "   2389/500000: episode: 2374, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2765.000 [2765.000, 2765.000],  loss: 231032.062500, mae: 1365.099365, mean_q: 5.629265\n",
      "wrong_move\n",
      "   2390/500000: episode: 2375, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 395.000 [395.000, 395.000],  loss: 25929.513672, mae: 1366.333984, mean_q: 5.505499\n",
      "wrong_move\n",
      "   2391/500000: episode: 2376, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3625.000 [3625.000, 3625.000],  loss: 45224.316406, mae: 1369.181396, mean_q: 5.488913\n",
      "wrong_move\n",
      "   2392/500000: episode: 2377, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3147.000 [3147.000, 3147.000],  loss: 65309.437500, mae: 1370.064453, mean_q: 5.481615\n",
      "wrong_move\n",
      "   2393/500000: episode: 2378, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 739.000 [739.000, 739.000],  loss: 78199.734375, mae: 1372.869629, mean_q: 5.488798\n",
      "wrong_move\n",
      "   2394/500000: episode: 2379, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 740.000 [740.000, 740.000],  loss: 38247.695312, mae: 1375.114014, mean_q: 5.570568\n",
      "wrong_move\n",
      "   2395/500000: episode: 2380, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1487.000 [1487.000, 1487.000],  loss: 61991.050781, mae: 1375.924316, mean_q: 5.490630\n",
      "wrong_move\n",
      "   2396/500000: episode: 2381, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2034.000 [2034.000, 2034.000],  loss: 408638.562500, mae: 1378.737549, mean_q: 5.432124\n",
      "wrong_move\n",
      "   2397/500000: episode: 2382, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1573.000 [1573.000, 1573.000],  loss: 41774.335938, mae: 1380.497070, mean_q: 5.482390\n",
      "wrong_move\n",
      "   2398/500000: episode: 2383, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 395.000 [395.000, 395.000],  loss: 411472.656250, mae: 1381.221313, mean_q: 5.505021\n",
      "wrong_move\n",
      "   2399/500000: episode: 2384, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2041.000 [2041.000, 2041.000],  loss: 18169.382812, mae: 1382.535767, mean_q: 5.461857\n",
      "wrong_move\n",
      "   2400/500000: episode: 2385, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 290.000 [290.000, 290.000],  loss: 26706.140625, mae: 1383.242920, mean_q: 5.461215\n",
      "wrong_move\n",
      "   2401/500000: episode: 2386, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 380.000 [380.000, 380.000],  loss: 1124798.375000, mae: 1382.352295, mean_q: 5.360666\n",
      "wrong_move\n",
      "   2402/500000: episode: 2387, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1099.000 [1099.000, 1099.000],  loss: 19277.457031, mae: 1379.971436, mean_q: 5.445324\n",
      "wrong_move\n",
      "   2403/500000: episode: 2388, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2347.000 [2347.000, 2347.000],  loss: 39161.617188, mae: 1376.063232, mean_q: 5.587928\n",
      "wrong_move\n",
      "   2404/500000: episode: 2389, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2837.000 [2837.000, 2837.000],  loss: 162246.687500, mae: 1372.574463, mean_q: 5.396340\n",
      "wrong_move\n",
      "   2405/500000: episode: 2390, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2837.000 [2837.000, 2837.000],  loss: 33227.851562, mae: 1370.024414, mean_q: 5.345750\n",
      "wrong_move\n",
      "   2406/500000: episode: 2391, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2618.000 [2618.000, 2618.000],  loss: 461245.843750, mae: 1370.732422, mean_q: 5.350806\n",
      "wrong_move\n",
      "   2407/500000: episode: 2392, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3536.000 [3536.000, 3536.000],  loss: 17646.984375, mae: 1373.539185, mean_q: 5.336834\n",
      "wrong_move\n",
      "   2408/500000: episode: 2393, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 578.000 [578.000, 578.000],  loss: 32082.146484, mae: 1378.049072, mean_q: 5.442114\n",
      "wrong_move\n",
      "   2409/500000: episode: 2394, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2837.000 [2837.000, 2837.000],  loss: 404361.500000, mae: 1382.724365, mean_q: 5.410444\n",
      "wrong_move\n",
      "   2410/500000: episode: 2395, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3976.000 [3976.000, 3976.000],  loss: 74042.835938, mae: 1384.615479, mean_q: 5.280036\n",
      "wrong_move\n",
      "   2412/500000: episode: 2396, duration: 0.182s, episode steps:   2, steps per second:  11, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 714.500 [380.000, 1049.000],  loss: 214067.218750, mae: 1388.505615, mean_q: 5.432244\n",
      "wrong_move\n",
      "   2413/500000: episode: 2397, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2618.000 [2618.000, 2618.000],  loss: 25030.113281, mae: 1392.005371, mean_q: 5.452956\n",
      "wrong_move\n",
      "   2414/500000: episode: 2398, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 926.000 [926.000, 926.000],  loss: 67224.070312, mae: 1394.804932, mean_q: 5.359823\n",
      "wrong_move\n",
      "   2415/500000: episode: 2399, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 178.000 [178.000, 178.000],  loss: 53765.890625, mae: 1399.005859, mean_q: 5.339053\n",
      "wrong_move\n",
      "   2416/500000: episode: 2400, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 855.000 [855.000, 855.000],  loss: 32424.718750, mae: 1400.587036, mean_q: 5.357672\n",
      "wrong_move\n",
      "   2417/500000: episode: 2401, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 979.000 [979.000, 979.000],  loss: 28372.019531, mae: 1402.935913, mean_q: 5.449028\n",
      "wrong_move\n",
      "   2418/500000: episode: 2402, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2655.000 [2655.000, 2655.000],  loss: 18364.042969, mae: 1405.206665, mean_q: 5.390462\n",
      "wrong_move\n",
      "   2419/500000: episode: 2403, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 872.000 [872.000, 872.000],  loss: 420312.687500, mae: 1406.378784, mean_q: 5.511082\n",
      "wrong_move\n",
      "   2420/500000: episode: 2404, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3964.000 [3964.000, 3964.000],  loss: 548819.312500, mae: 1408.946411, mean_q: 5.313353\n",
      "wrong_move\n",
      "   2421/500000: episode: 2405, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 979.000 [979.000, 979.000],  loss: 288835.875000, mae: 1409.506348, mean_q: 5.375830\n",
      "wrong_move\n",
      "   2422/500000: episode: 2406, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 64.000 [64.000, 64.000],  loss: 859093.062500, mae: 1407.805542, mean_q: 5.418213\n",
      "wrong_move\n",
      "   2423/500000: episode: 2407, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4089.000 [4089.000, 4089.000],  loss: 424604.312500, mae: 1405.592285, mean_q: 5.314370\n",
      "wrong_move\n",
      "   2424/500000: episode: 2408, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 855.000 [855.000, 855.000],  loss: 88898.968750, mae: 1400.595093, mean_q: 5.306077\n",
      "wrong_move\n",
      "   2425/500000: episode: 2409, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 93.000 [93.000, 93.000],  loss: 17732.509766, mae: 1396.550293, mean_q: 5.300157\n",
      "wrong_move\n",
      "   2426/500000: episode: 2410, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 979.000 [979.000, 979.000],  loss: 104429.765625, mae: 1396.130493, mean_q: 5.300745\n",
      "wrong_move\n",
      "   2427/500000: episode: 2411, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1533.000 [1533.000, 1533.000],  loss: 18493.650391, mae: 1398.679565, mean_q: 5.288302\n",
      "wrong_move\n",
      "   2428/500000: episode: 2412, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3976.000 [3976.000, 3976.000],  loss: 553955.625000, mae: 1401.641357, mean_q: 5.230167\n",
      "wrong_move\n",
      "   2429/500000: episode: 2413, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3924.000 [3924.000, 3924.000],  loss: 289499.375000, mae: 1401.638916, mean_q: 5.253563\n",
      "wrong_move\n",
      "   2430/500000: episode: 2414, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: 125473.171875, mae: 1403.273193, mean_q: 5.201797\n",
      "wrong_move\n",
      "   2431/500000: episode: 2415, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1483.000 [1483.000, 1483.000],  loss: 340663.218750, mae: 1404.956055, mean_q: 5.132878\n",
      "wrong_move\n",
      "   2432/500000: episode: 2416, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4089.000 [4089.000, 4089.000],  loss: 62072.277344, mae: 1408.830078, mean_q: 5.284655\n",
      "wrong_move\n",
      "   2433/500000: episode: 2417, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4089.000 [4089.000, 4089.000],  loss: 442370.750000, mae: 1412.939331, mean_q: 5.295065\n",
      "wrong_move\n",
      "   2434/500000: episode: 2418, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1533.000 [1533.000, 1533.000],  loss: 288763.125000, mae: 1418.499634, mean_q: 5.319514\n",
      "wrong_move\n",
      "   2435/500000: episode: 2419, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1317.000 [1317.000, 1317.000],  loss: 30033.771484, mae: 1419.930786, mean_q: 5.231778\n",
      "wrong_move\n",
      "   2436/500000: episode: 2420, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 855.000 [855.000, 855.000],  loss: 43908.710938, mae: 1420.267090, mean_q: 5.399221\n",
      "wrong_move\n",
      "   2437/500000: episode: 2421, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3025.000 [3025.000, 3025.000],  loss: 36347.402344, mae: 1421.157227, mean_q: 5.305373\n",
      "wrong_move\n",
      "   2438/500000: episode: 2422, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 855.000 [855.000, 855.000],  loss: 53770.750000, mae: 1422.244141, mean_q: 5.295941\n",
      "wrong_move\n",
      "   2439/500000: episode: 2423, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1368.000 [1368.000, 1368.000],  loss: 28048.832031, mae: 1422.676147, mean_q: 5.224465\n",
      "wrong_move\n",
      "   2440/500000: episode: 2424, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1696.000 [1696.000, 1696.000],  loss: 32815.007812, mae: 1423.756104, mean_q: 5.248310\n",
      "wrong_move\n",
      "   2441/500000: episode: 2425, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 855.000 [855.000, 855.000],  loss: 671203.250000, mae: 1423.838867, mean_q: 5.129942\n",
      "wrong_move\n",
      "   2442/500000: episode: 2426, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1056.000 [1056.000, 1056.000],  loss: 52266.921875, mae: 1419.796021, mean_q: 5.324494\n",
      "wrong_move\n",
      "   2443/500000: episode: 2427, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1696.000 [1696.000, 1696.000],  loss: 18109.123047, mae: 1417.340088, mean_q: 5.207475\n",
      "wrong_move\n",
      "   2444/500000: episode: 2428, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1317.000 [1317.000, 1317.000],  loss: 24894.031250, mae: 1414.225708, mean_q: 5.285048\n",
      "wrong_move\n",
      "   2445/500000: episode: 2429, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 91.000 [91.000, 91.000],  loss: 415409.843750, mae: 1410.071289, mean_q: 5.152324\n",
      "wrong_move\n",
      "   2446/500000: episode: 2430, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 891.000 [891.000, 891.000],  loss: 408264.656250, mae: 1406.510864, mean_q: 5.227684\n",
      "wrong_move\n",
      "   2447/500000: episode: 2431, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3025.000 [3025.000, 3025.000],  loss: 63165.578125, mae: 1404.531494, mean_q: 5.214681\n",
      "wrong_move\n",
      "   2448/500000: episode: 2432, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3193.000 [3193.000, 3193.000],  loss: 485068.625000, mae: 1404.773193, mean_q: 5.115898\n",
      "wrong_move\n",
      "   2449/500000: episode: 2433, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1696.000 [1696.000, 1696.000],  loss: 139448.562500, mae: 1404.063354, mean_q: 5.179451\n",
      "wrong_move\n",
      "   2450/500000: episode: 2434, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1696.000 [1696.000, 1696.000],  loss: 100552.039062, mae: 1404.231445, mean_q: 5.116283\n",
      "wrong_move\n",
      "   2451/500000: episode: 2435, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3193.000 [3193.000, 3193.000],  loss: 84991.304688, mae: 1405.643311, mean_q: 5.177262\n",
      "wrong_move\n",
      "   2452/500000: episode: 2436, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3193.000 [3193.000, 3193.000],  loss: 45737.757812, mae: 1408.938965, mean_q: 5.186042\n",
      "wrong_move\n",
      "   2453/500000: episode: 2437, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3006.000 [3006.000, 3006.000],  loss: 102914.609375, mae: 1414.703735, mean_q: 5.239457\n",
      "wrong_move\n",
      "   2454/500000: episode: 2438, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1378.000 [1378.000, 1378.000],  loss: 12318.880859, mae: 1419.118286, mean_q: 5.101559\n",
      "wrong_move\n",
      "   2455/500000: episode: 2439, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1967.000 [1967.000, 1967.000],  loss: 49799.386719, mae: 1423.954102, mean_q: 5.122244\n",
      "wrong_move\n",
      "   2456/500000: episode: 2440, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3315.000 [3315.000, 3315.000],  loss: 404599.000000, mae: 1427.618774, mean_q: 5.145606\n",
      "wrong_move\n",
      "   2457/500000: episode: 2441, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1266.000 [1266.000, 1266.000],  loss: 108449.578125, mae: 1429.849609, mean_q: 5.099568\n",
      "wrong_move\n",
      "   2458/500000: episode: 2442, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1621.000 [1621.000, 1621.000],  loss: 34527.054688, mae: 1430.153564, mean_q: 5.119412\n",
      "wrong_move\n",
      "   2459/500000: episode: 2443, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 993.000 [993.000, 993.000],  loss: 510759.250000, mae: 1430.821533, mean_q: 5.356491\n",
      "wrong_move\n",
      "   2460/500000: episode: 2444, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2487.000 [2487.000, 2487.000],  loss: 446831.343750, mae: 1432.280396, mean_q: 5.205280\n",
      "wrong_move\n",
      "   2461/500000: episode: 2445, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1711.000 [1711.000, 1711.000],  loss: 49654.187500, mae: 1435.451538, mean_q: 5.260870\n",
      "wrong_move\n",
      "   2462/500000: episode: 2446, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2982.000 [2982.000, 2982.000],  loss: 42035.554688, mae: 1437.593018, mean_q: 26.843712\n",
      "wrong_move\n",
      "   2463/500000: episode: 2447, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2982.000 [2982.000, 2982.000],  loss: 111080.765625, mae: 1437.881348, mean_q: 5.189846\n",
      "wrong_move\n",
      "   2464/500000: episode: 2448, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2982.000 [2982.000, 2982.000],  loss: 441699.437500, mae: 1437.172119, mean_q: 5.089909\n",
      "wrong_move\n",
      "   2465/500000: episode: 2449, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 290.000 [290.000, 290.000],  loss: 17909.945312, mae: 1434.511841, mean_q: 5.161428\n",
      "wrong_move\n",
      "   2466/500000: episode: 2450, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2982.000 [2982.000, 2982.000],  loss: 43275.519531, mae: 1432.447754, mean_q: 5.109660\n",
      "wrong_move\n",
      "   2467/500000: episode: 2451, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 447.000 [447.000, 447.000],  loss: 415295.812500, mae: 1430.255249, mean_q: 5.134309\n",
      "wrong_move\n",
      "   2468/500000: episode: 2452, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2982.000 [2982.000, 2982.000],  loss: 54828.988281, mae: 1428.289307, mean_q: 5.082890\n",
      "wrong_move\n",
      "   2469/500000: episode: 2453, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2992.000 [2992.000, 2992.000],  loss: 8375.976562, mae: 1426.920410, mean_q: 4.955486\n",
      "wrong_move\n",
      "   2470/500000: episode: 2454, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2294.000 [2294.000, 2294.000],  loss: 64722.160156, mae: 1427.007568, mean_q: 5.132014\n",
      "wrong_move\n",
      "   2471/500000: episode: 2455, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3025.000 [3025.000, 3025.000],  loss: 217785.281250, mae: 1428.627441, mean_q: 5.106738\n",
      "wrong_move\n",
      "   2472/500000: episode: 2456, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 590.000 [590.000, 590.000],  loss: 34425.101562, mae: 1431.069824, mean_q: 5.146591\n",
      "wrong_move\n",
      "   2473/500000: episode: 2457, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2612.000 [2612.000, 2612.000],  loss: 416172.937500, mae: 1433.153442, mean_q: 5.095754\n",
      "wrong_move\n",
      "   2474/500000: episode: 2458, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3287.000 [3287.000, 3287.000],  loss: 523970.406250, mae: 1434.913086, mean_q: 5.156232\n",
      "wrong_move\n",
      "   2475/500000: episode: 2459, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 590.000 [590.000, 590.000],  loss: 11779.725586, mae: 1438.028076, mean_q: 5.093863\n",
      "wrong_move\n",
      "   2476/500000: episode: 2460, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 590.000 [590.000, 590.000],  loss: 412079.468750, mae: 1441.217529, mean_q: 5.111109\n",
      "wrong_move\n",
      "   2477/500000: episode: 2461, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 306657.375000, mae: 1446.277344, mean_q: 5.222131\n",
      "wrong_move\n",
      "   2478/500000: episode: 2462, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2586.000 [2586.000, 2586.000],  loss: 14853.924805, mae: 1447.852295, mean_q: 5.220704\n",
      "wrong_move\n",
      "   2479/500000: episode: 2463, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3821.000 [3821.000, 3821.000],  loss: 26149.003906, mae: 1447.525269, mean_q: 5.163961\n",
      "wrong_move\n",
      "   2480/500000: episode: 2464, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3821.000 [3821.000, 3821.000],  loss: 46682.054688, mae: 1445.991943, mean_q: 5.146991\n",
      "wrong_move\n",
      "   2481/500000: episode: 2465, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3821.000 [3821.000, 3821.000],  loss: 59134.632812, mae: 1445.717285, mean_q: 5.003769\n",
      "wrong_move\n",
      "   2482/500000: episode: 2466, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3821.000 [3821.000, 3821.000],  loss: 158141.609375, mae: 1444.323486, mean_q: 5.144847\n",
      "wrong_move\n",
      "   2483/500000: episode: 2467, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 319076.437500, mae: 1438.052856, mean_q: 5.080213\n",
      "wrong_move\n",
      "   2484/500000: episode: 2468, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 109164.023438, mae: 1433.552002, mean_q: 4.977576\n",
      "wrong_move\n",
      "   2485/500000: episode: 2469, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1235.000 [1235.000, 1235.000],  loss: 420608.375000, mae: 1431.888428, mean_q: 5.044019\n",
      "wrong_move\n",
      "   2486/500000: episode: 2470, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1426.000 [1426.000, 1426.000],  loss: 401522.406250, mae: 1432.707520, mean_q: 5.019169\n",
      "wrong_move\n",
      "   2487/500000: episode: 2471, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3299.000 [3299.000, 3299.000],  loss: 391844.937500, mae: 1432.076050, mean_q: 5.102112\n",
      "wrong_move\n",
      "   2488/500000: episode: 2472, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2336.000 [2336.000, 2336.000],  loss: 208207.234375, mae: 1436.412354, mean_q: 5.037007\n",
      "wrong_move\n",
      "   2489/500000: episode: 2473, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1426.000 [1426.000, 1426.000],  loss: 114958.843750, mae: 1439.476807, mean_q: 4.953578\n",
      "wrong_move\n",
      "   2490/500000: episode: 2474, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 374.000 [374.000, 374.000],  loss: 76906.984375, mae: 1441.761719, mean_q: 5.002635\n",
      "wrong_move\n",
      "   2491/500000: episode: 2475, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1426.000 [1426.000, 1426.000],  loss: 31870.158203, mae: 1445.269043, mean_q: 5.104430\n",
      "wrong_move\n",
      "   2492/500000: episode: 2476, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1266.000 [1266.000, 1266.000],  loss: 39836.933594, mae: 1448.843506, mean_q: 4.935307\n",
      "wrong_move\n",
      "   2493/500000: episode: 2477, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1426.000 [1426.000, 1426.000],  loss: 24511.546875, mae: 1453.090698, mean_q: 4.951450\n",
      "wrong_move\n",
      "   2495/500000: episode: 2478, duration: 0.157s, episode steps:   2, steps per second:  13, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1998.500 [213.000, 3784.000],  loss: 115090.625000, mae: 1456.382812, mean_q: 5.015055\n",
      "wrong_move\n",
      "   2496/500000: episode: 2479, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4094.000 [4094.000, 4094.000],  loss: 289357.312500, mae: 1458.393677, mean_q: 5.083190\n",
      "wrong_move\n",
      "   2497/500000: episode: 2480, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1426.000 [1426.000, 1426.000],  loss: 23878.042969, mae: 1460.640015, mean_q: 5.131078\n",
      "wrong_move\n",
      "   2498/500000: episode: 2481, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 653.000 [653.000, 653.000],  loss: 324517.750000, mae: 1461.474365, mean_q: 5.163922\n",
      "wrong_move\n",
      "   2499/500000: episode: 2482, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3783.000 [3783.000, 3783.000],  loss: 497833.406250, mae: 1460.381714, mean_q: 5.009863\n",
      "wrong_move\n",
      "   2500/500000: episode: 2483, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 72685.156250, mae: 1456.321533, mean_q: 4.939080\n",
      "wrong_move\n",
      "   2501/500000: episode: 2484, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3606.000 [3606.000, 3606.000],  loss: 80607.968750, mae: 1451.030518, mean_q: 4.997491\n",
      "wrong_move\n",
      "   2502/500000: episode: 2485, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2694.000 [2694.000, 2694.000],  loss: 310055.562500, mae: 1445.801514, mean_q: 4.950280\n",
      "wrong_move\n",
      "   2503/500000: episode: 2486, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2612.000 [2612.000, 2612.000],  loss: 98755.203125, mae: 1440.399170, mean_q: 5.057652\n",
      "wrong_move\n",
      "   2504/500000: episode: 2487, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 965.000 [965.000, 965.000],  loss: 69766.671875, mae: 1439.277954, mean_q: 4.911654\n",
      "wrong_move\n",
      "   2505/500000: episode: 2488, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: 431787.906250, mae: 1443.103882, mean_q: 4.890272\n",
      "wrong_move\n",
      "   2506/500000: episode: 2489, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1491.000 [1491.000, 1491.000],  loss: 428953.437500, mae: 1449.240723, mean_q: 4.931149\n",
      "wrong_move\n",
      "   2507/500000: episode: 2490, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3976.000 [3976.000, 3976.000],  loss: 20187.568359, mae: 1455.155762, mean_q: 4.987165\n",
      "wrong_move\n",
      "   2508/500000: episode: 2491, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2473.000 [2473.000, 2473.000],  loss: 77284.820312, mae: 1460.335327, mean_q: 5.070294\n",
      "wrong_move\n",
      "   2509/500000: episode: 2492, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 965.000 [965.000, 965.000],  loss: 162438.234375, mae: 1465.584839, mean_q: 5.066463\n",
      "wrong_move\n",
      "   2510/500000: episode: 2493, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3856.000 [3856.000, 3856.000],  loss: 32833.894531, mae: 1469.368164, mean_q: 5.100058\n",
      "wrong_move\n",
      "   2511/500000: episode: 2494, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1712.000 [1712.000, 1712.000],  loss: 124860.007812, mae: 1469.134277, mean_q: 5.020908\n",
      "wrong_move\n",
      "   2512/500000: episode: 2495, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1532.000 [1532.000, 1532.000],  loss: 408301.968750, mae: 1464.915283, mean_q: 5.000617\n",
      "wrong_move\n",
      "   2513/500000: episode: 2496, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1712.000 [1712.000, 1712.000],  loss: 793332.312500, mae: 1460.099243, mean_q: 4.985838\n",
      "wrong_move\n",
      "   2514/500000: episode: 2497, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2905.000 [2905.000, 2905.000],  loss: 41442.144531, mae: 1456.707031, mean_q: 4.937563\n",
      "wrong_move\n",
      "   2515/500000: episode: 2498, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4026.000 [4026.000, 4026.000],  loss: 187376.921875, mae: 1455.974854, mean_q: 4.955037\n",
      "wrong_move\n",
      "   2516/500000: episode: 2499, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4074.000 [4074.000, 4074.000],  loss: 23888.009766, mae: 1457.498535, mean_q: 5.049387\n",
      "wrong_move\n",
      "   2517/500000: episode: 2500, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1712.000 [1712.000, 1712.000],  loss: 30481.050781, mae: 1461.609375, mean_q: 5.127727\n",
      "wrong_move\n",
      "   2518/500000: episode: 2501, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 783.000 [783.000, 783.000],  loss: 587469.437500, mae: 1468.252686, mean_q: 5.190371\n",
      "wrong_move\n",
      "   2519/500000: episode: 2502, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1322.000 [1322.000, 1322.000],  loss: 26938.953125, mae: 1472.373291, mean_q: 4.988985\n",
      "wrong_move\n",
      "   2520/500000: episode: 2503, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3856.000 [3856.000, 3856.000],  loss: 47431.781250, mae: 1475.155518, mean_q: 5.167308\n",
      "wrong_move\n",
      "   2521/500000: episode: 2504, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4051.000 [4051.000, 4051.000],  loss: 479452.687500, mae: 1473.436523, mean_q: 5.004317\n",
      "wrong_move\n",
      "   2522/500000: episode: 2505, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1712.000 [1712.000, 1712.000],  loss: 35653.691406, mae: 1472.413818, mean_q: 5.007400\n",
      "wrong_move\n",
      "   2523/500000: episode: 2506, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 294.000 [294.000, 294.000],  loss: 834584.187500, mae: 1470.767700, mean_q: 5.092414\n",
      "wrong_move\n",
      "   2524/500000: episode: 2507, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2694.000 [2694.000, 2694.000],  loss: 324726.625000, mae: 1469.047363, mean_q: 4.973663\n",
      "wrong_move\n",
      "   2525/500000: episode: 2508, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1712.000 [1712.000, 1712.000],  loss: 400813.906250, mae: 1467.453125, mean_q: 5.132582\n",
      "wrong_move\n",
      "   2526/500000: episode: 2509, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3446.000 [3446.000, 3446.000],  loss: 46955.132812, mae: 1467.613037, mean_q: 4.919150\n",
      "wrong_move\n",
      "   2527/500000: episode: 2510, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 607.000 [607.000, 607.000],  loss: 425293.968750, mae: 1468.317505, mean_q: 4.953887\n",
      "wrong_move\n",
      "   2528/500000: episode: 2511, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 801.000 [801.000, 801.000],  loss: 838880.375000, mae: 1468.349609, mean_q: 4.970554\n",
      "wrong_move\n",
      "   2529/500000: episode: 2512, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 801.000 [801.000, 801.000],  loss: 408008.718750, mae: 1467.376465, mean_q: 4.850228\n",
      "wrong_move\n",
      "   2530/500000: episode: 2513, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1387.000 [1387.000, 1387.000],  loss: 30203.015625, mae: 1468.484009, mean_q: 5.047281\n",
      "wrong_move\n",
      "   2531/500000: episode: 2514, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 362.000 [362.000, 362.000],  loss: 241185.984375, mae: 1472.408813, mean_q: 4.867667\n",
      "wrong_move\n",
      "   2532/500000: episode: 2515, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 783.000 [783.000, 783.000],  loss: 50195.902344, mae: 1479.447388, mean_q: 4.971509\n",
      "wrong_move\n",
      "   2533/500000: episode: 2516, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3323.000 [3323.000, 3323.000],  loss: 64395.421875, mae: 1487.646240, mean_q: 4.848292\n",
      "wrong_move\n",
      "   2534/500000: episode: 2517, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1049.000 [1049.000, 1049.000],  loss: 429577.718750, mae: 1496.429199, mean_q: 29.817135\n",
      "wrong_move\n",
      "   2535/500000: episode: 2518, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1613.000 [1613.000, 1613.000],  loss: 34496.386719, mae: 1500.910400, mean_q: 70.456215\n",
      "wrong_move\n",
      "   2536/500000: episode: 2519, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1482.000 [1482.000, 1482.000],  loss: 29658.796875, mae: 1499.744385, mean_q: 117.667877\n",
      "wrong_move\n",
      "   2537/500000: episode: 2520, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3428.000 [3428.000, 3428.000],  loss: 37929.242188, mae: 1495.307129, mean_q: 142.414764\n",
      "wrong_move\n",
      "   2538/500000: episode: 2521, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1049.000 [1049.000, 1049.000],  loss: 25899.556641, mae: 1489.886108, mean_q: 165.711212\n",
      "wrong_move\n",
      "   2539/500000: episode: 2522, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 295.000 [295.000, 295.000],  loss: 39691.246094, mae: 1484.703125, mean_q: 4.840925\n",
      "wrong_move\n",
      "   2540/500000: episode: 2523, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 888.000 [888.000, 888.000],  loss: 24970.453125, mae: 1477.487549, mean_q: 4.751511\n",
      "wrong_move\n",
      "   2541/500000: episode: 2524, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 653.000 [653.000, 653.000],  loss: 251143.921875, mae: 1471.044434, mean_q: 4.822436\n",
      "wrong_move\n",
      "   2542/500000: episode: 2525, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3293.000 [3293.000, 3293.000],  loss: 101501.164062, mae: 1462.279541, mean_q: 4.806276\n",
      "wrong_move\n",
      "   2543/500000: episode: 2526, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2031.000 [2031.000, 2031.000],  loss: 61179.304688, mae: 1459.022949, mean_q: 4.711596\n",
      "wrong_move\n",
      "   2544/500000: episode: 2527, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 295.000 [295.000, 295.000],  loss: 76516.085938, mae: 1459.657959, mean_q: 4.842263\n",
      "wrong_move\n",
      "   2545/500000: episode: 2528, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 295.000 [295.000, 295.000],  loss: 29877.943359, mae: 1463.259644, mean_q: 4.787784\n",
      "wrong_move\n",
      "   2546/500000: episode: 2529, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4092.000 [4092.000, 4092.000],  loss: 475269.937500, mae: 1469.026367, mean_q: 4.736804\n",
      "wrong_move\n",
      "   2547/500000: episode: 2530, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3110.000 [3110.000, 3110.000],  loss: 30828.281250, mae: 1471.071045, mean_q: 4.727134\n",
      "wrong_move\n",
      "   2548/500000: episode: 2531, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2187.000 [2187.000, 2187.000],  loss: 72761.062500, mae: 1475.863647, mean_q: 4.791420\n",
      "wrong_move\n",
      "   2549/500000: episode: 2532, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1930.000 [1930.000, 1930.000],  loss: 7285.108887, mae: 1484.516602, mean_q: 4.774530\n",
      "wrong_move\n",
      "   2550/500000: episode: 2533, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2128.000 [2128.000, 2128.000],  loss: 164259.796875, mae: 1492.264160, mean_q: 4.692332\n",
      "wrong_move\n",
      "   2551/500000: episode: 2534, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2162.000 [2162.000, 2162.000],  loss: 428234.468750, mae: 1499.255005, mean_q: 4.882461\n",
      "wrong_move\n",
      "   2552/500000: episode: 2535, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 653.000 [653.000, 653.000],  loss: 219838.562500, mae: 1501.192383, mean_q: 4.837116\n",
      "wrong_move\n",
      "   2553/500000: episode: 2536, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3603.000 [3603.000, 3603.000],  loss: 61642.757812, mae: 1501.005127, mean_q: 4.878782\n",
      "wrong_move\n",
      "   2554/500000: episode: 2537, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 888.000 [888.000, 888.000],  loss: 57596.597656, mae: 1498.291992, mean_q: 4.886861\n",
      "wrong_move\n",
      "   2555/500000: episode: 2538, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3785.000 [3785.000, 3785.000],  loss: 1010376.500000, mae: 1495.833984, mean_q: 4.692101\n",
      "wrong_move\n",
      "   2556/500000: episode: 2539, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2257.000 [2257.000, 2257.000],  loss: 70337.734375, mae: 1494.162720, mean_q: 4.888348\n",
      "wrong_move\n",
      "   2557/500000: episode: 2540, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 941.000 [941.000, 941.000],  loss: 565870.000000, mae: 1491.882324, mean_q: 4.922120\n",
      "wrong_move\n",
      "   2558/500000: episode: 2541, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3864.000 [3864.000, 3864.000],  loss: 421161.125000, mae: 1483.864380, mean_q: 4.745371\n",
      "wrong_move\n",
      "   2559/500000: episode: 2542, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 796.000 [796.000, 796.000],  loss: 36063.921875, mae: 1478.377563, mean_q: 4.753847\n",
      "wrong_move\n",
      "   2560/500000: episode: 2543, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2257.000 [2257.000, 2257.000],  loss: 367174.250000, mae: 1475.889404, mean_q: 4.832328\n",
      "wrong_move\n",
      "   2561/500000: episode: 2544, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2257.000 [2257.000, 2257.000],  loss: 620996.187500, mae: 1475.137695, mean_q: 4.636634\n",
      "wrong_move\n",
      "   2562/500000: episode: 2545, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2257.000 [2257.000, 2257.000],  loss: 25179.863281, mae: 1478.023193, mean_q: 4.830644\n",
      "wrong_move\n",
      "   2563/500000: episode: 2546, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2257.000 [2257.000, 2257.000],  loss: 39224.554688, mae: 1483.387207, mean_q: 4.717432\n",
      "wrong_move\n",
      "   2564/500000: episode: 2547, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2580.000 [2580.000, 2580.000],  loss: 404798.093750, mae: 1489.956543, mean_q: 4.675125\n",
      "wrong_move\n",
      "   2565/500000: episode: 2548, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2580.000 [2580.000, 2580.000],  loss: 42248.191406, mae: 1494.274902, mean_q: 4.695446\n",
      "wrong_move\n",
      "   2566/500000: episode: 2549, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1987.000 [1987.000, 1987.000],  loss: 45032.746094, mae: 1498.395142, mean_q: 4.835480\n",
      "wrong_move\n",
      "   2567/500000: episode: 2550, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3864.000 [3864.000, 3864.000],  loss: 92864.734375, mae: 1502.406738, mean_q: 4.774393\n",
      "wrong_move\n",
      "   2568/500000: episode: 2551, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 861.000 [861.000, 861.000],  loss: 333329.718750, mae: 1504.483398, mean_q: 4.812050\n",
      "wrong_move\n",
      "   2569/500000: episode: 2552, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2257.000 [2257.000, 2257.000],  loss: 584827.375000, mae: 1506.188965, mean_q: 4.861151\n",
      "wrong_move\n",
      "   2570/500000: episode: 2553, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3933.000 [3933.000, 3933.000],  loss: 36454.664062, mae: 1509.898560, mean_q: 4.832998\n",
      "wrong_move\n",
      "   2571/500000: episode: 2554, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1029.000 [1029.000, 1029.000],  loss: 400332.906250, mae: 1514.550293, mean_q: 4.831137\n",
      "wrong_move\n",
      "   2572/500000: episode: 2555, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1987.000 [1987.000, 1987.000],  loss: 266490.812500, mae: 1517.097290, mean_q: 4.867691\n",
      "wrong_move\n",
      "   2573/500000: episode: 2556, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2257.000 [2257.000, 2257.000],  loss: 353827.500000, mae: 1516.142090, mean_q: 4.821410\n",
      "wrong_move\n",
      "   2574/500000: episode: 2557, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2357.000 [2357.000, 2357.000],  loss: 295612.625000, mae: 1510.691406, mean_q: 4.833721\n",
      "wrong_move\n",
      "   2575/500000: episode: 2558, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2439.000 [2439.000, 2439.000],  loss: 18006.070312, mae: 1501.056641, mean_q: 4.714699\n",
      "wrong_move\n",
      "   2576/500000: episode: 2559, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1930.000 [1930.000, 1930.000],  loss: 414105.687500, mae: 1495.318237, mean_q: 4.774078\n",
      "wrong_move\n",
      "   2577/500000: episode: 2560, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 51.000 [51.000, 51.000],  loss: 205647.406250, mae: 1493.080566, mean_q: 4.869023\n",
      "wrong_move\n",
      "   2578/500000: episode: 2561, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1930.000 [1930.000, 1930.000],  loss: 420620.937500, mae: 1492.787598, mean_q: 5.009321\n",
      "wrong_move\n",
      "   2579/500000: episode: 2562, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2484.000 [2484.000, 2484.000],  loss: 19003.015625, mae: 1493.727905, mean_q: 4.868851\n",
      "wrong_move\n",
      "   2580/500000: episode: 2563, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2484.000 [2484.000, 2484.000],  loss: 25831.695312, mae: 1496.804932, mean_q: 4.813352\n",
      "wrong_move\n",
      "   2581/500000: episode: 2564, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2580.000 [2580.000, 2580.000],  loss: 17414.103516, mae: 1496.587402, mean_q: 4.920701\n",
      "wrong_move\n",
      "   2582/500000: episode: 2565, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 285.000 [285.000, 285.000],  loss: 69414.953125, mae: 1497.938477, mean_q: 4.818466\n",
      "wrong_move\n",
      "   2583/500000: episode: 2566, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2905.000 [2905.000, 2905.000],  loss: 164283.906250, mae: 1499.184082, mean_q: 4.809971\n",
      "wrong_move\n",
      "   2584/500000: episode: 2567, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 141.000 [141.000, 141.000],  loss: 37066.875000, mae: 1501.624878, mean_q: 4.829640\n",
      "wrong_move\n",
      "   2585/500000: episode: 2568, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3434.000 [3434.000, 3434.000],  loss: 100824.796875, mae: 1506.133545, mean_q: 4.819144\n",
      "wrong_move\n",
      "   2586/500000: episode: 2569, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1389.000 [1389.000, 1389.000],  loss: 20505.791016, mae: 1508.156860, mean_q: 4.979953\n",
      "wrong_move\n",
      "   2587/500000: episode: 2570, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 366.000 [366.000, 366.000],  loss: 441926.750000, mae: 1510.296265, mean_q: 4.940376\n",
      "wrong_move\n",
      "   2588/500000: episode: 2571, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3785.000 [3785.000, 3785.000],  loss: 498139.781250, mae: 1514.078735, mean_q: 4.896457\n",
      "wrong_move\n",
      "   2589/500000: episode: 2572, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2580.000 [2580.000, 2580.000],  loss: 88660.507812, mae: 1518.855469, mean_q: 5.003301\n",
      "wrong_move\n",
      "   2590/500000: episode: 2573, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1389.000 [1389.000, 1389.000],  loss: 51619.019531, mae: 1520.474365, mean_q: 4.835928\n",
      "wrong_move\n",
      "   2591/500000: episode: 2574, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1487.000 [1487.000, 1487.000],  loss: 263353.812500, mae: 1519.664795, mean_q: 4.819495\n",
      "wrong_move\n",
      "   2592/500000: episode: 2575, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 888.000 [888.000, 888.000],  loss: 75646.359375, mae: 1515.599487, mean_q: 4.906000\n",
      "wrong_move\n",
      "   2593/500000: episode: 2576, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1180.000 [1180.000, 1180.000],  loss: 223436.859375, mae: 1508.393066, mean_q: 5.057761\n",
      "wrong_move\n",
      "   2594/500000: episode: 2577, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3958.000 [3958.000, 3958.000],  loss: 422038.375000, mae: 1498.721680, mean_q: 4.944047\n",
      "wrong_move\n",
      "   2595/500000: episode: 2578, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 941.000 [941.000, 941.000],  loss: 439343.187500, mae: 1491.317261, mean_q: 4.932547\n",
      "wrong_move\n",
      "   2596/500000: episode: 2579, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 888.000 [888.000, 888.000],  loss: 14043.085938, mae: 1486.173340, mean_q: 4.903903\n",
      "wrong_move\n",
      "   2597/500000: episode: 2580, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 819.000 [819.000, 819.000],  loss: 438465.000000, mae: 1485.168701, mean_q: 4.743574\n",
      "wrong_move\n",
      "   2598/500000: episode: 2581, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 941.000 [941.000, 941.000],  loss: 30831.015625, mae: 1486.975586, mean_q: 4.855413\n",
      "wrong_move\n",
      "   2599/500000: episode: 2582, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3785.000 [3785.000, 3785.000],  loss: 68500.640625, mae: 1489.642090, mean_q: 4.914990\n",
      "wrong_move\n",
      "   2600/500000: episode: 2583, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 888.000 [888.000, 888.000],  loss: 44848.031250, mae: 1494.792969, mean_q: 4.868464\n",
      "wrong_move\n",
      "   2601/500000: episode: 2584, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1389.000 [1389.000, 1389.000],  loss: 831052.625000, mae: 1498.065674, mean_q: 4.858330\n",
      "wrong_move\n",
      "   2602/500000: episode: 2585, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 581.000 [581.000, 581.000],  loss: 110721.414062, mae: 1501.436768, mean_q: 4.891438\n",
      "wrong_move\n",
      "   2603/500000: episode: 2586, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2580.000 [2580.000, 2580.000],  loss: 816006.062500, mae: 1505.389648, mean_q: 4.967622\n",
      "wrong_move\n",
      "   2604/500000: episode: 2587, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3187.000 [3187.000, 3187.000],  loss: 169888.890625, mae: 1509.119385, mean_q: 4.936532\n",
      "wrong_move\n",
      "   2605/500000: episode: 2588, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2445.000 [2445.000, 2445.000],  loss: 445584.750000, mae: 1510.327393, mean_q: 4.884869\n",
      "wrong_move\n",
      "   2606/500000: episode: 2589, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1267.000 [1267.000, 1267.000],  loss: 42322.750000, mae: 1513.937500, mean_q: 4.889013\n",
      "wrong_move\n",
      "   2607/500000: episode: 2590, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 735.000 [735.000, 735.000],  loss: 30707.896484, mae: 1517.464233, mean_q: 4.911498\n",
      "wrong_move\n",
      "   2608/500000: episode: 2591, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3103.000 [3103.000, 3103.000],  loss: 411360.656250, mae: 1521.780762, mean_q: 4.898488\n",
      "wrong_move\n",
      "   2609/500000: episode: 2592, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3740.000 [3740.000, 3740.000],  loss: 24091.980469, mae: 1524.903809, mean_q: 5.066047\n",
      "wrong_move\n",
      "   2610/500000: episode: 2593, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2064.000 [2064.000, 2064.000],  loss: 25035.408203, mae: 1529.494263, mean_q: 4.947282\n",
      "wrong_move\n",
      "   2611/500000: episode: 2594, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3785.000 [3785.000, 3785.000],  loss: 416247.593750, mae: 1535.130371, mean_q: 4.824477\n",
      "wrong_move\n",
      "   2612/500000: episode: 2595, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2655.000 [2655.000, 2655.000],  loss: 478954.031250, mae: 1539.635986, mean_q: 4.896411\n",
      "wrong_move\n",
      "   2613/500000: episode: 2596, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 854.000 [854.000, 854.000],  loss: 213811.140625, mae: 1541.450317, mean_q: 4.892951\n",
      "wrong_move\n",
      "   2614/500000: episode: 2597, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2367.000 [2367.000, 2367.000],  loss: 466873.375000, mae: 1542.127441, mean_q: 4.882651\n",
      "wrong_move\n",
      "   2615/500000: episode: 2598, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2907.000 [2907.000, 2907.000],  loss: 308006.875000, mae: 1540.193115, mean_q: 4.918551\n",
      "wrong_move\n",
      "   2616/500000: episode: 2599, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 819.000 [819.000, 819.000],  loss: 613093.687500, mae: 1537.765015, mean_q: 4.833680\n",
      "wrong_move\n",
      "   2617/500000: episode: 2600, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 840.000 [840.000, 840.000],  loss: 445426.812500, mae: 1536.620117, mean_q: 4.767260\n",
      "wrong_move\n",
      "   2618/500000: episode: 2601, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 819.000 [819.000, 819.000],  loss: 632308.187500, mae: 1533.676025, mean_q: 4.840856\n",
      "wrong_move\n",
      "   2619/500000: episode: 2602, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2498.000 [2498.000, 2498.000],  loss: 139086.531250, mae: 1533.052368, mean_q: 4.877483\n",
      "wrong_move\n",
      "   2620/500000: episode: 2603, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 854.000 [854.000, 854.000],  loss: 805510.937500, mae: 1532.326416, mean_q: 4.870113\n",
      "wrong_move\n",
      "   2621/500000: episode: 2604, duration: 0.168s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2942.000 [2942.000, 2942.000],  loss: 419366.875000, mae: 1526.412598, mean_q: 4.904120\n",
      "wrong_move\n",
      "   2622/500000: episode: 2605, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2498.000 [2498.000, 2498.000],  loss: 433429.531250, mae: 1526.251587, mean_q: 4.803730\n",
      "wrong_move\n",
      "   2623/500000: episode: 2606, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 854.000 [854.000, 854.000],  loss: 305519.437500, mae: 1531.671143, mean_q: 4.716850\n",
      "wrong_move\n",
      "   2624/500000: episode: 2607, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1536.000 [1536.000, 1536.000],  loss: 105470.179688, mae: 1534.635864, mean_q: 4.804025\n",
      "wrong_move\n",
      "   2625/500000: episode: 2608, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2653.000 [2653.000, 2653.000],  loss: 253103.156250, mae: 1539.383911, mean_q: 4.739137\n",
      "wrong_move\n",
      "   2626/500000: episode: 2609, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3590.000 [3590.000, 3590.000],  loss: 121737.242188, mae: 1538.008545, mean_q: 4.793106\n",
      "wrong_move\n",
      "   2627/500000: episode: 2610, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2184.000 [2184.000, 2184.000],  loss: 371731.625000, mae: 1538.047485, mean_q: 4.805735\n",
      "wrong_move\n",
      "   2628/500000: episode: 2611, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2653.000 [2653.000, 2653.000],  loss: 50509.132812, mae: 1535.200928, mean_q: 4.827184\n",
      "wrong_move\n",
      "   2629/500000: episode: 2612, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1986.000 [1986.000, 1986.000],  loss: 100283.445312, mae: 1532.316895, mean_q: 4.802433\n",
      "wrong_move\n",
      "   2630/500000: episode: 2613, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3590.000 [3590.000, 3590.000],  loss: 34542.359375, mae: 1531.425293, mean_q: 4.786193\n",
      "wrong_move\n",
      "   2631/500000: episode: 2614, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1957.000 [1957.000, 1957.000],  loss: 19450.687500, mae: 1531.287476, mean_q: 4.841306\n",
      "wrong_move\n",
      "   2632/500000: episode: 2615, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1434.000 [1434.000, 1434.000],  loss: 29590.451172, mae: 1529.241699, mean_q: 4.882435\n",
      "wrong_move\n",
      "   2633/500000: episode: 2616, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1692.000 [1692.000, 1692.000],  loss: 235066.375000, mae: 1527.745605, mean_q: 5.004395\n",
      "wrong_move\n",
      "   2634/500000: episode: 2617, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3590.000 [3590.000, 3590.000],  loss: 47956.867188, mae: 1527.027832, mean_q: 4.884834\n",
      "wrong_move\n",
      "   2635/500000: episode: 2618, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2822.000 [2822.000, 2822.000],  loss: 55816.726562, mae: 1529.467529, mean_q: 4.827472\n",
      "wrong_move\n",
      "   2636/500000: episode: 2619, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3590.000 [3590.000, 3590.000],  loss: 29129.902344, mae: 1530.343018, mean_q: 4.812155\n",
      "wrong_move\n",
      "   2637/500000: episode: 2620, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2572.000 [2572.000, 2572.000],  loss: 35119.531250, mae: 1532.326660, mean_q: 4.823688\n",
      "wrong_move\n",
      "   2638/500000: episode: 2621, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3590.000 [3590.000, 3590.000],  loss: 517990.562500, mae: 1538.390869, mean_q: 4.845141\n",
      "wrong_move\n",
      "   2639/500000: episode: 2622, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2006.000 [2006.000, 2006.000],  loss: 25001.710938, mae: 1545.568848, mean_q: 4.806513\n",
      "wrong_move\n",
      "   2640/500000: episode: 2623, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1904.000 [1904.000, 1904.000],  loss: 75350.281250, mae: 1547.486572, mean_q: 4.776227\n",
      "wrong_move\n",
      "   2641/500000: episode: 2624, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 331.000 [331.000, 331.000],  loss: 192544.281250, mae: 1551.386963, mean_q: 4.853024\n",
      "wrong_move\n",
      "   2642/500000: episode: 2625, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3602.000 [3602.000, 3602.000],  loss: 339260.125000, mae: 1552.079590, mean_q: 4.854238\n",
      "wrong_move\n",
      "   2643/500000: episode: 2626, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4076.000 [4076.000, 4076.000],  loss: 33320.757812, mae: 1547.397217, mean_q: 4.839399\n",
      "wrong_move\n",
      "   2644/500000: episode: 2627, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3525.000 [3525.000, 3525.000],  loss: 77219.359375, mae: 1542.357178, mean_q: 4.948407\n",
      "wrong_move\n",
      "   2645/500000: episode: 2628, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2267.000 [2267.000, 2267.000],  loss: 34523.976562, mae: 1539.131470, mean_q: 4.862750\n",
      "wrong_move\n",
      "   2646/500000: episode: 2629, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4076.000 [4076.000, 4076.000],  loss: 421049.312500, mae: 1539.726929, mean_q: 4.833400\n",
      "wrong_move\n",
      "   2647/500000: episode: 2630, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4076.000 [4076.000, 4076.000],  loss: 113487.343750, mae: 1541.615234, mean_q: 4.887572\n",
      "wrong_move\n",
      "   2648/500000: episode: 2631, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4076.000 [4076.000, 4076.000],  loss: 49426.191406, mae: 1541.066895, mean_q: 4.770867\n",
      "wrong_move\n",
      "   2649/500000: episode: 2632, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1993.000 [1993.000, 1993.000],  loss: 22616.658203, mae: 1541.421631, mean_q: 4.925673\n",
      "wrong_move\n",
      "   2650/500000: episode: 2633, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4076.000 [4076.000, 4076.000],  loss: 66475.078125, mae: 1544.295410, mean_q: 4.915723\n",
      "wrong_move\n",
      "   2651/500000: episode: 2634, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4076.000 [4076.000, 4076.000],  loss: 28131.421875, mae: 1547.291992, mean_q: 4.875199\n",
      "wrong_move\n",
      "   2652/500000: episode: 2635, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 191.000 [191.000, 191.000],  loss: 382931.031250, mae: 1547.583740, mean_q: 4.785726\n",
      "wrong_move\n",
      "   2653/500000: episode: 2636, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 352.000 [352.000, 352.000],  loss: 459146.843750, mae: 1545.361572, mean_q: 4.796596\n",
      "wrong_move\n",
      "   2654/500000: episode: 2637, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2052.000 [2052.000, 2052.000],  loss: 82018.851562, mae: 1541.458252, mean_q: 4.875420\n",
      "wrong_move\n",
      "   2655/500000: episode: 2638, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3525.000 [3525.000, 3525.000],  loss: 594933.687500, mae: 1537.911499, mean_q: 4.941914\n",
      "wrong_move\n",
      "   2656/500000: episode: 2639, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 221.000 [221.000, 221.000],  loss: 61927.195312, mae: 1536.771362, mean_q: 4.874729\n",
      "wrong_move\n",
      "   2657/500000: episode: 2640, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 221.000 [221.000, 221.000],  loss: 88393.578125, mae: 1536.160400, mean_q: 4.941943\n",
      "wrong_move\n",
      "   2658/500000: episode: 2641, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 221.000 [221.000, 221.000],  loss: 36157.531250, mae: 1536.537598, mean_q: 4.864655\n",
      "wrong_move\n",
      "   2659/500000: episode: 2642, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 221.000 [221.000, 221.000],  loss: 46243.265625, mae: 1535.511230, mean_q: 4.866804\n",
      "wrong_move\n",
      "   2661/500000: episode: 2643, duration: 0.185s, episode steps:   2, steps per second:  11, episode reward: -5031.000, mean reward: -2515.500 [-5000.000, -31.000], mean action: 1511.500 [1256.000, 1767.000],  loss: 289732.250000, mae: 1531.105103, mean_q: 4.821518\n",
      "wrong_move\n",
      "   2662/500000: episode: 2644, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2448.000 [2448.000, 2448.000],  loss: 37840.195312, mae: 1529.707642, mean_q: 4.970692\n",
      "wrong_move\n",
      "   2663/500000: episode: 2645, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1568.000 [1568.000, 1568.000],  loss: 412957.156250, mae: 1532.270996, mean_q: 4.891888\n",
      "wrong_move\n",
      "   2664/500000: episode: 2646, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2726.000 [2726.000, 2726.000],  loss: 27226.128906, mae: 1532.738403, mean_q: 4.914754\n",
      "wrong_move\n",
      "   2665/500000: episode: 2647, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1880.000 [1880.000, 1880.000],  loss: 30690.863281, mae: 1536.408203, mean_q: 4.905952\n",
      "wrong_move\n",
      "   2666/500000: episode: 2648, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 221.000 [221.000, 221.000],  loss: 43126.015625, mae: 1540.950928, mean_q: 4.888513\n",
      "wrong_move\n",
      "   2667/500000: episode: 2649, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1890.000 [1890.000, 1890.000],  loss: 204223.000000, mae: 1545.875977, mean_q: 4.878462\n",
      "wrong_move\n",
      "   2668/500000: episode: 2650, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3880.000 [3880.000, 3880.000],  loss: 285803.718750, mae: 1549.444458, mean_q: 4.870975\n",
      "wrong_move\n",
      "   2669/500000: episode: 2651, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3525.000 [3525.000, 3525.000],  loss: 496196.750000, mae: 1551.219482, mean_q: 4.967239\n",
      "wrong_move\n",
      "   2670/500000: episode: 2652, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2969.000 [2969.000, 2969.000],  loss: 102904.679688, mae: 1555.000854, mean_q: 4.901733\n",
      "wrong_move\n",
      "   2671/500000: episode: 2653, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3004.000 [3004.000, 3004.000],  loss: 102536.109375, mae: 1558.725830, mean_q: 4.883531\n",
      "wrong_move\n",
      "   2672/500000: episode: 2654, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2767.000 [2767.000, 2767.000],  loss: 25433.441406, mae: 1559.879028, mean_q: 4.803396\n",
      "wrong_move\n",
      "   2673/500000: episode: 2655, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2969.000 [2969.000, 2969.000],  loss: 83172.781250, mae: 1559.491943, mean_q: 4.995732\n",
      "wrong_move\n",
      "   2674/500000: episode: 2656, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3243.000 [3243.000, 3243.000],  loss: 101235.765625, mae: 1557.065063, mean_q: 5.109190\n",
      "wrong_move\n",
      "   2675/500000: episode: 2657, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3381.000 [3381.000, 3381.000],  loss: 41393.179688, mae: 1553.290527, mean_q: 5.102295\n",
      "wrong_move\n",
      "   2676/500000: episode: 2658, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 672.000 [672.000, 672.000],  loss: 49686.648438, mae: 1549.743896, mean_q: 4.895393\n",
      "wrong_move\n",
      "   2677/500000: episode: 2659, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2969.000 [2969.000, 2969.000],  loss: 69743.359375, mae: 1544.534668, mean_q: 4.814775\n",
      "wrong_move\n",
      "   2678/500000: episode: 2660, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2969.000 [2969.000, 2969.000],  loss: 632111.687500, mae: 1538.715576, mean_q: 4.917087\n",
      "wrong_move\n",
      "   2679/500000: episode: 2661, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3166.000 [3166.000, 3166.000],  loss: 1487470.000000, mae: 1539.753784, mean_q: 5.069391\n",
      "wrong_move\n",
      "   2680/500000: episode: 2662, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3352.000 [3352.000, 3352.000],  loss: 401427.593750, mae: 1542.913208, mean_q: 6.285479\n",
      "wrong_move\n",
      "   2681/500000: episode: 2663, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1296.000 [1296.000, 1296.000],  loss: 419318.843750, mae: 1548.230957, mean_q: 4.834270\n",
      "wrong_move\n",
      "   2682/500000: episode: 2664, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2969.000 [2969.000, 2969.000],  loss: 38712.828125, mae: 1551.755615, mean_q: 4.869933\n",
      "wrong_move\n",
      "   2683/500000: episode: 2665, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3832.000 [3832.000, 3832.000],  loss: 53482.050781, mae: 1552.899658, mean_q: 4.956695\n",
      "wrong_move\n",
      "   2684/500000: episode: 2666, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 398717.250000, mae: 1552.719360, mean_q: 67.093010\n",
      "wrong_move\n",
      "   2685/500000: episode: 2667, duration: 0.142s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 60096.046875, mae: 1553.696533, mean_q: 260.332550\n",
      "wrong_move\n",
      "   2686/500000: episode: 2668, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 535697.687500, mae: 1557.284668, mean_q: 423.087372\n",
      "wrong_move\n",
      "   2687/500000: episode: 2669, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3329.000 [3329.000, 3329.000],  loss: 52344.292969, mae: 1565.075806, mean_q: 549.407104\n",
      "wrong_move\n",
      "   2688/500000: episode: 2670, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 950.000 [950.000, 950.000],  loss: 37694.457031, mae: 1571.660645, mean_q: 476.503235\n",
      "wrong_move\n",
      "   2689/500000: episode: 2671, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1589.000 [1589.000, 1589.000],  loss: 422330.281250, mae: 1575.658569, mean_q: 416.650024\n",
      "wrong_move\n",
      "   2690/500000: episode: 2672, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 376.000 [376.000, 376.000],  loss: 137488.484375, mae: 1578.790283, mean_q: 367.824066\n",
      "wrong_move\n",
      "   2691/500000: episode: 2673, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 80.000 [80.000, 80.000],  loss: 24788.363281, mae: 1578.486084, mean_q: 327.496368\n",
      "wrong_move\n",
      "   2692/500000: episode: 2674, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 42323.234375, mae: 1576.395386, mean_q: 294.372223\n",
      "wrong_move\n",
      "   2693/500000: episode: 2675, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 517997.312500, mae: 1574.753296, mean_q: 267.554810\n",
      "wrong_move\n",
      "   2694/500000: episode: 2676, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 46087.445312, mae: 1573.383789, mean_q: 245.369568\n",
      "wrong_move\n",
      "   2695/500000: episode: 2677, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4005.000 [4005.000, 4005.000],  loss: 469452.750000, mae: 1574.332275, mean_q: 227.731171\n",
      "wrong_move\n",
      "   2696/500000: episode: 2678, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 452601.593750, mae: 1576.252075, mean_q: 213.637604\n",
      "wrong_move\n",
      "   2697/500000: episode: 2679, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1454.000 [1454.000, 1454.000],  loss: 402289.937500, mae: 1577.237061, mean_q: 202.781494\n",
      "wrong_move\n",
      "   2698/500000: episode: 2680, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4043.000 [4043.000, 4043.000],  loss: 34680.296875, mae: 1570.347900, mean_q: 4.892412\n",
      "wrong_move\n",
      "   2699/500000: episode: 2681, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 958.000 [958.000, 958.000],  loss: 29260.000000, mae: 1562.946289, mean_q: 4.906263\n",
      "wrong_move\n",
      "   2700/500000: episode: 2682, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1015.000 [1015.000, 1015.000],  loss: 237150.062500, mae: 1557.113892, mean_q: 4.877563\n",
      "wrong_move\n",
      "   2701/500000: episode: 2683, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2825.000 [2825.000, 2825.000],  loss: 56077.609375, mae: 1555.707275, mean_q: 4.854612\n",
      "wrong_move\n",
      "   2702/500000: episode: 2684, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3047.000 [3047.000, 3047.000],  loss: 106438.343750, mae: 1558.552612, mean_q: 4.872654\n",
      "wrong_move\n",
      "   2703/500000: episode: 2685, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 911.000 [911.000, 911.000],  loss: 44226.562500, mae: 1563.095947, mean_q: 4.853914\n",
      "wrong_move\n",
      "   2704/500000: episode: 2686, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2181.000 [2181.000, 2181.000],  loss: 78780.781250, mae: 1569.429199, mean_q: 4.862315\n",
      "wrong_move\n",
      "   2705/500000: episode: 2687, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1118.000 [1118.000, 1118.000],  loss: 47507.167969, mae: 1574.144287, mean_q: 4.813671\n",
      "wrong_move\n",
      "   2706/500000: episode: 2688, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1588.000 [1588.000, 1588.000],  loss: 38578.281250, mae: 1576.714355, mean_q: 4.899520\n",
      "wrong_move\n",
      "   2707/500000: episode: 2689, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2652.000 [2652.000, 2652.000],  loss: 49913.863281, mae: 1577.797363, mean_q: 4.744013\n",
      "wrong_move\n",
      "   2708/500000: episode: 2690, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1601.000 [1601.000, 1601.000],  loss: 420158.593750, mae: 1577.143066, mean_q: 15.218912\n",
      "wrong_move\n",
      "   2709/500000: episode: 2691, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2732.000 [2732.000, 2732.000],  loss: 218843.546875, mae: 1576.840210, mean_q: 4.853980\n",
      "wrong_move\n",
      "   2710/500000: episode: 2692, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2732.000 [2732.000, 2732.000],  loss: 535080.812500, mae: 1573.076904, mean_q: 4.869387\n",
      "wrong_move\n",
      "   2711/500000: episode: 2693, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2944.000 [2944.000, 2944.000],  loss: 443615.500000, mae: 1563.536255, mean_q: 4.816794\n",
      "wrong_move\n",
      "   2712/500000: episode: 2694, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2259.000 [2259.000, 2259.000],  loss: 180765.828125, mae: 1559.105225, mean_q: 4.931829\n",
      "wrong_move\n",
      "   2713/500000: episode: 2695, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3798.000 [3798.000, 3798.000],  loss: 612423.812500, mae: 1557.458130, mean_q: 4.814295\n",
      "wrong_move\n",
      "   2714/500000: episode: 2696, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3798.000 [3798.000, 3798.000],  loss: 41358.675781, mae: 1559.146973, mean_q: 4.812563\n",
      "wrong_move\n",
      "   2715/500000: episode: 2697, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3798.000 [3798.000, 3798.000],  loss: 55460.648438, mae: 1563.038696, mean_q: 19.241310\n",
      "wrong_move\n",
      "   2716/500000: episode: 2698, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3798.000 [3798.000, 3798.000],  loss: 214484.718750, mae: 1567.145020, mean_q: 4.834278\n",
      "wrong_move\n",
      "   2717/500000: episode: 2699, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2663.000 [2663.000, 2663.000],  loss: 38084.328125, mae: 1573.602295, mean_q: 4.902063\n",
      "wrong_move\n",
      "   2718/500000: episode: 2700, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2503.000 [2503.000, 2503.000],  loss: 848441.750000, mae: 1582.014160, mean_q: 4.803444\n",
      "wrong_move\n",
      "   2719/500000: episode: 2701, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2732.000 [2732.000, 2732.000],  loss: 693080.875000, mae: 1588.857666, mean_q: 4.868731\n",
      "wrong_move\n",
      "   2720/500000: episode: 2702, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2829.000 [2829.000, 2829.000],  loss: 36854.746094, mae: 1591.279053, mean_q: 4.923693\n",
      "wrong_move\n",
      "   2721/500000: episode: 2703, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2354.000 [2354.000, 2354.000],  loss: 36448.726562, mae: 1591.827148, mean_q: 4.819122\n",
      "wrong_move\n",
      "   2722/500000: episode: 2704, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3788.000 [3788.000, 3788.000],  loss: 94198.578125, mae: 1590.923584, mean_q: 4.903066\n",
      "wrong_move\n",
      "   2723/500000: episode: 2705, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3902.000 [3902.000, 3902.000],  loss: 189353.187500, mae: 1587.287109, mean_q: 4.906812\n",
      "wrong_move\n",
      "   2724/500000: episode: 2706, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1354.000 [1354.000, 1354.000],  loss: 34200.652344, mae: 1586.771851, mean_q: 4.858664\n",
      "wrong_move\n",
      "   2725/500000: episode: 2707, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2752.000 [2752.000, 2752.000],  loss: 26000.718750, mae: 1584.938477, mean_q: 4.877792\n",
      "wrong_move\n",
      "   2726/500000: episode: 2708, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2944.000 [2944.000, 2944.000],  loss: 463909.000000, mae: 1583.930054, mean_q: 4.801228\n",
      "wrong_move\n",
      "   2727/500000: episode: 2709, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2181.000 [2181.000, 2181.000],  loss: 43863.812500, mae: 1584.304932, mean_q: 4.819633\n",
      "wrong_move\n",
      "   2728/500000: episode: 2710, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2944.000 [2944.000, 2944.000],  loss: 50425.070312, mae: 1588.517212, mean_q: 7.676010\n",
      "wrong_move\n",
      "   2729/500000: episode: 2711, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1166.000 [1166.000, 1166.000],  loss: 39488.382812, mae: 1592.166992, mean_q: 4.800396\n",
      "wrong_move\n",
      "   2730/500000: episode: 2712, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2944.000 [2944.000, 2944.000],  loss: 400538.812500, mae: 1596.694580, mean_q: 4.893779\n",
      "wrong_move\n",
      "   2731/500000: episode: 2713, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3520.000 [3520.000, 3520.000],  loss: 458184.843750, mae: 1601.904541, mean_q: 5.012777\n",
      "wrong_move\n",
      "   2732/500000: episode: 2714, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1803.000 [1803.000, 1803.000],  loss: 229882.000000, mae: 1607.526611, mean_q: 4.772549\n",
      "wrong_move\n",
      "   2733/500000: episode: 2715, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1988.000 [1988.000, 1988.000],  loss: 20564.015625, mae: 1609.309082, mean_q: 4.790225\n",
      "wrong_move\n",
      "   2734/500000: episode: 2716, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 984.000 [984.000, 984.000],  loss: 34631.890625, mae: 1609.111572, mean_q: 4.926366\n",
      "wrong_move\n",
      "   2735/500000: episode: 2717, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2944.000 [2944.000, 2944.000],  loss: 59681.074219, mae: 1605.898682, mean_q: 4.817832\n",
      "wrong_move\n",
      "   2736/500000: episode: 2718, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4054.000 [4054.000, 4054.000],  loss: 23846.531250, mae: 1601.213867, mean_q: 4.879790\n",
      "wrong_move\n",
      "   2737/500000: episode: 2719, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1569.000 [1569.000, 1569.000],  loss: 71040.625000, mae: 1599.789551, mean_q: 4.758810\n",
      "wrong_move\n",
      "   2739/500000: episode: 2720, duration: 0.134s, episode steps:   2, steps per second:  15, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1569.000 [1569.000, 1569.000],  loss: 296830.625000, mae: 1602.674316, mean_q: 5.314580\n",
      "wrong_move\n",
      "   2740/500000: episode: 2721, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1043.000 [1043.000, 1043.000],  loss: 22039.582031, mae: 1604.622314, mean_q: 4.794933\n",
      "wrong_move\n",
      "   2741/500000: episode: 2722, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2374.000 [2374.000, 2374.000],  loss: 809404.750000, mae: 1605.198975, mean_q: 4.798489\n",
      "wrong_move\n",
      "   2742/500000: episode: 2723, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3778.000 [3778.000, 3778.000],  loss: 38934.332031, mae: 1608.390015, mean_q: 4.771791\n",
      "wrong_move\n",
      "   2743/500000: episode: 2724, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 341.000 [341.000, 341.000],  loss: 96927.570312, mae: 1610.357910, mean_q: 45.557617\n",
      "wrong_move\n",
      "   2744/500000: episode: 2725, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4054.000 [4054.000, 4054.000],  loss: 84329.195312, mae: 1608.709229, mean_q: 4.717468\n",
      "wrong_move\n",
      "   2745/500000: episode: 2726, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2026.000 [2026.000, 2026.000],  loss: 56823.484375, mae: 1606.386963, mean_q: 4.778562\n",
      "wrong_move\n",
      "   2746/500000: episode: 2727, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2234.000 [2234.000, 2234.000],  loss: 899534.562500, mae: 1602.779907, mean_q: 4.682275\n",
      "wrong_move\n",
      "   2747/500000: episode: 2728, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4054.000 [4054.000, 4054.000],  loss: 824828.062500, mae: 1589.355957, mean_q: 21.680511\n",
      "wrong_move\n",
      "   2748/500000: episode: 2729, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 779.000 [779.000, 779.000],  loss: 426069.437500, mae: 1575.357422, mean_q: 4.700498\n",
      "wrong_move\n",
      "   2749/500000: episode: 2730, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2020.000 [2020.000, 2020.000],  loss: 145529.125000, mae: 1568.676514, mean_q: 4.730636\n",
      "wrong_move\n",
      "   2750/500000: episode: 2731, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3562.000 [3562.000, 3562.000],  loss: 375794.343750, mae: 1567.721924, mean_q: 17.405548\n",
      "wrong_move\n",
      "   2751/500000: episode: 2732, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2020.000 [2020.000, 2020.000],  loss: 491038.406250, mae: 1572.714600, mean_q: 4.969060\n",
      "wrong_move\n",
      "   2752/500000: episode: 2733, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3703.000 [3703.000, 3703.000],  loss: 39987.394531, mae: 1582.469238, mean_q: 4.812333\n",
      "wrong_move\n",
      "   2753/500000: episode: 2734, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2020.000 [2020.000, 2020.000],  loss: 104727.085938, mae: 1591.771606, mean_q: 4.800768\n",
      "wrong_move\n",
      "   2754/500000: episode: 2735, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3439.000 [3439.000, 3439.000],  loss: 215766.812500, mae: 1599.977051, mean_q: 4.838682\n",
      "wrong_move\n",
      "   2756/500000: episode: 2736, duration: 0.114s, episode steps:   2, steps per second:  18, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 598.000 [521.000, 675.000],  loss: 58617.121094, mae: 1606.646851, mean_q: 4.759575\n",
      "wrong_move\n",
      "   2757/500000: episode: 2737, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2679.000 [2679.000, 2679.000],  loss: 47422.570312, mae: 1609.941528, mean_q: 4.865799\n",
      "wrong_move\n",
      "   2758/500000: episode: 2738, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1484.000 [1484.000, 1484.000],  loss: 18978.992188, mae: 1611.393311, mean_q: 5.027718\n",
      "wrong_move\n",
      "   2759/500000: episode: 2739, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 241.000 [241.000, 241.000],  loss: 878369.500000, mae: 1609.705078, mean_q: 13.432347\n",
      "wrong_move\n",
      "   2760/500000: episode: 2740, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4069.000 [4069.000, 4069.000],  loss: 75706.953125, mae: 1609.347900, mean_q: 17.787958\n",
      "wrong_move\n",
      "   2761/500000: episode: 2741, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 443.000 [443.000, 443.000],  loss: 87199.984375, mae: 1605.970459, mean_q: 14.534754\n",
      "wrong_move\n",
      "   2762/500000: episode: 2742, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2424.000 [2424.000, 2424.000],  loss: 23441.568359, mae: 1605.805664, mean_q: 7.574637\n",
      "wrong_move\n",
      "   2763/500000: episode: 2743, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 418.000 [418.000, 418.000],  loss: 40328.316406, mae: 1603.290649, mean_q: 4.753600\n",
      "wrong_move\n",
      "   2764/500000: episode: 2744, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3058.000 [3058.000, 3058.000],  loss: 165446.656250, mae: 1603.871094, mean_q: 4.941245\n",
      "wrong_move\n",
      "   2765/500000: episode: 2745, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 241.000 [241.000, 241.000],  loss: 147992.437500, mae: 1606.474243, mean_q: 4.818760\n",
      "wrong_move\n",
      "   2766/500000: episode: 2746, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 241.000 [241.000, 241.000],  loss: 599808.125000, mae: 1609.098633, mean_q: 4.842863\n",
      "wrong_move\n",
      "   2767/500000: episode: 2747, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1628.000 [1628.000, 1628.000],  loss: 105100.179688, mae: 1603.994873, mean_q: 10.487713\n",
      "wrong_move\n",
      "   2768/500000: episode: 2748, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 624.000 [624.000, 624.000],  loss: 607225.687500, mae: 1602.488403, mean_q: 4.753627\n",
      "wrong_move\n",
      "   2769/500000: episode: 2749, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1969.000 [1969.000, 1969.000],  loss: 66288.867188, mae: 1598.493408, mean_q: 4.790447\n",
      "wrong_move\n",
      "   2770/500000: episode: 2750, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4054.000 [4054.000, 4054.000],  loss: 43363.394531, mae: 1597.460205, mean_q: 4.882998\n",
      "wrong_move\n",
      "   2771/500000: episode: 2751, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1445.000 [1445.000, 1445.000],  loss: 564283.687500, mae: 1597.985718, mean_q: 4.808118\n",
      "wrong_move\n",
      "   2772/500000: episode: 2752, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4054.000 [4054.000, 4054.000],  loss: 13237.527344, mae: 1598.109985, mean_q: 4.857603\n",
      "wrong_move\n",
      "   2773/500000: episode: 2753, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 996.000 [996.000, 996.000],  loss: 441263.875000, mae: 1600.485840, mean_q: 4.940358\n",
      "wrong_move\n",
      "   2774/500000: episode: 2754, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2803.000 [2803.000, 2803.000],  loss: 45925.921875, mae: 1603.704956, mean_q: 4.801038\n",
      "wrong_move\n",
      "   2775/500000: episode: 2755, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 341.000 [341.000, 341.000],  loss: 216605.750000, mae: 1607.988770, mean_q: 4.779881\n",
      "wrong_move\n",
      "   2776/500000: episode: 2756, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1354.000 [1354.000, 1354.000],  loss: 109480.546875, mae: 1612.082031, mean_q: 4.828432\n",
      "wrong_move\n",
      "   2777/500000: episode: 2757, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 628.000 [628.000, 628.000],  loss: 62631.859375, mae: 1616.405029, mean_q: 4.703437\n",
      "wrong_move\n",
      "   2778/500000: episode: 2758, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 62.000 [62.000, 62.000],  loss: 309453.562500, mae: 1622.019043, mean_q: 4.937606\n",
      "wrong_move\n",
      "   2779/500000: episode: 2759, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1399.000 [1399.000, 1399.000],  loss: 519607.750000, mae: 1626.120361, mean_q: 4.707470\n",
      "wrong_move\n",
      "   2780/500000: episode: 2760, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3512.000 [3512.000, 3512.000],  loss: 366588.343750, mae: 1623.756104, mean_q: 4.912290\n",
      "wrong_move\n",
      "   2781/500000: episode: 2761, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3599.000 [3599.000, 3599.000],  loss: 114614.539062, mae: 1621.017090, mean_q: 4.831500\n",
      "wrong_move\n",
      "   2782/500000: episode: 2762, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1296.000 [1296.000, 1296.000],  loss: 21010.298828, mae: 1620.329834, mean_q: 5.024302\n",
      "wrong_move\n",
      "   2783/500000: episode: 2763, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2190.000 [2190.000, 2190.000],  loss: 532538.875000, mae: 1619.416748, mean_q: 4.804509\n",
      "wrong_move\n",
      "   2784/500000: episode: 2764, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3108.000 [3108.000, 3108.000],  loss: 86069.546875, mae: 1620.217285, mean_q: 4.786273\n",
      "wrong_move\n",
      "   2785/500000: episode: 2765, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3854.000 [3854.000, 3854.000],  loss: 94504.015625, mae: 1626.238037, mean_q: 4.896199\n",
      "wrong_move\n",
      "   2786/500000: episode: 2766, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3854.000 [3854.000, 3854.000],  loss: 42260.058594, mae: 1634.841309, mean_q: 4.832665\n",
      "wrong_move\n",
      "   2787/500000: episode: 2767, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 706.000 [706.000, 706.000],  loss: 121493.140625, mae: 1643.870117, mean_q: 4.818724\n",
      "wrong_move\n",
      "   2788/500000: episode: 2768, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3854.000 [3854.000, 3854.000],  loss: 57526.914062, mae: 1647.144775, mean_q: 4.879778\n",
      "wrong_move\n",
      "   2789/500000: episode: 2769, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1969.000 [1969.000, 1969.000],  loss: 66318.562500, mae: 1653.321533, mean_q: 4.729066\n",
      "wrong_move\n",
      "   2790/500000: episode: 2770, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3854.000 [3854.000, 3854.000],  loss: 488023.718750, mae: 1658.632935, mean_q: 4.810865\n",
      "wrong_move\n",
      "   2791/500000: episode: 2771, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3854.000 [3854.000, 3854.000],  loss: 82893.421875, mae: 1661.202637, mean_q: 4.746496\n",
      "wrong_move\n",
      "   2792/500000: episode: 2772, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 341.000 [341.000, 341.000],  loss: 27687.005859, mae: 1656.085083, mean_q: 5.003714\n",
      "wrong_move\n",
      "   2793/500000: episode: 2773, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1899.000 [1899.000, 1899.000],  loss: 33792.976562, mae: 1649.183594, mean_q: 4.630375\n",
      "wrong_move\n",
      "   2794/500000: episode: 2774, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1969.000 [1969.000, 1969.000],  loss: 419400.937500, mae: 1644.353027, mean_q: 4.677981\n",
      "wrong_move\n",
      "   2795/500000: episode: 2775, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1973.000 [1973.000, 1973.000],  loss: 157887.546875, mae: 1642.942627, mean_q: 4.752539\n",
      "wrong_move\n",
      "   2796/500000: episode: 2776, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1346.000 [1346.000, 1346.000],  loss: 263460.375000, mae: 1641.727539, mean_q: 4.680447\n",
      "wrong_move\n",
      "   2797/500000: episode: 2777, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 92.000 [92.000, 92.000],  loss: 397474.656250, mae: 1638.957275, mean_q: 4.655673\n",
      "wrong_move\n",
      "   2798/500000: episode: 2778, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1523.000 [1523.000, 1523.000],  loss: 52577.472656, mae: 1627.892822, mean_q: 10.432592\n",
      "wrong_move\n",
      "   2799/500000: episode: 2779, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 628.000 [628.000, 628.000],  loss: 34185.886719, mae: 1620.184692, mean_q: 4.651758\n",
      "wrong_move\n",
      "   2800/500000: episode: 2780, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1969.000 [1969.000, 1969.000],  loss: 22787.746094, mae: 1618.048706, mean_q: 4.503737\n",
      "wrong_move\n",
      "   2801/500000: episode: 2781, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3302.000 [3302.000, 3302.000],  loss: 49809.828125, mae: 1619.748535, mean_q: 4.548891\n",
      "wrong_move\n",
      "   2802/500000: episode: 2782, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 284.000 [284.000, 284.000],  loss: 40163.273438, mae: 1623.436890, mean_q: 4.657205\n",
      "wrong_move\n",
      "   2803/500000: episode: 2783, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3301.000 [3301.000, 3301.000],  loss: 431845.218750, mae: 1627.025513, mean_q: 4.549907\n",
      "wrong_move\n",
      "   2804/500000: episode: 2784, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1181.000 [1181.000, 1181.000],  loss: 405316.750000, mae: 1634.957275, mean_q: 4.581993\n",
      "wrong_move\n",
      "   2805/500000: episode: 2785, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1969.000 [1969.000, 1969.000],  loss: 38609.699219, mae: 1643.420166, mean_q: 4.592497\n",
      "wrong_move\n",
      "   2806/500000: episode: 2786, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1647.000 [1647.000, 1647.000],  loss: 34963.195312, mae: 1651.771240, mean_q: 4.553474\n",
      "wrong_move\n",
      "   2807/500000: episode: 2787, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 628.000 [628.000, 628.000],  loss: 807841.000000, mae: 1655.885742, mean_q: 4.518217\n",
      "wrong_move\n",
      "   2808/500000: episode: 2788, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2479.000 [2479.000, 2479.000],  loss: 212121.609375, mae: 1656.358887, mean_q: 4.531515\n",
      "wrong_move\n",
      "   2809/500000: episode: 2789, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 140.000 [140.000, 140.000],  loss: 248452.734375, mae: 1659.388184, mean_q: 4.506054\n",
      "wrong_move\n",
      "   2810/500000: episode: 2790, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2067.000 [2067.000, 2067.000],  loss: 24043.671875, mae: 1659.600098, mean_q: 4.593992\n",
      "wrong_move\n",
      "   2811/500000: episode: 2791, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2008.000 [2008.000, 2008.000],  loss: 40794.750000, mae: 1659.658447, mean_q: 4.589680\n",
      "wrong_move\n",
      "   2812/500000: episode: 2792, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3570.000 [3570.000, 3570.000],  loss: 25387.332031, mae: 1658.494385, mean_q: 4.505659\n",
      "wrong_move\n",
      "   2813/500000: episode: 2793, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2808.000 [2808.000, 2808.000],  loss: 120010.914062, mae: 1658.701904, mean_q: 4.508131\n",
      "wrong_move\n",
      "   2814/500000: episode: 2794, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3206.000 [3206.000, 3206.000],  loss: 78433.656250, mae: 1657.045776, mean_q: 4.479507\n",
      "wrong_move\n",
      "   2815/500000: episode: 2795, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1969.000 [1969.000, 1969.000],  loss: 77011.875000, mae: 1654.660522, mean_q: 4.534885\n",
      "wrong_move\n",
      "   2816/500000: episode: 2796, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1969.000 [1969.000, 1969.000],  loss: 423741.593750, mae: 1651.165405, mean_q: 4.540045\n",
      "wrong_move\n",
      "   2817/500000: episode: 2797, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3570.000 [3570.000, 3570.000],  loss: 426966.375000, mae: 1648.262329, mean_q: 4.534201\n",
      "wrong_move\n",
      "   2818/500000: episode: 2798, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3595.000 [3595.000, 3595.000],  loss: 96414.578125, mae: 1647.683716, mean_q: 4.487080\n",
      "wrong_move\n",
      "   2819/500000: episode: 2799, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1569.000 [1569.000, 1569.000],  loss: 31754.343750, mae: 1651.278320, mean_q: 4.498156\n",
      "wrong_move\n",
      "   2820/500000: episode: 2800, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 768.000 [768.000, 768.000],  loss: 52900.507812, mae: 1659.274170, mean_q: 4.581053\n",
      "wrong_move\n",
      "   2821/500000: episode: 2801, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 237.000 [237.000, 237.000],  loss: 62106.996094, mae: 1663.879150, mean_q: 4.508032\n",
      "wrong_move\n",
      "   2822/500000: episode: 2802, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3805.000 [3805.000, 3805.000],  loss: 419333.812500, mae: 1663.937256, mean_q: 4.477169\n",
      "wrong_move\n",
      "   2823/500000: episode: 2803, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 250.000 [250.000, 250.000],  loss: 59034.281250, mae: 1664.319702, mean_q: 4.486530\n",
      "wrong_move\n",
      "   2824/500000: episode: 2804, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 938.000 [938.000, 938.000],  loss: 821819.375000, mae: 1662.323120, mean_q: 4.537016\n",
      "wrong_move\n",
      "   2825/500000: episode: 2805, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3805.000 [3805.000, 3805.000],  loss: 423784.718750, mae: 1664.332764, mean_q: 4.506637\n",
      "wrong_move\n",
      "   2826/500000: episode: 2806, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1936.000 [1936.000, 1936.000],  loss: 79287.812500, mae: 1661.883301, mean_q: 4.497228\n",
      "wrong_move\n",
      "   2827/500000: episode: 2807, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3658.000 [3658.000, 3658.000],  loss: 850886.875000, mae: 1659.492920, mean_q: 4.484433\n",
      "wrong_move\n",
      "   2828/500000: episode: 2808, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3805.000 [3805.000, 3805.000],  loss: 43840.070312, mae: 1658.267578, mean_q: 4.513459\n",
      "wrong_move\n",
      "   2829/500000: episode: 2809, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2117.000 [2117.000, 2117.000],  loss: 427267.875000, mae: 1655.856689, mean_q: 4.525023\n",
      "wrong_move\n",
      "   2830/500000: episode: 2810, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3805.000 [3805.000, 3805.000],  loss: 183458.187500, mae: 1653.011230, mean_q: 4.400561\n",
      "wrong_move\n",
      "   2831/500000: episode: 2811, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3238.000 [3238.000, 3238.000],  loss: 429463.750000, mae: 1655.166748, mean_q: 4.392195\n",
      "wrong_move\n",
      "   2832/500000: episode: 2812, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2334.000 [2334.000, 2334.000],  loss: 98224.742188, mae: 1657.260376, mean_q: 4.475368\n",
      "wrong_move\n",
      "   2833/500000: episode: 2813, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3805.000 [3805.000, 3805.000],  loss: 57523.125000, mae: 1662.799805, mean_q: 4.417150\n",
      "wrong_move\n",
      "   2834/500000: episode: 2814, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3489.000 [3489.000, 3489.000],  loss: 26193.365234, mae: 1670.194824, mean_q: 4.482068\n",
      "wrong_move\n",
      "   2835/500000: episode: 2815, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 373.000 [373.000, 373.000],  loss: 74874.203125, mae: 1676.976929, mean_q: 4.522012\n",
      "wrong_move\n",
      "   2836/500000: episode: 2816, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 901.000 [901.000, 901.000],  loss: 53626.339844, mae: 1681.911133, mean_q: 4.409876\n",
      "wrong_move\n",
      "   2837/500000: episode: 2817, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: 21742.953125, mae: 1687.411987, mean_q: 432.199036\n",
      "wrong_move\n",
      "   2838/500000: episode: 2818, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3176.000 [3176.000, 3176.000],  loss: 113348.914062, mae: 1691.106201, mean_q: 1172.655762\n",
      "wrong_move\n",
      "   2839/500000: episode: 2819, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: 470328.125000, mae: 1688.727295, mean_q: 1769.339844\n",
      "wrong_move\n",
      "   2840/500000: episode: 2820, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: 554313.187500, mae: 1686.577637, mean_q: 2251.062500\n",
      "wrong_move\n",
      "   2841/500000: episode: 2821, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: 971287.125000, mae: 1682.026611, mean_q: 2635.802490\n",
      "wrong_move\n",
      "   2842/500000: episode: 2822, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: 40242.531250, mae: 1678.148926, mean_q: 1795.762817\n",
      "wrong_move\n",
      "   2843/500000: episode: 2823, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: 102800.406250, mae: 1674.351440, mean_q: 1117.327026\n",
      "wrong_move\n",
      "   2844/500000: episode: 2824, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1686.000 [1686.000, 1686.000],  loss: 422041.562500, mae: 1676.158203, mean_q: 571.382263\n",
      "wrong_move\n",
      "   2845/500000: episode: 2825, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3805.000 [3805.000, 3805.000],  loss: 34007.687500, mae: 1676.103271, mean_q: 6.415849\n",
      "wrong_move\n",
      "   2846/500000: episode: 2826, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1958.000 [1958.000, 1958.000],  loss: 418826.625000, mae: 1677.859375, mean_q: 4.289710\n",
      "wrong_move\n",
      "   2847/500000: episode: 2827, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 760.000 [760.000, 760.000],  loss: 226996.500000, mae: 1683.105225, mean_q: 4.347600\n",
      "wrong_move\n",
      "   2848/500000: episode: 2828, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 483.000 [483.000, 483.000],  loss: 420767.906250, mae: 1686.184570, mean_q: 4.505924\n",
      "wrong_move\n",
      "   2849/500000: episode: 2829, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2483.000 [2483.000, 2483.000],  loss: 518911.843750, mae: 1694.466675, mean_q: 4.369803\n",
      "wrong_move\n",
      "   2850/500000: episode: 2830, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 760.000 [760.000, 760.000],  loss: 32043.726562, mae: 1692.156860, mean_q: 4.304131\n",
      "wrong_move\n",
      "   2851/500000: episode: 2831, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3291.000 [3291.000, 3291.000],  loss: 106320.796875, mae: 1687.377197, mean_q: 4.351682\n",
      "wrong_move\n",
      "   2852/500000: episode: 2832, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2483.000 [2483.000, 2483.000],  loss: 419816.125000, mae: 1685.709961, mean_q: 4.316708\n",
      "wrong_move\n",
      "   2853/500000: episode: 2833, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3013.000 [3013.000, 3013.000],  loss: 73252.046875, mae: 1683.440552, mean_q: 4.478018\n",
      "wrong_move\n",
      "   2854/500000: episode: 2834, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: 428339.625000, mae: 1678.867798, mean_q: 4.425960\n",
      "wrong_move\n",
      "   2855/500000: episode: 2835, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 491.000 [491.000, 491.000],  loss: 356937.718750, mae: 1677.900513, mean_q: 4.403999\n",
      "wrong_move\n",
      "   2856/500000: episode: 2836, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1123.000 [1123.000, 1123.000],  loss: 36510.468750, mae: 1677.529297, mean_q: 4.461596\n",
      "wrong_move\n",
      "   2857/500000: episode: 2837, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2483.000 [2483.000, 2483.000],  loss: 40850.695312, mae: 1679.749756, mean_q: 4.377001\n",
      "wrong_move\n",
      "   2858/500000: episode: 2838, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3082.000 [3082.000, 3082.000],  loss: 326816.718750, mae: 1680.833008, mean_q: 4.373453\n",
      "wrong_move\n",
      "   2859/500000: episode: 2839, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2483.000 [2483.000, 2483.000],  loss: 505227.593750, mae: 1679.138184, mean_q: 4.343248\n",
      "wrong_move\n",
      "   2860/500000: episode: 2840, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3012.000 [3012.000, 3012.000],  loss: 59345.914062, mae: 1677.941162, mean_q: 4.447259\n",
      "wrong_move\n",
      "   2861/500000: episode: 2841, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2039.000 [2039.000, 2039.000],  loss: 391364.500000, mae: 1677.432251, mean_q: 4.365718\n",
      "wrong_move\n",
      "   2862/500000: episode: 2842, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3154.000 [3154.000, 3154.000],  loss: 127467.140625, mae: 1676.460449, mean_q: 4.372040\n",
      "wrong_move\n",
      "   2863/500000: episode: 2843, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2039.000 [2039.000, 2039.000],  loss: 112344.234375, mae: 1677.487793, mean_q: 4.354845\n",
      "wrong_move\n",
      "   2864/500000: episode: 2844, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1504.000 [1504.000, 1504.000],  loss: 289718.687500, mae: 1683.675049, mean_q: 4.431044\n",
      "wrong_move\n",
      "   2865/500000: episode: 2845, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2039.000 [2039.000, 2039.000],  loss: 27200.921875, mae: 1684.057983, mean_q: 4.335721\n",
      "wrong_move\n",
      "   2866/500000: episode: 2846, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2039.000 [2039.000, 2039.000],  loss: 55191.000000, mae: 1683.840576, mean_q: 4.377301\n",
      "wrong_move\n",
      "   2867/500000: episode: 2847, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3154.000 [3154.000, 3154.000],  loss: 213570.203125, mae: 1682.130371, mean_q: 4.349012\n",
      "wrong_move\n",
      "   2868/500000: episode: 2848, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 800.000 [800.000, 800.000],  loss: 434316.812500, mae: 1680.963745, mean_q: 4.370324\n",
      "wrong_move\n",
      "   2869/500000: episode: 2849, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 190.000 [190.000, 190.000],  loss: 94228.703125, mae: 1681.469116, mean_q: 4.291287\n",
      "wrong_move\n",
      "   2870/500000: episode: 2850, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3154.000 [3154.000, 3154.000],  loss: 19368.476562, mae: 1682.941284, mean_q: 4.377748\n",
      "wrong_move\n",
      "   2871/500000: episode: 2851, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3103.000 [3103.000, 3103.000],  loss: 35704.281250, mae: 1684.363525, mean_q: 4.389019\n",
      "wrong_move\n",
      "   2872/500000: episode: 2852, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3154.000 [3154.000, 3154.000],  loss: 34901.890625, mae: 1685.898438, mean_q: 4.452413\n",
      "wrong_move\n",
      "   2873/500000: episode: 2853, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3154.000 [3154.000, 3154.000],  loss: 42272.023438, mae: 1684.045166, mean_q: 4.429272\n",
      "wrong_move\n",
      "   2875/500000: episode: 2854, duration: 0.149s, episode steps:   2, steps per second:  13, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2417.500 [1747.000, 3088.000],  loss: 286125.437500, mae: 1682.929688, mean_q: 4.377986\n",
      "wrong_move\n",
      "   2876/500000: episode: 2855, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3687.000 [3687.000, 3687.000],  loss: 47763.265625, mae: 1684.356445, mean_q: 4.377110\n",
      "wrong_move\n",
      "   2877/500000: episode: 2856, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3687.000 [3687.000, 3687.000],  loss: 52993.496094, mae: 1686.907959, mean_q: 4.387594\n",
      "wrong_move\n",
      "   2878/500000: episode: 2857, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 382.000 [382.000, 382.000],  loss: 407544.750000, mae: 1689.818604, mean_q: 4.377567\n",
      "wrong_move\n",
      "   2879/500000: episode: 2858, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3206.000 [3206.000, 3206.000],  loss: 490393.062500, mae: 1687.447266, mean_q: 4.309987\n",
      "wrong_move\n",
      "   2880/500000: episode: 2859, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3068.000 [3068.000, 3068.000],  loss: 98372.750000, mae: 1686.894531, mean_q: 4.363701\n",
      "wrong_move\n",
      "   2881/500000: episode: 2860, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 918.000 [918.000, 918.000],  loss: 34455.273438, mae: 1691.164917, mean_q: 4.369213\n",
      "wrong_move\n",
      "   2882/500000: episode: 2861, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3291.000 [3291.000, 3291.000],  loss: 423723.187500, mae: 1696.251221, mean_q: 4.512881\n",
      "wrong_move\n",
      "   2883/500000: episode: 2862, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3206.000 [3206.000, 3206.000],  loss: 40764.296875, mae: 1700.560791, mean_q: 4.482171\n",
      "wrong_move\n",
      "   2884/500000: episode: 2863, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3827.000 [3827.000, 3827.000],  loss: 435535.750000, mae: 1705.060547, mean_q: 4.374412\n",
      "wrong_move\n",
      "   2885/500000: episode: 2864, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2317.000 [2317.000, 2317.000],  loss: 31349.478516, mae: 1703.421997, mean_q: 4.368357\n",
      "wrong_move\n",
      "   2886/500000: episode: 2865, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1864.000 [1864.000, 1864.000],  loss: 65603.523438, mae: 1697.135376, mean_q: 4.395366\n",
      "wrong_move\n",
      "   2887/500000: episode: 2866, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2117.000 [2117.000, 2117.000],  loss: 476680.718750, mae: 1694.827393, mean_q: 4.406187\n",
      "wrong_move\n",
      "   2888/500000: episode: 2867, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3687.000 [3687.000, 3687.000],  loss: 77758.460938, mae: 1695.361816, mean_q: 4.371923\n",
      "wrong_move\n",
      "   2889/500000: episode: 2868, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1261.000 [1261.000, 1261.000],  loss: 431980.156250, mae: 1697.374512, mean_q: 4.294759\n",
      "wrong_move\n",
      "   2890/500000: episode: 2869, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3687.000 [3687.000, 3687.000],  loss: 49418.617188, mae: 1697.701416, mean_q: 4.352032\n",
      "wrong_move\n",
      "   2891/500000: episode: 2870, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2620.000 [2620.000, 2620.000],  loss: 41542.687500, mae: 1696.958862, mean_q: 4.295726\n",
      "wrong_move\n",
      "   2892/500000: episode: 2871, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3291.000 [3291.000, 3291.000],  loss: 437465.406250, mae: 1697.393921, mean_q: 4.331472\n",
      "wrong_move\n",
      "   2893/500000: episode: 2872, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4017.000 [4017.000, 4017.000],  loss: 87229.554688, mae: 1698.235107, mean_q: 4.324903\n",
      "wrong_move\n",
      "   2894/500000: episode: 2873, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2794.000 [2794.000, 2794.000],  loss: 253101.656250, mae: 1699.638794, mean_q: 4.443968\n",
      "wrong_move\n",
      "   2895/500000: episode: 2874, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2794.000 [2794.000, 2794.000],  loss: 54399.703125, mae: 1698.606323, mean_q: 4.331087\n",
      "wrong_move\n",
      "   2896/500000: episode: 2875, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2384.000 [2384.000, 2384.000],  loss: 38424.753906, mae: 1699.090942, mean_q: 4.321001\n",
      "wrong_move\n",
      "   2897/500000: episode: 2876, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2794.000 [2794.000, 2794.000],  loss: 27613.562500, mae: 1698.564697, mean_q: 4.301413\n",
      "wrong_move\n",
      "   2898/500000: episode: 2877, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2794.000 [2794.000, 2794.000],  loss: 205269.406250, mae: 1700.231201, mean_q: 4.364769\n",
      "wrong_move\n",
      "   2899/500000: episode: 2878, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 866.000 [866.000, 866.000],  loss: 416663.187500, mae: 1704.287109, mean_q: 4.329440\n",
      "wrong_move\n",
      "   2900/500000: episode: 2879, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1377.000 [1377.000, 1377.000],  loss: 22102.984375, mae: 1706.806885, mean_q: 10.710300\n",
      "wrong_move\n",
      "   2901/500000: episode: 2880, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 460.000 [460.000, 460.000],  loss: 140400.562500, mae: 1709.654297, mean_q: 11.534756\n",
      "wrong_move\n",
      "   2902/500000: episode: 2881, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1494.000 [1494.000, 1494.000],  loss: 509224.250000, mae: 1712.448608, mean_q: 8.881670\n",
      "wrong_move\n",
      "   2903/500000: episode: 2882, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2794.000 [2794.000, 2794.000],  loss: 87209.234375, mae: 1715.058472, mean_q: 4.281046\n",
      "wrong_move\n",
      "   2904/500000: episode: 2883, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 789.000 [789.000, 789.000],  loss: 29519.947266, mae: 1714.156494, mean_q: 4.345476\n",
      "wrong_move\n",
      "   2905/500000: episode: 2884, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4062.000 [4062.000, 4062.000],  loss: 160037.546875, mae: 1713.362183, mean_q: 9.052212\n",
      "wrong_move\n",
      "   2906/500000: episode: 2885, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2385.000 [2385.000, 2385.000],  loss: 32906.062500, mae: 1716.163940, mean_q: 5.623264\n",
      "wrong_move\n",
      "   2907/500000: episode: 2886, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 628.000 [628.000, 628.000],  loss: 22409.484375, mae: 1723.463135, mean_q: 7.446936\n",
      "wrong_move\n",
      "   2908/500000: episode: 2887, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2794.000 [2794.000, 2794.000],  loss: 79489.453125, mae: 1730.515869, mean_q: 15.082897\n",
      "wrong_move\n",
      "   2909/500000: episode: 2888, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1789.000 [1789.000, 1789.000],  loss: 690249.375000, mae: 1735.290649, mean_q: 4.496321\n",
      "wrong_move\n",
      "   2910/500000: episode: 2889, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2126.000 [2126.000, 2126.000],  loss: 440796.531250, mae: 1733.907715, mean_q: 4.683999\n",
      "wrong_move\n",
      "   2911/500000: episode: 2890, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 458.000 [458.000, 458.000],  loss: 97736.031250, mae: 1729.085938, mean_q: 8.769098\n",
      "wrong_move\n",
      "   2912/500000: episode: 2891, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1789.000 [1789.000, 1789.000],  loss: 46600.531250, mae: 1722.206421, mean_q: 6.857433\n",
      "wrong_move\n",
      "   2913/500000: episode: 2892, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1789.000 [1789.000, 1789.000],  loss: 224811.000000, mae: 1712.600586, mean_q: 5.985088\n",
      "wrong_move\n",
      "   2914/500000: episode: 2893, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1789.000 [1789.000, 1789.000],  loss: 115246.992188, mae: 1700.754517, mean_q: 11.090797\n",
      "wrong_move\n",
      "   2915/500000: episode: 2894, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1789.000 [1789.000, 1789.000],  loss: 43985.468750, mae: 1691.835938, mean_q: 11.607853\n",
      "wrong_move\n",
      "   2916/500000: episode: 2895, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1789.000 [1789.000, 1789.000],  loss: 65273.289062, mae: 1688.105469, mean_q: 10.134251\n",
      "wrong_move\n",
      "   2917/500000: episode: 2896, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4094.000 [4094.000, 4094.000],  loss: 189361.875000, mae: 1688.417480, mean_q: 8.826413\n",
      "wrong_move\n",
      "   2918/500000: episode: 2897, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1789.000 [1789.000, 1789.000],  loss: 76993.953125, mae: 1686.433838, mean_q: 32.563660\n",
      "wrong_move\n",
      "   2919/500000: episode: 2898, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2126.000 [2126.000, 2126.000],  loss: 81024.593750, mae: 1691.176147, mean_q: 10.082150\n",
      "wrong_move\n",
      "   2920/500000: episode: 2899, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 351.000 [351.000, 351.000],  loss: 43598.937500, mae: 1697.120605, mean_q: 5.747717\n",
      "wrong_move\n",
      "   2921/500000: episode: 2900, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3969.000 [3969.000, 3969.000],  loss: 143959.625000, mae: 1703.329468, mean_q: 7.875928\n",
      "wrong_move\n",
      "   2922/500000: episode: 2901, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1336.000 [1336.000, 1336.000],  loss: 43034.726562, mae: 1711.505005, mean_q: 4.541270\n",
      "wrong_move\n",
      "   2923/500000: episode: 2902, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1336.000 [1336.000, 1336.000],  loss: 410916.468750, mae: 1720.967041, mean_q: 10.566410\n",
      "wrong_move\n",
      "   2924/500000: episode: 2903, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1336.000 [1336.000, 1336.000],  loss: 37682.695312, mae: 1727.013428, mean_q: 10.427381\n",
      "wrong_move\n",
      "   2925/500000: episode: 2904, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3258.000 [3258.000, 3258.000],  loss: 217204.656250, mae: 1733.514771, mean_q: 11.971919\n",
      "wrong_move\n",
      "   2926/500000: episode: 2905, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1336.000 [1336.000, 1336.000],  loss: 23043.324219, mae: 1736.688232, mean_q: 19.513502\n",
      "wrong_move\n",
      "   2927/500000: episode: 2906, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1336.000 [1336.000, 1336.000],  loss: 129714.843750, mae: 1737.320068, mean_q: 4.643411\n",
      "wrong_move\n",
      "   2928/500000: episode: 2907, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3258.000 [3258.000, 3258.000],  loss: 36036.429688, mae: 1734.287598, mean_q: 4.581041\n",
      "wrong_move\n",
      "   2929/500000: episode: 2908, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3309.000 [3309.000, 3309.000],  loss: 43252.812500, mae: 1725.766235, mean_q: 4.581329\n",
      "wrong_move\n",
      "   2930/500000: episode: 2909, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1336.000 [1336.000, 1336.000],  loss: 459470.281250, mae: 1716.871094, mean_q: 4.593887\n",
      "wrong_move\n",
      "   2931/500000: episode: 2910, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3251.000 [3251.000, 3251.000],  loss: 810596.250000, mae: 1707.620117, mean_q: 4.516022\n",
      "wrong_move\n",
      "   2932/500000: episode: 2911, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1269.000 [1269.000, 1269.000],  loss: 42450.269531, mae: 1700.561523, mean_q: 4.533242\n",
      "wrong_move\n",
      "   2933/500000: episode: 2912, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1211.000 [1211.000, 1211.000],  loss: 275394.656250, mae: 1700.411377, mean_q: 4.496121\n",
      "wrong_move\n",
      "   2934/500000: episode: 2913, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1822.000 [1822.000, 1822.000],  loss: 32166.789062, mae: 1700.068970, mean_q: 4.590570\n",
      "wrong_move\n",
      "   2935/500000: episode: 2914, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2718.000 [2718.000, 2718.000],  loss: 794740.125000, mae: 1704.006592, mean_q: 4.597269\n",
      "wrong_move\n",
      "   2936/500000: episode: 2915, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1791.000 [1791.000, 1791.000],  loss: 40678.175781, mae: 1704.195801, mean_q: 4.498566\n",
      "wrong_move\n",
      "   2937/500000: episode: 2916, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2282.000 [2282.000, 2282.000],  loss: 93839.085938, mae: 1708.050781, mean_q: 4.895321\n",
      "wrong_move\n",
      "   2938/500000: episode: 2917, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 158.000 [158.000, 158.000],  loss: 452034.031250, mae: 1710.626953, mean_q: 4.547734\n",
      "wrong_move\n",
      "   2939/500000: episode: 2918, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3723.000 [3723.000, 3723.000],  loss: 394035.343750, mae: 1707.970581, mean_q: 4.575126\n",
      "wrong_move\n",
      "   2940/500000: episode: 2919, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 707.000 [707.000, 707.000],  loss: 355836.000000, mae: 1704.289307, mean_q: 4.489762\n",
      "wrong_move\n",
      "   2941/500000: episode: 2920, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2890.000 [2890.000, 2890.000],  loss: 232538.375000, mae: 1697.552124, mean_q: 4.554183\n",
      "wrong_move\n",
      "   2942/500000: episode: 2921, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: 50981.980469, mae: 1690.643799, mean_q: 4.499878\n",
      "wrong_move\n",
      "   2943/500000: episode: 2922, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1847.000 [1847.000, 1847.000],  loss: 16970.205078, mae: 1684.979004, mean_q: 4.494907\n",
      "wrong_move\n",
      "   2944/500000: episode: 2923, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1479.000 [1479.000, 1479.000],  loss: 35098.910156, mae: 1684.628662, mean_q: 4.494852\n",
      "wrong_move\n",
      "   2945/500000: episode: 2924, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2790.000 [2790.000, 2790.000],  loss: 68744.656250, mae: 1693.716431, mean_q: 4.328103\n",
      "wrong_move\n",
      "   2946/500000: episode: 2925, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1847.000 [1847.000, 1847.000],  loss: 56637.507812, mae: 1705.411865, mean_q: 4.409910\n",
      "wrong_move\n",
      "   2947/500000: episode: 2926, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 172.000 [172.000, 172.000],  loss: 448547.093750, mae: 1713.603027, mean_q: 4.579669\n",
      "wrong_move\n",
      "   2948/500000: episode: 2927, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4024.000 [4024.000, 4024.000],  loss: 421839.718750, mae: 1717.927490, mean_q: 4.467463\n",
      "wrong_move\n",
      "   2949/500000: episode: 2928, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2141.000 [2141.000, 2141.000],  loss: 35555.601562, mae: 1722.312012, mean_q: 4.563670\n",
      "wrong_move\n",
      "   2950/500000: episode: 2929, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 41.000 [41.000, 41.000],  loss: 46433.710938, mae: 1724.629150, mean_q: 4.544674\n",
      "wrong_move\n",
      "   2951/500000: episode: 2930, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4004.000 [4004.000, 4004.000],  loss: 445289.718750, mae: 1729.768921, mean_q: 4.577832\n",
      "wrong_move\n",
      "   2952/500000: episode: 2931, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4051.000 [4051.000, 4051.000],  loss: 36471.796875, mae: 1733.007568, mean_q: 23.621120\n",
      "wrong_move\n",
      "   2953/500000: episode: 2932, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3397.000 [3397.000, 3397.000],  loss: 87393.265625, mae: 1733.422852, mean_q: 4.529582\n",
      "wrong_move\n",
      "   2954/500000: episode: 2933, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1356.000 [1356.000, 1356.000],  loss: 114912.515625, mae: 1730.856934, mean_q: 4.532402\n",
      "wrong_move\n",
      "   2955/500000: episode: 2934, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3250.000 [3250.000, 3250.000],  loss: 61511.808594, mae: 1725.586548, mean_q: 4.501426\n",
      "wrong_move\n",
      "   2957/500000: episode: 2935, duration: 0.172s, episode steps:   2, steps per second:  12, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1801.500 [881.000, 2722.000],  loss: 69432.632812, mae: 1720.627563, mean_q: 4.648456\n",
      "wrong_move\n",
      "   2958/500000: episode: 2936, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1049.000 [1049.000, 1049.000],  loss: 73095.546875, mae: 1720.022217, mean_q: 12.030729\n",
      "wrong_move\n",
      "   2959/500000: episode: 2937, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2141.000 [2141.000, 2141.000],  loss: 1235891.250000, mae: 1719.677612, mean_q: 4.556711\n",
      "wrong_move\n",
      "   2960/500000: episode: 2938, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 972.000 [972.000, 972.000],  loss: 87917.343750, mae: 1719.397705, mean_q: 15.166390\n",
      "wrong_move\n",
      "   2961/500000: episode: 2939, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1049.000 [1049.000, 1049.000],  loss: 534877.750000, mae: 1721.558228, mean_q: 4.586716\n",
      "wrong_move\n",
      "   2962/500000: episode: 2940, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3231.000 [3231.000, 3231.000],  loss: 51966.460938, mae: 1727.923340, mean_q: 4.539846\n",
      "wrong_move\n",
      "   2963/500000: episode: 2941, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 800.000 [800.000, 800.000],  loss: 66149.640625, mae: 1737.315918, mean_q: 4.687965\n",
      "wrong_move\n",
      "   2964/500000: episode: 2942, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 458.000 [458.000, 458.000],  loss: 368554.718750, mae: 1743.763550, mean_q: 4.456714\n",
      "wrong_move\n",
      "   2965/500000: episode: 2943, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 915.000 [915.000, 915.000],  loss: 60923.730469, mae: 1740.158691, mean_q: 4.536018\n",
      "wrong_move\n",
      "   2966/500000: episode: 2944, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 800.000 [800.000, 800.000],  loss: 251216.531250, mae: 1737.092529, mean_q: 4.481237\n",
      "wrong_move\n",
      "   2967/500000: episode: 2945, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 326.000 [326.000, 326.000],  loss: 496435.968750, mae: 1734.748535, mean_q: 4.520134\n",
      "wrong_move\n",
      "   2968/500000: episode: 2946, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3083.000 [3083.000, 3083.000],  loss: 449285.531250, mae: 1735.547363, mean_q: 4.524960\n",
      "wrong_move\n",
      "   2969/500000: episode: 2947, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1632.000 [1632.000, 1632.000],  loss: 426337.500000, mae: 1732.408813, mean_q: 4.497364\n",
      "wrong_move\n",
      "   2970/500000: episode: 2948, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1632.000 [1632.000, 1632.000],  loss: 135646.593750, mae: 1728.477905, mean_q: 4.510831\n",
      "wrong_move\n",
      "   2971/500000: episode: 2949, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 458.000 [458.000, 458.000],  loss: 93192.304688, mae: 1725.000000, mean_q: 4.406901\n",
      "wrong_move\n",
      "   2972/500000: episode: 2950, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 710.000 [710.000, 710.000],  loss: 67846.343750, mae: 1725.320068, mean_q: 4.390030\n",
      "wrong_move\n",
      "   2973/500000: episode: 2951, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3819.000 [3819.000, 3819.000],  loss: 44978.484375, mae: 1729.473877, mean_q: 4.371332\n",
      "wrong_move\n",
      "   2974/500000: episode: 2952, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 326.000 [326.000, 326.000],  loss: 70241.953125, mae: 1730.656006, mean_q: 4.361066\n",
      "wrong_move\n",
      "   2975/500000: episode: 2953, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 710.000 [710.000, 710.000],  loss: 60518.093750, mae: 1732.785156, mean_q: 4.378806\n",
      "wrong_move\n",
      "   2976/500000: episode: 2954, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1632.000 [1632.000, 1632.000],  loss: 433294.062500, mae: 1733.571899, mean_q: 4.286428\n",
      "wrong_move\n",
      "   2977/500000: episode: 2955, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1431.000 [1431.000, 1431.000],  loss: 421320.906250, mae: 1731.615601, mean_q: 4.322306\n",
      "wrong_move\n",
      "   2978/500000: episode: 2956, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 173.000 [173.000, 173.000],  loss: 19582.417969, mae: 1730.328735, mean_q: 4.322768\n",
      "wrong_move\n",
      "   2979/500000: episode: 2957, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 710.000 [710.000, 710.000],  loss: 30662.992188, mae: 1731.604126, mean_q: 4.447656\n",
      "wrong_move\n",
      "   2980/500000: episode: 2958, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 458.000 [458.000, 458.000],  loss: 418486.156250, mae: 1732.663818, mean_q: 5.613249\n",
      "wrong_move\n",
      "   2981/500000: episode: 2959, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1724.000 [1724.000, 1724.000],  loss: 91134.296875, mae: 1734.633545, mean_q: 4.459571\n",
      "wrong_move\n",
      "   2982/500000: episode: 2960, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 224.000 [224.000, 224.000],  loss: 105393.179688, mae: 1736.039307, mean_q: 12.348215\n",
      "wrong_move\n",
      "   2983/500000: episode: 2961, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3761.000 [3761.000, 3761.000],  loss: 251767.250000, mae: 1738.595459, mean_q: 24.839569\n",
      "wrong_move\n",
      "   2984/500000: episode: 2962, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 710.000 [710.000, 710.000],  loss: 987897.812500, mae: 1741.739990, mean_q: 13.856348\n",
      "wrong_move\n",
      "   2985/500000: episode: 2963, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 326.000 [326.000, 326.000],  loss: 740041.625000, mae: 1743.691650, mean_q: 17.780079\n",
      "wrong_move\n",
      "   2986/500000: episode: 2964, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 710.000 [710.000, 710.000],  loss: 58387.375000, mae: 1741.132568, mean_q: 25.566357\n",
      "wrong_move\n",
      "   2987/500000: episode: 2965, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3126.000 [3126.000, 3126.000],  loss: 183504.937500, mae: 1733.783936, mean_q: 32.723167\n",
      "wrong_move\n",
      "   2988/500000: episode: 2966, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 710.000 [710.000, 710.000],  loss: 43823.718750, mae: 1729.368408, mean_q: 39.374222\n",
      "wrong_move\n",
      "   2989/500000: episode: 2967, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1951.000 [1951.000, 1951.000],  loss: 251562.187500, mae: 1727.199829, mean_q: 43.404797\n",
      "wrong_move\n",
      "   2990/500000: episode: 2968, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4004.000 [4004.000, 4004.000],  loss: 86093.320312, mae: 1725.433960, mean_q: 31.618389\n",
      "wrong_move\n",
      "   2991/500000: episode: 2969, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 56.000 [56.000, 56.000],  loss: 432149.125000, mae: 1728.355957, mean_q: 27.543282\n",
      "wrong_move\n",
      "   2992/500000: episode: 2970, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 19227.535156, mae: 1735.104492, mean_q: 15.980453\n",
      "wrong_move\n",
      "   2993/500000: episode: 2971, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 245586.218750, mae: 1743.277100, mean_q: 9.826634\n",
      "wrong_move\n",
      "   2994/500000: episode: 2972, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1417.000 [1417.000, 1417.000],  loss: 37454.902344, mae: 1746.064941, mean_q: 23.413218\n",
      "wrong_move\n",
      "   2995/500000: episode: 2973, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3623.000 [3623.000, 3623.000],  loss: 29468.062500, mae: 1751.776978, mean_q: 24.759972\n",
      "wrong_move\n",
      "   2996/500000: episode: 2974, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1492.000 [1492.000, 1492.000],  loss: 109679.375000, mae: 1755.505615, mean_q: 17.824842\n",
      "wrong_move\n",
      "   2997/500000: episode: 2975, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4004.000 [4004.000, 4004.000],  loss: 67878.109375, mae: 1760.230957, mean_q: 23.047470\n",
      "wrong_move\n",
      "   2998/500000: episode: 2976, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 160.000 [160.000, 160.000],  loss: 228001.093750, mae: 1760.823975, mean_q: 8.694521\n",
      "wrong_move\n",
      "   2999/500000: episode: 2977, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1492.000 [1492.000, 1492.000],  loss: 599473.437500, mae: 1763.270996, mean_q: 12.658748\n",
      "wrong_move\n",
      "   3000/500000: episode: 2978, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 723.000 [723.000, 723.000],  loss: 111507.015625, mae: 1761.821045, mean_q: 10.442942\n",
      "wrong_move\n",
      "   3001/500000: episode: 2979, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1647.000 [1647.000, 1647.000],  loss: 256308.093750, mae: 1759.158447, mean_q: 10.395793\n",
      "wrong_move\n",
      "   3002/500000: episode: 2980, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2267.000 [2267.000, 2267.000],  loss: 32082.160156, mae: 1753.276855, mean_q: 9.456619\n",
      "wrong_move\n",
      "   3003/500000: episode: 2981, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 710.000 [710.000, 710.000],  loss: 64971.890625, mae: 1748.139893, mean_q: 5.216553\n",
      "wrong_move\n",
      "   3004/500000: episode: 2982, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: 192543.734375, mae: 1744.765259, mean_q: 10.317869\n",
      "wrong_move\n",
      "   3005/500000: episode: 2983, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 710.000 [710.000, 710.000],  loss: 714131.250000, mae: 1740.229492, mean_q: 8.514103\n",
      "wrong_move\n",
      "   3006/500000: episode: 2984, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2587.000 [2587.000, 2587.000],  loss: 771163.875000, mae: 1735.680664, mean_q: 10.395439\n",
      "wrong_move\n",
      "   3007/500000: episode: 2985, duration: 0.118s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2842.000 [2842.000, 2842.000],  loss: 76136.281250, mae: 1731.335205, mean_q: 4.259721\n",
      "wrong_move\n",
      "   3008/500000: episode: 2986, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 967.000 [967.000, 967.000],  loss: 100253.484375, mae: 1734.666138, mean_q: 4.260817\n",
      "wrong_move\n",
      "   3009/500000: episode: 2987, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1356.000 [1356.000, 1356.000],  loss: 430725.062500, mae: 1744.566162, mean_q: 4.338007\n",
      "wrong_move\n",
      "   3010/500000: episode: 2988, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3623.000 [3623.000, 3623.000],  loss: 150564.390625, mae: 1754.381836, mean_q: 4.310354\n",
      "wrong_move\n",
      "   3011/500000: episode: 2989, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2594.000 [2594.000, 2594.000],  loss: 31211.400391, mae: 1761.795776, mean_q: 4.345868\n",
      "wrong_move\n",
      "   3012/500000: episode: 2990, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1885.000 [1885.000, 1885.000],  loss: 116432.296875, mae: 1766.541016, mean_q: 4.327424\n",
      "wrong_move\n",
      "   3013/500000: episode: 2991, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1951.000 [1951.000, 1951.000],  loss: 281345.656250, mae: 1768.051514, mean_q: 4.277681\n",
      "wrong_move\n",
      "   3014/500000: episode: 2992, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1417.000 [1417.000, 1417.000],  loss: 65939.601562, mae: 1766.887329, mean_q: 4.327636\n",
      "wrong_move\n",
      "   3015/500000: episode: 2993, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1885.000 [1885.000, 1885.000],  loss: 96189.765625, mae: 1765.637695, mean_q: 4.305588\n",
      "wrong_move\n",
      "   3016/500000: episode: 2994, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2587.000 [2587.000, 2587.000],  loss: 130436.765625, mae: 1769.031250, mean_q: 4.326031\n",
      "wrong_move\n",
      "   3017/500000: episode: 2995, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1885.000 [1885.000, 1885.000],  loss: 112697.218750, mae: 1772.460938, mean_q: 4.243689\n",
      "wrong_move\n",
      "   3018/500000: episode: 2996, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3110.000 [3110.000, 3110.000],  loss: 70408.109375, mae: 1775.045898, mean_q: 4.242258\n",
      "wrong_move\n",
      "   3019/500000: episode: 2997, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2819.000 [2819.000, 2819.000],  loss: 54932.671875, mae: 1773.615234, mean_q: 4.362142\n",
      "wrong_move\n",
      "   3020/500000: episode: 2998, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3622.000 [3622.000, 3622.000],  loss: 446917.281250, mae: 1771.994141, mean_q: 4.358096\n",
      "wrong_move\n",
      "   3021/500000: episode: 2999, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1417.000 [1417.000, 1417.000],  loss: 30521.902344, mae: 1769.760498, mean_q: 4.371367\n",
      "wrong_move\n",
      "   3022/500000: episode: 3000, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1347.000 [1347.000, 1347.000],  loss: 78013.851562, mae: 1769.639648, mean_q: 4.392221\n",
      "wrong_move\n",
      "   3023/500000: episode: 3001, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 222.000 [222.000, 222.000],  loss: 78981.164062, mae: 1769.802734, mean_q: 4.441737\n",
      "wrong_move\n",
      "   3024/500000: episode: 3002, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3327.000 [3327.000, 3327.000],  loss: 45463.394531, mae: 1769.418579, mean_q: 4.259097\n",
      "wrong_move\n",
      "   3025/500000: episode: 3003, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3622.000 [3622.000, 3622.000],  loss: 175531.046875, mae: 1770.774048, mean_q: 4.383414\n",
      "wrong_move\n",
      "   3026/500000: episode: 3004, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 670.000 [670.000, 670.000],  loss: 557590.312500, mae: 1773.847412, mean_q: 4.419608\n",
      "wrong_move\n",
      "   3027/500000: episode: 3005, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3622.000 [3622.000, 3622.000],  loss: 58556.593750, mae: 1771.707642, mean_q: 4.258211\n",
      "wrong_move\n",
      "   3028/500000: episode: 3006, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 502.000 [502.000, 502.000],  loss: 592029.125000, mae: 1767.757324, mean_q: 4.302394\n",
      "wrong_move\n",
      "   3029/500000: episode: 3007, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1417.000 [1417.000, 1417.000],  loss: 63990.195312, mae: 1761.755493, mean_q: 4.308725\n",
      "wrong_move\n",
      "   3030/500000: episode: 3008, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3327.000 [3327.000, 3327.000],  loss: 629026.000000, mae: 1762.652710, mean_q: 4.102297\n",
      "wrong_move\n",
      "   3031/500000: episode: 3009, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3622.000 [3622.000, 3622.000],  loss: 40190.062500, mae: 1761.214478, mean_q: 4.188437\n",
      "wrong_move\n",
      "   3032/500000: episode: 3010, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3164.000 [3164.000, 3164.000],  loss: 29456.546875, mae: 1763.534668, mean_q: 4.349239\n",
      "wrong_move\n",
      "   3033/500000: episode: 3011, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 218.000 [218.000, 218.000],  loss: 51710.273438, mae: 1768.134033, mean_q: 4.205544\n",
      "wrong_move\n",
      "   3034/500000: episode: 3012, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 541.000 [541.000, 541.000],  loss: 53420.960938, mae: 1769.484497, mean_q: 4.292747\n",
      "wrong_move\n",
      "   3035/500000: episode: 3013, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3622.000 [3622.000, 3622.000],  loss: 446641.781250, mae: 1770.738525, mean_q: 4.245301\n",
      "wrong_move\n",
      "   3036/500000: episode: 3014, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3622.000 [3622.000, 3622.000],  loss: 31808.337891, mae: 1772.486938, mean_q: 4.196059\n",
      "wrong_move\n",
      "   3037/500000: episode: 3015, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3622.000 [3622.000, 3622.000],  loss: 42308.476562, mae: 1773.343506, mean_q: 4.361915\n",
      "wrong_move\n",
      "   3038/500000: episode: 3016, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3372.000 [3372.000, 3372.000],  loss: 51488.195312, mae: 1772.612549, mean_q: 4.244100\n",
      "wrong_move\n",
      "   3039/500000: episode: 3017, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3622.000 [3622.000, 3622.000],  loss: 46604.859375, mae: 1773.736572, mean_q: 4.232244\n",
      "wrong_move\n",
      "   3040/500000: episode: 3018, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 27338.673828, mae: 1770.738159, mean_q: 4.303628\n",
      "wrong_move\n",
      "   3041/500000: episode: 3019, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3527.000 [3527.000, 3527.000],  loss: 234661.671875, mae: 1769.979980, mean_q: 4.258998\n",
      "wrong_move\n",
      "   3042/500000: episode: 3020, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3622.000 [3622.000, 3622.000],  loss: 242598.437500, mae: 1770.269287, mean_q: 4.241336\n",
      "wrong_move\n",
      "   3043/500000: episode: 3021, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2587.000 [2587.000, 2587.000],  loss: 51070.839844, mae: 1769.728882, mean_q: 4.292125\n",
      "wrong_move\n",
      "   3044/500000: episode: 3022, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3622.000 [3622.000, 3622.000],  loss: 47590.308594, mae: 1768.727417, mean_q: 4.264585\n",
      "wrong_move\n",
      "   3045/500000: episode: 3023, duration: 0.118s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2791.000 [2791.000, 2791.000],  loss: 46646.011719, mae: 1773.352295, mean_q: 4.250268\n",
      "wrong_move\n",
      "   3046/500000: episode: 3024, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3063.000 [3063.000, 3063.000],  loss: 97916.867188, mae: 1776.632446, mean_q: 4.412305\n",
      "wrong_move\n",
      "   3047/500000: episode: 3025, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3327.000 [3327.000, 3327.000],  loss: 38547.390625, mae: 1774.245850, mean_q: 4.198384\n",
      "wrong_move\n",
      "   3048/500000: episode: 3026, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2853.000 [2853.000, 2853.000],  loss: 1777429.500000, mae: 1769.502563, mean_q: 4.263268\n",
      "wrong_move\n",
      "   3049/500000: episode: 3027, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1223.000 [1223.000, 1223.000],  loss: 149202.593750, mae: 1766.827148, mean_q: 4.283217\n",
      "wrong_move\n",
      "   3050/500000: episode: 3028, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1885.000 [1885.000, 1885.000],  loss: 48880.031250, mae: 1767.035889, mean_q: 4.188643\n",
      "wrong_move\n",
      "   3051/500000: episode: 3029, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1885.000 [1885.000, 1885.000],  loss: 48216.296875, mae: 1767.219604, mean_q: 4.380321\n",
      "wrong_move\n",
      "   3052/500000: episode: 3030, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 676.000 [676.000, 676.000],  loss: 286495.312500, mae: 1773.331543, mean_q: 4.242609\n",
      "wrong_move\n",
      "   3053/500000: episode: 3031, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2800.000 [2800.000, 2800.000],  loss: 440739.468750, mae: 1787.121094, mean_q: 4.210401\n",
      "wrong_move\n",
      "   3054/500000: episode: 3032, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 209.000 [209.000, 209.000],  loss: 64573.261719, mae: 1798.330078, mean_q: 4.229182\n",
      "wrong_move\n",
      "   3055/500000: episode: 3033, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1897.000 [1897.000, 1897.000],  loss: 174663.062500, mae: 1802.018555, mean_q: 4.252741\n",
      "wrong_move\n",
      "   3056/500000: episode: 3034, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1223.000 [1223.000, 1223.000],  loss: 44015.570312, mae: 1800.156494, mean_q: 4.263359\n",
      "wrong_move\n",
      "   3057/500000: episode: 3035, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 207.000 [207.000, 207.000],  loss: 95048.742188, mae: 1795.634033, mean_q: 4.219194\n",
      "wrong_move\n",
      "   3058/500000: episode: 3036, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 562.000 [562.000, 562.000],  loss: 440115.281250, mae: 1794.362549, mean_q: 4.268374\n",
      "wrong_move\n",
      "   3059/500000: episode: 3037, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3096.000 [3096.000, 3096.000],  loss: 112714.382812, mae: 1791.587158, mean_q: 4.147536\n",
      "wrong_move\n",
      "   3060/500000: episode: 3038, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1134.000 [1134.000, 1134.000],  loss: 52209.375000, mae: 1787.656738, mean_q: 4.157518\n",
      "wrong_move\n",
      "   3061/500000: episode: 3039, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3096.000 [3096.000, 3096.000],  loss: 55042.035156, mae: 1787.999023, mean_q: 4.237457\n",
      "wrong_move\n",
      "   3062/500000: episode: 3040, duration: 0.044s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 562.000 [562.000, 562.000],  loss: 327699.718750, mae: 1785.934570, mean_q: 4.165614\n",
      "wrong_move\n",
      "   3063/500000: episode: 3041, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3852.000 [3852.000, 3852.000],  loss: 94998.984375, mae: 1778.651611, mean_q: 4.186286\n",
      "wrong_move\n",
      "   3064/500000: episode: 3042, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2804.000 [2804.000, 2804.000],  loss: 138086.312500, mae: 1773.879150, mean_q: 4.214692\n",
      "wrong_move\n",
      "   3065/500000: episode: 3043, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 688.000 [688.000, 688.000],  loss: 46921.988281, mae: 1775.747070, mean_q: 4.250333\n",
      "wrong_move\n",
      "   3066/500000: episode: 3044, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 21.000 [21.000, 21.000],  loss: 62452.562500, mae: 1779.699219, mean_q: 4.167477\n",
      "wrong_move\n",
      "   3067/500000: episode: 3045, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1440.000 [1440.000, 1440.000],  loss: 37370.023438, mae: 1786.554443, mean_q: 4.221257\n",
      "wrong_move\n",
      "   3068/500000: episode: 3046, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 738.000 [738.000, 738.000],  loss: 289479.750000, mae: 1794.417114, mean_q: 4.048430\n",
      "wrong_move\n",
      "   3069/500000: episode: 3047, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3535.000 [3535.000, 3535.000],  loss: 840254.250000, mae: 1799.680176, mean_q: 4.022896\n",
      "wrong_move\n",
      "   3070/500000: episode: 3048, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3241.000 [3241.000, 3241.000],  loss: 17953.578125, mae: 1795.215820, mean_q: 4.128304\n",
      "wrong_move\n",
      "   3071/500000: episode: 3049, duration: 0.148s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 562.000 [562.000, 562.000],  loss: 62826.937500, mae: 1793.181641, mean_q: 4.195835\n",
      "wrong_move\n",
      "   3072/500000: episode: 3050, duration: 0.152s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1978.000 [1978.000, 1978.000],  loss: 272156.062500, mae: 1787.996582, mean_q: 4.071974\n",
      "wrong_move\n",
      "   3073/500000: episode: 3051, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3535.000 [3535.000, 3535.000],  loss: 35110.515625, mae: 1780.947510, mean_q: 4.046573\n",
      "wrong_move\n",
      "   3074/500000: episode: 3052, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3100.000 [3100.000, 3100.000],  loss: 302600.156250, mae: 1779.672729, mean_q: 4.227445\n",
      "wrong_move\n",
      "   3075/500000: episode: 3053, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 562.000 [562.000, 562.000],  loss: 28752.712891, mae: 1782.590332, mean_q: 4.056856\n",
      "wrong_move\n",
      "   3076/500000: episode: 3054, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 655.000 [655.000, 655.000],  loss: 299844.218750, mae: 1786.229126, mean_q: 3.975462\n",
      "wrong_move\n",
      "   3077/500000: episode: 3055, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 365.000 [365.000, 365.000],  loss: 17948.007812, mae: 1791.655762, mean_q: 4.149621\n",
      "wrong_move\n",
      "   3078/500000: episode: 3056, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 562.000 [562.000, 562.000],  loss: 77542.179688, mae: 1796.658081, mean_q: 4.001329\n",
      "wrong_move\n",
      "   3079/500000: episode: 3057, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1498.000 [1498.000, 1498.000],  loss: 98134.937500, mae: 1801.736450, mean_q: 4.009983\n",
      "wrong_move\n",
      "   3080/500000: episode: 3058, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2098.000 [2098.000, 2098.000],  loss: 123668.031250, mae: 1805.561768, mean_q: 4.078062\n",
      "wrong_move\n",
      "   3081/500000: episode: 3059, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2643.000 [2643.000, 2643.000],  loss: 55453.492188, mae: 1805.458374, mean_q: 4.106261\n",
      "wrong_move\n",
      "   3082/500000: episode: 3060, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2247.000 [2247.000, 2247.000],  loss: 163011.703125, mae: 1802.076172, mean_q: 4.037691\n",
      "wrong_move\n",
      "   3083/500000: episode: 3061, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2517.000 [2517.000, 2517.000],  loss: 34986.847656, mae: 1795.941650, mean_q: 4.046283\n",
      "wrong_move\n",
      "   3084/500000: episode: 3062, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 609.000 [609.000, 609.000],  loss: 32737.867188, mae: 1789.949463, mean_q: 4.109127\n",
      "wrong_move\n",
      "   3085/500000: episode: 3063, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2611.000 [2611.000, 2611.000],  loss: 511742.875000, mae: 1785.047974, mean_q: 4.048400\n",
      "wrong_move\n",
      "   3086/500000: episode: 3064, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3794.000 [3794.000, 3794.000],  loss: 83501.929688, mae: 1778.981445, mean_q: 3.927723\n",
      "wrong_move\n",
      "   3087/500000: episode: 3065, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2637.000 [2637.000, 2637.000],  loss: 87219.625000, mae: 1777.882324, mean_q: 4.176698\n",
      "wrong_move\n",
      "   3088/500000: episode: 3066, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2199.000 [2199.000, 2199.000],  loss: 34235.863281, mae: 1783.838867, mean_q: 4.015134\n",
      "wrong_move\n",
      "   3089/500000: episode: 3067, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3139.000 [3139.000, 3139.000],  loss: 101173.750000, mae: 1793.394287, mean_q: 4.076896\n",
      "wrong_move\n",
      "   3090/500000: episode: 3068, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2021.000 [2021.000, 2021.000],  loss: 497551.750000, mae: 1804.374512, mean_q: 4.112497\n",
      "wrong_move\n",
      "   3091/500000: episode: 3069, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3490.000 [3490.000, 3490.000],  loss: 63496.910156, mae: 1808.320068, mean_q: 4.144737\n",
      "wrong_move\n",
      "   3092/500000: episode: 3070, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2095.000 [2095.000, 2095.000],  loss: 56168.031250, mae: 1816.203613, mean_q: 4.105567\n",
      "wrong_move\n",
      "   3093/500000: episode: 3071, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3837.000 [3837.000, 3837.000],  loss: 99279.062500, mae: 1821.395264, mean_q: 4.082467\n",
      "wrong_move\n",
      "   3094/500000: episode: 3072, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3139.000 [3139.000, 3139.000],  loss: 91205.531250, mae: 1825.715332, mean_q: 4.120557\n",
      "wrong_move\n",
      "   3095/500000: episode: 3073, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 26.000 [26.000, 26.000],  loss: 519961.937500, mae: 1823.071045, mean_q: 4.174806\n",
      "wrong_move\n",
      "   3096/500000: episode: 3074, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2588.000 [2588.000, 2588.000],  loss: 209465.062500, mae: 1812.539185, mean_q: 4.195840\n",
      "wrong_move\n",
      "   3097/500000: episode: 3075, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 26.000 [26.000, 26.000],  loss: 88846.804688, mae: 1799.621094, mean_q: 3.959283\n",
      "wrong_move\n",
      "   3098/500000: episode: 3076, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 738.000 [738.000, 738.000],  loss: 381801.531250, mae: 1791.107178, mean_q: 4.050958\n",
      "wrong_move\n",
      "   3099/500000: episode: 3077, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 26.000 [26.000, 26.000],  loss: 510455.625000, mae: 1781.560059, mean_q: 4.057073\n",
      "wrong_move\n",
      "   3100/500000: episode: 3078, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2247.000 [2247.000, 2247.000],  loss: 412454.312500, mae: 1781.198242, mean_q: 4.023046\n",
      "wrong_move\n",
      "   3101/500000: episode: 3079, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2056.000 [2056.000, 2056.000],  loss: 138562.093750, mae: 1789.090576, mean_q: 4.013910\n",
      "wrong_move\n",
      "   3102/500000: episode: 3080, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1281.000 [1281.000, 1281.000],  loss: 840639.437500, mae: 1800.078979, mean_q: 4.057178\n",
      "wrong_move\n",
      "   3103/500000: episode: 3081, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2769.000 [2769.000, 2769.000],  loss: 84976.953125, mae: 1808.636719, mean_q: 4.103728\n",
      "wrong_move\n",
      "   3104/500000: episode: 3082, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 738.000 [738.000, 738.000],  loss: 35491.867188, mae: 1819.517334, mean_q: 4.051522\n",
      "wrong_move\n",
      "   3105/500000: episode: 3083, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 200.000 [200.000, 200.000],  loss: 418901.062500, mae: 1827.213989, mean_q: 3.993374\n",
      "wrong_move\n",
      "   3106/500000: episode: 3084, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1193.000 [1193.000, 1193.000],  loss: 31406.722656, mae: 1832.159546, mean_q: 4.061359\n",
      "wrong_move\n",
      "   3107/500000: episode: 3085, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1193.000 [1193.000, 1193.000],  loss: 121956.218750, mae: 1834.601074, mean_q: 4.038065\n",
      "wrong_move\n",
      "   3108/500000: episode: 3086, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 573.000 [573.000, 573.000],  loss: 43900.109375, mae: 1839.231445, mean_q: 4.101740\n",
      "wrong_move\n",
      "   3109/500000: episode: 3087, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1511.000 [1511.000, 1511.000],  loss: 53175.187500, mae: 1840.164185, mean_q: 4.000805\n",
      "wrong_move\n",
      "   3110/500000: episode: 3088, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1303.000 [1303.000, 1303.000],  loss: 145343.906250, mae: 1838.750854, mean_q: 3.983132\n",
      "wrong_move\n",
      "   3111/500000: episode: 3089, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2776.000 [2776.000, 2776.000],  loss: 42512.789062, mae: 1831.660400, mean_q: 4.076367\n",
      "wrong_move\n",
      "   3112/500000: episode: 3090, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1193.000 [1193.000, 1193.000],  loss: 127496.468750, mae: 1823.500000, mean_q: 3.927207\n",
      "wrong_move\n",
      "   3113/500000: episode: 3091, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2862.000 [2862.000, 2862.000],  loss: 40055.523438, mae: 1811.486938, mean_q: 3.917083\n",
      "wrong_move\n",
      "   3114/500000: episode: 3092, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 573.000 [573.000, 573.000],  loss: 221971.250000, mae: 1802.846924, mean_q: 4.090098\n",
      "wrong_move\n",
      "   3115/500000: episode: 3093, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: 518685.250000, mae: 1799.003174, mean_q: 3.975424\n",
      "wrong_move\n",
      "   3116/500000: episode: 3094, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 880.000 [880.000, 880.000],  loss: 1212897.000000, mae: 1804.459351, mean_q: 3.973261\n",
      "wrong_move\n",
      "   3117/500000: episode: 3095, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2517.000 [2517.000, 2517.000],  loss: 87168.210938, mae: 1814.529053, mean_q: 3.979468\n",
      "wrong_move\n",
      "   3118/500000: episode: 3096, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2149.000 [2149.000, 2149.000],  loss: 54616.855469, mae: 1825.828857, mean_q: 4.000289\n",
      "wrong_move\n",
      "   3119/500000: episode: 3097, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2880.000 [2880.000, 2880.000],  loss: 39916.507812, mae: 1840.815674, mean_q: 4.037267\n",
      "wrong_move\n",
      "   3120/500000: episode: 3098, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1137.000 [1137.000, 1137.000],  loss: 77799.890625, mae: 1850.217529, mean_q: 4.017280\n",
      "wrong_move\n",
      "   3121/500000: episode: 3099, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3181.000 [3181.000, 3181.000],  loss: 198193.062500, mae: 1853.958740, mean_q: 4.012022\n",
      "wrong_move\n",
      "   3122/500000: episode: 3100, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 272.000 [272.000, 272.000],  loss: 33743.593750, mae: 1853.686646, mean_q: 4.044993\n",
      "wrong_move\n",
      "   3123/500000: episode: 3101, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2551.000 [2551.000, 2551.000],  loss: 278439.156250, mae: 1854.375122, mean_q: 4.080594\n",
      "wrong_move\n",
      "   3124/500000: episode: 3102, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 161.000 [161.000, 161.000],  loss: 373494.093750, mae: 1848.220947, mean_q: 4.072748\n",
      "wrong_move\n",
      "   3125/500000: episode: 3103, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 880.000 [880.000, 880.000],  loss: 757050.125000, mae: 1841.195801, mean_q: 4.090136\n",
      "wrong_move\n",
      "   3126/500000: episode: 3104, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1141.000 [1141.000, 1141.000],  loss: 42872.867188, mae: 1839.246338, mean_q: 4.066843\n",
      "wrong_move\n",
      "   3127/500000: episode: 3105, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 554.000 [554.000, 554.000],  loss: 422172.531250, mae: 1837.373535, mean_q: 4.081655\n",
      "wrong_move\n",
      "   3128/500000: episode: 3106, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3075.000 [3075.000, 3075.000],  loss: 53227.664062, mae: 1839.009155, mean_q: 4.019001\n",
      "wrong_move\n",
      "   3129/500000: episode: 3107, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3017.000 [3017.000, 3017.000],  loss: 133198.312500, mae: 1847.215210, mean_q: 4.046909\n",
      "wrong_move\n",
      "   3130/500000: episode: 3108, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3118.000 [3118.000, 3118.000],  loss: 114238.750000, mae: 1860.386719, mean_q: 4.082591\n",
      "wrong_move\n",
      "   3131/500000: episode: 3109, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 233.000 [233.000, 233.000],  loss: 29364.078125, mae: 1868.323242, mean_q: 4.037889\n",
      "wrong_move\n",
      "   3132/500000: episode: 3110, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2862.000 [2862.000, 2862.000],  loss: 458747.843750, mae: 1869.322510, mean_q: 4.050365\n",
      "wrong_move\n",
      "   3133/500000: episode: 3111, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 371.000 [371.000, 371.000],  loss: 385532.812500, mae: 1871.657837, mean_q: 4.028012\n",
      "wrong_move\n",
      "   3134/500000: episode: 3112, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4094.000 [4094.000, 4094.000],  loss: 41389.804688, mae: 1868.123535, mean_q: 3.936444\n",
      "wrong_move\n",
      "   3135/500000: episode: 3113, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 200.000 [200.000, 200.000],  loss: 447293.812500, mae: 1863.594238, mean_q: 3.963469\n",
      "wrong_move\n",
      "   3136/500000: episode: 3114, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 161.000 [161.000, 161.000],  loss: 420423.718750, mae: 1856.514404, mean_q: 4.047993\n",
      "wrong_move\n",
      "   3137/500000: episode: 3115, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2174.000 [2174.000, 2174.000],  loss: 56535.914062, mae: 1848.310547, mean_q: 4.103612\n",
      "wrong_move\n",
      "   3138/500000: episode: 3116, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2935.000 [2935.000, 2935.000],  loss: 168929.531250, mae: 1846.120239, mean_q: 4.055696\n",
      "wrong_move\n",
      "   3139/500000: episode: 3117, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 161.000 [161.000, 161.000],  loss: 71304.585938, mae: 1841.624023, mean_q: 16.556746\n",
      "wrong_move\n",
      "   3140/500000: episode: 3118, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1552.000 [1552.000, 1552.000],  loss: 464064.906250, mae: 1843.130981, mean_q: 4.027765\n",
      "wrong_move\n",
      "   3141/500000: episode: 3119, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1552.000 [1552.000, 1552.000],  loss: 133448.390625, mae: 1844.786255, mean_q: 4.046149\n",
      "wrong_move\n",
      "   3142/500000: episode: 3120, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 161.000 [161.000, 161.000],  loss: 640438.437500, mae: 1852.306152, mean_q: 4.027749\n",
      "wrong_move\n",
      "   3143/500000: episode: 3121, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2371.000 [2371.000, 2371.000],  loss: 83833.140625, mae: 1859.315430, mean_q: 4.110666\n",
      "wrong_move\n",
      "   3144/500000: episode: 3122, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 598.000 [598.000, 598.000],  loss: 483916.843750, mae: 1862.133423, mean_q: 4.028444\n",
      "wrong_move\n",
      "   3145/500000: episode: 3123, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3510.000 [3510.000, 3510.000],  loss: 42803.507812, mae: 1860.849121, mean_q: 3.959896\n",
      "wrong_move\n",
      "   3146/500000: episode: 3124, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3075.000 [3075.000, 3075.000],  loss: 50339.726562, mae: 1852.393799, mean_q: 4.197807\n",
      "wrong_move\n",
      "   3147/500000: episode: 3125, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3075.000 [3075.000, 3075.000],  loss: 124888.281250, mae: 1838.424072, mean_q: 16.684786\n",
      "wrong_move\n",
      "   3148/500000: episode: 3126, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3510.000 [3510.000, 3510.000],  loss: 50950.015625, mae: 1827.499756, mean_q: 4.024175\n",
      "wrong_move\n",
      "   3149/500000: episode: 3127, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 147.000 [147.000, 147.000],  loss: 51338.953125, mae: 1819.579102, mean_q: 4.100847\n",
      "wrong_move\n",
      "   3150/500000: episode: 3128, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3510.000 [3510.000, 3510.000],  loss: 38928.050781, mae: 1815.317871, mean_q: 4.069659\n",
      "wrong_move\n",
      "   3151/500000: episode: 3129, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 686.000 [686.000, 686.000],  loss: 33972.804688, mae: 1819.034912, mean_q: 4.010319\n",
      "wrong_move\n",
      "   3152/500000: episode: 3130, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1478.000 [1478.000, 1478.000],  loss: 442781.343750, mae: 1826.091553, mean_q: 4.110831\n",
      "wrong_move\n",
      "   3153/500000: episode: 3131, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3510.000 [3510.000, 3510.000],  loss: 425007.343750, mae: 1838.205811, mean_q: 4.037361\n",
      "wrong_move\n",
      "   3154/500000: episode: 3132, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3510.000 [3510.000, 3510.000],  loss: 533843.625000, mae: 1851.047119, mean_q: 4.078354\n",
      "wrong_move\n",
      "   3155/500000: episode: 3133, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1649.000 [1649.000, 1649.000],  loss: 368311.812500, mae: 1861.386475, mean_q: 4.001890\n",
      "wrong_move\n",
      "   3156/500000: episode: 3134, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 694.000 [694.000, 694.000],  loss: 43052.972656, mae: 1869.481201, mean_q: 4.012784\n",
      "wrong_move\n",
      "   3157/500000: episode: 3135, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1301.000 [1301.000, 1301.000],  loss: 54508.574219, mae: 1873.135376, mean_q: 4.132228\n",
      "wrong_move\n",
      "   3158/500000: episode: 3136, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3945.000 [3945.000, 3945.000],  loss: 52669.687500, mae: 1874.110352, mean_q: 25.245287\n",
      "wrong_move\n",
      "   3159/500000: episode: 3137, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1122.000 [1122.000, 1122.000],  loss: 467138.750000, mae: 1876.342041, mean_q: 4.132760\n",
      "wrong_move\n",
      "   3160/500000: episode: 3138, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1478.000 [1478.000, 1478.000],  loss: 455312.687500, mae: 1879.462891, mean_q: 11.592288\n",
      "wrong_move\n",
      "   3161/500000: episode: 3139, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 450.000 [450.000, 450.000],  loss: 417789.125000, mae: 1880.411377, mean_q: 4.087021\n",
      "wrong_move\n",
      "   3162/500000: episode: 3140, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1478.000 [1478.000, 1478.000],  loss: 42700.718750, mae: 1872.303711, mean_q: 4.137335\n",
      "wrong_move\n",
      "   3163/500000: episode: 3141, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2291.000 [2291.000, 2291.000],  loss: 465489.343750, mae: 1859.957520, mean_q: 4.383471\n",
      "wrong_move\n",
      "   3164/500000: episode: 3142, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1478.000 [1478.000, 1478.000],  loss: 127741.093750, mae: 1850.586426, mean_q: 4.073313\n",
      "wrong_move\n",
      "   3165/500000: episode: 3143, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1277.000 [1277.000, 1277.000],  loss: 419552.906250, mae: 1843.102661, mean_q: 4.094874\n",
      "wrong_move\n",
      "   3166/500000: episode: 3144, duration: 0.143s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1306.000 [1306.000, 1306.000],  loss: 828480.562500, mae: 1841.745117, mean_q: 4.013301\n",
      "wrong_move\n",
      "   3167/500000: episode: 3145, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1277.000 [1277.000, 1277.000],  loss: 204173.406250, mae: 1849.026611, mean_q: 36.054665\n",
      "wrong_move\n",
      "   3168/500000: episode: 3146, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 62334.125000, mae: 1861.134521, mean_q: 3.957898\n",
      "wrong_move\n",
      "   3169/500000: episode: 3147, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1277.000 [1277.000, 1277.000],  loss: 80852.921875, mae: 1874.923828, mean_q: 4.158581\n",
      "wrong_move\n",
      "   3170/500000: episode: 3148, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1277.000 [1277.000, 1277.000],  loss: 180602.906250, mae: 1882.069580, mean_q: 10.074413\n",
      "wrong_move\n",
      "   3171/500000: episode: 3149, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3944.000 [3944.000, 3944.000],  loss: 46795.035156, mae: 1888.745361, mean_q: 4.102839\n",
      "wrong_move\n",
      "   3172/500000: episode: 3150, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 79.000 [79.000, 79.000],  loss: 38737.839844, mae: 1886.597656, mean_q: 19.661537\n",
      "wrong_move\n",
      "   3173/500000: episode: 3151, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1917.000 [1917.000, 1917.000],  loss: 66575.460938, mae: 1877.962524, mean_q: 4.176128\n",
      "wrong_move\n",
      "   3174/500000: episode: 3152, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1917.000 [1917.000, 1917.000],  loss: 110658.312500, mae: 1868.159668, mean_q: 34.352318\n",
      "wrong_move\n",
      "   3175/500000: episode: 3153, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2992.000 [2992.000, 2992.000],  loss: 468810.500000, mae: 1857.427856, mean_q: 4.171016\n",
      "wrong_move\n",
      "   3176/500000: episode: 3154, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2279.000 [2279.000, 2279.000],  loss: 63411.480469, mae: 1849.715576, mean_q: 17.382872\n",
      "wrong_move\n",
      "   3177/500000: episode: 3155, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3002.000 [3002.000, 3002.000],  loss: 106848.640625, mae: 1846.990845, mean_q: 4.116043\n",
      "wrong_move\n",
      "   3178/500000: episode: 3156, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1422.000 [1422.000, 1422.000],  loss: 77391.546875, mae: 1848.510498, mean_q: 3.957080\n",
      "wrong_move\n",
      "   3179/500000: episode: 3157, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3705.000 [3705.000, 3705.000],  loss: 294577.531250, mae: 1846.392456, mean_q: 4.086021\n",
      "wrong_move\n",
      "   3180/500000: episode: 3158, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2920.000 [2920.000, 2920.000],  loss: 45704.703125, mae: 1855.668701, mean_q: 4.236174\n",
      "wrong_move\n",
      "   3181/500000: episode: 3159, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2683.000 [2683.000, 2683.000],  loss: 37146.011719, mae: 1864.973511, mean_q: 4.136244\n",
      "wrong_move\n",
      "   3182/500000: episode: 3160, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 528.000 [528.000, 528.000],  loss: 43828.179688, mae: 1870.290283, mean_q: 7.193730\n",
      "wrong_move\n",
      "   3183/500000: episode: 3161, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 598.000 [598.000, 598.000],  loss: 47521.425781, mae: 1869.954224, mean_q: 4.037941\n",
      "wrong_move\n",
      "   3184/500000: episode: 3162, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2683.000 [2683.000, 2683.000],  loss: 469812.875000, mae: 1870.439209, mean_q: 4.136181\n",
      "wrong_move\n",
      "   3185/500000: episode: 3163, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2683.000 [2683.000, 2683.000],  loss: 44233.863281, mae: 1871.505371, mean_q: 6.011953\n",
      "wrong_move\n",
      "   3186/500000: episode: 3164, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3988.000 [3988.000, 3988.000],  loss: 114598.328125, mae: 1877.107056, mean_q: 4.073542\n",
      "wrong_move\n",
      "   3187/500000: episode: 3165, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2265.000 [2265.000, 2265.000],  loss: 485792.031250, mae: 1879.793457, mean_q: 4.175867\n",
      "wrong_move\n",
      "   3188/500000: episode: 3166, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2683.000 [2683.000, 2683.000],  loss: 84787.015625, mae: 1883.052734, mean_q: 4.114538\n",
      "wrong_move\n",
      "   3189/500000: episode: 3167, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2419.000 [2419.000, 2419.000],  loss: 166851.687500, mae: 1879.926514, mean_q: 4.087589\n",
      "wrong_move\n",
      "   3190/500000: episode: 3168, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3424.000 [3424.000, 3424.000],  loss: 30616.066406, mae: 1870.327393, mean_q: 3.955925\n",
      "wrong_move\n",
      "   3191/500000: episode: 3169, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 97.000 [97.000, 97.000],  loss: 29021.123047, mae: 1858.422852, mean_q: 4.028965\n",
      "wrong_move\n",
      "   3192/500000: episode: 3170, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 276.000 [276.000, 276.000],  loss: 44522.257812, mae: 1850.283447, mean_q: 18.661963\n",
      "wrong_move\n",
      "   3193/500000: episode: 3171, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3125.000 [3125.000, 3125.000],  loss: 58263.750000, mae: 1847.804932, mean_q: 4.008538\n",
      "wrong_move\n",
      "   3194/500000: episode: 3172, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3060.000 [3060.000, 3060.000],  loss: 430676.125000, mae: 1850.374268, mean_q: 37.627762\n",
      "wrong_move\n",
      "   3195/500000: episode: 3173, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2658.000 [2658.000, 2658.000],  loss: 167288.453125, mae: 1859.394409, mean_q: 4.191812\n",
      "wrong_move\n",
      "   3196/500000: episode: 3174, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1728.000 [1728.000, 1728.000],  loss: 138239.781250, mae: 1871.846924, mean_q: 48.329727\n",
      "wrong_move\n",
      "   3197/500000: episode: 3175, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2348.000 [2348.000, 2348.000],  loss: 132896.875000, mae: 1881.631836, mean_q: 26.936953\n",
      "wrong_move\n",
      "   3198/500000: episode: 3176, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2452.000 [2452.000, 2452.000],  loss: 440236.062500, mae: 1887.543457, mean_q: 4.100912\n",
      "wrong_move\n",
      "   3199/500000: episode: 3177, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2899.000 [2899.000, 2899.000],  loss: 106799.234375, mae: 1892.488159, mean_q: 13.125576\n",
      "wrong_move\n",
      "   3200/500000: episode: 3178, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1440.000 [1440.000, 1440.000],  loss: 106549.890625, mae: 1898.189941, mean_q: 47.647049\n",
      "wrong_move\n",
      "   3201/500000: episode: 3179, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1575.000 [1575.000, 1575.000],  loss: 42095.636719, mae: 1899.912231, mean_q: 4.054489\n",
      "wrong_move\n",
      "   3202/500000: episode: 3180, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2711.000 [2711.000, 2711.000],  loss: 29878.419922, mae: 1896.238403, mean_q: 4.119505\n",
      "wrong_move\n",
      "   3203/500000: episode: 3181, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 16.000 [16.000, 16.000],  loss: 481953.656250, mae: 1892.678589, mean_q: 4.088449\n",
      "wrong_move\n",
      "   3204/500000: episode: 3182, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1903.000 [1903.000, 1903.000],  loss: 849888.687500, mae: 1883.289185, mean_q: 30.880558\n",
      "wrong_move\n",
      "   3205/500000: episode: 3183, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 392.000 [392.000, 392.000],  loss: 385916.000000, mae: 1868.865723, mean_q: 4.074403\n",
      "wrong_move\n",
      "   3206/500000: episode: 3184, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 216.000 [216.000, 216.000],  loss: 62343.414062, mae: 1855.279663, mean_q: 3.944369\n",
      "wrong_move\n",
      "   3207/500000: episode: 3185, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2077.000 [2077.000, 2077.000],  loss: 425296.343750, mae: 1848.076416, mean_q: 4.097337\n",
      "wrong_move\n",
      "   3208/500000: episode: 3186, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3002.000 [3002.000, 3002.000],  loss: 51723.082031, mae: 1848.216797, mean_q: 13.119078\n",
      "wrong_move\n",
      "   3209/500000: episode: 3187, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 343.000 [343.000, 343.000],  loss: 52051.062500, mae: 1846.867188, mean_q: 4.141219\n",
      "wrong_move\n",
      "   3210/500000: episode: 3188, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3735.000 [3735.000, 3735.000],  loss: 103646.054688, mae: 1845.410889, mean_q: 23.399462\n",
      "wrong_move\n",
      "   3211/500000: episode: 3189, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 343.000 [343.000, 343.000],  loss: 49482.171875, mae: 1843.713135, mean_q: 3.997682\n",
      "wrong_move\n",
      "   3212/500000: episode: 3190, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 832.000 [832.000, 832.000],  loss: 59540.066406, mae: 1838.274170, mean_q: 4.106870\n",
      "wrong_move\n",
      "   3213/500000: episode: 3191, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1890.000 [1890.000, 1890.000],  loss: 152476.218750, mae: 1837.453735, mean_q: 22.138630\n",
      "wrong_move\n",
      "   3214/500000: episode: 3192, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 343.000 [343.000, 343.000],  loss: 338778.000000, mae: 1841.540283, mean_q: 4.094151\n",
      "wrong_move\n",
      "   3215/500000: episode: 3193, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3600.000 [3600.000, 3600.000],  loss: 74493.609375, mae: 1841.301880, mean_q: 4.102634\n",
      "wrong_move\n",
      "   3216/500000: episode: 3194, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1293.000 [1293.000, 1293.000],  loss: 382298.562500, mae: 1846.103760, mean_q: 4.140399\n",
      "wrong_move\n",
      "   3217/500000: episode: 3195, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 343.000 [343.000, 343.000],  loss: 44337.398438, mae: 1856.475952, mean_q: 62.819443\n",
      "wrong_move\n",
      "   3218/500000: episode: 3196, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 343.000 [343.000, 343.000],  loss: 424078.375000, mae: 1863.977783, mean_q: 4.088598\n",
      "wrong_move\n",
      "   3219/500000: episode: 3197, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 797.000 [797.000, 797.000],  loss: 55029.343750, mae: 1871.607056, mean_q: 4.076703\n",
      "wrong_move\n",
      "   3220/500000: episode: 3198, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 216.000 [216.000, 216.000],  loss: 55991.980469, mae: 1879.579102, mean_q: 8.220136\n",
      "wrong_move\n",
      "   3221/500000: episode: 3199, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2547.000 [2547.000, 2547.000],  loss: 41236.281250, mae: 1884.011963, mean_q: 58.400524\n",
      "wrong_move\n",
      "   3222/500000: episode: 3200, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 797.000 [797.000, 797.000],  loss: 66508.859375, mae: 1888.431641, mean_q: 48.269188\n",
      "wrong_move\n",
      "   3223/500000: episode: 3201, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1877.000 [1877.000, 1877.000],  loss: 433815.218750, mae: 1888.354736, mean_q: 4.209313\n",
      "wrong_move\n",
      "   3224/500000: episode: 3202, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3808.000 [3808.000, 3808.000],  loss: 63208.816406, mae: 1886.745850, mean_q: 45.891071\n",
      "wrong_move\n",
      "   3225/500000: episode: 3203, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 261.000 [261.000, 261.000],  loss: 69686.546875, mae: 1889.780884, mean_q: 59.945766\n",
      "wrong_move\n",
      "   3227/500000: episode: 3204, duration: 0.148s, episode steps:   2, steps per second:  13, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2029.500 [1160.000, 2899.000],  loss: 408795.250000, mae: 1892.591797, mean_q: 42.792561\n",
      "wrong_move\n",
      "   3228/500000: episode: 3205, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 832.000 [832.000, 832.000],  loss: 78806.921875, mae: 1891.767700, mean_q: 5.018312\n",
      "wrong_move\n",
      "   3229/500000: episode: 3206, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2899.000 [2899.000, 2899.000],  loss: 800738.500000, mae: 1884.654297, mean_q: 49.817764\n",
      "wrong_move\n",
      "   3230/500000: episode: 3207, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2899.000 [2899.000, 2899.000],  loss: 185851.000000, mae: 1877.632080, mean_q: 22.868484\n",
      "wrong_move\n",
      "   3231/500000: episode: 3208, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1557.000 [1557.000, 1557.000],  loss: 452006.750000, mae: 1877.577148, mean_q: 4.142012\n",
      "wrong_move\n",
      "   3232/500000: episode: 3209, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 261.000 [261.000, 261.000],  loss: 90907.414062, mae: 1881.737793, mean_q: 54.603584\n",
      "wrong_move\n",
      "   3233/500000: episode: 3210, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 832.000 [832.000, 832.000],  loss: 641087.062500, mae: 1885.724365, mean_q: 33.864651\n",
      "wrong_move\n",
      "   3234/500000: episode: 3211, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1557.000 [1557.000, 1557.000],  loss: 191571.796875, mae: 1884.257324, mean_q: 6.689342\n",
      "wrong_move\n",
      "   3235/500000: episode: 3212, duration: 0.133s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 261.000 [261.000, 261.000],  loss: 261792.531250, mae: 1891.097412, mean_q: 44.657944\n",
      "wrong_move\n",
      "   3236/500000: episode: 3213, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3536.000 [3536.000, 3536.000],  loss: 51565.789062, mae: 1893.143066, mean_q: 64.587173\n",
      "wrong_move\n",
      "   3237/500000: episode: 3214, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3182.000 [3182.000, 3182.000],  loss: 91744.789062, mae: 1898.014526, mean_q: 45.671783\n",
      "wrong_move\n",
      "   3238/500000: episode: 3215, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 832.000 [832.000, 832.000],  loss: 119513.906250, mae: 1905.555054, mean_q: 4.112613\n",
      "wrong_move\n",
      "   3239/500000: episode: 3216, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3900.000 [3900.000, 3900.000],  loss: 47931.027344, mae: 1911.795654, mean_q: 8.531123\n",
      "wrong_move\n",
      "   3240/500000: episode: 3217, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 832.000 [832.000, 832.000],  loss: 78228.289062, mae: 1911.864624, mean_q: 12.423645\n",
      "wrong_move\n",
      "   3241/500000: episode: 3218, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1614.000 [1614.000, 1614.000],  loss: 256355.781250, mae: 1909.869263, mean_q: 19.600647\n",
      "wrong_move\n",
      "   3242/500000: episode: 3219, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3298.000 [3298.000, 3298.000],  loss: 713489.375000, mae: 1906.211792, mean_q: 38.839760\n",
      "wrong_move\n",
      "   3243/500000: episode: 3220, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3188.000 [3188.000, 3188.000],  loss: 435928.812500, mae: 1902.659912, mean_q: 25.771725\n",
      "wrong_move\n",
      "   3244/500000: episode: 3221, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 832.000 [832.000, 832.000],  loss: 49218.640625, mae: 1895.974854, mean_q: 64.762978\n",
      "wrong_move\n",
      "   3245/500000: episode: 3222, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3598.000 [3598.000, 3598.000],  loss: 35484.476562, mae: 1893.887573, mean_q: 4.206511\n",
      "wrong_move\n",
      "   3246/500000: episode: 3223, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2990.000 [2990.000, 2990.000],  loss: 44330.718750, mae: 1897.365479, mean_q: 44.971626\n",
      "wrong_move\n",
      "   3247/500000: episode: 3224, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2335.000 [2335.000, 2335.000],  loss: 64465.011719, mae: 1902.763428, mean_q: 4.209658\n",
      "wrong_move\n",
      "   3248/500000: episode: 3225, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1283.000 [1283.000, 1283.000],  loss: 152324.593750, mae: 1908.359863, mean_q: 4.119413\n",
      "wrong_move\n",
      "   3249/500000: episode: 3226, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 832.000 [832.000, 832.000],  loss: 167767.781250, mae: 1914.577637, mean_q: 26.816444\n",
      "wrong_move\n",
      "   3250/500000: episode: 3227, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2311.000 [2311.000, 2311.000],  loss: 497130.250000, mae: 1920.272705, mean_q: 9.847733\n",
      "wrong_move\n",
      "   3251/500000: episode: 3228, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3640.000 [3640.000, 3640.000],  loss: 79270.234375, mae: 1924.720825, mean_q: 39.604237\n",
      "wrong_move\n",
      "   3252/500000: episode: 3229, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 666.000 [666.000, 666.000],  loss: 56751.781250, mae: 1924.203491, mean_q: 37.805466\n",
      "wrong_move\n",
      "   3253/500000: episode: 3230, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3935.000 [3935.000, 3935.000],  loss: 48331.652344, mae: 1919.522705, mean_q: 8.250156\n",
      "wrong_move\n",
      "   3254/500000: episode: 3231, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1687.000 [1687.000, 1687.000],  loss: 69884.406250, mae: 1912.533691, mean_q: 4.122302\n",
      "wrong_move\n",
      "   3255/500000: episode: 3232, duration: 0.156s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3322.000 [3322.000, 3322.000],  loss: 91905.750000, mae: 1901.854492, mean_q: 34.318195\n",
      "wrong_move\n",
      "   3256/500000: episode: 3233, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 467.000 [467.000, 467.000],  loss: 676317.125000, mae: 1897.433838, mean_q: 3.991941\n",
      "wrong_move\n",
      "   3257/500000: episode: 3234, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1706.000 [1706.000, 1706.000],  loss: 62689.644531, mae: 1892.676514, mean_q: 4.877754\n",
      "wrong_move\n",
      "   3258/500000: episode: 3235, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1437.000 [1437.000, 1437.000],  loss: 17940.203125, mae: 1893.574219, mean_q: 28.927534\n",
      "wrong_move\n",
      "   3259/500000: episode: 3236, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1116.000 [1116.000, 1116.000],  loss: 57508.757812, mae: 1900.415771, mean_q: 6.651752\n",
      "wrong_move\n",
      "   3260/500000: episode: 3237, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3640.000 [3640.000, 3640.000],  loss: 34230.054688, mae: 1907.225098, mean_q: 4.105350\n",
      "wrong_move\n",
      "   3262/500000: episode: 3238, duration: 0.140s, episode steps:   2, steps per second:  14, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 974.000 [974.000, 974.000],  loss: 473583.093750, mae: 1910.943481, mean_q: 30.731012\n",
      "wrong_move\n",
      "   3263/500000: episode: 3239, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3382.000 [3382.000, 3382.000],  loss: 99007.234375, mae: 1915.542725, mean_q: 4.119752\n",
      "wrong_move\n",
      "   3264/500000: episode: 3240, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1076.000 [1076.000, 1076.000],  loss: 449852.812500, mae: 1912.990723, mean_q: 14.572503\n",
      "wrong_move\n",
      "   3265/500000: episode: 3241, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1706.000 [1706.000, 1706.000],  loss: 264959.781250, mae: 1914.016235, mean_q: 4.136396\n",
      "wrong_move\n",
      "   3266/500000: episode: 3242, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1302.000 [1302.000, 1302.000],  loss: 139229.156250, mae: 1911.202393, mean_q: 4.063219\n",
      "wrong_move\n",
      "   3267/500000: episode: 3243, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3288.000 [3288.000, 3288.000],  loss: 45343.898438, mae: 1905.743042, mean_q: 4.076273\n",
      "wrong_move\n",
      "   3268/500000: episode: 3244, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 844.000 [844.000, 844.000],  loss: 214060.343750, mae: 1899.443481, mean_q: 4.046296\n",
      "wrong_move\n",
      "   3269/500000: episode: 3245, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1706.000 [1706.000, 1706.000],  loss: 53028.523438, mae: 1893.914795, mean_q: 57.541016\n",
      "wrong_move\n",
      "   3270/500000: episode: 3246, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1706.000 [1706.000, 1706.000],  loss: 450959.531250, mae: 1890.335571, mean_q: 30.425095\n",
      "wrong_move\n",
      "   3271/500000: episode: 3247, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 467.000 [467.000, 467.000],  loss: 137265.437500, mae: 1882.600220, mean_q: 41.795017\n",
      "wrong_move\n",
      "   3272/500000: episode: 3248, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 98.000 [98.000, 98.000],  loss: 211441.625000, mae: 1878.005127, mean_q: 19.762774\n",
      "wrong_move\n",
      "   3273/500000: episode: 3249, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 98.000 [98.000, 98.000],  loss: 50796.007812, mae: 1879.103760, mean_q: 4.069679\n",
      "wrong_move\n",
      "   3274/500000: episode: 3250, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2144.000 [2144.000, 2144.000],  loss: 144980.562500, mae: 1881.726318, mean_q: 13.600868\n",
      "wrong_move\n",
      "   3275/500000: episode: 3251, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3233.000 [3233.000, 3233.000],  loss: 92006.046875, mae: 1887.279785, mean_q: 27.810291\n",
      "wrong_move\n",
      "   3276/500000: episode: 3252, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2644.000 [2644.000, 2644.000],  loss: 74016.234375, mae: 1892.036987, mean_q: 6.197866\n",
      "wrong_move\n",
      "   3277/500000: episode: 3253, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 183.000 [183.000, 183.000],  loss: 92920.460938, mae: 1903.179688, mean_q: 4.097255\n",
      "wrong_move\n",
      "   3278/500000: episode: 3254, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1077.000 [1077.000, 1077.000],  loss: 36149.031250, mae: 1917.438721, mean_q: 4.105128\n",
      "wrong_move\n",
      "   3279/500000: episode: 3255, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1821.000 [1821.000, 1821.000],  loss: 81600.320312, mae: 1928.333862, mean_q: 4.124946\n",
      "wrong_move\n",
      "   3280/500000: episode: 3256, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3289.000 [3289.000, 3289.000],  loss: 88994.265625, mae: 1936.136353, mean_q: 4.073362\n",
      "wrong_move\n",
      "   3281/500000: episode: 3257, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1005.000 [1005.000, 1005.000],  loss: 422290.500000, mae: 1941.597656, mean_q: 5.748995\n",
      "wrong_move\n",
      "   3282/500000: episode: 3258, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1815.000 [1815.000, 1815.000],  loss: 21338.849609, mae: 1943.269287, mean_q: 4.127392\n",
      "wrong_move\n",
      "   3283/500000: episode: 3259, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2272.000 [2272.000, 2272.000],  loss: 84915.171875, mae: 1944.207397, mean_q: 4.120723\n",
      "wrong_move\n",
      "   3284/500000: episode: 3260, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2297.000 [2297.000, 2297.000],  loss: 53149.019531, mae: 1943.229248, mean_q: 38.130962\n",
      "wrong_move\n",
      "   3285/500000: episode: 3261, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1005.000 [1005.000, 1005.000],  loss: 48001.007812, mae: 1937.582764, mean_q: 7.888362\n",
      "wrong_move\n",
      "   3286/500000: episode: 3262, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 844.000 [844.000, 844.000],  loss: 52739.300781, mae: 1925.066284, mean_q: 4.113222\n",
      "wrong_move\n",
      "   3287/500000: episode: 3263, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2262.000 [2262.000, 2262.000],  loss: 69087.390625, mae: 1915.361206, mean_q: 15.277982\n",
      "wrong_move\n",
      "   3288/500000: episode: 3264, duration: 0.141s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1313.000 [1313.000, 1313.000],  loss: 50057.199219, mae: 1909.756104, mean_q: 4.098121\n",
      "wrong_move\n",
      "   3289/500000: episode: 3265, duration: 0.169s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3719.000 [3719.000, 3719.000],  loss: 512691.781250, mae: 1904.345581, mean_q: 4.110988\n",
      "wrong_move\n",
      "   3290/500000: episode: 3266, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3719.000 [3719.000, 3719.000],  loss: 104015.578125, mae: 1898.105957, mean_q: 12.081124\n",
      "wrong_move\n",
      "   3291/500000: episode: 3267, duration: 0.148s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1747.000 [1747.000, 1747.000],  loss: 85616.953125, mae: 1900.875000, mean_q: 3.862283\n",
      "wrong_move\n",
      "   3292/500000: episode: 3268, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3719.000 [3719.000, 3719.000],  loss: 71019.101562, mae: 1906.043335, mean_q: 3.822091\n",
      "wrong_move\n",
      "   3293/500000: episode: 3269, duration: 0.146s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1116.000 [1116.000, 1116.000],  loss: 474085.031250, mae: 1915.322021, mean_q: 4.020617\n",
      "wrong_move\n",
      "   3294/500000: episode: 3270, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3719.000 [3719.000, 3719.000],  loss: 255770.187500, mae: 1921.842407, mean_q: 3.993014\n",
      "wrong_move\n",
      "   3295/500000: episode: 3271, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1116.000 [1116.000, 1116.000],  loss: 22643.218750, mae: 1927.184082, mean_q: 3.988070\n",
      "wrong_move\n",
      "   3296/500000: episode: 3272, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 559.000 [559.000, 559.000],  loss: 92189.937500, mae: 1927.921143, mean_q: 3.999737\n",
      "wrong_move\n",
      "   3297/500000: episode: 3273, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3719.000 [3719.000, 3719.000],  loss: 164917.312500, mae: 1925.060425, mean_q: 3.954909\n",
      "wrong_move\n",
      "   3298/500000: episode: 3274, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3719.000 [3719.000, 3719.000],  loss: 68433.773438, mae: 1917.497070, mean_q: 4.038006\n",
      "wrong_move\n",
      "   3299/500000: episode: 3275, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2265.000 [2265.000, 2265.000],  loss: 144097.140625, mae: 1916.731079, mean_q: 11.853225\n",
      "wrong_move\n",
      "   3300/500000: episode: 3276, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3926.000 [3926.000, 3926.000],  loss: 485826.625000, mae: 1921.853760, mean_q: 3.938570\n",
      "wrong_move\n",
      "   3301/500000: episode: 3277, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2090.000 [2090.000, 2090.000],  loss: 73420.640625, mae: 1928.178711, mean_q: 7.613307\n",
      "wrong_move\n",
      "   3302/500000: episode: 3278, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 702.000 [702.000, 702.000],  loss: 441315.343750, mae: 1931.846924, mean_q: 3.844930\n",
      "wrong_move\n",
      "   3303/500000: episode: 3279, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3719.000 [3719.000, 3719.000],  loss: 44825.683594, mae: 1935.603516, mean_q: 3.993486\n",
      "wrong_move\n",
      "   3304/500000: episode: 3280, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 702.000 [702.000, 702.000],  loss: 61559.070312, mae: 1939.937866, mean_q: 3.944030\n",
      "wrong_move\n",
      "   3305/500000: episode: 3281, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 345.000 [345.000, 345.000],  loss: 466349.281250, mae: 1940.880249, mean_q: 4.009059\n",
      "wrong_move\n",
      "   3306/500000: episode: 3282, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1524.000 [1524.000, 1524.000],  loss: 46123.390625, mae: 1942.528320, mean_q: 3.815994\n",
      "wrong_move\n",
      "   3307/500000: episode: 3283, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 702.000 [702.000, 702.000],  loss: 97522.148438, mae: 1943.059082, mean_q: 3.879219\n",
      "wrong_move\n",
      "   3308/500000: episode: 3284, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1313.000 [1313.000, 1313.000],  loss: 79158.289062, mae: 1942.808594, mean_q: 3.962583\n",
      "wrong_move\n",
      "   3309/500000: episode: 3285, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 702.000 [702.000, 702.000],  loss: 421772.812500, mae: 1945.921631, mean_q: 3.945713\n",
      "wrong_move\n",
      "   3310/500000: episode: 3286, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 702.000 [702.000, 702.000],  loss: 123931.820312, mae: 1939.939697, mean_q: 3.994256\n",
      "wrong_move\n",
      "   3311/500000: episode: 3287, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2997.000 [2997.000, 2997.000],  loss: 41632.250000, mae: 1934.101562, mean_q: 3.880920\n",
      "wrong_move\n",
      "   3312/500000: episode: 3288, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1688.000 [1688.000, 1688.000],  loss: 23541.824219, mae: 1929.463867, mean_q: 3.884033\n",
      "wrong_move\n",
      "   3313/500000: episode: 3289, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 595.000 [595.000, 595.000],  loss: 463030.406250, mae: 1923.779785, mean_q: 3.792783\n",
      "wrong_move\n",
      "   3314/500000: episode: 3290, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1780.000 [1780.000, 1780.000],  loss: 404614.000000, mae: 1918.259033, mean_q: 3.867362\n",
      "wrong_move\n",
      "   3315/500000: episode: 3291, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2919.000 [2919.000, 2919.000],  loss: 417062.125000, mae: 1916.330811, mean_q: 3.815247\n",
      "wrong_move\n",
      "   3316/500000: episode: 3292, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1486.000 [1486.000, 1486.000],  loss: 56053.023438, mae: 1914.973267, mean_q: 3.792778\n",
      "wrong_move\n",
      "   3317/500000: episode: 3293, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2960.000 [2960.000, 2960.000],  loss: 31203.894531, mae: 1920.847778, mean_q: 3.846105\n",
      "wrong_move\n",
      "   3318/500000: episode: 3294, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1949.000 [1949.000, 1949.000],  loss: 36835.359375, mae: 1927.313721, mean_q: 3.846430\n",
      "wrong_move\n",
      "   3319/500000: episode: 3295, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 702.000 [702.000, 702.000],  loss: 516618.531250, mae: 1932.489258, mean_q: 3.869494\n",
      "wrong_move\n",
      "   3320/500000: episode: 3296, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1780.000 [1780.000, 1780.000],  loss: 614025.312500, mae: 1932.235718, mean_q: 3.845377\n",
      "wrong_move\n",
      "   3321/500000: episode: 3297, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3061.000 [3061.000, 3061.000],  loss: 135993.281250, mae: 1916.986816, mean_q: 3.741443\n",
      "wrong_move\n",
      "   3322/500000: episode: 3298, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1313.000 [1313.000, 1313.000],  loss: 295100.218750, mae: 1905.625854, mean_q: 3.706902\n",
      "wrong_move\n",
      "   3323/500000: episode: 3299, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1313.000 [1313.000, 1313.000],  loss: 63366.703125, mae: 1906.763550, mean_q: 3.730813\n",
      "wrong_move\n",
      "   3324/500000: episode: 3300, duration: 0.166s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1332.000 [1332.000, 1332.000],  loss: 39797.816406, mae: 1913.054565, mean_q: 3.789208\n",
      "wrong_move\n",
      "   3326/500000: episode: 3301, duration: 0.204s, episode steps:   2, steps per second:  10, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2653.500 [1308.000, 3999.000],  loss: 77443.757812, mae: 1922.897461, mean_q: 15.725244\n",
      "wrong_move\n",
      "   3327/500000: episode: 3302, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3583.000 [3583.000, 3583.000],  loss: 294502.062500, mae: 1927.265137, mean_q: 13.186446\n",
      "wrong_move\n",
      "   3328/500000: episode: 3303, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 585.000 [585.000, 585.000],  loss: 53670.429688, mae: 1935.207031, mean_q: 3.789340\n",
      "wrong_move\n",
      "   3329/500000: episode: 3304, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 679.000 [679.000, 679.000],  loss: 428038.406250, mae: 1946.422607, mean_q: 13.556675\n",
      "wrong_move\n",
      "   3330/500000: episode: 3305, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 860.000 [860.000, 860.000],  loss: 527142.062500, mae: 1956.263672, mean_q: 3.691188\n",
      "wrong_move\n",
      "   3331/500000: episode: 3306, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 795.000 [795.000, 795.000],  loss: 456824.437500, mae: 1967.288574, mean_q: 3.755661\n",
      "wrong_move\n",
      "   3332/500000: episode: 3307, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3197.000 [3197.000, 3197.000],  loss: 587709.187500, mae: 1973.987061, mean_q: 3.692497\n",
      "wrong_move\n",
      "   3333/500000: episode: 3308, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 781.000 [781.000, 781.000],  loss: 41511.304688, mae: 1978.653564, mean_q: 3.667352\n",
      "wrong_move\n",
      "   3334/500000: episode: 3309, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3927.000 [3927.000, 3927.000],  loss: 503591.093750, mae: 1977.084961, mean_q: 3.771274\n",
      "wrong_move\n",
      "   3335/500000: episode: 3310, duration: 0.155s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2258.000 [2258.000, 2258.000],  loss: 898198.500000, mae: 1966.270752, mean_q: 3.651533\n",
      "wrong_move\n",
      "   3336/500000: episode: 3311, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 282.000 [282.000, 282.000],  loss: 413487.531250, mae: 1960.501465, mean_q: 3.654540\n",
      "wrong_move\n",
      "   3337/500000: episode: 3312, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3424.000 [3424.000, 3424.000],  loss: 449613.312500, mae: 1956.823975, mean_q: 3.670493\n",
      "wrong_move\n",
      "   3338/500000: episode: 3313, duration: 0.154s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2320.000 [2320.000, 2320.000],  loss: 46895.640625, mae: 1953.731445, mean_q: 3.667897\n",
      "wrong_move\n",
      "   3339/500000: episode: 3314, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3927.000 [3927.000, 3927.000],  loss: 70804.515625, mae: 1952.845215, mean_q: 3.697407\n",
      "wrong_move\n",
      "   3340/500000: episode: 3315, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 282.000 [282.000, 282.000],  loss: 247776.890625, mae: 1952.938721, mean_q: 3.611751\n",
      "wrong_move\n",
      "   3341/500000: episode: 3316, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1667.000 [1667.000, 1667.000],  loss: 60513.281250, mae: 1957.874268, mean_q: 3.609359\n",
      "wrong_move\n",
      "   3342/500000: episode: 3317, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 700.000 [700.000, 700.000],  loss: 285696.562500, mae: 1967.767822, mean_q: 3.585369\n",
      "wrong_move\n",
      "   3343/500000: episode: 3318, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 282.000 [282.000, 282.000],  loss: 70796.468750, mae: 1971.445312, mean_q: 3.595717\n",
      "wrong_move\n",
      "   3344/500000: episode: 3319, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1849.000 [1849.000, 1849.000],  loss: 447594.656250, mae: 1977.785156, mean_q: 3.596117\n",
      "wrong_move\n",
      "   3345/500000: episode: 3320, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 187.000 [187.000, 187.000],  loss: 882917.625000, mae: 1981.198364, mean_q: 3.686379\n",
      "wrong_move\n",
      "   3346/500000: episode: 3321, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 679.000 [679.000, 679.000],  loss: 33234.445312, mae: 1978.066162, mean_q: 3.694299\n",
      "wrong_move\n",
      "   3347/500000: episode: 3322, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1308.000 [1308.000, 1308.000],  loss: 67483.523438, mae: 1972.595215, mean_q: 377.859985\n",
      "wrong_move\n",
      "   3348/500000: episode: 3323, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1308.000 [1308.000, 1308.000],  loss: 41034.207031, mae: 1969.502930, mean_q: 1025.436646\n",
      "wrong_move\n",
      "   3349/500000: episode: 3324, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1308.000 [1308.000, 1308.000],  loss: 435799.750000, mae: 1971.375854, mean_q: 1551.570679\n",
      "wrong_move\n",
      "   3350/500000: episode: 3325, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1873.000 [1873.000, 1873.000],  loss: 85647.500000, mae: 1974.575928, mean_q: 1979.758423\n",
      "wrong_move\n",
      "   3351/500000: episode: 3326, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3097.000 [3097.000, 3097.000],  loss: 100108.953125, mae: 1977.174805, mean_q: 2326.874512\n",
      "wrong_move\n",
      "   3352/500000: episode: 3327, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 313.000 [313.000, 313.000],  loss: 866293.125000, mae: 1981.507324, mean_q: 2611.196777\n",
      "wrong_move\n",
      "   3353/500000: episode: 3328, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1308.000 [1308.000, 1308.000],  loss: 457930.531250, mae: 1982.942383, mean_q: 2839.041504\n",
      "wrong_move\n",
      "   3354/500000: episode: 3329, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 937.000 [937.000, 937.000],  loss: 300850.062500, mae: 1980.038452, mean_q: 3017.741943\n",
      "wrong_move\n",
      "   3355/500000: episode: 3330, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1308.000 [1308.000, 1308.000],  loss: 504903.500000, mae: 1975.554688, mean_q: 3158.927246\n",
      "wrong_move\n",
      "   3356/500000: episode: 3331, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1308.000 [1308.000, 1308.000],  loss: 41402.742188, mae: 1963.863892, mean_q: 3260.687500\n",
      "wrong_move\n",
      "   3358/500000: episode: 3332, duration: 0.139s, episode steps:   2, steps per second:  14, episode reward: -5051.000, mean reward: -2525.500 [-5000.000, -51.000], mean action: 1308.000 [1308.000, 1308.000],  loss: 34304.718750, mae: 1950.395752, mean_q: 3356.134033\n",
      "wrong_move\n",
      "   3359/500000: episode: 3333, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1308.000 [1308.000, 1308.000],  loss: 42482.972656, mae: 1941.148193, mean_q: 3431.076904\n",
      "wrong_move\n",
      "   3360/500000: episode: 3334, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1308.000 [1308.000, 1308.000],  loss: 1228102.875000, mae: 1944.554199, mean_q: 3493.307617\n",
      "wrong_move\n",
      "   3361/500000: episode: 3335, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 628.000 [628.000, 628.000],  loss: 462477.031250, mae: 1948.288574, mean_q: 2775.052490\n",
      "wrong_move\n",
      "   3362/500000: episode: 3336, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1308.000 [1308.000, 1308.000],  loss: 951992.625000, mae: 1955.667847, mean_q: 2195.586426\n",
      "wrong_move\n",
      "   3363/500000: episode: 3337, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1308.000 [1308.000, 1308.000],  loss: 138572.250000, mae: 1964.509033, mean_q: 1362.011841\n",
      "wrong_move\n",
      "   3364/500000: episode: 3338, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1308.000 [1308.000, 1308.000],  loss: 174411.218750, mae: 1979.732544, mean_q: 633.751404\n",
      "wrong_move\n",
      "   3365/500000: episode: 3339, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1160.000 [1160.000, 1160.000],  loss: 77240.187500, mae: 1992.271729, mean_q: 91.930466\n",
      "wrong_move\n",
      "   3366/500000: episode: 3340, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3438.000 [3438.000, 3438.000],  loss: 70324.375000, mae: 2001.613159, mean_q: 76.823792\n",
      "wrong_move\n",
      "   3367/500000: episode: 3341, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4095.000 [4095.000, 4095.000],  loss: 191719.625000, mae: 2001.671631, mean_q: 79.835129\n",
      "wrong_move\n",
      "   3368/500000: episode: 3342, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1160.000 [1160.000, 1160.000],  loss: 169542.562500, mae: 2002.527710, mean_q: 79.232162\n",
      "wrong_move\n",
      "   3369/500000: episode: 3343, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1160.000 [1160.000, 1160.000],  loss: 88454.984375, mae: 1990.285889, mean_q: 93.513153\n",
      "wrong_move\n",
      "   3370/500000: episode: 3344, duration: 0.141s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1945.000 [1945.000, 1945.000],  loss: 43891.585938, mae: 1978.471436, mean_q: 93.155098\n",
      "wrong_move\n",
      "   3371/500000: episode: 3345, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 273.000 [273.000, 273.000],  loss: 33071.878906, mae: 1966.933594, mean_q: 97.326790\n",
      "wrong_move\n",
      "   3372/500000: episode: 3346, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1160.000 [1160.000, 1160.000],  loss: 139841.359375, mae: 1960.854004, mean_q: 90.004494\n",
      "wrong_move\n",
      "   3373/500000: episode: 3347, duration: 0.142s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1160.000 [1160.000, 1160.000],  loss: 356780.468750, mae: 1962.024170, mean_q: 89.769966\n",
      "wrong_move\n",
      "   3374/500000: episode: 3348, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1160.000 [1160.000, 1160.000],  loss: 244143.734375, mae: 1969.625244, mean_q: 89.363319\n",
      "wrong_move\n",
      "   3375/500000: episode: 3349, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1160.000 [1160.000, 1160.000],  loss: 419309.906250, mae: 1977.846680, mean_q: 93.512726\n",
      "wrong_move\n",
      "   3376/500000: episode: 3350, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1160.000 [1160.000, 1160.000],  loss: 41385.281250, mae: 1985.228882, mean_q: 99.362671\n",
      "wrong_move\n",
      "   3377/500000: episode: 3351, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1727.000 [1727.000, 1727.000],  loss: 739962.750000, mae: 1986.993042, mean_q: 86.299255\n",
      "wrong_move\n",
      "   3378/500000: episode: 3352, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1034.000 [1034.000, 1034.000],  loss: 517723.093750, mae: 1979.683350, mean_q: 95.015007\n",
      "wrong_move\n",
      "   3379/500000: episode: 3353, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 533.000 [533.000, 533.000],  loss: 640595.312500, mae: 1977.572998, mean_q: 101.879578\n",
      "wrong_move\n",
      "   3380/500000: episode: 3354, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1160.000 [1160.000, 1160.000],  loss: 476620.562500, mae: 1984.243652, mean_q: 98.665451\n",
      "wrong_move\n",
      "   3381/500000: episode: 3355, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2320.000 [2320.000, 2320.000],  loss: 57439.281250, mae: 1996.812622, mean_q: 103.597961\n",
      "wrong_move\n",
      "   3382/500000: episode: 3356, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1160.000 [1160.000, 1160.000],  loss: 64853.535156, mae: 2008.873047, mean_q: 101.916733\n",
      "wrong_move\n",
      "   3383/500000: episode: 3357, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3857.000 [3857.000, 3857.000],  loss: 827255.750000, mae: 2016.033691, mean_q: 105.518906\n",
      "wrong_move\n",
      "   3384/500000: episode: 3358, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1160.000 [1160.000, 1160.000],  loss: 88776.179688, mae: 2013.128662, mean_q: 98.892654\n",
      "wrong_move\n",
      "   3385/500000: episode: 3359, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1160.000 [1160.000, 1160.000],  loss: 173626.750000, mae: 2002.591797, mean_q: 102.552094\n",
      "wrong_move\n",
      "   3386/500000: episode: 3360, duration: 0.141s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3113.000 [3113.000, 3113.000],  loss: 20708.238281, mae: 1991.983887, mean_q: 96.496735\n",
      "wrong_move\n",
      "   3387/500000: episode: 3361, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 137.000 [137.000, 137.000],  loss: 36473.460938, mae: 1986.153198, mean_q: 6.154918\n",
      "wrong_move\n",
      "   3388/500000: episode: 3362, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3054.000 [3054.000, 3054.000],  loss: 165396.718750, mae: 1984.787842, mean_q: 3.569082\n",
      "wrong_move\n",
      "   3389/500000: episode: 3363, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4084.000 [4084.000, 4084.000],  loss: 58452.550781, mae: 1984.842773, mean_q: 3.476255\n",
      "wrong_move\n",
      "   3390/500000: episode: 3364, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3054.000 [3054.000, 3054.000],  loss: 55965.027344, mae: 1989.603271, mean_q: 3.525132\n",
      "wrong_move\n",
      "   3391/500000: episode: 3365, duration: 0.143s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2854.000 [2854.000, 2854.000],  loss: 75178.507812, mae: 1996.811279, mean_q: 3.559480\n",
      "wrong_move\n",
      "   3392/500000: episode: 3366, duration: 0.149s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 360.000 [360.000, 360.000],  loss: 60148.062500, mae: 2003.836670, mean_q: 3.423279\n",
      "wrong_move\n",
      "   3393/500000: episode: 3367, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4015.000 [4015.000, 4015.000],  loss: 13816.191406, mae: 2010.561279, mean_q: 3.533602\n",
      "wrong_move\n",
      "   3394/500000: episode: 3368, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4084.000 [4084.000, 4084.000],  loss: 68624.101562, mae: 2017.937744, mean_q: 3.611997\n",
      "wrong_move\n",
      "   3395/500000: episode: 3369, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 574.000 [574.000, 574.000],  loss: 216075.375000, mae: 2019.978760, mean_q: 3.516812\n",
      "wrong_move\n",
      "   3396/500000: episode: 3370, duration: 0.165s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3337.000 [3337.000, 3337.000],  loss: 100111.859375, mae: 2016.072510, mean_q: 3.578756\n",
      "wrong_move\n",
      "   3397/500000: episode: 3371, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 705.000 [705.000, 705.000],  loss: 481035.218750, mae: 2013.440063, mean_q: 3.483152\n",
      "wrong_move\n",
      "   3398/500000: episode: 3372, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1045.000 [1045.000, 1045.000],  loss: 60524.421875, mae: 2012.033691, mean_q: 3.586760\n",
      "wrong_move\n",
      "   3399/500000: episode: 3373, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 847.000 [847.000, 847.000],  loss: 21983.789062, mae: 2008.627686, mean_q: 3.449700\n",
      "wrong_move\n",
      "   3400/500000: episode: 3374, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1663.000 [1663.000, 1663.000],  loss: 103436.976562, mae: 2001.486328, mean_q: 3.572228\n",
      "wrong_move\n",
      "   3401/500000: episode: 3375, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1230.000 [1230.000, 1230.000],  loss: 61032.539062, mae: 1992.209717, mean_q: 3.475630\n",
      "wrong_move\n",
      "   3402/500000: episode: 3376, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 705.000 [705.000, 705.000],  loss: 53412.527344, mae: 1988.632080, mean_q: 3.439975\n",
      "wrong_move\n",
      "   3403/500000: episode: 3377, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3326.000 [3326.000, 3326.000],  loss: 84857.304688, mae: 1990.911377, mean_q: 3.514301\n",
      "wrong_move\n",
      "   3404/500000: episode: 3378, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3712.000 [3712.000, 3712.000],  loss: 68906.531250, mae: 1995.637451, mean_q: 3.580835\n",
      "wrong_move\n",
      "   3405/500000: episode: 3379, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1215.000 [1215.000, 1215.000],  loss: 45760.546875, mae: 2000.138794, mean_q: 3.653650\n",
      "wrong_move\n",
      "   3406/500000: episode: 3380, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3757.000 [3757.000, 3757.000],  loss: 26373.527344, mae: 2005.867188, mean_q: 3.603627\n",
      "wrong_move\n",
      "   3407/500000: episode: 3381, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3679.000 [3679.000, 3679.000],  loss: 52661.292969, mae: 2011.473633, mean_q: 3.631374\n",
      "wrong_move\n",
      "   3408/500000: episode: 3382, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1443.000 [1443.000, 1443.000],  loss: 145982.656250, mae: 2013.661133, mean_q: 5.892856\n",
      "wrong_move\n",
      "   3409/500000: episode: 3383, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 417.000 [417.000, 417.000],  loss: 82221.601562, mae: 2018.967285, mean_q: 3.692335\n",
      "wrong_move\n",
      "   3410/500000: episode: 3384, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1315.000 [1315.000, 1315.000],  loss: 135154.828125, mae: 2019.287598, mean_q: 3.692066\n",
      "wrong_move\n",
      "   3411/500000: episode: 3385, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2846.000 [2846.000, 2846.000],  loss: 58176.023438, mae: 2013.749023, mean_q: 8.900055\n",
      "wrong_move\n",
      "   3412/500000: episode: 3386, duration: 0.143s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 440893.656250, mae: 2014.985596, mean_q: 33.662148\n",
      "wrong_move\n",
      "   3413/500000: episode: 3387, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1315.000 [1315.000, 1315.000],  loss: 66809.453125, mae: 2017.582642, mean_q: 3.591372\n",
      "wrong_move\n",
      "   3414/500000: episode: 3388, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3757.000 [3757.000, 3757.000],  loss: 128480.312500, mae: 2015.779541, mean_q: 11.821471\n",
      "wrong_move\n",
      "   3415/500000: episode: 3389, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3616.000 [3616.000, 3616.000],  loss: 98985.804688, mae: 2002.650635, mean_q: 23.903933\n",
      "wrong_move\n",
      "   3416/500000: episode: 3390, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2320.000 [2320.000, 2320.000],  loss: 54670.640625, mae: 1981.016968, mean_q: 3.599432\n",
      "wrong_move\n",
      "   3417/500000: episode: 3391, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 575.000 [575.000, 575.000],  loss: 118856.375000, mae: 1964.471436, mean_q: 3.635376\n",
      "wrong_move\n",
      "   3418/500000: episode: 3392, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1664.000 [1664.000, 1664.000],  loss: 45618.605469, mae: 1957.786621, mean_q: 41.711151\n",
      "wrong_move\n",
      "   3419/500000: episode: 3393, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 360.000 [360.000, 360.000],  loss: 432895.687500, mae: 1959.002197, mean_q: 37.753109\n",
      "wrong_move\n",
      "   3420/500000: episode: 3394, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 575.000 [575.000, 575.000],  loss: 77941.250000, mae: 1969.187134, mean_q: 29.358822\n",
      "wrong_move\n",
      "   3421/500000: episode: 3395, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3390.000 [3390.000, 3390.000],  loss: 48503.804688, mae: 1988.167969, mean_q: 4.116021\n",
      "wrong_move\n",
      "   3422/500000: episode: 3396, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 360.000 [360.000, 360.000],  loss: 37091.343750, mae: 2012.282715, mean_q: 3.604425\n",
      "wrong_move\n",
      "   3423/500000: episode: 3397, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3659.000 [3659.000, 3659.000],  loss: 657803.125000, mae: 2032.822632, mean_q: 3.627053\n",
      "wrong_move\n",
      "   3424/500000: episode: 3398, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 360.000 [360.000, 360.000],  loss: 724803.062500, mae: 2029.707031, mean_q: 3.478335\n",
      "wrong_move\n",
      "   3425/500000: episode: 3399, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 360.000 [360.000, 360.000],  loss: 87365.554688, mae: 2022.460693, mean_q: 3.592431\n",
      "wrong_move\n",
      "   3426/500000: episode: 3400, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2117.000 [2117.000, 2117.000],  loss: 424436.687500, mae: 2011.958252, mean_q: 3.522690\n",
      "wrong_move\n",
      "   3427/500000: episode: 3401, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3857.000 [3857.000, 3857.000],  loss: 105314.546875, mae: 2002.792969, mean_q: 36.724812\n",
      "wrong_move\n",
      "   3428/500000: episode: 3402, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3757.000 [3757.000, 3757.000],  loss: 252142.500000, mae: 1996.061523, mean_q: 3.535868\n",
      "wrong_move\n",
      "   3429/500000: episode: 3403, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1069.000 [1069.000, 1069.000],  loss: 153542.031250, mae: 1996.700684, mean_q: 3.563177\n",
      "wrong_move\n",
      "   3430/500000: episode: 3404, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2337.000 [2337.000, 2337.000],  loss: 76436.304688, mae: 1993.594482, mean_q: 3.511171\n",
      "wrong_move\n",
      "   3431/500000: episode: 3405, duration: 0.149s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2040.000 [2040.000, 2040.000],  loss: 58349.523438, mae: 1990.436523, mean_q: 3.652774\n",
      "wrong_move\n",
      "   3432/500000: episode: 3406, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1045.000 [1045.000, 1045.000],  loss: 49500.585938, mae: 1991.839233, mean_q: 3.555369\n",
      "wrong_move\n",
      "   3433/500000: episode: 3407, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2565.000 [2565.000, 2565.000],  loss: 647206.312500, mae: 1994.840576, mean_q: 3.580366\n",
      "wrong_move\n",
      "   3434/500000: episode: 3408, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3659.000 [3659.000, 3659.000],  loss: 105324.093750, mae: 1994.963623, mean_q: 3.551217\n",
      "wrong_move\n",
      "   3435/500000: episode: 3409, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2742.000 [2742.000, 2742.000],  loss: 453427.000000, mae: 1999.300293, mean_q: 26.294680\n",
      "wrong_move\n",
      "   3436/500000: episode: 3410, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2001.000 [2001.000, 2001.000],  loss: 512426.062500, mae: 2008.187500, mean_q: 3.782628\n",
      "wrong_move\n",
      "   3437/500000: episode: 3411, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3757.000 [3757.000, 3757.000],  loss: 453789.093750, mae: 2012.021362, mean_q: 3.474308\n",
      "wrong_move\n",
      "   3438/500000: episode: 3412, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 451.000 [451.000, 451.000],  loss: 51759.316406, mae: 2015.102173, mean_q: 4.606401\n",
      "wrong_move\n",
      "   3439/500000: episode: 3413, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 575.000 [575.000, 575.000],  loss: 411844.031250, mae: 2015.635986, mean_q: 3.571208\n",
      "wrong_move\n",
      "   3440/500000: episode: 3414, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2759.000 [2759.000, 2759.000],  loss: 81516.890625, mae: 2015.153931, mean_q: 3.671503\n",
      "wrong_move\n",
      "   3441/500000: episode: 3415, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 575.000 [575.000, 575.000],  loss: 504830.437500, mae: 2013.871216, mean_q: 3.646380\n",
      "wrong_move\n",
      "   3442/500000: episode: 3416, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2012.000 [2012.000, 2012.000],  loss: 68973.890625, mae: 2009.673950, mean_q: 3.591470\n",
      "wrong_move\n",
      "   3443/500000: episode: 3417, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 575.000 [575.000, 575.000],  loss: 52586.503906, mae: 2012.076660, mean_q: 3.616924\n",
      "wrong_move\n",
      "   3444/500000: episode: 3418, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2917.000 [2917.000, 2917.000],  loss: 446694.375000, mae: 2017.669922, mean_q: 3.525153\n",
      "wrong_move\n",
      "   3445/500000: episode: 3419, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 575.000 [575.000, 575.000],  loss: 546194.312500, mae: 2026.409668, mean_q: 3.762192\n",
      "wrong_move\n",
      "   3446/500000: episode: 3420, duration: 0.147s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1813.000 [1813.000, 1813.000],  loss: 100793.039062, mae: 2037.961792, mean_q: 18.391262\n",
      "wrong_move\n",
      "   3447/500000: episode: 3421, duration: 0.154s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2854.000 [2854.000, 2854.000],  loss: 480939.812500, mae: 2052.769775, mean_q: 3.638536\n",
      "wrong_move\n",
      "   3448/500000: episode: 3422, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4026.000 [4026.000, 4026.000],  loss: 36898.894531, mae: 2066.245117, mean_q: 11.066000\n",
      "wrong_move\n",
      "   3449/500000: episode: 3423, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 100.000 [100.000, 100.000],  loss: 212711.187500, mae: 2071.562012, mean_q: 3.582776\n",
      "wrong_move\n",
      "   3450/500000: episode: 3424, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 336.000 [336.000, 336.000],  loss: 368813.625000, mae: 2068.899902, mean_q: 3.617192\n",
      "wrong_move\n",
      "   3451/500000: episode: 3425, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1420.000 [1420.000, 1420.000],  loss: 68232.875000, mae: 2054.422852, mean_q: 3.551413\n",
      "wrong_move\n",
      "   3452/500000: episode: 3426, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1420.000 [1420.000, 1420.000],  loss: 42455.468750, mae: 2040.589600, mean_q: 4.596892\n",
      "wrong_move\n",
      "   3453/500000: episode: 3427, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 424.000 [424.000, 424.000],  loss: 249967.875000, mae: 2030.263672, mean_q: 3.565152\n",
      "wrong_move\n",
      "   3454/500000: episode: 3428, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2854.000 [2854.000, 2854.000],  loss: 417932.031250, mae: 2023.712402, mean_q: 3.652282\n",
      "wrong_move\n",
      "   3455/500000: episode: 3429, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3513.000 [3513.000, 3513.000],  loss: 41590.046875, mae: 2017.653320, mean_q: 3.578969\n",
      "wrong_move\n",
      "   3456/500000: episode: 3430, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 268.000 [268.000, 268.000],  loss: 147968.281250, mae: 2015.285767, mean_q: 3.507169\n",
      "wrong_move\n",
      "   3457/500000: episode: 3431, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2725.000 [2725.000, 2725.000],  loss: 192911.203125, mae: 2022.948242, mean_q: 3.543274\n",
      "wrong_move\n",
      "   3458/500000: episode: 3432, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1420.000 [1420.000, 1420.000],  loss: 60330.207031, mae: 2031.803345, mean_q: 3.516184\n",
      "wrong_move\n",
      "   3459/500000: episode: 3433, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2854.000 [2854.000, 2854.000],  loss: 609899.750000, mae: 2033.106445, mean_q: 3.575332\n",
      "wrong_move\n",
      "   3460/500000: episode: 3434, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3170.000 [3170.000, 3170.000],  loss: 275754.843750, mae: 2030.972412, mean_q: 18.228184\n",
      "wrong_move\n",
      "   3461/500000: episode: 3435, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 409.000 [409.000, 409.000],  loss: 35756.824219, mae: 2028.595947, mean_q: 3.581544\n",
      "wrong_move\n",
      "   3462/500000: episode: 3436, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3066.000 [3066.000, 3066.000],  loss: 99225.617188, mae: 2029.219482, mean_q: 3.579216\n",
      "wrong_move\n",
      "   3463/500000: episode: 3437, duration: 0.118s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3117.000 [3117.000, 3117.000],  loss: 76878.546875, mae: 2026.890869, mean_q: 3.538794\n",
      "wrong_move\n",
      "   3464/500000: episode: 3438, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3117.000 [3117.000, 3117.000],  loss: 63091.078125, mae: 2023.722168, mean_q: 3.452906\n",
      "wrong_move\n",
      "   3465/500000: episode: 3439, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 100.000 [100.000, 100.000],  loss: 87964.000000, mae: 2019.263428, mean_q: 3.547952\n",
      "wrong_move\n",
      "   3466/500000: episode: 3440, duration: 0.167s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2669.000 [2669.000, 2669.000],  loss: 149426.078125, mae: 2021.531250, mean_q: 38.904312\n",
      "wrong_move\n",
      "   3467/500000: episode: 3441, duration: 0.154s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3170.000 [3170.000, 3170.000],  loss: 82351.500000, mae: 2027.957275, mean_q: 3.615306\n",
      "wrong_move\n",
      "   3468/500000: episode: 3442, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2801.000 [2801.000, 2801.000],  loss: 450691.531250, mae: 2037.531860, mean_q: 3.593320\n",
      "wrong_move\n",
      "   3469/500000: episode: 3443, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2669.000 [2669.000, 2669.000],  loss: 83731.531250, mae: 2039.725464, mean_q: 3.657769\n",
      "wrong_move\n",
      "   3470/500000: episode: 3444, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2669.000 [2669.000, 2669.000],  loss: 732844.875000, mae: 2041.188843, mean_q: 3.612755\n",
      "wrong_move\n",
      "   3471/500000: episode: 3445, duration: 0.148s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 409.000 [409.000, 409.000],  loss: 46508.250000, mae: 2042.910522, mean_q: 3.557690\n",
      "wrong_move\n",
      "   3472/500000: episode: 3446, duration: 0.157s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1845.000 [1845.000, 1845.000],  loss: 398518.156250, mae: 2046.931763, mean_q: 76.945663\n",
      "wrong_move\n",
      "   3473/500000: episode: 3447, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1238.000 [1238.000, 1238.000],  loss: 35713.414062, mae: 2045.578613, mean_q: 6.090879\n",
      "wrong_move\n",
      "   3474/500000: episode: 3448, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2512.000 [2512.000, 2512.000],  loss: 78510.382812, mae: 2048.927002, mean_q: 3.594486\n",
      "wrong_move\n",
      "   3475/500000: episode: 3449, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1749.000 [1749.000, 1749.000],  loss: 105278.109375, mae: 2051.622559, mean_q: 18.988321\n",
      "wrong_move\n",
      "   3476/500000: episode: 3450, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 798.000 [798.000, 798.000],  loss: 452090.000000, mae: 2054.148438, mean_q: 16.736023\n",
      "wrong_move\n",
      "   3477/500000: episode: 3451, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1749.000 [1749.000, 1749.000],  loss: 39717.281250, mae: 2049.031250, mean_q: 24.291164\n",
      "wrong_move\n",
      "   3478/500000: episode: 3452, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3190.000 [3190.000, 3190.000],  loss: 461199.750000, mae: 2046.114868, mean_q: 24.545971\n",
      "wrong_move\n",
      "   3479/500000: episode: 3453, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 977.000 [977.000, 977.000],  loss: 167049.968750, mae: 2045.392822, mean_q: 39.082378\n",
      "wrong_move\n",
      "   3480/500000: episode: 3454, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 82837.296875, mae: 2044.570312, mean_q: 18.755138\n",
      "wrong_move\n",
      "   3482/500000: episode: 3455, duration: 0.171s, episode steps:   2, steps per second:  12, episode reward: -5021.000, mean reward: -2510.500 [-5000.000, -21.000], mean action: 951.000 [153.000, 1749.000],  loss: 762519.625000, mae: 2035.160400, mean_q: 91.036392\n",
      "wrong_move\n",
      "   3483/500000: episode: 3456, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 355.000 [355.000, 355.000],  loss: 23062.914062, mae: 2020.710693, mean_q: 176.960480\n",
      "wrong_move\n",
      "   3484/500000: episode: 3457, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 661.000 [661.000, 661.000],  loss: 83423.937500, mae: 2016.040771, mean_q: 180.450455\n",
      "wrong_move\n",
      "   3485/500000: episode: 3458, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 59654.386719, mae: 2017.792847, mean_q: 71.996490\n",
      "wrong_move\n",
      "   3486/500000: episode: 3459, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3190.000 [3190.000, 3190.000],  loss: 336194.031250, mae: 2024.231445, mean_q: 22.383507\n",
      "wrong_move\n",
      "   3487/500000: episode: 3460, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 100.000 [100.000, 100.000],  loss: 129902.070312, mae: 2027.524658, mean_q: 68.663322\n",
      "wrong_move\n",
      "   3488/500000: episode: 3461, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 684.000 [684.000, 684.000],  loss: 470008.750000, mae: 2031.951416, mean_q: 76.629623\n",
      "wrong_move\n",
      "   3489/500000: episode: 3462, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2341.000 [2341.000, 2341.000],  loss: 64161.804688, mae: 2040.314941, mean_q: 159.208786\n",
      "wrong_move\n",
      "   3490/500000: episode: 3463, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1736.000 [1736.000, 1736.000],  loss: 312863.093750, mae: 2038.235352, mean_q: 165.646347\n",
      "wrong_move\n",
      "   3491/500000: episode: 3464, duration: 0.148s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 574.000 [574.000, 574.000],  loss: 40409.945312, mae: 2037.999512, mean_q: 51.718468\n",
      "wrong_move\n",
      "   3492/500000: episode: 3465, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1140.000 [1140.000, 1140.000],  loss: 34366.578125, mae: 2039.099976, mean_q: 29.509171\n",
      "wrong_move\n",
      "   3493/500000: episode: 3466, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2025.000 [2025.000, 2025.000],  loss: 456610.375000, mae: 2040.754395, mean_q: 138.017914\n",
      "wrong_move\n",
      "   3494/500000: episode: 3467, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2870.000 [2870.000, 2870.000],  loss: 79816.953125, mae: 2038.043579, mean_q: 46.235329\n",
      "wrong_move\n",
      "   3495/500000: episode: 3468, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2725.000 [2725.000, 2725.000],  loss: 386221.500000, mae: 2037.393799, mean_q: 209.754303\n",
      "wrong_move\n",
      "   3496/500000: episode: 3469, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3190.000 [3190.000, 3190.000],  loss: 51297.828125, mae: 2030.334106, mean_q: 87.893288\n",
      "wrong_move\n",
      "   3497/500000: episode: 3470, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2645.000 [2645.000, 2645.000],  loss: 21770.578125, mae: 2030.514526, mean_q: 94.570923\n",
      "wrong_move\n",
      "   3498/500000: episode: 3471, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1041.000 [1041.000, 1041.000],  loss: 600130.000000, mae: 2036.169434, mean_q: 180.423935\n",
      "wrong_move\n",
      "   3499/500000: episode: 3472, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3190.000 [3190.000, 3190.000],  loss: 968380.687500, mae: 2042.506348, mean_q: 254.182861\n",
      "wrong_move\n",
      "   3500/500000: episode: 3473, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2725.000 [2725.000, 2725.000],  loss: 39473.957031, mae: 2045.983276, mean_q: 165.626633\n",
      "wrong_move\n",
      "   3501/500000: episode: 3474, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1908.000 [1908.000, 1908.000],  loss: 263035.968750, mae: 2052.114258, mean_q: 264.929657\n",
      "wrong_move\n",
      "   3503/500000: episode: 3475, duration: 0.126s, episode steps:   2, steps per second:  16, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 298925.062500, mae: 2058.778809, mean_q: 122.890968\n",
      "wrong_move\n",
      "   3504/500000: episode: 3476, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1086.000 [1086.000, 1086.000],  loss: 191054.062500, mae: 2053.221680, mean_q: 103.964287\n",
      "wrong_move\n",
      "   3505/500000: episode: 3477, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1749.000 [1749.000, 1749.000],  loss: 435159.218750, mae: 2046.299805, mean_q: 82.981506\n",
      "wrong_move\n",
      "   3506/500000: episode: 3478, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 574.000 [574.000, 574.000],  loss: 96634.132812, mae: 2039.970947, mean_q: 263.592773\n",
      "wrong_move\n",
      "   3507/500000: episode: 3479, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2868.000 [2868.000, 2868.000],  loss: 84336.046875, mae: 2036.710449, mean_q: 116.245895\n",
      "wrong_move\n",
      "   3508/500000: episode: 3480, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1041.000 [1041.000, 1041.000],  loss: 86476.257812, mae: 2037.454346, mean_q: 115.745728\n",
      "wrong_move\n",
      "   3509/500000: episode: 3481, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1021.000 [1021.000, 1021.000],  loss: 38540.351562, mae: 2035.714844, mean_q: 147.139801\n",
      "wrong_move\n",
      "   3510/500000: episode: 3482, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1021.000 [1021.000, 1021.000],  loss: 249859.687500, mae: 2033.675781, mean_q: 244.298248\n",
      "wrong_move\n",
      "   3511/500000: episode: 3483, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1021.000 [1021.000, 1021.000],  loss: 106373.421875, mae: 2035.828125, mean_q: 361.113708\n",
      "wrong_move\n",
      "   3512/500000: episode: 3484, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1041.000 [1041.000, 1041.000],  loss: 95715.359375, mae: 2041.892456, mean_q: 48.731796\n",
      "wrong_move\n",
      "   3513/500000: episode: 3485, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 48068.699219, mae: 2051.434326, mean_q: 122.319687\n",
      "wrong_move\n",
      "   3514/500000: episode: 3486, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1749.000 [1749.000, 1749.000],  loss: 72871.695312, mae: 2060.661133, mean_q: 97.745186\n",
      "wrong_move\n",
      "   3515/500000: episode: 3487, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1021.000 [1021.000, 1021.000],  loss: 430407.937500, mae: 2066.911133, mean_q: 136.938034\n",
      "wrong_move\n",
      "   3516/500000: episode: 3488, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1021.000 [1021.000, 1021.000],  loss: 36700.496094, mae: 2066.420410, mean_q: 249.371246\n",
      "wrong_move\n",
      "   3517/500000: episode: 3489, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 871100.125000, mae: 2062.016602, mean_q: 54.912617\n",
      "wrong_move\n",
      "   3518/500000: episode: 3490, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1807.000 [1807.000, 1807.000],  loss: 65490.785156, mae: 2045.897461, mean_q: 166.313629\n",
      "wrong_move\n",
      "   3519/500000: episode: 3491, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2816.000 [2816.000, 2816.000],  loss: 41926.828125, mae: 2035.031860, mean_q: 3.769421\n",
      "wrong_move\n",
      "   3520/500000: episode: 3492, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 976.000 [976.000, 976.000],  loss: 82351.953125, mae: 2028.536987, mean_q: 3.803552\n",
      "wrong_move\n",
      "   3521/500000: episode: 3493, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2378.000 [2378.000, 2378.000],  loss: 33299.074219, mae: 2023.673340, mean_q: 70.141342\n",
      "wrong_move\n",
      "   3522/500000: episode: 3494, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2816.000 [2816.000, 2816.000],  loss: 516003.437500, mae: 2025.224365, mean_q: 155.599380\n",
      "wrong_move\n",
      "   3523/500000: episode: 3495, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 976.000 [976.000, 976.000],  loss: 53875.535156, mae: 2025.715820, mean_q: 99.545357\n",
      "wrong_move\n",
      "   3524/500000: episode: 3496, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1302.000 [1302.000, 1302.000],  loss: 41992.652344, mae: 2030.665039, mean_q: 121.843880\n",
      "wrong_move\n",
      "   3525/500000: episode: 3497, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3030.000 [3030.000, 3030.000],  loss: 32609.011719, mae: 2043.028809, mean_q: 42.680466\n",
      "wrong_move\n",
      "   3526/500000: episode: 3498, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 976.000 [976.000, 976.000],  loss: 347972.312500, mae: 2056.215088, mean_q: 3.927068\n",
      "wrong_move\n",
      "   3527/500000: episode: 3499, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 976.000 [976.000, 976.000],  loss: 77826.656250, mae: 2070.264648, mean_q: 3.878185\n",
      "wrong_move\n",
      "   3528/500000: episode: 3500, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1908.000 [1908.000, 1908.000],  loss: 132842.875000, mae: 2080.975098, mean_q: 3.775669\n",
      "wrong_move\n",
      "   3529/500000: episode: 3501, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2641.000 [2641.000, 2641.000],  loss: 506558.187500, mae: 2091.499512, mean_q: 4.120620\n",
      "wrong_move\n",
      "   3530/500000: episode: 3502, duration: 0.141s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2638.000 [2638.000, 2638.000],  loss: 339956.750000, mae: 2092.562988, mean_q: 203.904327\n",
      "wrong_move\n",
      "   3531/500000: episode: 3503, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1102.000 [1102.000, 1102.000],  loss: 650192.562500, mae: 2083.326660, mean_q: 3.813562\n",
      "wrong_move\n",
      "   3532/500000: episode: 3504, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 976.000 [976.000, 976.000],  loss: 539588.187500, mae: 2065.976074, mean_q: 16.686939\n",
      "wrong_move\n",
      "   3533/500000: episode: 3505, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 193.000 [193.000, 193.000],  loss: 630933.437500, mae: 2043.863159, mean_q: 3.711258\n",
      "wrong_move\n",
      "   3534/500000: episode: 3506, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 11.000 [11.000, 11.000],  loss: 63047.710938, mae: 2032.919678, mean_q: 26.150810\n",
      "wrong_move\n",
      "   3535/500000: episode: 3507, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 11.000 [11.000, 11.000],  loss: 72451.765625, mae: 2032.807251, mean_q: 99.679100\n",
      "wrong_move\n",
      "   3536/500000: episode: 3508, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1861.000 [1861.000, 1861.000],  loss: 42681.273438, mae: 2035.437744, mean_q: 3.840956\n",
      "wrong_move\n",
      "   3537/500000: episode: 3509, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 312.000 [312.000, 312.000],  loss: 514985.843750, mae: 2042.707764, mean_q: 73.482269\n",
      "wrong_move\n",
      "   3538/500000: episode: 3510, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2872.000 [2872.000, 2872.000],  loss: 51090.750000, mae: 2055.143555, mean_q: 24.795870\n",
      "wrong_move\n",
      "   3539/500000: episode: 3511, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3404.000 [3404.000, 3404.000],  loss: 52437.609375, mae: 2062.698730, mean_q: 7.290730\n",
      "wrong_move\n",
      "   3540/500000: episode: 3512, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1277.000 [1277.000, 1277.000],  loss: 72617.843750, mae: 2068.868164, mean_q: 85.955704\n",
      "wrong_move\n",
      "   3541/500000: episode: 3513, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3588.000 [3588.000, 3588.000],  loss: 114874.664062, mae: 2071.212891, mean_q: 112.783028\n",
      "wrong_move\n",
      "   3542/500000: episode: 3514, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3587.000 [3587.000, 3587.000],  loss: 229011.156250, mae: 2073.707520, mean_q: 42.256214\n",
      "wrong_move\n",
      "   3543/500000: episode: 3515, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3458.000 [3458.000, 3458.000],  loss: 81345.492188, mae: 2076.562744, mean_q: 51.630157\n",
      "wrong_move\n",
      "   3544/500000: episode: 3516, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2874.000 [2874.000, 2874.000],  loss: 482317.718750, mae: 2079.985840, mean_q: 30.695711\n",
      "wrong_move\n",
      "   3546/500000: episode: 3517, duration: 0.122s, episode steps:   2, steps per second:  16, episode reward: -5901.000, mean reward: -2950.500 [-5000.000, -901.000], mean action: 1866.500 [332.000, 3401.000],  loss: 57301.074219, mae: 2083.579102, mean_q: 125.884514\n",
      "wrong_move\n",
      "   3547/500000: episode: 3518, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1667.000 [1667.000, 1667.000],  loss: 28724.238281, mae: 2084.407715, mean_q: 32.163063\n",
      "wrong_move\n",
      "   3548/500000: episode: 3519, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 325.000 [325.000, 325.000],  loss: 64177.015625, mae: 2078.834961, mean_q: 127.460022\n",
      "wrong_move\n",
      "   3549/500000: episode: 3520, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1575.000 [1575.000, 1575.000],  loss: 294048.562500, mae: 2069.665527, mean_q: 137.874390\n",
      "wrong_move\n",
      "   3550/500000: episode: 3521, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2115.000 [2115.000, 2115.000],  loss: 72202.804688, mae: 2056.311035, mean_q: 33.995480\n",
      "wrong_move\n",
      "   3551/500000: episode: 3522, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2134.000 [2134.000, 2134.000],  loss: 104311.046875, mae: 2049.732910, mean_q: 240.367233\n",
      "wrong_move\n",
      "   3552/500000: episode: 3523, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3456.000 [3456.000, 3456.000],  loss: 77919.382812, mae: 2041.560425, mean_q: 33.275341\n",
      "wrong_move\n",
      "   3553/500000: episode: 3524, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3765.000 [3765.000, 3765.000],  loss: 464416.625000, mae: 2039.726074, mean_q: 34.333427\n",
      "wrong_move\n",
      "   3554/500000: episode: 3525, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 207.000 [207.000, 207.000],  loss: 495565.437500, mae: 2042.111572, mean_q: 269.700226\n",
      "wrong_move\n",
      "   3555/500000: episode: 3526, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2874.000 [2874.000, 2874.000],  loss: 475260.062500, mae: 2046.916626, mean_q: 161.294037\n",
      "wrong_move\n",
      "   3556/500000: episode: 3527, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3480.000 [3480.000, 3480.000],  loss: 58674.781250, mae: 2061.083496, mean_q: 21.703924\n",
      "wrong_move\n",
      "   3557/500000: episode: 3528, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3385.000 [3385.000, 3385.000],  loss: 528592.312500, mae: 2077.189697, mean_q: 31.358192\n",
      "wrong_move\n",
      "   3558/500000: episode: 3529, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1186.000 [1186.000, 1186.000],  loss: 470640.000000, mae: 2087.206055, mean_q: 160.612549\n",
      "wrong_move\n",
      "   3559/500000: episode: 3530, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 825.000 [825.000, 825.000],  loss: 1210191.250000, mae: 2086.024902, mean_q: 45.876289\n",
      "wrong_move\n",
      "   3560/500000: episode: 3531, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1779.000 [1779.000, 1779.000],  loss: 93959.140625, mae: 2084.782715, mean_q: 60.522903\n",
      "wrong_move\n",
      "   3561/500000: episode: 3532, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1985.000 [1985.000, 1985.000],  loss: 478982.531250, mae: 2083.623535, mean_q: 53.373863\n",
      "wrong_move\n",
      "   3562/500000: episode: 3533, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2268.000 [2268.000, 2268.000],  loss: 525642.687500, mae: 2088.789795, mean_q: 92.014236\n",
      "wrong_move\n",
      "   3563/500000: episode: 3534, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3401.000 [3401.000, 3401.000],  loss: 51929.304688, mae: 2088.086914, mean_q: 93.415764\n",
      "wrong_move\n",
      "   3564/500000: episode: 3535, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2352.000 [2352.000, 2352.000],  loss: 486410.781250, mae: 2089.574219, mean_q: 152.237793\n",
      "wrong_move\n",
      "   3565/500000: episode: 3536, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 220.000 [220.000, 220.000],  loss: 119674.898438, mae: 2091.016113, mean_q: 3.535587\n",
      "wrong_move\n",
      "   3566/500000: episode: 3537, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2874.000 [2874.000, 2874.000],  loss: 127905.140625, mae: 2096.574707, mean_q: 4.153996\n",
      "wrong_move\n",
      "   3567/500000: episode: 3538, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 837.000 [837.000, 837.000],  loss: 478258.406250, mae: 2095.303955, mean_q: 49.427914\n",
      "wrong_move\n",
      "   3568/500000: episode: 3539, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4008.000 [4008.000, 4008.000],  loss: 89659.843750, mae: 2099.331543, mean_q: 118.951805\n",
      "wrong_move\n",
      "   3569/500000: episode: 3540, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 80.000 [80.000, 80.000],  loss: 464869.656250, mae: 2105.918457, mean_q: 184.367233\n",
      "wrong_move\n",
      "   3570/500000: episode: 3541, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3903.000 [3903.000, 3903.000],  loss: 1027001.500000, mae: 2104.394531, mean_q: 351.084839\n",
      "wrong_move\n",
      "   3571/500000: episode: 3542, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 921.000 [921.000, 921.000],  loss: 15440.790039, mae: 2092.108887, mean_q: 338.455139\n",
      "wrong_move\n",
      "   3572/500000: episode: 3543, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2060.000 [2060.000, 2060.000],  loss: 236523.265625, mae: 2077.234131, mean_q: 68.116409\n",
      "wrong_move\n",
      "   3573/500000: episode: 3544, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 96870.085938, mae: 2063.852295, mean_q: 397.835907\n",
      "wrong_move\n",
      "   3574/500000: episode: 3545, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2094.000 [2094.000, 2094.000],  loss: 66869.851562, mae: 2054.847412, mean_q: 70.258293\n",
      "wrong_move\n",
      "   3575/500000: episode: 3546, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2270.000 [2270.000, 2270.000],  loss: 83378.500000, mae: 2055.353027, mean_q: 229.542801\n",
      "wrong_move\n",
      "   3576/500000: episode: 3547, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 761.000 [761.000, 761.000],  loss: 121018.203125, mae: 2064.875488, mean_q: 148.652557\n",
      "wrong_move\n",
      "   3577/500000: episode: 3548, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2407.000 [2407.000, 2407.000],  loss: 432469.375000, mae: 2082.514648, mean_q: 49.513847\n",
      "wrong_move\n",
      "   3578/500000: episode: 3549, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2407.000 [2407.000, 2407.000],  loss: 55143.402344, mae: 2099.051514, mean_q: 294.991058\n",
      "wrong_move\n",
      "   3579/500000: episode: 3550, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3150.000 [3150.000, 3150.000],  loss: 32853.089844, mae: 2110.520996, mean_q: 287.454132\n",
      "wrong_move\n",
      "   3580/500000: episode: 3551, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2672.000 [2672.000, 2672.000],  loss: 74040.515625, mae: 2118.312988, mean_q: 509.679810\n",
      "wrong_move\n",
      "   3581/500000: episode: 3552, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1022.000 [1022.000, 1022.000],  loss: 96178.046875, mae: 2118.255615, mean_q: 250.358154\n",
      "wrong_move\n",
      "   3582/500000: episode: 3553, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 181.000 [181.000, 181.000],  loss: 156279.875000, mae: 2112.975830, mean_q: 503.561157\n",
      "wrong_move\n",
      "   3583/500000: episode: 3554, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4090.000 [4090.000, 4090.000],  loss: 88487.304688, mae: 2106.747559, mean_q: 337.062042\n",
      "wrong_move\n",
      "   3584/500000: episode: 3555, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 332.000 [332.000, 332.000],  loss: 50584.957031, mae: 2102.001465, mean_q: 153.615540\n",
      "wrong_move\n",
      "   3585/500000: episode: 3556, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 875.000 [875.000, 875.000],  loss: 838893.875000, mae: 2101.117432, mean_q: 230.543594\n",
      "wrong_move\n",
      "   3586/500000: episode: 3557, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 166.000 [166.000, 166.000],  loss: 322490.781250, mae: 2099.461914, mean_q: 100.894661\n",
      "wrong_move\n",
      "   3587/500000: episode: 3558, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4026.000 [4026.000, 4026.000],  loss: 706126.312500, mae: 2103.536621, mean_q: 236.504379\n",
      "wrong_move\n",
      "   3588/500000: episode: 3559, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 332.000 [332.000, 332.000],  loss: 736977.500000, mae: 2110.829346, mean_q: 343.909912\n",
      "wrong_move\n",
      "   3589/500000: episode: 3560, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2589.000 [2589.000, 2589.000],  loss: 143582.843750, mae: 2116.115723, mean_q: 194.498810\n",
      "wrong_move\n",
      "   3590/500000: episode: 3561, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2441.000 [2441.000, 2441.000],  loss: 69865.593750, mae: 2117.555908, mean_q: 395.155945\n",
      "wrong_move\n",
      "   3591/500000: episode: 3562, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3385.000 [3385.000, 3385.000],  loss: 104553.843750, mae: 2118.813477, mean_q: 346.340302\n",
      "wrong_move\n",
      "   3592/500000: episode: 3563, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1605.000 [1605.000, 1605.000],  loss: 483289.218750, mae: 2114.333008, mean_q: 3.661788\n",
      "wrong_move\n",
      "   3593/500000: episode: 3564, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3385.000 [3385.000, 3385.000],  loss: 153671.406250, mae: 2107.525879, mean_q: 3.670791\n",
      "wrong_move\n",
      "   3594/500000: episode: 3565, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1022.000 [1022.000, 1022.000],  loss: 457414.562500, mae: 2094.225586, mean_q: 274.356476\n",
      "wrong_move\n",
      "   3595/500000: episode: 3566, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2744.000 [2744.000, 2744.000],  loss: 45516.519531, mae: 2079.518311, mean_q: 3.606450\n",
      "wrong_move\n",
      "   3596/500000: episode: 3567, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1144.000 [1144.000, 1144.000],  loss: 16638.880859, mae: 2074.076416, mean_q: 3.506583\n",
      "wrong_move\n",
      "   3597/500000: episode: 3568, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 700.000 [700.000, 700.000],  loss: 393719.468750, mae: 2073.262207, mean_q: 3.569358\n",
      "wrong_move\n",
      "   3598/500000: episode: 3569, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 700.000 [700.000, 700.000],  loss: 473286.750000, mae: 2068.376465, mean_q: 282.131958\n",
      "wrong_move\n",
      "   3599/500000: episode: 3570, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1022.000 [1022.000, 1022.000],  loss: 473098.437500, mae: 2066.433594, mean_q: 14.182473\n",
      "wrong_move\n",
      "   3600/500000: episode: 3571, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1144.000 [1144.000, 1144.000],  loss: 442077.687500, mae: 2067.930176, mean_q: 187.750427\n",
      "wrong_move\n",
      "   3601/500000: episode: 3572, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 700.000 [700.000, 700.000],  loss: 34993.464844, mae: 2065.034180, mean_q: 256.869629\n",
      "wrong_move\n",
      "   3602/500000: episode: 3573, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3072.000 [3072.000, 3072.000],  loss: 93683.156250, mae: 2062.361816, mean_q: 301.404297\n",
      "wrong_move\n",
      "   3603/500000: episode: 3574, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1144.000 [1144.000, 1144.000],  loss: 133025.359375, mae: 2069.101318, mean_q: 228.461502\n",
      "wrong_move\n",
      "   3604/500000: episode: 3575, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2662.000 [2662.000, 2662.000],  loss: 828374.375000, mae: 2082.628418, mean_q: 27.594761\n",
      "wrong_move\n",
      "   3605/500000: episode: 3576, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1991.000 [1991.000, 1991.000],  loss: 72849.539062, mae: 2098.029053, mean_q: 40.944580\n",
      "wrong_move\n",
      "   3606/500000: episode: 3577, duration: 0.147s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3292.000 [3292.000, 3292.000],  loss: 81594.414062, mae: 2111.231201, mean_q: 360.838104\n",
      "wrong_move\n",
      "   3607/500000: episode: 3578, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1183.000 [1183.000, 1183.000],  loss: 184215.859375, mae: 2117.126709, mean_q: 384.822510\n",
      "wrong_move\n",
      "   3608/500000: episode: 3579, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1624.000 [1624.000, 1624.000],  loss: 69582.171875, mae: 2118.898926, mean_q: 4.697522\n",
      "wrong_move\n",
      "   3609/500000: episode: 3580, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1210.000 [1210.000, 1210.000],  loss: 412457.562500, mae: 2114.922852, mean_q: 367.652893\n",
      "wrong_move\n",
      "   3610/500000: episode: 3581, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2490.000 [2490.000, 2490.000],  loss: 190140.578125, mae: 2113.372314, mean_q: 4.217812\n",
      "wrong_move\n",
      "   3611/500000: episode: 3582, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3503.000 [3503.000, 3503.000],  loss: 14908.726562, mae: 2110.443848, mean_q: 3.446351\n",
      "wrong_move\n",
      "   3612/500000: episode: 3583, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1144.000 [1144.000, 1144.000],  loss: 88234.726562, mae: 2112.173340, mean_q: 3.437559\n",
      "wrong_move\n",
      "   3613/500000: episode: 3584, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1126.000 [1126.000, 1126.000],  loss: 140218.468750, mae: 2114.563477, mean_q: 344.064880\n",
      "wrong_move\n",
      "   3614/500000: episode: 3585, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1144.000 [1144.000, 1144.000],  loss: 280916.406250, mae: 2116.861328, mean_q: 201.580429\n",
      "wrong_move\n",
      "   3615/500000: episode: 3586, duration: 0.133s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 567.000 [567.000, 567.000],  loss: 850890.625000, mae: 2108.777588, mean_q: 75.107964\n",
      "wrong_move\n",
      "   3616/500000: episode: 3587, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3150.000 [3150.000, 3150.000],  loss: 107469.484375, mae: 2105.512207, mean_q: 3.535206\n",
      "wrong_move\n",
      "   3617/500000: episode: 3588, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1719.000 [1719.000, 1719.000],  loss: 494891.781250, mae: 2111.604248, mean_q: 146.543091\n",
      "wrong_move\n",
      "   3618/500000: episode: 3589, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: 320881.093750, mae: 2125.844238, mean_q: 75.092514\n",
      "wrong_move\n",
      "   3619/500000: episode: 3590, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3150.000 [3150.000, 3150.000],  loss: 561649.000000, mae: 2132.614502, mean_q: 17.010181\n",
      "wrong_move\n",
      "   3620/500000: episode: 3591, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1098.000 [1098.000, 1098.000],  loss: 218326.578125, mae: 2138.526123, mean_q: 30.496965\n",
      "wrong_move\n",
      "   3621/500000: episode: 3592, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1496368.500000, mae: 2152.404785, mean_q: 3.581553\n",
      "wrong_move\n",
      "   3622/500000: episode: 3593, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2276.000 [2276.000, 2276.000],  loss: 77453.882812, mae: 2163.461670, mean_q: 3.427987\n",
      "wrong_move\n",
      "   3623/500000: episode: 3594, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3085.000 [3085.000, 3085.000],  loss: 34534.582031, mae: 2165.051758, mean_q: 72.524818\n",
      "wrong_move\n",
      "   3624/500000: episode: 3595, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 484.000 [484.000, 484.000],  loss: 423208.437500, mae: 2159.843750, mean_q: 7.699593\n",
      "wrong_move\n",
      "   3625/500000: episode: 3596, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1872.000 [1872.000, 1872.000],  loss: 520996.625000, mae: 2158.110840, mean_q: 113.121269\n",
      "wrong_move\n",
      "   3626/500000: episode: 3597, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2443.000 [2443.000, 2443.000],  loss: 58098.046875, mae: 2147.994629, mean_q: 34.952431\n",
      "wrong_move\n",
      "   3627/500000: episode: 3598, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2929.000 [2929.000, 2929.000],  loss: 144112.421875, mae: 2136.706055, mean_q: 6.003674\n",
      "wrong_move\n",
      "   3628/500000: episode: 3599, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1098.000 [1098.000, 1098.000],  loss: 170859.156250, mae: 2128.056396, mean_q: 215.361984\n",
      "wrong_move\n",
      "   3629/500000: episode: 3600, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 484.000 [484.000, 484.000],  loss: 87546.062500, mae: 2116.008789, mean_q: 3.591362\n",
      "wrong_move\n",
      "   3630/500000: episode: 3601, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 882.000 [882.000, 882.000],  loss: 440569.875000, mae: 2113.808105, mean_q: 3.469054\n",
      "wrong_move\n",
      "   3631/500000: episode: 3602, duration: 0.158s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3079.000 [3079.000, 3079.000],  loss: 151489.281250, mae: 2116.125977, mean_q: 241.831482\n",
      "wrong_move\n",
      "   3632/500000: episode: 3603, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4090.000 [4090.000, 4090.000],  loss: 138116.046875, mae: 2115.106445, mean_q: 3.536970\n",
      "wrong_move\n",
      "   3633/500000: episode: 3604, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1768.000 [1768.000, 1768.000],  loss: 2359848.500000, mae: 2110.052246, mean_q: 251.066223\n",
      "wrong_move\n",
      "   3634/500000: episode: 3605, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 185.000 [185.000, 185.000],  loss: 35570.406250, mae: 2116.948730, mean_q: 3.478154\n",
      "wrong_move\n",
      "   3635/500000: episode: 3606, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 484.000 [484.000, 484.000],  loss: 91967.093750, mae: 2125.935547, mean_q: 3.390824\n",
      "wrong_move\n",
      "   3636/500000: episode: 3607, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 185.000 [185.000, 185.000],  loss: 478951.750000, mae: 2129.362793, mean_q: 231.656525\n",
      "wrong_move\n",
      "   3637/500000: episode: 3608, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 185.000 [185.000, 185.000],  loss: 58856.414062, mae: 2128.034668, mean_q: 3.446609\n",
      "wrong_move\n",
      "   3638/500000: episode: 3609, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 185.000 [185.000, 185.000],  loss: 123280.765625, mae: 2123.472168, mean_q: 65.796959\n",
      "wrong_move\n",
      "   3639/500000: episode: 3610, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2688.000 [2688.000, 2688.000],  loss: 77224.531250, mae: 2123.618652, mean_q: 3.493408\n",
      "wrong_move\n",
      "   3640/500000: episode: 3611, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3638.000 [3638.000, 3638.000],  loss: 63569.292969, mae: 2125.840820, mean_q: 221.535278\n",
      "wrong_move\n",
      "   3641/500000: episode: 3612, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3485.000 [3485.000, 3485.000],  loss: 78129.734375, mae: 2130.771729, mean_q: 97.675735\n",
      "wrong_move\n",
      "   3642/500000: episode: 3613, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2464.000 [2464.000, 2464.000],  loss: 53827548.000000, mae: 2135.715820, mean_q: 191.764832\n",
      "wrong_move\n",
      "   3643/500000: episode: 3614, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 793.000 [793.000, 793.000],  loss: 99025.351562, mae: 2079.564697, mean_q: 3.564427\n",
      "wrong_move\n",
      "   3644/500000: episode: 3615, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 185.000 [185.000, 185.000],  loss: 488600.593750, mae: 2047.015625, mean_q: 202.860901\n",
      "wrong_move\n",
      "   3645/500000: episode: 3616, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 185.000 [185.000, 185.000],  loss: 131510.375000, mae: 2029.696655, mean_q: 168.299637\n",
      "wrong_move\n",
      "   3646/500000: episode: 3617, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2535.000 [2535.000, 2535.000],  loss: 580583.437500, mae: 2028.910889, mean_q: 268.424561\n",
      "wrong_move\n",
      "   3647/500000: episode: 3618, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3468.000 [3468.000, 3468.000],  loss: 471956.281250, mae: 2033.976807, mean_q: 356.911285\n",
      "wrong_move\n",
      "   3648/500000: episode: 3619, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 185.000 [185.000, 185.000],  loss: 511018.031250, mae: 2048.423828, mean_q: 382.620941\n",
      "wrong_move\n",
      "   3649/500000: episode: 3620, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2486.000 [2486.000, 2486.000],  loss: 1103307.875000, mae: 2065.847412, mean_q: 128.565125\n",
      "wrong_move\n",
      "   3650/500000: episode: 3621, duration: 0.146s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2309.000 [2309.000, 2309.000],  loss: 180229.281250, mae: 2079.430420, mean_q: 511.261169\n",
      "wrong_move\n",
      "   3651/500000: episode: 3622, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 185.000 [185.000, 185.000],  loss: 88764.898438, mae: 2097.916992, mean_q: 307.141113\n",
      "wrong_move\n",
      "   3652/500000: episode: 3623, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 267.000 [267.000, 267.000],  loss: 594808.687500, mae: 2113.865723, mean_q: 463.555084\n",
      "wrong_move\n",
      "   3653/500000: episode: 3624, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2535.000 [2535.000, 2535.000],  loss: 106090.234375, mae: 2119.054688, mean_q: 404.332764\n",
      "wrong_move\n",
      "   3654/500000: episode: 3625, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 185.000 [185.000, 185.000],  loss: 505898.468750, mae: 2120.853271, mean_q: 495.297546\n",
      "wrong_move\n",
      "   3655/500000: episode: 3626, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 141.000 [141.000, 141.000],  loss: 107724.507812, mae: 2117.251465, mean_q: 325.252808\n",
      "wrong_move\n",
      "   3656/500000: episode: 3627, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 468299.125000, mae: 2115.741211, mean_q: 398.278503\n",
      "wrong_move\n",
      "   3657/500000: episode: 3628, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 185.000 [185.000, 185.000],  loss: 468446.312500, mae: 2115.550293, mean_q: 500.139435\n",
      "wrong_move\n",
      "   3658/500000: episode: 3629, duration: 0.162s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 0.000 [0.000, 0.000],  loss: 830895.187500, mae: 2119.424072, mean_q: 502.123688\n",
      "wrong_move\n",
      "   3659/500000: episode: 3630, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3437.000 [3437.000, 3437.000],  loss: 156221.593750, mae: 2119.237061, mean_q: 538.127075\n",
      "wrong_move\n",
      "   3660/500000: episode: 3631, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1874.000 [1874.000, 1874.000],  loss: 175181.671875, mae: 2119.536377, mean_q: 453.415314\n",
      "wrong_move\n",
      "   3661/500000: episode: 3632, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 53905.500000, mae: 2115.527344, mean_q: 380.684692\n",
      "wrong_move\n",
      "   3662/500000: episode: 3633, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 850.000 [850.000, 850.000],  loss: 462052.312500, mae: 2110.935303, mean_q: 409.797516\n",
      "wrong_move\n",
      "   3663/500000: episode: 3634, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1333.000 [1333.000, 1333.000],  loss: 483372.656250, mae: 2112.087891, mean_q: 619.888672\n",
      "wrong_move\n",
      "   3664/500000: episode: 3635, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2741.000 [2741.000, 2741.000],  loss: 273028.906250, mae: 2116.138428, mean_q: 422.080292\n",
      "wrong_move\n",
      "   3665/500000: episode: 3636, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2998.000 [2998.000, 2998.000],  loss: 737382.125000, mae: 2124.674316, mean_q: 501.480713\n",
      "wrong_move\n",
      "   3666/500000: episode: 3637, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 0.000 [0.000, 0.000],  loss: 478319.937500, mae: 2131.852539, mean_q: 494.777496\n",
      "wrong_move\n",
      "   3667/500000: episode: 3638, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 735.000 [735.000, 735.000],  loss: 445470.687500, mae: 2134.607666, mean_q: 493.399963\n",
      "wrong_move\n",
      "   3668/500000: episode: 3639, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 0.000 [0.000, 0.000],  loss: 131463.234375, mae: 2133.190430, mean_q: 467.642456\n",
      "wrong_move\n",
      "   3669/500000: episode: 3640, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1872.000 [1872.000, 1872.000],  loss: 128957.335938, mae: 2137.078613, mean_q: 395.449707\n",
      "wrong_move\n",
      "   3670/500000: episode: 3641, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3613.000 [3613.000, 3613.000],  loss: 478763.187500, mae: 2142.321533, mean_q: 368.206055\n",
      "wrong_move\n",
      "   3671/500000: episode: 3642, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2177.000 [2177.000, 2177.000],  loss: 269750.687500, mae: 2145.779785, mean_q: 466.667877\n",
      "wrong_move\n",
      "   3672/500000: episode: 3643, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2344.000 [2344.000, 2344.000],  loss: 209241.562500, mae: 2154.575439, mean_q: 222.368225\n",
      "wrong_move\n",
      "   3673/500000: episode: 3644, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3954.000 [3954.000, 3954.000],  loss: 96283.250000, mae: 2162.594482, mean_q: 439.626343\n",
      "wrong_move\n",
      "   3674/500000: episode: 3645, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3801.000 [3801.000, 3801.000],  loss: 499608.187500, mae: 2167.676270, mean_q: 436.915527\n",
      "wrong_move\n",
      "   3675/500000: episode: 3646, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 546.000 [546.000, 546.000],  loss: 34038836.000000, mae: 2165.220703, mean_q: 469.592743\n",
      "wrong_move\n",
      "   3676/500000: episode: 3647, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3643.000 [3643.000, 3643.000],  loss: 324883.875000, mae: 2144.776367, mean_q: 507.868927\n",
      "wrong_move\n",
      "   3677/500000: episode: 3648, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1704.000 [1704.000, 1704.000],  loss: 508652.156250, mae: 2126.188721, mean_q: 550.124695\n",
      "wrong_move\n",
      "   3678/500000: episode: 3649, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3503.000 [3503.000, 3503.000],  loss: 136363.953125, mae: 2109.863525, mean_q: 583.700623\n",
      "wrong_move\n",
      "   3679/500000: episode: 3650, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 614.000 [614.000, 614.000],  loss: 82025.484375, mae: 2103.315186, mean_q: 631.854614\n",
      "wrong_move\n",
      "   3680/500000: episode: 3651, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2575.000 [2575.000, 2575.000],  loss: 66686.148438, mae: 2101.264893, mean_q: 508.969879\n",
      "wrong_move\n",
      "   3681/500000: episode: 3652, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2358.000 [2358.000, 2358.000],  loss: 125249.867188, mae: 2106.389648, mean_q: 732.521423\n",
      "wrong_move\n",
      "   3682/500000: episode: 3653, duration: 0.142s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3954.000 [3954.000, 3954.000],  loss: 110121.632812, mae: 2110.878906, mean_q: 703.277649\n",
      "wrong_move\n",
      "   3683/500000: episode: 3654, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1874.000 [1874.000, 1874.000],  loss: 115901.882812, mae: 2116.718262, mean_q: 741.582275\n",
      "wrong_move\n",
      "   3684/500000: episode: 3655, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 546.000 [546.000, 546.000],  loss: 196499.609375, mae: 2122.666748, mean_q: 726.498291\n",
      "wrong_move\n",
      "   3685/500000: episode: 3656, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3503.000 [3503.000, 3503.000],  loss: 88717.843750, mae: 2129.330322, mean_q: 981.963379\n",
      "wrong_move\n",
      "   3687/500000: episode: 3657, duration: 0.197s, episode steps:   2, steps per second:  10, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 3503.000 [3503.000, 3503.000],  loss: 413966.968750, mae: 2135.192871, mean_q: 950.461914\n",
      "wrong_move\n",
      "   3688/500000: episode: 3658, duration: 0.149s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 160061.843750, mae: 2152.728516, mean_q: 761.034424\n",
      "wrong_move\n",
      "   3689/500000: episode: 3659, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 546.000 [546.000, 546.000],  loss: 79647.687500, mae: 2165.052246, mean_q: 775.325562\n",
      "wrong_move\n",
      "   3690/500000: episode: 3660, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 546.000 [546.000, 546.000],  loss: 185220.890625, mae: 2175.876465, mean_q: 834.440796\n",
      "wrong_move\n",
      "   3691/500000: episode: 3661, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2827.000 [2827.000, 2827.000],  loss: 282746.000000, mae: 2183.053711, mean_q: 874.462708\n",
      "wrong_move\n",
      "   3692/500000: episode: 3662, duration: 0.141s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 633.000 [633.000, 633.000],  loss: 174082.328125, mae: 2187.216309, mean_q: 601.510315\n",
      "wrong_move\n",
      "   3693/500000: episode: 3663, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 164.000 [164.000, 164.000],  loss: 1999593.375000, mae: 2195.470215, mean_q: 845.742249\n",
      "wrong_move\n",
      "   3694/500000: episode: 3664, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 546.000 [546.000, 546.000],  loss: 573427.312500, mae: 2193.758301, mean_q: 949.577515\n",
      "wrong_move\n",
      "   3695/500000: episode: 3665, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3284.000 [3284.000, 3284.000],  loss: 141254.796875, mae: 2189.196777, mean_q: 992.240051\n",
      "wrong_move\n",
      "   3696/500000: episode: 3666, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1544.000 [1544.000, 1544.000],  loss: 481730.906250, mae: 2181.272461, mean_q: 972.277649\n",
      "wrong_move\n",
      "   3697/500000: episode: 3667, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: 649835.875000, mae: 2176.774902, mean_q: 944.110657\n",
      "wrong_move\n",
      "   3698/500000: episode: 3668, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1961.000 [1961.000, 1961.000],  loss: 451618.500000, mae: 2166.081543, mean_q: 805.340027\n",
      "wrong_move\n",
      "   3699/500000: episode: 3669, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 546.000 [546.000, 546.000],  loss: 38222.226562, mae: 2154.195557, mean_q: 811.221252\n",
      "wrong_move\n",
      "   3700/500000: episode: 3670, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2038.000 [2038.000, 2038.000],  loss: 70241.109375, mae: 2144.872070, mean_q: 818.511169\n",
      "wrong_move\n",
      "   3701/500000: episode: 3671, duration: 0.143s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2038.000 [2038.000, 2038.000],  loss: 150955.031250, mae: 2142.303955, mean_q: 979.035095\n",
      "wrong_move\n",
      "   3702/500000: episode: 3672, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2344.000 [2344.000, 2344.000],  loss: 189931.062500, mae: 2142.279541, mean_q: 961.486328\n",
      "wrong_move\n",
      "   3703/500000: episode: 3673, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1241.000 [1241.000, 1241.000],  loss: 693542.000000, mae: 2137.788330, mean_q: 867.134766\n",
      "wrong_move\n",
      "   3704/500000: episode: 3674, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2038.000 [2038.000, 2038.000],  loss: 82436.515625, mae: 2133.491211, mean_q: 791.042419\n",
      "wrong_move\n",
      "   3705/500000: episode: 3675, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2344.000 [2344.000, 2344.000],  loss: 562011.625000, mae: 2137.568115, mean_q: 618.948425\n",
      "wrong_move\n",
      "   3706/500000: episode: 3676, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 314.000 [314.000, 314.000],  loss: 85287.375000, mae: 2144.408936, mean_q: 769.108337\n",
      "wrong_move\n",
      "   3707/500000: episode: 3677, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1132.000 [1132.000, 1132.000],  loss: 90684.515625, mae: 2144.128662, mean_q: 795.379272\n",
      "wrong_move\n",
      "   3708/500000: episode: 3678, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 215.000 [215.000, 215.000],  loss: 88602.554688, mae: 2145.673340, mean_q: 190.910431\n",
      "wrong_move\n",
      "   3709/500000: episode: 3679, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2038.000 [2038.000, 2038.000],  loss: 534183.312500, mae: 2152.742188, mean_q: 571.775818\n",
      "wrong_move\n",
      "   3710/500000: episode: 3680, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4072.000 [4072.000, 4072.000],  loss: 107626.781250, mae: 2165.430176, mean_q: 595.101013\n",
      "wrong_move\n",
      "   3711/500000: episode: 3681, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1583.000 [1583.000, 1583.000],  loss: 81177.937500, mae: 2175.379883, mean_q: 560.355347\n",
      "wrong_move\n",
      "   3712/500000: episode: 3682, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1467.000 [1467.000, 1467.000],  loss: 2084389.875000, mae: 2186.933105, mean_q: 551.039062\n",
      "wrong_move\n",
      "   3713/500000: episode: 3683, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 406.000 [406.000, 406.000],  loss: 95860.593750, mae: 2192.955078, mean_q: 597.077209\n",
      "wrong_move\n",
      "   3714/500000: episode: 3684, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 406.000 [406.000, 406.000],  loss: 108509.726562, mae: 2198.249023, mean_q: 474.766174\n",
      "wrong_move\n",
      "   3715/500000: episode: 3685, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3166.000 [3166.000, 3166.000],  loss: 139475.421875, mae: 2203.517334, mean_q: 98.206650\n",
      "wrong_move\n",
      "   3716/500000: episode: 3686, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 664.000 [664.000, 664.000],  loss: 893675.625000, mae: 2209.610840, mean_q: 264.003387\n",
      "wrong_move\n",
      "   3717/500000: episode: 3687, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 157.000 [157.000, 157.000],  loss: 470261.187500, mae: 2213.832520, mean_q: 80.634468\n",
      "wrong_move\n",
      "   3718/500000: episode: 3688, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 800.000 [800.000, 800.000],  loss: 48111.992188, mae: 2212.492920, mean_q: 374.567169\n",
      "wrong_move\n",
      "   3719/500000: episode: 3689, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3451.000 [3451.000, 3451.000],  loss: 282852.250000, mae: 2211.027344, mean_q: 564.853943\n",
      "wrong_move\n",
      "   3720/500000: episode: 3690, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 406.000 [406.000, 406.000],  loss: 105757.195312, mae: 2195.928955, mean_q: 351.681641\n",
      "wrong_move\n",
      "   3721/500000: episode: 3691, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2139.000 [2139.000, 2139.000],  loss: 48677.875000, mae: 2184.715332, mean_q: 209.543945\n",
      "wrong_move\n",
      "   3722/500000: episode: 3692, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1865.000 [1865.000, 1865.000],  loss: 91878.406250, mae: 2176.459961, mean_q: 88.560417\n",
      "wrong_move\n",
      "   3723/500000: episode: 3693, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 149.000 [149.000, 149.000],  loss: 507213.656250, mae: 2175.450439, mean_q: 247.293564\n",
      "wrong_move\n",
      "   3724/500000: episode: 3694, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 149.000 [149.000, 149.000],  loss: 129241.343750, mae: 2178.359375, mean_q: 414.072662\n",
      "wrong_move\n",
      "   3725/500000: episode: 3695, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 658.000 [658.000, 658.000],  loss: 73241.156250, mae: 2182.781738, mean_q: 163.279846\n",
      "wrong_move\n",
      "   3726/500000: episode: 3696, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2499.000 [2499.000, 2499.000],  loss: 102271.156250, mae: 2191.018799, mean_q: 372.977783\n",
      "wrong_move\n",
      "   3727/500000: episode: 3697, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 406.000 [406.000, 406.000],  loss: 289459.218750, mae: 2194.489990, mean_q: 187.710327\n",
      "wrong_move\n",
      "   3728/500000: episode: 3698, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 149.000 [149.000, 149.000],  loss: 516311.312500, mae: 2199.306152, mean_q: 147.375031\n",
      "wrong_move\n",
      "   3729/500000: episode: 3699, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1204.000 [1204.000, 1204.000],  loss: 467003.937500, mae: 2196.001953, mean_q: 294.464355\n",
      "wrong_move\n",
      "   3730/500000: episode: 3700, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 406.000 [406.000, 406.000],  loss: 465705.500000, mae: 2195.470703, mean_q: 654.324402\n",
      "wrong_move\n",
      "   3731/500000: episode: 3701, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3252.000 [3252.000, 3252.000],  loss: 79331.289062, mae: 2198.764160, mean_q: 92.772987\n",
      "wrong_move\n",
      "   3732/500000: episode: 3702, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2460.000 [2460.000, 2460.000],  loss: 30401476.000000, mae: 2204.204590, mean_q: 547.756531\n",
      "wrong_move\n",
      "   3733/500000: episode: 3703, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 518.000 [518.000, 518.000],  loss: 88454.382812, mae: 2204.267334, mean_q: 355.743958\n",
      "wrong_move\n",
      "   3734/500000: episode: 3704, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3451.000 [3451.000, 3451.000],  loss: 462054.500000, mae: 2207.316650, mean_q: 463.060211\n",
      "wrong_move\n",
      "   3735/500000: episode: 3705, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 518.000 [518.000, 518.000],  loss: 51387.105469, mae: 2205.134521, mean_q: 152.638596\n",
      "wrong_move\n",
      "   3736/500000: episode: 3706, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 518.000 [518.000, 518.000],  loss: 104928.421875, mae: 2206.267578, mean_q: 101.090683\n",
      "wrong_move\n",
      "   3737/500000: episode: 3707, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 518.000 [518.000, 518.000],  loss: 56862.816406, mae: 2208.759277, mean_q: 121.929611\n",
      "wrong_move\n",
      "   3738/500000: episode: 3708, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1974.000 [1974.000, 1974.000],  loss: 492405.750000, mae: 2219.712891, mean_q: 96.492073\n",
      "wrong_move\n",
      "   3739/500000: episode: 3709, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 518.000 [518.000, 518.000],  loss: 208803.031250, mae: 2227.332031, mean_q: 178.124054\n",
      "wrong_move\n",
      "   3740/500000: episode: 3710, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3451.000 [3451.000, 3451.000],  loss: 52740.230469, mae: 2226.474121, mean_q: 77.951508\n",
      "wrong_move\n",
      "   3741/500000: episode: 3711, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3876.000 [3876.000, 3876.000],  loss: 445896.437500, mae: 2221.413574, mean_q: 155.581482\n",
      "wrong_move\n",
      "   3742/500000: episode: 3712, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 518.000 [518.000, 518.000],  loss: 236201.156250, mae: 2215.731934, mean_q: 211.625885\n",
      "wrong_move\n",
      "   3743/500000: episode: 3713, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3174.000 [3174.000, 3174.000],  loss: 121031.570312, mae: 2208.583252, mean_q: 138.618408\n",
      "wrong_move\n",
      "   3744/500000: episode: 3714, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 758.000 [758.000, 758.000],  loss: 34788.566406, mae: 2206.870117, mean_q: 7.983295\n",
      "wrong_move\n",
      "   3745/500000: episode: 3715, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2449.000 [2449.000, 2449.000],  loss: 51479.785156, mae: 2205.644043, mean_q: 77.841133\n",
      "wrong_move\n",
      "   3746/500000: episode: 3716, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 518.000 [518.000, 518.000],  loss: 450423.593750, mae: 2201.422607, mean_q: 37.093590\n",
      "wrong_move\n",
      "   3747/500000: episode: 3717, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 518.000 [518.000, 518.000],  loss: 295679.281250, mae: 2197.866699, mean_q: 11.226847\n",
      "wrong_move\n",
      "   3748/500000: episode: 3718, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 518.000 [518.000, 518.000],  loss: 163974.953125, mae: 2194.769043, mean_q: 312.151794\n",
      "wrong_move\n",
      "   3749/500000: episode: 3719, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3484.000 [3484.000, 3484.000],  loss: 1062330.250000, mae: 2197.726562, mean_q: 130.879013\n",
      "wrong_move\n",
      "   3750/500000: episode: 3720, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1001.000 [1001.000, 1001.000],  loss: 118198.968750, mae: 2200.702148, mean_q: 64.843376\n",
      "wrong_move\n",
      "   3751/500000: episode: 3721, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2002.000 [2002.000, 2002.000],  loss: 187316.265625, mae: 2203.284668, mean_q: 116.275551\n",
      "wrong_move\n",
      "   3752/500000: episode: 3722, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2585.000 [2585.000, 2585.000],  loss: 71397.625000, mae: 2206.708008, mean_q: 93.399330\n",
      "wrong_move\n",
      "   3753/500000: episode: 3723, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2002.000 [2002.000, 2002.000],  loss: 86500.718750, mae: 2204.144775, mean_q: 60.027691\n",
      "wrong_move\n",
      "   3754/500000: episode: 3724, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2585.000 [2585.000, 2585.000],  loss: 81445.984375, mae: 2203.587891, mean_q: 52.301556\n",
      "wrong_move\n",
      "   3755/500000: episode: 3725, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3842.000 [3842.000, 3842.000],  loss: 116633.515625, mae: 2206.364746, mean_q: 203.593887\n",
      "wrong_move\n",
      "   3756/500000: episode: 3726, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1724.000 [1724.000, 1724.000],  loss: 329186.281250, mae: 2212.146484, mean_q: 21.311359\n",
      "wrong_move\n",
      "   3757/500000: episode: 3727, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 832.000 [832.000, 832.000],  loss: 75990.992188, mae: 2216.209473, mean_q: 425.870178\n",
      "wrong_move\n",
      "   3758/500000: episode: 3728, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2002.000 [2002.000, 2002.000],  loss: 181873.781250, mae: 2220.848633, mean_q: 176.578918\n",
      "wrong_move\n",
      "   3759/500000: episode: 3729, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3318.000 [3318.000, 3318.000],  loss: 33818.015625, mae: 2229.617188, mean_q: 518.067505\n",
      "wrong_move\n",
      "   3760/500000: episode: 3730, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1001.000 [1001.000, 1001.000],  loss: 429038.000000, mae: 2231.317383, mean_q: 3.073511\n",
      "wrong_move\n",
      "   3761/500000: episode: 3731, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1503.000 [1503.000, 1503.000],  loss: 130061.078125, mae: 2237.079590, mean_q: 483.905792\n",
      "wrong_move\n",
      "   3762/500000: episode: 3732, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1784.000 [1784.000, 1784.000],  loss: 91361.312500, mae: 2235.850342, mean_q: 104.594116\n",
      "wrong_move\n",
      "   3763/500000: episode: 3733, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3842.000 [3842.000, 3842.000],  loss: 33042.023438, mae: 2234.243408, mean_q: 479.763519\n",
      "wrong_move\n",
      "   3764/500000: episode: 3734, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1416.000 [1416.000, 1416.000],  loss: 429591.031250, mae: 2226.962891, mean_q: 107.471581\n",
      "wrong_move\n",
      "   3765/500000: episode: 3735, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 652.000 [652.000, 652.000],  loss: 240389.046875, mae: 2214.315430, mean_q: 96.757912\n",
      "wrong_move\n",
      "   3766/500000: episode: 3736, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1784.000 [1784.000, 1784.000],  loss: 162108.171875, mae: 2206.574463, mean_q: 283.596985\n",
      "wrong_move\n",
      "   3767/500000: episode: 3737, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3543.000 [3543.000, 3543.000],  loss: 18754964.000000, mae: 2204.568848, mean_q: 651.340027\n",
      "wrong_move\n",
      "   3768/500000: episode: 3738, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1264.000 [1264.000, 1264.000],  loss: 26882.929688, mae: 2201.945312, mean_q: 166.362305\n",
      "wrong_move\n",
      "   3769/500000: episode: 3739, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3842.000 [3842.000, 3842.000],  loss: 188497.109375, mae: 2200.566650, mean_q: 72.296616\n",
      "wrong_move\n",
      "   3770/500000: episode: 3740, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 431.000 [431.000, 431.000],  loss: 58396.847656, mae: 2203.401123, mean_q: 367.732361\n",
      "wrong_move\n",
      "   3771/500000: episode: 3741, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 215.000 [215.000, 215.000],  loss: 54071.214844, mae: 2211.832520, mean_q: 3.138701\n",
      "wrong_move\n",
      "   3772/500000: episode: 3742, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2798.000 [2798.000, 2798.000],  loss: 70486.179688, mae: 2224.856445, mean_q: 381.354004\n",
      "wrong_move\n",
      "   3773/500000: episode: 3743, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3842.000 [3842.000, 3842.000],  loss: 511333.187500, mae: 2230.311035, mean_q: 226.905655\n",
      "wrong_move\n",
      "   3774/500000: episode: 3744, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3370.000 [3370.000, 3370.000],  loss: 481548.281250, mae: 2235.441406, mean_q: 523.315063\n",
      "wrong_move\n",
      "   3775/500000: episode: 3745, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1527.000 [1527.000, 1527.000],  loss: 51104.882812, mae: 2233.538574, mean_q: 138.189316\n",
      "wrong_move\n",
      "   3776/500000: episode: 3746, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3573.000 [3573.000, 3573.000],  loss: 496852.312500, mae: 2237.857666, mean_q: 22.668163\n",
      "wrong_move\n",
      "   3777/500000: episode: 3747, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3842.000 [3842.000, 3842.000],  loss: 302430.281250, mae: 2244.634033, mean_q: 251.540421\n",
      "wrong_move\n",
      "   3778/500000: episode: 3748, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1865.000 [1865.000, 1865.000],  loss: 31931.066406, mae: 2246.469238, mean_q: 126.139107\n",
      "wrong_move\n",
      "   3779/500000: episode: 3749, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1865.000 [1865.000, 1865.000],  loss: 367476.968750, mae: 2246.128906, mean_q: 77.813995\n",
      "wrong_move\n",
      "   3780/500000: episode: 3750, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3405.000 [3405.000, 3405.000],  loss: 109017.437500, mae: 2240.622803, mean_q: 603.240662\n",
      "wrong_move\n",
      "   3781/500000: episode: 3751, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1865.000 [1865.000, 1865.000],  loss: 43826.453125, mae: 2229.703125, mean_q: 34.411541\n",
      "wrong_move\n",
      "   3782/500000: episode: 3752, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3842.000 [3842.000, 3842.000],  loss: 143237.921875, mae: 2222.213379, mean_q: 3.162255\n",
      "wrong_move\n",
      "   3783/500000: episode: 3753, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1865.000 [1865.000, 1865.000],  loss: 169536.343750, mae: 2214.062988, mean_q: 3.020899\n",
      "wrong_move\n",
      "   3784/500000: episode: 3754, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1865.000 [1865.000, 1865.000],  loss: 169413.031250, mae: 2209.207520, mean_q: 80.214348\n",
      "wrong_move\n",
      "   3785/500000: episode: 3755, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 867.000 [867.000, 867.000],  loss: 709233.125000, mae: 2212.070312, mean_q: 113.878868\n",
      "wrong_move\n",
      "   3786/500000: episode: 3756, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2528.000 [2528.000, 2528.000],  loss: 624054.250000, mae: 2219.179199, mean_q: 704.562134\n",
      "wrong_move\n",
      "   3787/500000: episode: 3757, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1003.000 [1003.000, 1003.000],  loss: 75105.695312, mae: 2224.367676, mean_q: 3.051484\n",
      "wrong_move\n",
      "   3788/500000: episode: 3758, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4076.000 [4076.000, 4076.000],  loss: 155225.437500, mae: 2235.441162, mean_q: 49.555473\n",
      "wrong_move\n",
      "   3789/500000: episode: 3759, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3699.000 [3699.000, 3699.000],  loss: 149879.328125, mae: 2246.560059, mean_q: 65.364082\n",
      "wrong_move\n",
      "   3790/500000: episode: 3760, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1865.000 [1865.000, 1865.000],  loss: 85098.015625, mae: 2251.078369, mean_q: 42.237568\n",
      "wrong_move\n",
      "   3791/500000: episode: 3761, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3881.000 [3881.000, 3881.000],  loss: 293278.437500, mae: 2252.977783, mean_q: 3.058754\n",
      "wrong_move\n",
      "   3792/500000: episode: 3762, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2217.000 [2217.000, 2217.000],  loss: 145070.906250, mae: 2250.460938, mean_q: 3.083400\n",
      "wrong_move\n",
      "   3793/500000: episode: 3763, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2657.000 [2657.000, 2657.000],  loss: 457840.250000, mae: 2249.650879, mean_q: 173.110077\n",
      "wrong_move\n",
      "   3794/500000: episode: 3764, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2519.000 [2519.000, 2519.000],  loss: 43594.343750, mae: 2247.431152, mean_q: 23.681000\n",
      "wrong_move\n",
      "   3795/500000: episode: 3765, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3318.000 [3318.000, 3318.000],  loss: 265238.968750, mae: 2239.876953, mean_q: 818.923096\n",
      "wrong_move\n",
      "   3796/500000: episode: 3766, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4059.000 [4059.000, 4059.000],  loss: 90190.500000, mae: 2220.218994, mean_q: 672.583252\n",
      "wrong_move\n",
      "   3797/500000: episode: 3767, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2710.000 [2710.000, 2710.000],  loss: 190710.078125, mae: 2198.983887, mean_q: 56.283867\n",
      "wrong_move\n",
      "   3798/500000: episode: 3768, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1337.000 [1337.000, 1337.000],  loss: 39435.234375, mae: 2192.732422, mean_q: 3.085889\n",
      "wrong_move\n",
      "   3799/500000: episode: 3769, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1053.000 [1053.000, 1053.000],  loss: 75395.429688, mae: 2195.255859, mean_q: 3.027128\n",
      "wrong_move\n",
      "   3800/500000: episode: 3770, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3168.000 [3168.000, 3168.000],  loss: 656647.375000, mae: 2201.781494, mean_q: 3.082830\n",
      "wrong_move\n",
      "   3801/500000: episode: 3771, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 244269.375000, mae: 2211.010742, mean_q: 240.859344\n",
      "wrong_move\n",
      "   3802/500000: episode: 3772, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 54.000 [54.000, 54.000],  loss: 91516.351562, mae: 2218.057373, mean_q: 54.382343\n",
      "wrong_move\n",
      "   3803/500000: episode: 3773, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1671.000 [1671.000, 1671.000],  loss: 116380.093750, mae: 2210.936035, mean_q: 32.367924\n",
      "wrong_move\n",
      "   3804/500000: episode: 3774, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2605.000 [2605.000, 2605.000],  loss: 222098.078125, mae: 2195.080566, mean_q: 583.632202\n",
      "wrong_move\n",
      "   3805/500000: episode: 3775, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3427.000 [3427.000, 3427.000],  loss: 121609.562500, mae: 2181.559570, mean_q: 2.966839\n",
      "wrong_move\n",
      "   3806/500000: episode: 3776, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 902.000 [902.000, 902.000],  loss: 69188.890625, mae: 2178.958984, mean_q: 8.963797\n",
      "wrong_move\n",
      "   3807/500000: episode: 3777, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: 493568.625000, mae: 2186.703613, mean_q: 3.043919\n",
      "wrong_move\n",
      "   3808/500000: episode: 3778, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1572.000 [1572.000, 1572.000],  loss: 162515.218750, mae: 2203.332031, mean_q: 60.762077\n",
      "wrong_move\n",
      "   3809/500000: episode: 3779, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3484.000 [3484.000, 3484.000],  loss: 461780.687500, mae: 2226.529785, mean_q: 258.312561\n",
      "wrong_move\n",
      "   3810/500000: episode: 3780, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4025.000 [4025.000, 4025.000],  loss: 234749.500000, mae: 2245.286865, mean_q: 671.881836\n",
      "wrong_move\n",
      "   3811/500000: episode: 3781, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1990.000 [1990.000, 1990.000],  loss: 5758648.500000, mae: 2258.832520, mean_q: 577.818176\n",
      "wrong_move\n",
      "   3812/500000: episode: 3782, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2528.000 [2528.000, 2528.000],  loss: 129447.531250, mae: 2261.237061, mean_q: 16.950861\n",
      "wrong_move\n",
      "   3813/500000: episode: 3783, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1990.000 [1990.000, 1990.000],  loss: 111308.875000, mae: 2263.787354, mean_q: 42.190475\n",
      "wrong_move\n",
      "   3814/500000: episode: 3784, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3879.000 [3879.000, 3879.000],  loss: 65080.691406, mae: 2266.467773, mean_q: 515.323120\n",
      "wrong_move\n",
      "   3815/500000: episode: 3785, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1285.000 [1285.000, 1285.000],  loss: 1076169.500000, mae: 2264.005615, mean_q: 16.361088\n",
      "wrong_move\n",
      "   3816/500000: episode: 3786, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 138266.500000, mae: 2255.342041, mean_q: 74.564278\n",
      "wrong_move\n",
      "   3817/500000: episode: 3787, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 645584.500000, mae: 2251.399414, mean_q: 40.451782\n",
      "wrong_move\n",
      "   3818/500000: episode: 3788, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 863.000 [863.000, 863.000],  loss: 814369.000000, mae: 2244.319824, mean_q: 243.192398\n",
      "wrong_move\n",
      "   3819/500000: episode: 3789, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 863.000 [863.000, 863.000],  loss: 5335131.500000, mae: 2235.903320, mean_q: 814.123474\n",
      "wrong_move\n",
      "   3820/500000: episode: 3790, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1902.000 [1902.000, 1902.000],  loss: 75060.929688, mae: 2222.530273, mean_q: 3.184458\n",
      "wrong_move\n",
      "   3821/500000: episode: 3791, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 519.000 [519.000, 519.000],  loss: 354735.593750, mae: 2221.243164, mean_q: 239.600342\n",
      "wrong_move\n",
      "   3822/500000: episode: 3792, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2999.000 [2999.000, 2999.000],  loss: 474339.906250, mae: 2221.750488, mean_q: 323.513916\n",
      "wrong_move\n",
      "   3823/500000: episode: 3793, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3586.000 [3586.000, 3586.000],  loss: 118392.093750, mae: 2225.229248, mean_q: 396.694550\n",
      "wrong_move\n",
      "   3824/500000: episode: 3794, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1763.000 [1763.000, 1763.000],  loss: 71441.601562, mae: 2231.736816, mean_q: 143.677811\n",
      "wrong_move\n",
      "   3825/500000: episode: 3795, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1763.000 [1763.000, 1763.000],  loss: 38582.472656, mae: 2239.495117, mean_q: 70.885353\n",
      "wrong_move\n",
      "   3826/500000: episode: 3796, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3484.000 [3484.000, 3484.000],  loss: 522010.843750, mae: 2249.306152, mean_q: 127.937897\n",
      "wrong_move\n",
      "   3827/500000: episode: 3797, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 430.000 [430.000, 430.000],  loss: 503254.468750, mae: 2259.108887, mean_q: 70.151993\n",
      "wrong_move\n",
      "   3828/500000: episode: 3798, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3473.000 [3473.000, 3473.000],  loss: 73123.554688, mae: 2262.402100, mean_q: 142.528778\n",
      "wrong_move\n",
      "   3829/500000: episode: 3799, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1777.000 [1777.000, 1777.000],  loss: 61296.695312, mae: 2260.879150, mean_q: 123.607132\n",
      "wrong_move\n",
      "   3830/500000: episode: 3800, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2999.000 [2999.000, 2999.000],  loss: 97909.796875, mae: 2255.408203, mean_q: 104.402817\n",
      "wrong_move\n",
      "   3831/500000: episode: 3801, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3586.000 [3586.000, 3586.000],  loss: 91357.265625, mae: 2253.628418, mean_q: 95.274124\n",
      "wrong_move\n",
      "   3832/500000: episode: 3802, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3586.000 [3586.000, 3586.000],  loss: 25234.226562, mae: 2255.037354, mean_q: 109.518478\n",
      "wrong_move\n",
      "   3833/500000: episode: 3803, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1788.000 [1788.000, 1788.000],  loss: 66475.703125, mae: 2252.454590, mean_q: 132.036682\n",
      "wrong_move\n",
      "   3834/500000: episode: 3804, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 571.000 [571.000, 571.000],  loss: 67313.265625, mae: 2251.482666, mean_q: 430.992096\n",
      "wrong_move\n",
      "   3835/500000: episode: 3805, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 86805.429688, mae: 2250.516113, mean_q: 217.847168\n",
      "wrong_move\n",
      "   3836/500000: episode: 3806, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3586.000 [3586.000, 3586.000],  loss: 469264.531250, mae: 2256.354736, mean_q: 830.849731\n",
      "wrong_move\n",
      "   3837/500000: episode: 3807, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3586.000 [3586.000, 3586.000],  loss: 398272.500000, mae: 2257.683350, mean_q: 405.429016\n",
      "wrong_move\n",
      "   3838/500000: episode: 3808, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2755.000 [2755.000, 2755.000],  loss: 247475.859375, mae: 2250.259277, mean_q: 80.013237\n",
      "wrong_move\n",
      "   3839/500000: episode: 3809, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2163.000 [2163.000, 2163.000],  loss: 492910.031250, mae: 2245.270996, mean_q: 91.927017\n",
      "wrong_move\n",
      "   3840/500000: episode: 3810, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2163.000 [2163.000, 2163.000],  loss: 679233.687500, mae: 2245.221191, mean_q: 86.544907\n",
      "wrong_move\n",
      "   3841/500000: episode: 3811, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2760.000 [2760.000, 2760.000],  loss: 328474.281250, mae: 2255.064453, mean_q: 199.605392\n",
      "wrong_move\n",
      "   3842/500000: episode: 3812, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1609.000 [1609.000, 1609.000],  loss: 458220.218750, mae: 2263.719238, mean_q: 176.209625\n",
      "wrong_move\n",
      "   3843/500000: episode: 3813, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2168.000 [2168.000, 2168.000],  loss: 780592.187500, mae: 2269.199219, mean_q: 86.948616\n",
      "wrong_move\n",
      "   3844/500000: episode: 3814, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 446.000 [446.000, 446.000],  loss: 541390.375000, mae: 2266.127197, mean_q: 228.748962\n",
      "wrong_move\n",
      "   3845/500000: episode: 3815, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 104.000 [104.000, 104.000],  loss: 58728.292969, mae: 2264.874268, mean_q: 110.343750\n",
      "wrong_move\n",
      "   3846/500000: episode: 3816, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3470.000 [3470.000, 3470.000],  loss: 78353.578125, mae: 2261.788574, mean_q: 181.772141\n",
      "wrong_move\n",
      "   3847/500000: episode: 3817, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1278.000 [1278.000, 1278.000],  loss: 57904.796875, mae: 2250.505371, mean_q: 385.032501\n",
      "wrong_move\n",
      "   3848/500000: episode: 3818, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 80.000 [80.000, 80.000],  loss: 188249.562500, mae: 2241.227539, mean_q: 271.797791\n",
      "wrong_move\n",
      "   3849/500000: episode: 3819, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2673.000 [2673.000, 2673.000],  loss: 516866.937500, mae: 2233.450684, mean_q: 154.737457\n",
      "wrong_move\n",
      "   3850/500000: episode: 3820, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1167.000 [1167.000, 1167.000],  loss: 268425.312500, mae: 2225.329102, mean_q: 227.851959\n",
      "wrong_move\n",
      "   3851/500000: episode: 3821, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2999.000 [2999.000, 2999.000],  loss: 74995.359375, mae: 2227.210938, mean_q: 198.642563\n",
      "wrong_move\n",
      "   3852/500000: episode: 3822, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2834.000 [2834.000, 2834.000],  loss: 95804.218750, mae: 2236.391113, mean_q: 179.322083\n",
      "wrong_move\n",
      "   3853/500000: episode: 3823, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1295.000 [1295.000, 1295.000],  loss: 537199.000000, mae: 2246.062256, mean_q: 168.234497\n",
      "wrong_move\n",
      "   3854/500000: episode: 3824, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3701.000 [3701.000, 3701.000],  loss: 512427.031250, mae: 2248.679932, mean_q: 293.310791\n",
      "wrong_move\n",
      "   3855/500000: episode: 3825, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2163.000 [2163.000, 2163.000],  loss: 68893.492188, mae: 2246.943359, mean_q: 296.079895\n",
      "wrong_move\n",
      "   3856/500000: episode: 3826, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 51521.804688, mae: 2244.742188, mean_q: 112.482635\n",
      "wrong_move\n",
      "   3857/500000: episode: 3827, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2999.000 [2999.000, 2999.000],  loss: 198746.218750, mae: 2240.639893, mean_q: 591.951599\n",
      "wrong_move\n",
      "   3858/500000: episode: 3828, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2163.000 [2163.000, 2163.000],  loss: 271942.562500, mae: 2234.689941, mean_q: 784.669739\n",
      "wrong_move\n",
      "   3859/500000: episode: 3829, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2047.000 [2047.000, 2047.000],  loss: 524107.468750, mae: 2223.951416, mean_q: 119.595108\n",
      "wrong_move\n",
      "   3860/500000: episode: 3830, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3363.000 [3363.000, 3363.000],  loss: 61077.203125, mae: 2226.906250, mean_q: 230.476013\n",
      "wrong_move\n",
      "   3861/500000: episode: 3831, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 493.000 [493.000, 493.000],  loss: 866979.187500, mae: 2232.527344, mean_q: 219.379242\n",
      "wrong_move\n",
      "   3862/500000: episode: 3832, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1884.000 [1884.000, 1884.000],  loss: 184707.093750, mae: 2235.495117, mean_q: 62.221500\n",
      "wrong_move\n",
      "   3863/500000: episode: 3833, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 493.000 [493.000, 493.000],  loss: 6090412.000000, mae: 2244.298340, mean_q: 551.450867\n",
      "wrong_move\n",
      "   3864/500000: episode: 3834, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 459.000 [459.000, 459.000],  loss: 718901.687500, mae: 2253.058594, mean_q: 145.532730\n",
      "wrong_move\n",
      "   3865/500000: episode: 3835, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1469.000 [1469.000, 1469.000],  loss: 225707.093750, mae: 2258.198242, mean_q: 719.470642\n",
      "wrong_move\n",
      "   3866/500000: episode: 3836, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 459.000 [459.000, 459.000],  loss: 410067.750000, mae: 2256.186523, mean_q: 98.241806\n",
      "wrong_move\n",
      "   3867/500000: episode: 3837, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 459.000 [459.000, 459.000],  loss: 504044.718750, mae: 2252.072998, mean_q: 69.877800\n",
      "wrong_move\n",
      "   3868/500000: episode: 3838, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 459.000 [459.000, 459.000],  loss: 142298.875000, mae: 2250.908691, mean_q: 186.834732\n",
      "wrong_move\n",
      "   3869/500000: episode: 3839, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1270.000 [1270.000, 1270.000],  loss: 422774.968750, mae: 2248.943115, mean_q: 107.940132\n",
      "wrong_move\n",
      "   3870/500000: episode: 3840, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3938.000 [3938.000, 3938.000],  loss: 113667.929688, mae: 2254.141357, mean_q: 29.384655\n",
      "wrong_move\n",
      "   3871/500000: episode: 3841, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3341.000 [3341.000, 3341.000],  loss: 506147.375000, mae: 2260.842773, mean_q: 64.277161\n",
      "wrong_move\n",
      "   3872/500000: episode: 3842, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1295.000 [1295.000, 1295.000],  loss: 434673.000000, mae: 2270.420166, mean_q: 376.792542\n",
      "wrong_move\n",
      "   3873/500000: episode: 3843, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 459.000 [459.000, 459.000],  loss: 110883.359375, mae: 2275.342529, mean_q: 56.053135\n",
      "wrong_move\n",
      "   3874/500000: episode: 3844, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 185224.343750, mae: 2277.981934, mean_q: 77.674034\n",
      "wrong_move\n",
      "   3875/500000: episode: 3845, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 38.000 [38.000, 38.000],  loss: 756927.250000, mae: 2278.658203, mean_q: 56.259628\n",
      "wrong_move\n",
      "   3876/500000: episode: 3846, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 83.000 [83.000, 83.000],  loss: 942533.000000, mae: 2262.136475, mean_q: 198.890289\n",
      "wrong_move\n",
      "   3877/500000: episode: 3847, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3955.000 [3955.000, 3955.000],  loss: 115060.117188, mae: 2246.144531, mean_q: 615.777344\n",
      "wrong_move\n",
      "   3878/500000: episode: 3848, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 459.000 [459.000, 459.000],  loss: 294292.031250, mae: 2233.858398, mean_q: 175.178574\n",
      "wrong_move\n",
      "   3879/500000: episode: 3849, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 493.000 [493.000, 493.000],  loss: 126908.031250, mae: 2230.778320, mean_q: 11.883462\n",
      "wrong_move\n",
      "   3880/500000: episode: 3850, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 459.000 [459.000, 459.000],  loss: 99775.781250, mae: 2237.446777, mean_q: 79.844810\n",
      "wrong_move\n",
      "   3881/500000: episode: 3851, duration: 0.148s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1409.000 [1409.000, 1409.000],  loss: 869377.125000, mae: 2247.542480, mean_q: 27.736279\n",
      "wrong_move\n",
      "   3882/500000: episode: 3852, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 733.000 [733.000, 733.000],  loss: 165774.515625, mae: 2263.633545, mean_q: 52.536182\n",
      "wrong_move\n",
      "   3883/500000: episode: 3853, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2745.000 [2745.000, 2745.000],  loss: 352522.312500, mae: 2285.539307, mean_q: 27.411842\n",
      "wrong_move\n",
      "   3884/500000: episode: 3854, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 459.000 [459.000, 459.000],  loss: 445319.468750, mae: 2295.631348, mean_q: 24.519978\n",
      "wrong_move\n",
      "   3885/500000: episode: 3855, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2859.000 [2859.000, 2859.000],  loss: 5245140.500000, mae: 2294.352539, mean_q: 599.536438\n",
      "wrong_move\n",
      "   3886/500000: episode: 3856, duration: 0.145s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 73129.156250, mae: 2284.258057, mean_q: 430.455231\n",
      "wrong_move\n",
      "   3887/500000: episode: 3857, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3443.000 [3443.000, 3443.000],  loss: 154015.171875, mae: 2272.219482, mean_q: 49.718983\n",
      "wrong_move\n",
      "   3888/500000: episode: 3858, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2550.000 [2550.000, 2550.000],  loss: 823651.187500, mae: 2267.673340, mean_q: 644.173584\n",
      "wrong_move\n",
      "   3889/500000: episode: 3859, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1201.000 [1201.000, 1201.000],  loss: 98297.500000, mae: 2263.715576, mean_q: 33.581970\n",
      "wrong_move\n",
      "   3890/500000: episode: 3860, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 519.000 [519.000, 519.000],  loss: 88143.039062, mae: 2264.477539, mean_q: 132.098694\n",
      "wrong_move\n",
      "   3891/500000: episode: 3861, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1674.000 [1674.000, 1674.000],  loss: 259324.750000, mae: 2265.021729, mean_q: 132.853012\n",
      "wrong_move\n",
      "   3892/500000: episode: 3862, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 126.000 [126.000, 126.000],  loss: 6117157.500000, mae: 2269.791504, mean_q: 615.259216\n",
      "wrong_move\n",
      "   3893/500000: episode: 3863, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 718.000 [718.000, 718.000],  loss: 5666526.000000, mae: 2274.711426, mean_q: 501.879333\n",
      "wrong_move\n",
      "   3894/500000: episode: 3864, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 247.000 [247.000, 247.000],  loss: 183872.375000, mae: 2291.257812, mean_q: 98.752586\n",
      "wrong_move\n",
      "   3895/500000: episode: 3865, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 913.000 [913.000, 913.000],  loss: 62938.140625, mae: 2310.202881, mean_q: 146.083908\n",
      "wrong_move\n",
      "   3896/500000: episode: 3866, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1835.000 [1835.000, 1835.000],  loss: 437921.937500, mae: 2325.713623, mean_q: 93.141464\n",
      "wrong_move\n",
      "   3897/500000: episode: 3867, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3205.000 [3205.000, 3205.000],  loss: 56915.867188, mae: 2335.346680, mean_q: 62.864357\n",
      "wrong_move\n",
      "   3898/500000: episode: 3868, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2393.000 [2393.000, 2393.000],  loss: 179157.656250, mae: 2338.294922, mean_q: 144.062683\n",
      "wrong_move\n",
      "   3899/500000: episode: 3869, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3589.000 [3589.000, 3589.000],  loss: 109381.289062, mae: 2340.231445, mean_q: 209.371841\n",
      "wrong_move\n",
      "   3900/500000: episode: 3870, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1065.000 [1065.000, 1065.000],  loss: 117208.101562, mae: 2334.774658, mean_q: 403.592194\n",
      "wrong_move\n",
      "   3901/500000: episode: 3871, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1977.000 [1977.000, 1977.000],  loss: 35429.179688, mae: 2317.003906, mean_q: 25.347202\n",
      "wrong_move\n",
      "   3902/500000: episode: 3872, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3646.000 [3646.000, 3646.000],  loss: 168979.968750, mae: 2295.834473, mean_q: 46.740837\n",
      "wrong_move\n",
      "   3903/500000: episode: 3873, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2010.000 [2010.000, 2010.000],  loss: 32423.542969, mae: 2278.781738, mean_q: 3.288714\n",
      "wrong_move\n",
      "   3904/500000: episode: 3874, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 247.000 [247.000, 247.000],  loss: 213177.187500, mae: 2270.063232, mean_q: 196.904892\n",
      "wrong_move\n",
      "   3905/500000: episode: 3875, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1409.000 [1409.000, 1409.000],  loss: 62698.089844, mae: 2270.260254, mean_q: 322.713593\n",
      "wrong_move\n",
      "   3906/500000: episode: 3876, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2325.000 [2325.000, 2325.000],  loss: 78987.031250, mae: 2275.040039, mean_q: 434.098022\n",
      "wrong_move\n",
      "   3907/500000: episode: 3877, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 247.000 [247.000, 247.000],  loss: 488943.468750, mae: 2279.936523, mean_q: 31.571072\n",
      "wrong_move\n",
      "   3908/500000: episode: 3878, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1762.000 [1762.000, 1762.000],  loss: 90027.570312, mae: 2288.038574, mean_q: 151.912659\n",
      "wrong_move\n",
      "   3909/500000: episode: 3879, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1091.000 [1091.000, 1091.000],  loss: 62013.386719, mae: 2294.324707, mean_q: 67.723839\n",
      "wrong_move\n",
      "   3910/500000: episode: 3880, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1495.000 [1495.000, 1495.000],  loss: 805961.500000, mae: 2304.544434, mean_q: 55.253559\n",
      "wrong_move\n",
      "   3911/500000: episode: 3881, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1295.000 [1295.000, 1295.000],  loss: 71164.875000, mae: 2306.174316, mean_q: 68.704483\n",
      "wrong_move\n",
      "   3912/500000: episode: 3882, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1022.000 [1022.000, 1022.000],  loss: 462304.125000, mae: 2300.437988, mean_q: 3.457281\n",
      "wrong_move\n",
      "   3913/500000: episode: 3883, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1295.000 [1295.000, 1295.000],  loss: 6671644.000000, mae: 2298.211426, mean_q: 83.430344\n",
      "wrong_move\n",
      "   3914/500000: episode: 3884, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1295.000 [1295.000, 1295.000],  loss: 664619.250000, mae: 2291.128906, mean_q: 25.197260\n",
      "wrong_move\n",
      "   3915/500000: episode: 3885, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1122.000 [1122.000, 1122.000],  loss: 3442321.750000, mae: 2282.193604, mean_q: 460.500854\n",
      "wrong_move\n",
      "   3916/500000: episode: 3886, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1295.000 [1295.000, 1295.000],  loss: 59532.792969, mae: 2276.951660, mean_q: 10.633873\n",
      "wrong_move\n",
      "   3917/500000: episode: 3887, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1295.000 [1295.000, 1295.000],  loss: 214208.781250, mae: 2276.820801, mean_q: 128.181564\n",
      "wrong_move\n",
      "   3918/500000: episode: 3888, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3514.000 [3514.000, 3514.000],  loss: 37633.035156, mae: 2287.490479, mean_q: 11.356770\n",
      "wrong_move\n",
      "   3919/500000: episode: 3889, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 917.000 [917.000, 917.000],  loss: 63577.992188, mae: 2298.571289, mean_q: 112.672501\n",
      "wrong_move\n",
      "   3920/500000: episode: 3890, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1830.000 [1830.000, 1830.000],  loss: 468059.125000, mae: 2309.309082, mean_q: 3.190593\n",
      "wrong_move\n",
      "   3921/500000: episode: 3891, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 917.000 [917.000, 917.000],  loss: 44865.847656, mae: 2326.092041, mean_q: 110.756699\n",
      "wrong_move\n",
      "   3922/500000: episode: 3892, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 155.000 [155.000, 155.000],  loss: 1919588.625000, mae: 2342.216309, mean_q: 188.813446\n",
      "wrong_move\n",
      "   3923/500000: episode: 3893, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 155.000 [155.000, 155.000],  loss: 62344.355469, mae: 2356.210693, mean_q: 75.172676\n",
      "wrong_move\n",
      "   3924/500000: episode: 3894, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2273.000 [2273.000, 2273.000],  loss: 45867.734375, mae: 2361.801514, mean_q: 73.504196\n",
      "wrong_move\n",
      "   3925/500000: episode: 3895, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1108.000 [1108.000, 1108.000],  loss: 57971.968750, mae: 2359.009033, mean_q: 287.537720\n",
      "wrong_move\n",
      "   3926/500000: episode: 3896, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1091.000 [1091.000, 1091.000],  loss: 242705.031250, mae: 2350.414551, mean_q: 59.542519\n",
      "wrong_move\n",
      "   3927/500000: episode: 3897, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3496.000 [3496.000, 3496.000],  loss: 430296.718750, mae: 2340.454102, mean_q: 3.278409\n",
      "wrong_move\n",
      "   3928/500000: episode: 3898, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1563.000 [1563.000, 1563.000],  loss: 13175955.000000, mae: 2324.432617, mean_q: 562.511353\n",
      "wrong_move\n",
      "   3929/500000: episode: 3899, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2896.000 [2896.000, 2896.000],  loss: 251733.781250, mae: 2290.505371, mean_q: 3.095784\n",
      "wrong_move\n",
      "   3930/500000: episode: 3900, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1474.000 [1474.000, 1474.000],  loss: 144835.359375, mae: 2272.376953, mean_q: 32.498055\n",
      "wrong_move\n",
      "   3931/500000: episode: 3901, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 913.000 [913.000, 913.000],  loss: 463912.031250, mae: 2267.058594, mean_q: 282.037170\n",
      "wrong_move\n",
      "   3932/500000: episode: 3902, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1598.000 [1598.000, 1598.000],  loss: 231058.953125, mae: 2272.520996, mean_q: 5.548861\n",
      "wrong_move\n",
      "   3933/500000: episode: 3903, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1270.000 [1270.000, 1270.000],  loss: 377752.875000, mae: 2284.850586, mean_q: 49.473038\n",
      "wrong_move\n",
      "   3934/500000: episode: 3904, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1563.000 [1563.000, 1563.000],  loss: 81418.421875, mae: 2296.182373, mean_q: 203.859756\n",
      "wrong_move\n",
      "   3935/500000: episode: 3905, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1598.000 [1598.000, 1598.000],  loss: 450296.656250, mae: 2309.933105, mean_q: 100.649261\n",
      "wrong_move\n",
      "   3936/500000: episode: 3906, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 620.000 [620.000, 620.000],  loss: 107585.203125, mae: 2317.522461, mean_q: 9.155519\n",
      "wrong_move\n",
      "   3937/500000: episode: 3907, duration: 0.145s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1453.000 [1453.000, 1453.000],  loss: 70074.343750, mae: 2319.635498, mean_q: 228.374481\n",
      "wrong_move\n",
      "   3938/500000: episode: 3908, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1158.000 [1158.000, 1158.000],  loss: 142584.390625, mae: 2316.712891, mean_q: 173.337692\n",
      "wrong_move\n",
      "   3939/500000: episode: 3909, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1258.000 [1258.000, 1258.000],  loss: 84256.562500, mae: 2311.236328, mean_q: 316.235626\n",
      "wrong_move\n",
      "   3940/500000: episode: 3910, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 674.000 [674.000, 674.000],  loss: 496069.437500, mae: 2301.176270, mean_q: 9.079517\n",
      "wrong_move\n",
      "   3941/500000: episode: 3911, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 674.000 [674.000, 674.000],  loss: 109754.414062, mae: 2297.270020, mean_q: 101.213219\n",
      "wrong_move\n",
      "   3942/500000: episode: 3912, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 674.000 [674.000, 674.000],  loss: 54619.414062, mae: 2294.324707, mean_q: 78.418854\n",
      "wrong_move\n",
      "   3943/500000: episode: 3913, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 674.000 [674.000, 674.000],  loss: 599050.375000, mae: 2294.464600, mean_q: 269.438690\n",
      "wrong_move\n",
      "   3944/500000: episode: 3914, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 223926.046875, mae: 2294.481201, mean_q: 155.424408\n",
      "wrong_move\n",
      "   3945/500000: episode: 3915, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3949.000 [3949.000, 3949.000],  loss: 309578.562500, mae: 2294.215332, mean_q: 25.258541\n",
      "wrong_move\n",
      "   3946/500000: episode: 3916, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1263.000 [1263.000, 1263.000],  loss: 443799.906250, mae: 2298.183350, mean_q: 25.566917\n",
      "wrong_move\n",
      "   3947/500000: episode: 3917, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2631.000 [2631.000, 2631.000],  loss: 641662.687500, mae: 2307.416016, mean_q: 91.849403\n",
      "wrong_move\n",
      "   3948/500000: episode: 3918, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2135.000 [2135.000, 2135.000],  loss: 437397.156250, mae: 2313.736328, mean_q: 46.142208\n",
      "wrong_move\n",
      "   3949/500000: episode: 3919, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2859.000 [2859.000, 2859.000],  loss: 154315.093750, mae: 2324.101074, mean_q: 248.560425\n",
      "wrong_move\n",
      "   3950/500000: episode: 3920, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1381.000 [1381.000, 1381.000],  loss: 842460.625000, mae: 2322.300293, mean_q: 75.841789\n",
      "wrong_move\n",
      "   3951/500000: episode: 3921, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2289.000 [2289.000, 2289.000],  loss: 53260.121094, mae: 2303.250732, mean_q: 383.407349\n",
      "wrong_move\n",
      "   3952/500000: episode: 3922, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2891.000 [2891.000, 2891.000],  loss: 219116.546875, mae: 2292.269531, mean_q: 3.086031\n",
      "wrong_move\n",
      "   3953/500000: episode: 3923, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1868.000 [1868.000, 1868.000],  loss: 485160.125000, mae: 2287.406738, mean_q: 199.451218\n",
      "wrong_move\n",
      "   3954/500000: episode: 3924, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3925.000 [3925.000, 3925.000],  loss: 172910.265625, mae: 2286.748291, mean_q: 91.614418\n",
      "wrong_move\n",
      "   3955/500000: episode: 3925, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 674.000 [674.000, 674.000],  loss: 439888.937500, mae: 2288.037109, mean_q: 212.272705\n",
      "wrong_move\n",
      "   3956/500000: episode: 3926, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 445.000 [445.000, 445.000],  loss: 4607158.500000, mae: 2295.959473, mean_q: 113.892319\n",
      "wrong_move\n",
      "   3957/500000: episode: 3927, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3925.000 [3925.000, 3925.000],  loss: 69630.968750, mae: 2309.673828, mean_q: 396.735321\n",
      "wrong_move\n",
      "   3958/500000: episode: 3928, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3925.000 [3925.000, 3925.000],  loss: 86618.695312, mae: 2323.769043, mean_q: 577.330505\n",
      "wrong_move\n",
      "   3959/500000: episode: 3929, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3756.000 [3756.000, 3756.000],  loss: 72037.898438, mae: 2333.199707, mean_q: 206.592270\n",
      "wrong_move\n",
      "   3960/500000: episode: 3930, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1244.000 [1244.000, 1244.000],  loss: 67538.125000, mae: 2340.168945, mean_q: 126.601479\n",
      "wrong_move\n",
      "   3961/500000: episode: 3931, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3445.000 [3445.000, 3445.000],  loss: 139995.750000, mae: 2340.685303, mean_q: 313.690155\n",
      "wrong_move\n",
      "   3962/500000: episode: 3932, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1613.000 [1613.000, 1613.000],  loss: 76058.843750, mae: 2338.685059, mean_q: 98.681801\n",
      "wrong_move\n",
      "   3963/500000: episode: 3933, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 17.000 [17.000, 17.000],  loss: 46580.562500, mae: 2345.597900, mean_q: 123.028191\n",
      "wrong_move\n",
      "   3964/500000: episode: 3934, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3445.000 [3445.000, 3445.000],  loss: 516675.437500, mae: 2349.431152, mean_q: 371.226440\n",
      "wrong_move\n",
      "   3965/500000: episode: 3935, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2555.000 [2555.000, 2555.000],  loss: 109564.742188, mae: 2354.316650, mean_q: 309.937775\n",
      "wrong_move\n",
      "   3966/500000: episode: 3936, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3251.000 [3251.000, 3251.000],  loss: 459626.000000, mae: 2363.673828, mean_q: 249.184326\n",
      "wrong_move\n",
      "   3967/500000: episode: 3937, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3904.000 [3904.000, 3904.000],  loss: 75556.718750, mae: 2357.918457, mean_q: 721.833069\n",
      "wrong_move\n",
      "   3968/500000: episode: 3938, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3271.000 [3271.000, 3271.000],  loss: 56887.507812, mae: 2346.697998, mean_q: 752.970581\n",
      "wrong_move\n",
      "   3969/500000: episode: 3939, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1194.000 [1194.000, 1194.000],  loss: 929456.125000, mae: 2338.459961, mean_q: 981.522583\n",
      "wrong_move\n",
      "   3970/500000: episode: 3940, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 76946.609375, mae: 2329.334717, mean_q: 1094.234863\n",
      "wrong_move\n",
      "   3971/500000: episode: 3941, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 416189.187500, mae: 2322.708008, mean_q: 1258.321045\n",
      "wrong_move\n",
      "   3972/500000: episode: 3942, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 370577.468750, mae: 2312.695312, mean_q: 1353.727905\n",
      "wrong_move\n",
      "   3973/500000: episode: 3943, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 50244.445312, mae: 2306.985840, mean_q: 1333.122803\n",
      "wrong_move\n",
      "   3974/500000: episode: 3944, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1359.000 [1359.000, 1359.000],  loss: 318575.437500, mae: 2305.210938, mean_q: 1415.803223\n",
      "wrong_move\n",
      "   3975/500000: episode: 3945, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 49772.601562, mae: 2306.962646, mean_q: 1564.842773\n",
      "wrong_move\n",
      "   3976/500000: episode: 3946, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 665.000 [665.000, 665.000],  loss: 76516.359375, mae: 2311.483887, mean_q: 1379.002441\n",
      "wrong_move\n",
      "   3977/500000: episode: 3947, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 476.000 [476.000, 476.000],  loss: 209829.203125, mae: 2318.129639, mean_q: 1389.482910\n",
      "wrong_move\n",
      "   3978/500000: episode: 3948, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2570.000 [2570.000, 2570.000],  loss: 1028990.500000, mae: 2322.165771, mean_q: 1274.285522\n",
      "wrong_move\n",
      "   3979/500000: episode: 3949, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1542871.000000, mae: 2319.685547, mean_q: 1411.891113\n",
      "wrong_move\n",
      "   3980/500000: episode: 3950, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 35807.210938, mae: 2319.339844, mean_q: 1229.921631\n",
      "wrong_move\n",
      "   3981/500000: episode: 3951, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 435250.500000, mae: 2324.517090, mean_q: 1124.275757\n",
      "wrong_move\n",
      "   3982/500000: episode: 3952, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 171.000 [171.000, 171.000],  loss: 183688.187500, mae: 2332.762207, mean_q: 1059.911621\n",
      "wrong_move\n",
      "   3983/500000: episode: 3953, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1666.000 [1666.000, 1666.000],  loss: 69658.921875, mae: 2342.219727, mean_q: 1052.118896\n",
      "wrong_move\n",
      "   3984/500000: episode: 3954, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 586.000 [586.000, 586.000],  loss: 90366.093750, mae: 2352.236572, mean_q: 1158.593628\n",
      "wrong_move\n",
      "   3985/500000: episode: 3955, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: 846263.125000, mae: 2364.053711, mean_q: 858.121033\n",
      "wrong_move\n",
      "   3987/500000: episode: 3956, duration: 0.093s, episode steps:   2, steps per second:  21, episode reward: -4971.000, mean reward: -2485.500 [-5000.000, 29.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 126390.906250, mae: 2375.510254, mean_q: 931.092285\n",
      "wrong_move\n",
      "   3988/500000: episode: 3957, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3984.000 [3984.000, 3984.000],  loss: 283683.531250, mae: 2376.631836, mean_q: 939.292358\n",
      "wrong_move\n",
      "   3989/500000: episode: 3958, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 151563.609375, mae: 2374.748779, mean_q: 1029.979004\n",
      "wrong_move\n",
      "   3991/500000: episode: 3959, duration: 0.100s, episode steps:   2, steps per second:  20, episode reward: -4941.000, mean reward: -2470.500 [-5000.000, 59.000], mean action: 2339.500 [1957.000, 2722.000],  loss: 132283.125000, mae: 2365.836670, mean_q: 890.335632\n",
      "wrong_move\n",
      "   3992/500000: episode: 3960, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 310.000 [310.000, 310.000],  loss: 126733.601562, mae: 2353.159180, mean_q: 922.062134\n",
      "wrong_move\n",
      "   3993/500000: episode: 3961, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 902.000 [902.000, 902.000],  loss: 404787.281250, mae: 2340.997559, mean_q: 789.540771\n",
      "wrong_move\n",
      "   3994/500000: episode: 3962, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3271.000 [3271.000, 3271.000],  loss: 782191.687500, mae: 2333.004395, mean_q: 753.976074\n",
      "wrong_move\n",
      "   3995/500000: episode: 3963, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 153.000 [153.000, 153.000],  loss: 107141.429688, mae: 2323.105957, mean_q: 632.374023\n",
      "wrong_move\n",
      "   3996/500000: episode: 3964, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1603.000 [1603.000, 1603.000],  loss: 142317.328125, mae: 2320.511963, mean_q: 557.727905\n",
      "wrong_move\n",
      "   3997/500000: episode: 3965, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 321.000 [321.000, 321.000],  loss: 25846.453125, mae: 2321.198975, mean_q: 536.392700\n",
      "wrong_move\n",
      "   3998/500000: episode: 3966, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 101307.859375, mae: 2321.562988, mean_q: 409.787109\n",
      "wrong_move\n",
      "   3999/500000: episode: 3967, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 66329.031250, mae: 2320.457764, mean_q: 533.218384\n",
      "wrong_move\n",
      "   4000/500000: episode: 3968, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 62826.539062, mae: 2337.365479, mean_q: 593.637817\n",
      "wrong_move\n",
      "   4001/500000: episode: 3969, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 135657.812500, mae: 2368.177979, mean_q: 1081.118164\n",
      "wrong_move\n",
      "   4002/500000: episode: 3970, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 818426.750000, mae: 2385.942139, mean_q: 1530.369995\n",
      "wrong_move\n",
      "   4003/500000: episode: 3971, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 88218.187500, mae: 2392.379150, mean_q: 1900.762695\n",
      "wrong_move\n",
      "   4004/500000: episode: 3972, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 117787.890625, mae: 2389.753906, mean_q: 2035.092651\n",
      "wrong_move\n",
      "   4005/500000: episode: 3973, duration: 0.157s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 944448.875000, mae: 2379.221680, mean_q: 2286.090332\n",
      "wrong_move\n",
      "   4007/500000: episode: 3974, duration: 0.216s, episode steps:   2, steps per second:   9, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1349325.250000, mae: 2358.358154, mean_q: 2302.676514\n",
      "wrong_move\n",
      "   4008/500000: episode: 3975, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3246.000 [3246.000, 3246.000],  loss: 108319.156250, mae: 2326.712646, mean_q: 2217.557617\n",
      "wrong_move\n",
      "   4009/500000: episode: 3976, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 235664.281250, mae: 2310.854980, mean_q: 2174.785156\n",
      "wrong_move\n",
      "   4010/500000: episode: 3977, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 84387.867188, mae: 2307.610840, mean_q: 2180.198242\n",
      "wrong_move\n",
      "   4011/500000: episode: 3978, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 478.000 [478.000, 478.000],  loss: 45580.898438, mae: 2310.187012, mean_q: 2021.429077\n",
      "wrong_move\n",
      "   4012/500000: episode: 3979, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 91798.820312, mae: 2298.753906, mean_q: 2031.561890\n",
      "wrong_move\n",
      "   4013/500000: episode: 3980, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2974.000 [2974.000, 2974.000],  loss: 46415.000000, mae: 2277.794922, mean_q: 1907.175049\n",
      "wrong_move\n",
      "   4014/500000: episode: 3981, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1441.000 [1441.000, 1441.000],  loss: 528673.437500, mae: 2267.554199, mean_q: 1784.294922\n",
      "wrong_move\n",
      "   4015/500000: episode: 3982, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 175877.250000, mae: 2262.575684, mean_q: 1826.228516\n",
      "wrong_move\n",
      "   4016/500000: episode: 3983, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1776.000 [1776.000, 1776.000],  loss: 326280.281250, mae: 2261.063477, mean_q: 1908.482910\n",
      "wrong_move\n",
      "   4017/500000: episode: 3984, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 480002.468750, mae: 2266.820557, mean_q: 1801.283936\n",
      "wrong_move\n",
      "   4018/500000: episode: 3985, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 57852.984375, mae: 2281.916504, mean_q: 1782.622559\n",
      "wrong_move\n",
      "   4019/500000: episode: 3986, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 104542.156250, mae: 2301.421631, mean_q: 1609.169922\n",
      "wrong_move\n",
      "   4020/500000: episode: 3987, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1804850.750000, mae: 2320.313965, mean_q: 2215.190186\n",
      "wrong_move\n",
      "   4021/500000: episode: 3988, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 863.000 [863.000, 863.000],  loss: 121116.593750, mae: 2332.933594, mean_q: 1687.093750\n",
      "wrong_move\n",
      "   4022/500000: episode: 3989, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 141607.187500, mae: 2341.221680, mean_q: 1608.218628\n",
      "wrong_move\n",
      "   4023/500000: episode: 3990, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1313171.250000, mae: 2344.974121, mean_q: 1568.794922\n",
      "wrong_move\n",
      "   4025/500000: episode: 3991, duration: 0.065s, episode steps:   2, steps per second:  31, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2448.500 [2175.000, 2722.000],  loss: 657587.125000, mae: 2366.928223, mean_q: 1634.718384\n",
      "wrong_move\n",
      "   4026/500000: episode: 3992, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 380432.093750, mae: 2397.313232, mean_q: 1966.319702\n",
      "wrong_move\n",
      "   4027/500000: episode: 3993, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 135003.406250, mae: 2412.937012, mean_q: 3027.568359\n",
      "wrong_move\n",
      "   4028/500000: episode: 3994, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 213768.250000, mae: 2415.053955, mean_q: 2656.914062\n",
      "wrong_move\n",
      "   4029/500000: episode: 3995, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 960497.187500, mae: 2406.665771, mean_q: 2597.472900\n",
      "wrong_move\n",
      "   4030/500000: episode: 3996, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 105193.453125, mae: 2391.208496, mean_q: 2701.657715\n",
      "wrong_move\n",
      "   4031/500000: episode: 3997, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1399.000 [1399.000, 1399.000],  loss: 1597332.375000, mae: 2377.095947, mean_q: 2848.636230\n",
      "wrong_move\n",
      "   4032/500000: episode: 3998, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 135517.906250, mae: 2358.851318, mean_q: 2899.031494\n",
      "wrong_move\n",
      "   4033/500000: episode: 3999, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 348270.312500, mae: 2340.822021, mean_q: 2721.288574\n",
      "wrong_move\n",
      "   4034/500000: episode: 4000, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 104962.343750, mae: 2328.923096, mean_q: 2929.929443\n",
      "wrong_move\n",
      "   4035/500000: episode: 4001, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 95949.351562, mae: 2317.302246, mean_q: 2821.723145\n",
      "wrong_move\n",
      "   4036/500000: episode: 4002, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3013.000 [3013.000, 3013.000],  loss: 979729.062500, mae: 2310.785889, mean_q: 2753.527832\n",
      "wrong_move\n",
      "   4037/500000: episode: 4003, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3761.000 [3761.000, 3761.000],  loss: 223105.687500, mae: 2310.190674, mean_q: 2807.829346\n",
      "wrong_move\n",
      "   4038/500000: episode: 4004, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1397969.375000, mae: 2313.922363, mean_q: 2711.469482\n",
      "wrong_move\n",
      "   4040/500000: episode: 4005, duration: 0.188s, episode steps:   2, steps per second:  11, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 174404.968750, mae: 2315.224854, mean_q: 2365.414307\n",
      "wrong_move\n",
      "   4041/500000: episode: 4006, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 387911168.000000, mae: 2318.500488, mean_q: 2315.490723\n",
      "wrong_move\n",
      "   4043/500000: episode: 4007, duration: 0.135s, episode steps:   2, steps per second:  15, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 962569.250000, mae: 2382.916748, mean_q: 2895.609131\n",
      "wrong_move\n",
      "   4044/500000: episode: 4008, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 1835273.250000, mae: 2417.242188, mean_q: 3425.934326\n",
      "wrong_move\n",
      "   4045/500000: episode: 4009, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2468.000 [2468.000, 2468.000],  loss: 1247183.000000, mae: 2419.916504, mean_q: 3500.836426\n",
      "wrong_move\n",
      "   4046/500000: episode: 4010, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 513932.000000, mae: 2414.018799, mean_q: 3580.509766\n",
      "wrong_move\n",
      "   4047/500000: episode: 4011, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1543160.625000, mae: 2404.978271, mean_q: 3966.552246\n",
      "wrong_move\n",
      "   4048/500000: episode: 4012, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2460.000 [2460.000, 2460.000],  loss: 2000371.250000, mae: 2393.748779, mean_q: 3945.207520\n",
      "wrong_move\n",
      "   4049/500000: episode: 4013, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 170977.781250, mae: 2371.485596, mean_q: 3890.616211\n",
      "wrong_move\n",
      "   4050/500000: episode: 4014, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1301324.750000, mae: 2348.629883, mean_q: 3932.066406\n",
      "wrong_move\n",
      "   4051/500000: episode: 4015, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 3656118.500000, mae: 2327.530518, mean_q: 3486.440430\n",
      "wrong_move\n",
      "   4052/500000: episode: 4016, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 141844.125000, mae: 2295.986816, mean_q: 4053.947510\n",
      "wrong_move\n",
      "   4053/500000: episode: 4017, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 897.000 [897.000, 897.000],  loss: 1446072.000000, mae: 2264.926514, mean_q: 3155.930908\n",
      "wrong_move\n",
      "   4054/500000: episode: 4018, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2980.000 [2980.000, 2980.000],  loss: 465214.937500, mae: 2244.543945, mean_q: 3222.004883\n",
      "wrong_move\n",
      "   4055/500000: episode: 4019, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 118.000 [118.000, 118.000],  loss: 285358.531250, mae: 2229.525635, mean_q: 3154.375977\n",
      "wrong_move\n",
      "   4056/500000: episode: 4020, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 339606.718750, mae: 2219.001465, mean_q: 2707.413574\n",
      "wrong_move\n",
      "   4057/500000: episode: 4021, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3240.000 [3240.000, 3240.000],  loss: 584814.125000, mae: 2210.588135, mean_q: 2390.022461\n",
      "wrong_move\n",
      "   4058/500000: episode: 4022, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3084.000 [3084.000, 3084.000],  loss: 363007.468750, mae: 2211.848877, mean_q: 2306.330322\n",
      "wrong_move\n",
      "   4059/500000: episode: 4023, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3240.000 [3240.000, 3240.000],  loss: 123132.328125, mae: 2220.036133, mean_q: 1895.759277\n",
      "wrong_move\n",
      "   4060/500000: episode: 4024, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2983.000 [2983.000, 2983.000],  loss: 122128.804688, mae: 2234.423584, mean_q: 1877.458252\n",
      "wrong_move\n",
      "   4061/500000: episode: 4025, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3240.000 [3240.000, 3240.000],  loss: 960947.812500, mae: 2254.038086, mean_q: 1943.621460\n",
      "wrong_move\n",
      "   4062/500000: episode: 4026, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2050.000 [2050.000, 2050.000],  loss: 200356.484375, mae: 2269.865234, mean_q: 1735.526123\n",
      "wrong_move\n",
      "   4063/500000: episode: 4027, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 896.000 [896.000, 896.000],  loss: 1429865.625000, mae: 2288.328613, mean_q: 1342.299805\n",
      "wrong_move\n",
      "   4064/500000: episode: 4028, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3240.000 [3240.000, 3240.000],  loss: 533197.375000, mae: 2300.760010, mean_q: 1256.675537\n",
      "wrong_move\n",
      "   4065/500000: episode: 4029, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3240.000 [3240.000, 3240.000],  loss: 1711150.250000, mae: 2312.677734, mean_q: 1613.542725\n",
      "wrong_move\n",
      "   4066/500000: episode: 4030, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2050.000 [2050.000, 2050.000],  loss: 84595.156250, mae: 2317.277344, mean_q: 1189.807617\n",
      "wrong_move\n",
      "   4067/500000: episode: 4031, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3240.000 [3240.000, 3240.000],  loss: 926930.500000, mae: 2322.513184, mean_q: 1211.660156\n",
      "wrong_move\n",
      "   4068/500000: episode: 4032, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2983.000 [2983.000, 2983.000],  loss: 894202.062500, mae: 2323.388184, mean_q: 949.556152\n",
      "wrong_move\n",
      "   4069/500000: episode: 4033, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 545.000 [545.000, 545.000],  loss: 479209.312500, mae: 2325.760254, mean_q: 864.712463\n",
      "wrong_move\n",
      "   4070/500000: episode: 4034, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2050.000 [2050.000, 2050.000],  loss: 138658.875000, mae: 2325.254883, mean_q: 833.898926\n",
      "wrong_move\n",
      "   4071/500000: episode: 4035, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3240.000 [3240.000, 3240.000],  loss: 190787.281250, mae: 2324.173340, mean_q: 686.124634\n",
      "wrong_move\n",
      "   4072/500000: episode: 4036, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2915.000 [2915.000, 2915.000],  loss: 258612.781250, mae: 2323.308838, mean_q: 450.838348\n",
      "wrong_move\n",
      "   4073/500000: episode: 4037, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 612.000 [612.000, 612.000],  loss: 324446.093750, mae: 2328.140625, mean_q: 674.111938\n",
      "wrong_move\n",
      "   4074/500000: episode: 4038, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3240.000 [3240.000, 3240.000],  loss: 107540.453125, mae: 2332.288574, mean_q: 365.684814\n",
      "wrong_move\n",
      "   4075/500000: episode: 4039, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2983.000 [2983.000, 2983.000],  loss: 544788.187500, mae: 2338.923340, mean_q: 1141.170654\n",
      "wrong_move\n",
      "   4076/500000: episode: 4040, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2786.000 [2786.000, 2786.000],  loss: 991473.375000, mae: 2343.481934, mean_q: 473.454102\n",
      "wrong_move\n",
      "   4077/500000: episode: 4041, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2069.000 [2069.000, 2069.000],  loss: 881094.812500, mae: 2341.594727, mean_q: 368.884644\n",
      "wrong_move\n",
      "   4078/500000: episode: 4042, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1061.000 [1061.000, 1061.000],  loss: 79249.062500, mae: 2332.873047, mean_q: 280.004822\n",
      "wrong_move\n",
      "   4080/500000: episode: 4043, duration: 0.101s, episode steps:   2, steps per second:  20, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 177.500 [23.000, 332.000],  loss: 391088.250000, mae: 2328.948242, mean_q: 513.444214\n",
      "wrong_move\n",
      "   4081/500000: episode: 4044, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 23.000 [23.000, 23.000],  loss: 148801.093750, mae: 2331.628906, mean_q: 258.555237\n",
      "wrong_move\n",
      "   4082/500000: episode: 4045, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 23.000 [23.000, 23.000],  loss: 470787.125000, mae: 2343.392090, mean_q: 334.406006\n",
      "wrong_move\n",
      "   4083/500000: episode: 4046, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 23.000 [23.000, 23.000],  loss: 219793.312500, mae: 2355.205322, mean_q: 213.749084\n",
      "wrong_move\n",
      "   4084/500000: episode: 4047, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2830.000 [2830.000, 2830.000],  loss: 143152.281250, mae: 2359.047119, mean_q: 285.830536\n",
      "wrong_move\n",
      "   4085/500000: episode: 4048, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 545.000 [545.000, 545.000],  loss: 137309.781250, mae: 2361.967773, mean_q: 244.759842\n",
      "wrong_move\n",
      "   4086/500000: episode: 4049, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 23.000 [23.000, 23.000],  loss: 107529.750000, mae: 2365.515869, mean_q: 424.287323\n",
      "wrong_move\n",
      "   4087/500000: episode: 4050, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 23.000 [23.000, 23.000],  loss: 295674.343750, mae: 2367.262207, mean_q: 293.544128\n",
      "wrong_move\n",
      "   4088/500000: episode: 4051, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 23.000 [23.000, 23.000],  loss: 889406.437500, mae: 2364.417969, mean_q: 194.646484\n",
      "wrong_move\n",
      "   4089/500000: episode: 4052, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1061.000 [1061.000, 1061.000],  loss: 475499.343750, mae: 2356.705078, mean_q: 235.565521\n",
      "wrong_move\n",
      "   4090/500000: episode: 4053, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2983.000 [2983.000, 2983.000],  loss: 205854.796875, mae: 2352.584473, mean_q: 224.449646\n",
      "wrong_move\n",
      "   4091/500000: episode: 4054, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 23.000 [23.000, 23.000],  loss: 291955.687500, mae: 2353.105225, mean_q: 258.939850\n",
      "wrong_move\n",
      "   4092/500000: episode: 4055, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3339.000 [3339.000, 3339.000],  loss: 375413.312500, mae: 2356.870605, mean_q: 257.066681\n",
      "wrong_move\n",
      "   4093/500000: episode: 4056, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1507.000 [1507.000, 1507.000],  loss: 234528.187500, mae: 2362.166504, mean_q: 461.077850\n",
      "wrong_move\n",
      "   4094/500000: episode: 4057, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2719.000 [2719.000, 2719.000],  loss: 288482.406250, mae: 2359.732422, mean_q: 225.113770\n",
      "wrong_move\n",
      "   4095/500000: episode: 4058, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3414.000 [3414.000, 3414.000],  loss: 569450.125000, mae: 2359.828613, mean_q: 199.099640\n",
      "wrong_move\n",
      "   4096/500000: episode: 4059, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 23.000 [23.000, 23.000],  loss: 636315.687500, mae: 2363.404541, mean_q: 93.687073\n",
      "wrong_move\n",
      "   4097/500000: episode: 4060, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1764.000 [1764.000, 1764.000],  loss: 124304.750000, mae: 2360.252197, mean_q: 332.321014\n",
      "wrong_move\n",
      "   4098/500000: episode: 4061, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 671.000 [671.000, 671.000],  loss: 295395.281250, mae: 2353.601074, mean_q: 194.323395\n",
      "wrong_move\n",
      "   4099/500000: episode: 4062, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 545.000 [545.000, 545.000],  loss: 227891.250000, mae: 2344.322754, mean_q: 86.955048\n",
      "wrong_move\n",
      "   4100/500000: episode: 4063, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 142.000 [142.000, 142.000],  loss: 875202.625000, mae: 2332.871582, mean_q: 247.318665\n",
      "wrong_move\n",
      "   4101/500000: episode: 4064, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 903.000 [903.000, 903.000],  loss: 101103.828125, mae: 2314.936035, mean_q: 128.548386\n",
      "wrong_move\n",
      "   4102/500000: episode: 4065, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1061.000 [1061.000, 1061.000],  loss: 161605.468750, mae: 2304.197998, mean_q: 408.735260\n",
      "wrong_move\n",
      "   4103/500000: episode: 4066, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3982.000 [3982.000, 3982.000],  loss: 144514.687500, mae: 2301.470703, mean_q: 511.593384\n",
      "wrong_move\n",
      "   4104/500000: episode: 4067, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 671.000 [671.000, 671.000],  loss: 718388.312500, mae: 2305.610596, mean_q: 203.050415\n",
      "wrong_move\n",
      "   4105/500000: episode: 4068, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 671.000 [671.000, 671.000],  loss: 197066.046875, mae: 2316.227051, mean_q: 36.644005\n",
      "wrong_move\n",
      "   4106/500000: episode: 4069, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 274.000 [274.000, 274.000],  loss: 206518.250000, mae: 2331.170410, mean_q: 391.173126\n",
      "wrong_move\n",
      "   4107/500000: episode: 4070, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1486.000 [1486.000, 1486.000],  loss: 420017.750000, mae: 2344.296875, mean_q: 769.486572\n",
      "wrong_move\n",
      "   4108/500000: episode: 4071, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3751.000 [3751.000, 3751.000],  loss: 854474.312500, mae: 2357.890137, mean_q: 383.415253\n",
      "wrong_move\n",
      "   4109/500000: episode: 4072, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 897.000 [897.000, 897.000],  loss: 141503.296875, mae: 2369.013184, mean_q: 1399.430420\n",
      "wrong_move\n",
      "   4110/500000: episode: 4073, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1027.000 [1027.000, 1027.000],  loss: 462511.437500, mae: 2375.303711, mean_q: 105.237083\n",
      "wrong_move\n",
      "   4111/500000: episode: 4074, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 671.000 [671.000, 671.000],  loss: 458026.593750, mae: 2377.064209, mean_q: 105.563530\n",
      "wrong_move\n",
      "   4112/500000: episode: 4075, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2319.000 [2319.000, 2319.000],  loss: 876152.375000, mae: 2378.352783, mean_q: 142.772491\n",
      "wrong_move\n",
      "   4113/500000: episode: 4076, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2391.000 [2391.000, 2391.000],  loss: 205395.312500, mae: 2375.137695, mean_q: 463.933990\n",
      "wrong_move\n",
      "   4114/500000: episode: 4077, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4077.000 [4077.000, 4077.000],  loss: 99803.875000, mae: 2365.931641, mean_q: 13.899982\n",
      "wrong_move\n",
      "   4115/500000: episode: 4078, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 124537.351562, mae: 2358.009277, mean_q: 380.363770\n",
      "wrong_move\n",
      "   4116/500000: episode: 4079, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3636.000 [3636.000, 3636.000],  loss: 654671.562500, mae: 2348.089111, mean_q: 552.118225\n",
      "wrong_move\n",
      "   4117/500000: episode: 4080, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1667.000 [1667.000, 1667.000],  loss: 919678.812500, mae: 2338.005859, mean_q: 107.772179\n",
      "wrong_move\n",
      "   4118/500000: episode: 4081, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2864.000 [2864.000, 2864.000],  loss: 172042.812500, mae: 2334.825684, mean_q: 118.280876\n",
      "wrong_move\n",
      "   4119/500000: episode: 4082, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 897.000 [897.000, 897.000],  loss: 676450.187500, mae: 2335.246582, mean_q: 43.288837\n",
      "wrong_move\n",
      "   4120/500000: episode: 4083, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: 124004.343750, mae: 2334.685791, mean_q: 74.764336\n",
      "wrong_move\n",
      "   4121/500000: episode: 4084, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 243.000 [243.000, 243.000],  loss: 508784.343750, mae: 2338.872803, mean_q: 316.613983\n",
      "wrong_move\n",
      "   4122/500000: episode: 4085, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3636.000 [3636.000, 3636.000],  loss: 808519.000000, mae: 2347.756836, mean_q: 130.244827\n",
      "wrong_move\n",
      "   4123/500000: episode: 4086, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3673.000 [3673.000, 3673.000],  loss: 102116.000000, mae: 2361.322998, mean_q: 422.678772\n",
      "wrong_move\n",
      "   4124/500000: episode: 4087, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3901.000 [3901.000, 3901.000],  loss: 149454.187500, mae: 2373.405273, mean_q: 103.952225\n",
      "wrong_move\n",
      "   4125/500000: episode: 4088, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1028.000 [1028.000, 1028.000],  loss: 121728.531250, mae: 2381.814453, mean_q: 141.030121\n",
      "wrong_move\n",
      "   4126/500000: episode: 4089, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 302024.687500, mae: 2386.569824, mean_q: 438.881836\n",
      "wrong_move\n",
      "   4127/500000: episode: 4090, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3725.000 [3725.000, 3725.000],  loss: 345697.375000, mae: 2387.410645, mean_q: 169.350372\n",
      "wrong_move\n",
      "   4128/500000: episode: 4091, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 243.000 [243.000, 243.000],  loss: 258301.359375, mae: 2389.437988, mean_q: 375.280945\n",
      "wrong_move\n",
      "   4129/500000: episode: 4092, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 243.000 [243.000, 243.000],  loss: 487682.125000, mae: 2377.542969, mean_q: 228.136719\n",
      "wrong_move\n",
      "   4130/500000: episode: 4093, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3130.000 [3130.000, 3130.000],  loss: 150728.765625, mae: 2367.682129, mean_q: 65.111023\n",
      "wrong_move\n",
      "   4131/500000: episode: 4094, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2864.000 [2864.000, 2864.000],  loss: 576735.250000, mae: 2357.273682, mean_q: 110.604134\n",
      "wrong_move\n",
      "   4132/500000: episode: 4095, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1042.000 [1042.000, 1042.000],  loss: 841462.500000, mae: 2372.091309, mean_q: 113.689537\n",
      "wrong_move\n",
      "   4133/500000: episode: 4096, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3014.000 [3014.000, 3014.000],  loss: 444829.531250, mae: 2407.841553, mean_q: 249.507202\n",
      "wrong_move\n",
      "   4134/500000: episode: 4097, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2261.000 [2261.000, 2261.000],  loss: 279679.843750, mae: 2434.670898, mean_q: 261.047577\n",
      "wrong_move\n",
      "   4135/500000: episode: 4098, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 736.000 [736.000, 736.000],  loss: 184239.968750, mae: 2451.555420, mean_q: 407.338501\n",
      "wrong_move\n",
      "   4136/500000: episode: 4099, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2659.000 [2659.000, 2659.000],  loss: 181763.171875, mae: 2457.398682, mean_q: 952.780334\n",
      "wrong_move\n",
      "   4137/500000: episode: 4100, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2659.000 [2659.000, 2659.000],  loss: 835161.000000, mae: 2454.920654, mean_q: 813.189636\n",
      "wrong_move\n",
      "   4138/500000: episode: 4101, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2659.000 [2659.000, 2659.000],  loss: 550876.375000, mae: 2442.139648, mean_q: 856.407166\n",
      "wrong_move\n",
      "   4139/500000: episode: 4102, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 794.000 [794.000, 794.000],  loss: 296587.437500, mae: 2426.386963, mean_q: 1044.561035\n",
      "wrong_move\n",
      "   4140/500000: episode: 4103, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3333.000 [3333.000, 3333.000],  loss: 465628.156250, mae: 2407.953125, mean_q: 1112.998291\n",
      "wrong_move\n",
      "   4141/500000: episode: 4104, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2659.000 [2659.000, 2659.000],  loss: 1067824.125000, mae: 2389.497559, mean_q: 1169.385132\n",
      "wrong_move\n",
      "   4142/500000: episode: 4105, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 466475.593750, mae: 2372.663086, mean_q: 1184.856934\n",
      "wrong_move\n",
      "   4143/500000: episode: 4106, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1454.000 [1454.000, 1454.000],  loss: 314631.718750, mae: 2360.956543, mean_q: 1131.291992\n",
      "wrong_move\n",
      "   4144/500000: episode: 4107, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4007.000 [4007.000, 4007.000],  loss: 456370.468750, mae: 2356.709961, mean_q: 1391.788086\n",
      "wrong_move\n",
      "   4145/500000: episode: 4108, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1012.000 [1012.000, 1012.000],  loss: 443044.812500, mae: 2350.515625, mean_q: 1422.756348\n",
      "wrong_move\n",
      "   4146/500000: episode: 4109, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3574.000 [3574.000, 3574.000],  loss: 892132.312500, mae: 2341.734375, mean_q: 1474.221802\n",
      "wrong_move\n",
      "   4147/500000: episode: 4110, duration: 0.146s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 217278.656250, mae: 2332.996826, mean_q: 1426.670044\n",
      "wrong_move\n",
      "   4148/500000: episode: 4111, duration: 0.150s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 485.000 [485.000, 485.000],  loss: 273564.812500, mae: 2326.851318, mean_q: 1558.563110\n",
      "wrong_move\n",
      "   4149/500000: episode: 4112, duration: 0.146s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2241.000 [2241.000, 2241.000],  loss: 1797946.375000, mae: 2318.895996, mean_q: 1062.566162\n",
      "wrong_move\n",
      "   4150/500000: episode: 4113, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1357.000 [1357.000, 1357.000],  loss: 225788.687500, mae: 2312.370850, mean_q: 1083.780151\n",
      "wrong_move\n",
      "   4151/500000: episode: 4114, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 758.000 [758.000, 758.000],  loss: 586425.687500, mae: 2309.428711, mean_q: 1002.482971\n",
      "wrong_move\n",
      "   4152/500000: episode: 4115, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2659.000 [2659.000, 2659.000],  loss: 1273169.125000, mae: 2310.773193, mean_q: 895.083252\n",
      "wrong_move\n",
      "   4153/500000: episode: 4116, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2753.000 [2753.000, 2753.000],  loss: 292202.250000, mae: 2311.824219, mean_q: 1309.128174\n",
      "wrong_move\n",
      "   4154/500000: episode: 4117, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3574.000 [3574.000, 3574.000],  loss: 642369.500000, mae: 2318.644531, mean_q: 821.606567\n",
      "wrong_move\n",
      "   4155/500000: episode: 4118, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2326.000 [2326.000, 2326.000],  loss: 284439.500000, mae: 2326.471436, mean_q: 807.956055\n",
      "wrong_move\n",
      "   4156/500000: episode: 4119, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 304.000 [304.000, 304.000],  loss: 615463.562500, mae: 2336.354004, mean_q: 960.055359\n",
      "wrong_move\n",
      "   4157/500000: episode: 4120, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1964.000 [1964.000, 1964.000],  loss: 248254.937500, mae: 2342.104736, mean_q: 780.610718\n",
      "wrong_move\n",
      "   4158/500000: episode: 4121, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1316.000 [1316.000, 1316.000],  loss: 751498.625000, mae: 2351.003418, mean_q: 1060.642944\n",
      "wrong_move\n",
      "   4159/500000: episode: 4122, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1029.000 [1029.000, 1029.000],  loss: 308013.750000, mae: 2361.451172, mean_q: 794.624634\n",
      "wrong_move\n",
      "   4160/500000: episode: 4123, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2420.000 [2420.000, 2420.000],  loss: 664237.937500, mae: 2375.488037, mean_q: 960.585205\n",
      "wrong_move\n",
      "   4161/500000: episode: 4124, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3731.000 [3731.000, 3731.000],  loss: 327043.312500, mae: 2378.812012, mean_q: 846.778564\n",
      "wrong_move\n",
      "   4162/500000: episode: 4125, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 105942.070312, mae: 2381.486572, mean_q: 770.196106\n",
      "wrong_move\n",
      "   4163/500000: episode: 4126, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 472234.875000, mae: 2381.886719, mean_q: 633.437622\n",
      "wrong_move\n",
      "   4164/500000: episode: 4127, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3059.000 [3059.000, 3059.000],  loss: 697018.875000, mae: 2424.085938, mean_q: 1410.037109\n",
      "wrong_move\n",
      "   4165/500000: episode: 4128, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2659.000 [2659.000, 2659.000],  loss: 167869.656250, mae: 2499.374512, mean_q: 1614.414307\n",
      "wrong_move\n",
      "   4166/500000: episode: 4129, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 528167.562500, mae: 2557.635986, mean_q: 2072.715820\n",
      "wrong_move\n",
      "   4167/500000: episode: 4130, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2659.000 [2659.000, 2659.000],  loss: 167494.953125, mae: 2597.128174, mean_q: 2574.798828\n",
      "wrong_move\n",
      "   4168/500000: episode: 4131, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1499.000 [1499.000, 1499.000],  loss: 1985441.375000, mae: 2619.375000, mean_q: 2960.428711\n",
      "wrong_move\n",
      "   4169/500000: episode: 4132, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2397.000 [2397.000, 2397.000],  loss: 268542.312500, mae: 2619.278076, mean_q: 3231.447021\n",
      "wrong_move\n",
      "   4170/500000: episode: 4133, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3695.000 [3695.000, 3695.000],  loss: 3731967.000000, mae: 2607.696289, mean_q: 3490.309814\n",
      "wrong_move\n",
      "   4171/500000: episode: 4134, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2078.000 [2078.000, 2078.000],  loss: 2624916.000000, mae: 2583.097168, mean_q: 3530.022705\n",
      "wrong_move\n",
      "   4172/500000: episode: 4135, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2378.000 [2378.000, 2378.000],  loss: 1007232.500000, mae: 2552.920898, mean_q: 4016.078125\n",
      "wrong_move\n",
      "   4173/500000: episode: 4136, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2000.000 [2000.000, 2000.000],  loss: 15740960.000000, mae: 2518.829590, mean_q: 3792.811035\n",
      "wrong_move\n",
      "   4174/500000: episode: 4137, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 516.000 [516.000, 516.000],  loss: 181712.671875, mae: 2475.640625, mean_q: 3451.918945\n",
      "wrong_move\n",
      "   4175/500000: episode: 4138, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2659.000 [2659.000, 2659.000],  loss: 1004229.750000, mae: 2437.314941, mean_q: 3801.983398\n",
      "wrong_move\n",
      "   4176/500000: episode: 4139, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2659.000 [2659.000, 2659.000],  loss: 613806.750000, mae: 2404.291748, mean_q: 3744.796387\n",
      "wrong_move\n",
      "   4177/500000: episode: 4140, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2659.000 [2659.000, 2659.000],  loss: 768290.187500, mae: 2377.424072, mean_q: 3951.903320\n",
      "wrong_move\n",
      "   4178/500000: episode: 4141, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2659.000 [2659.000, 2659.000],  loss: 386906.312500, mae: 2356.594727, mean_q: 3242.712402\n",
      "wrong_move\n",
      "   4179/500000: episode: 4142, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1374.000 [1374.000, 1374.000],  loss: 585126.937500, mae: 2341.084229, mean_q: 3146.571533\n",
      "wrong_move\n",
      "   4180/500000: episode: 4143, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 332.000 [332.000, 332.000],  loss: 209727.296875, mae: 2329.196533, mean_q: 3122.531738\n",
      "wrong_move\n",
      "   4181/500000: episode: 4144, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 360725.281250, mae: 2322.743164, mean_q: 2942.517334\n",
      "wrong_move\n",
      "   4182/500000: episode: 4145, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 332.000 [332.000, 332.000],  loss: 398413.312500, mae: 2318.784180, mean_q: 3506.788574\n",
      "wrong_move\n",
      "   4183/500000: episode: 4146, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 332.000 [332.000, 332.000],  loss: 245342.875000, mae: 2317.813965, mean_q: 2941.840332\n",
      "wrong_move\n",
      "   4184/500000: episode: 4147, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 332.000 [332.000, 332.000],  loss: 609222.312500, mae: 2321.350098, mean_q: 3520.465576\n",
      "wrong_move\n",
      "   4185/500000: episode: 4148, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2233.000 [2233.000, 2233.000],  loss: 1916237.375000, mae: 2323.055664, mean_q: 2935.732178\n",
      "wrong_move\n",
      "   4186/500000: episode: 4149, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3895.000 [3895.000, 3895.000],  loss: 246170.859375, mae: 2326.916748, mean_q: 3172.032715\n",
      "wrong_move\n",
      "   4187/500000: episode: 4150, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 516.000 [516.000, 516.000],  loss: 253406.093750, mae: 2335.872070, mean_q: 2857.604004\n",
      "wrong_move\n",
      "   4188/500000: episode: 4151, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1331.000 [1331.000, 1331.000],  loss: 23469366.000000, mae: 2347.818359, mean_q: 3380.826172\n",
      "wrong_move\n",
      "   4189/500000: episode: 4152, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3752.000 [3752.000, 3752.000],  loss: 230381.734375, mae: 2351.276611, mean_q: 2626.416504\n",
      "wrong_move\n",
      "   4190/500000: episode: 4153, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 332.000 [332.000, 332.000],  loss: 282540.812500, mae: 2353.447266, mean_q: 2778.743164\n",
      "wrong_move\n",
      "   4191/500000: episode: 4154, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3237.000 [3237.000, 3237.000],  loss: 2159366.000000, mae: 2354.149902, mean_q: 3131.193604\n",
      "wrong_move\n",
      "   4192/500000: episode: 4155, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3237.000 [3237.000, 3237.000],  loss: 5890984960.000000, mae: 2372.109863, mean_q: 3236.779785\n",
      "wrong_move\n",
      "   4193/500000: episode: 4156, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 210156.718750, mae: 2435.497070, mean_q: 3456.918457\n",
      "wrong_move\n",
      "   4194/500000: episode: 4157, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 3621174.000000, mae: 2500.462402, mean_q: 4017.690918\n",
      "wrong_move\n",
      "   4195/500000: episode: 4158, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 337.000 [337.000, 337.000],  loss: 464109.687500, mae: 2547.381592, mean_q: 4533.994141\n",
      "wrong_move\n",
      "   4196/500000: episode: 4159, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 246931.203125, mae: 2579.850098, mean_q: 4722.661133\n",
      "wrong_move\n",
      "   4197/500000: episode: 4160, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1535.000 [1535.000, 1535.000],  loss: 1928892.750000, mae: 2600.673584, mean_q: 5195.771484\n",
      "wrong_move\n",
      "   4198/500000: episode: 4161, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 315242.750000, mae: 2608.765869, mean_q: 5897.330566\n",
      "wrong_move\n",
      "   4199/500000: episode: 4162, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 623851.125000, mae: 2607.749268, mean_q: 5462.239258\n",
      "wrong_move\n",
      "   4200/500000: episode: 4163, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 711432.750000, mae: 2600.235840, mean_q: 5525.884766\n",
      "wrong_move\n",
      "   4201/500000: episode: 4164, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 349162.750000, mae: 2589.572021, mean_q: 5764.437012\n",
      "wrong_move\n",
      "   4202/500000: episode: 4165, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 20.000 [20.000, 20.000],  loss: 1759048.000000, mae: 2577.714844, mean_q: 5768.410156\n",
      "wrong_move\n",
      "   4203/500000: episode: 4166, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 1811902.625000, mae: 2559.074219, mean_q: 5854.541504\n",
      "wrong_move\n",
      "   4204/500000: episode: 4167, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3265.000 [3265.000, 3265.000],  loss: 647382.937500, mae: 2539.013184, mean_q: 6057.488281\n",
      "wrong_move\n",
      "   4205/500000: episode: 4168, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 636094.750000, mae: 2520.484619, mean_q: 6051.024414\n",
      "wrong_move\n",
      "   4206/500000: episode: 4169, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 1776066.125000, mae: 2508.397217, mean_q: 5628.270508\n",
      "wrong_move\n",
      "   4207/500000: episode: 4170, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 551738.000000, mae: 2482.682861, mean_q: 5668.309570\n",
      "wrong_move\n",
      "   4208/500000: episode: 4171, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1125.000 [1125.000, 1125.000],  loss: 391864.500000, mae: 2459.289551, mean_q: 5556.914062\n",
      "wrong_move\n",
      "   4209/500000: episode: 4172, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 432344.250000, mae: 2438.976807, mean_q: 5608.150391\n",
      "wrong_move\n",
      "   4210/500000: episode: 4173, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 303891.250000, mae: 2420.674805, mean_q: 5451.167480\n",
      "wrong_move\n",
      "   4211/500000: episode: 4174, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 4529472.500000, mae: 2405.121826, mean_q: 5676.312012\n",
      "wrong_move\n",
      "   4212/500000: episode: 4175, duration: 0.165s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 332.000 [332.000, 332.000],  loss: 341254.437500, mae: 2386.130371, mean_q: 5302.483398\n",
      "wrong_move\n",
      "   4213/500000: episode: 4176, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 3219111.500000, mae: 2371.967773, mean_q: 5211.354492\n",
      "wrong_move\n",
      "   4214/500000: episode: 4177, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 624057.187500, mae: 2360.921631, mean_q: 6154.163086\n",
      "wrong_move\n",
      "   4215/500000: episode: 4178, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 288699.656250, mae: 2348.843750, mean_q: 5547.041992\n",
      "wrong_move\n",
      "   4216/500000: episode: 4179, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9508636.000000, mae: 2342.941406, mean_q: 4973.747070\n",
      "wrong_move\n",
      "   4217/500000: episode: 4180, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1919392.875000, mae: 2338.338379, mean_q: 5624.616699\n",
      "wrong_move\n",
      "   4218/500000: episode: 4181, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 296191.312500, mae: 2333.610840, mean_q: 4911.477539\n",
      "wrong_move\n",
      "   4219/500000: episode: 4182, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 952.000 [952.000, 952.000],  loss: 423398.312500, mae: 2333.087646, mean_q: 4921.348633\n",
      "wrong_move\n",
      "   4220/500000: episode: 4183, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 116876.203125, mae: 2335.980957, mean_q: 5128.378906\n",
      "wrong_move\n",
      "   4221/500000: episode: 4184, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 540.000 [540.000, 540.000],  loss: 373647.187500, mae: 2339.692383, mean_q: 5148.582031\n",
      "wrong_move\n",
      "   4222/500000: episode: 4185, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2661087.750000, mae: 2345.015381, mean_q: 5559.269531\n",
      "wrong_move\n",
      "   4223/500000: episode: 4186, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2000.000 [2000.000, 2000.000],  loss: 177769.718750, mae: 2347.212646, mean_q: 5097.513672\n",
      "wrong_move\n",
      "   4225/500000: episode: 4187, duration: 0.110s, episode steps:   2, steps per second:  18, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1272491.500000, mae: 2349.570312, mean_q: 4837.958496\n",
      "wrong_move\n",
      "   4226/500000: episode: 4188, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2000.000 [2000.000, 2000.000],  loss: 686860.250000, mae: 2349.181641, mean_q: 4877.872559\n",
      "wrong_move\n",
      "   4227/500000: episode: 4189, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 12.000 [12.000, 12.000],  loss: 3123396.250000, mae: 2344.428711, mean_q: 5468.061035\n",
      "wrong_move\n",
      "   4228/500000: episode: 4190, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 429054.250000, mae: 2330.753906, mean_q: 4728.566895\n",
      "wrong_move\n",
      "   4229/500000: episode: 4191, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3336.000 [3336.000, 3336.000],  loss: 258228.421875, mae: 2321.232178, mean_q: 4380.130859\n",
      "wrong_move\n",
      "   4230/500000: episode: 4192, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1996418.375000, mae: 2317.198975, mean_q: 4365.666016\n",
      "wrong_move\n",
      "   4231/500000: episode: 4193, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1826.000 [1826.000, 1826.000],  loss: 2062869.000000, mae: 2314.053223, mean_q: 4539.310547\n",
      "wrong_move\n",
      "   4232/500000: episode: 4194, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6239073280.000000, mae: 2329.820557, mean_q: 4530.753906\n",
      "wrong_move\n",
      "   4233/500000: episode: 4195, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 402944.187500, mae: 2382.428223, mean_q: 4855.016602\n",
      "wrong_move\n",
      "   4234/500000: episode: 4196, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 922476.750000, mae: 2438.707520, mean_q: 5468.254395\n",
      "wrong_move\n",
      "   4235/500000: episode: 4197, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4094.000 [4094.000, 4094.000],  loss: 20060756.000000, mae: 2480.052246, mean_q: 5896.406738\n",
      "wrong_move\n",
      "   4236/500000: episode: 4198, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 898527.312500, mae: 2504.055908, mean_q: 6187.004883\n",
      "wrong_move\n",
      "   4237/500000: episode: 4199, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2390.000 [2390.000, 2390.000],  loss: 2953382.750000, mae: 2519.343994, mean_q: 6721.151367\n",
      "wrong_move\n",
      "   4238/500000: episode: 4200, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2682309.000000, mae: 2529.670410, mean_q: 7657.125000\n",
      "wrong_move\n",
      "   4239/500000: episode: 4201, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2237324.000000, mae: 2529.231934, mean_q: 6667.934570\n",
      "wrong_move\n",
      "   4240/500000: episode: 4202, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 985878.687500, mae: 2525.603027, mean_q: 6720.314453\n",
      "wrong_move\n",
      "   4241/500000: episode: 4203, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 482032.062500, mae: 2517.992676, mean_q: 6752.354980\n",
      "wrong_move\n",
      "   4242/500000: episode: 4204, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 661229.625000, mae: 2518.695312, mean_q: 6821.434570\n",
      "wrong_move\n",
      "   4243/500000: episode: 4205, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 309277.406250, mae: 2500.923828, mean_q: 7258.726562\n",
      "wrong_move\n",
      "   4244/500000: episode: 4206, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3649052.000000, mae: 2529.035645, mean_q: 7359.683594\n",
      "wrong_move\n",
      "   4245/500000: episode: 4207, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 623892.875000, mae: 2571.314453, mean_q: 8093.001465\n",
      "wrong_move\n",
      "   4246/500000: episode: 4208, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 910515.875000, mae: 2602.867188, mean_q: 8553.461914\n",
      "wrong_move\n",
      "   4247/500000: episode: 4209, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3608014.000000, mae: 2621.145996, mean_q: 9005.528320\n",
      "wrong_move\n",
      "   4249/500000: episode: 4210, duration: 0.146s, episode steps:   2, steps per second:  14, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 1695.500 [669.000, 2722.000],  loss: 15170746.000000, mae: 2627.247070, mean_q: 10059.841797\n",
      "wrong_move\n",
      "   4250/500000: episode: 4211, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4017.000 [4017.000, 4017.000],  loss: 5602874.000000, mae: 2619.826172, mean_q: 10147.397461\n",
      "wrong_move\n",
      "   4251/500000: episode: 4212, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 415.000 [415.000, 415.000],  loss: 1108387.750000, mae: 2604.834717, mean_q: 10035.804688\n",
      "wrong_move\n",
      "   4252/500000: episode: 4213, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 521479.375000, mae: 2589.193359, mean_q: 9750.640625\n",
      "wrong_move\n",
      "   4253/500000: episode: 4214, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 739217.375000, mae: 2572.508789, mean_q: 10632.304688\n",
      "wrong_move\n",
      "   4254/500000: episode: 4215, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4094.000 [4094.000, 4094.000],  loss: 2006290.375000, mae: 2553.679443, mean_q: 9811.982422\n",
      "wrong_move\n",
      "   4255/500000: episode: 4216, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 37452608.000000, mae: 2534.530029, mean_q: 10447.642578\n",
      "wrong_move\n",
      "   4256/500000: episode: 4217, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 491134.937500, mae: 2507.518066, mean_q: 10045.078125\n",
      "wrong_move\n",
      "   4257/500000: episode: 4218, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3821.000 [3821.000, 3821.000],  loss: 1388317.250000, mae: 2485.322754, mean_q: 9770.399414\n",
      "wrong_move\n",
      "   4258/500000: episode: 4219, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9495496.000000, mae: 2467.255615, mean_q: 9909.619141\n",
      "wrong_move\n",
      "   4259/500000: episode: 4220, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1072665.875000, mae: 2444.958984, mean_q: 9622.396484\n",
      "wrong_move\n",
      "   4260/500000: episode: 4221, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7102560.500000, mae: 2425.406982, mean_q: 9596.115234\n",
      "wrong_move\n",
      "   4261/500000: episode: 4222, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7182013.500000, mae: 2406.987793, mean_q: 9436.620117\n",
      "wrong_move\n",
      "   4262/500000: episode: 4223, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1745.000 [1745.000, 1745.000],  loss: 358465.437500, mae: 2386.624023, mean_q: 9891.016602\n",
      "wrong_move\n",
      "   4263/500000: episode: 4224, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3998.000 [3998.000, 3998.000],  loss: 624565.125000, mae: 2370.778320, mean_q: 9559.250000\n",
      "wrong_move\n",
      "   4264/500000: episode: 4225, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2700.000 [2700.000, 2700.000],  loss: 1517370.375000, mae: 2358.763672, mean_q: 9053.149414\n",
      "wrong_move\n",
      "   4265/500000: episode: 4226, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 386650.812500, mae: 2363.363281, mean_q: 9037.407227\n",
      "wrong_move\n",
      "   4266/500000: episode: 4227, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3988728.000000, mae: 2338.656738, mean_q: 9504.138672\n",
      "wrong_move\n",
      "   4268/500000: episode: 4228, duration: 0.071s, episode steps:   2, steps per second:  28, episode reward: -4951.000, mean reward: -2475.500 [-5000.000, 49.000], mean action: 2498.500 [903.000, 4094.000],  loss: 874079.625000, mae: 2327.692871, mean_q: 9147.083008\n",
      "wrong_move\n",
      "   4269/500000: episode: 4229, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2538994.000000, mae: 2321.017090, mean_q: 8777.294922\n",
      "wrong_move\n",
      "   4270/500000: episode: 4230, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 759312.750000, mae: 2316.929199, mean_q: 9663.160156\n",
      "wrong_move\n",
      "   4271/500000: episode: 4231, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3961846.000000, mae: 2316.861084, mean_q: 9092.726562\n",
      "wrong_move\n",
      "   4272/500000: episode: 4232, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 134.000 [134.000, 134.000],  loss: 471386.875000, mae: 2316.397949, mean_q: 10150.585938\n",
      "wrong_move\n",
      "   4273/500000: episode: 4233, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 848714.812500, mae: 2314.596680, mean_q: 8495.824219\n",
      "wrong_move\n",
      "   4274/500000: episode: 4234, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4094.000 [4094.000, 4094.000],  loss: 3103166.500000, mae: 2316.752441, mean_q: 8889.660156\n",
      "wrong_move\n",
      "   4275/500000: episode: 4235, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 665104.375000, mae: 2314.876221, mean_q: 8739.007812\n",
      "wrong_move\n",
      "   4276/500000: episode: 4236, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1322100.875000, mae: 2313.766113, mean_q: 9081.386719\n",
      "wrong_move\n",
      "   4277/500000: episode: 4237, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2443.000 [2443.000, 2443.000],  loss: 4126838.000000, mae: 2314.419678, mean_q: 8299.734375\n",
      "wrong_move\n",
      "   4278/500000: episode: 4238, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 801200.187500, mae: 2312.924072, mean_q: 8817.704102\n",
      "wrong_move\n",
      "   4279/500000: episode: 4239, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1611.000 [1611.000, 1611.000],  loss: 15345394.000000, mae: 2315.103027, mean_q: 9343.159180\n",
      "wrong_move\n",
      "   4280/500000: episode: 4240, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2417624.500000, mae: 2311.195312, mean_q: 9234.150391\n",
      "wrong_move\n",
      "   4281/500000: episode: 4241, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 658456.937500, mae: 2310.977295, mean_q: 8589.496094\n",
      "wrong_move\n",
      "   4282/500000: episode: 4242, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 202.000 [202.000, 202.000],  loss: 559828.250000, mae: 2309.318604, mean_q: 8906.626953\n",
      "wrong_move\n",
      "   4283/500000: episode: 4243, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1211815.125000, mae: 2313.196777, mean_q: 8392.880859\n",
      "wrong_move\n",
      "   4284/500000: episode: 4244, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4015.000 [4015.000, 4015.000],  loss: 1142730.500000, mae: 2306.753418, mean_q: 8101.983398\n",
      "wrong_move\n",
      "   4285/500000: episode: 4245, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2569.000 [2569.000, 2569.000],  loss: 1922367.000000, mae: 2308.250977, mean_q: 8380.949219\n",
      "wrong_move\n",
      "   4286/500000: episode: 4246, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2468.000 [2468.000, 2468.000],  loss: 783495.375000, mae: 2307.859131, mean_q: 8552.409180\n",
      "wrong_move\n",
      "   4287/500000: episode: 4247, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2683.000 [2683.000, 2683.000],  loss: 565428.500000, mae: 2306.770020, mean_q: 8389.507812\n",
      "wrong_move\n",
      "   4288/500000: episode: 4248, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 26.000 [26.000, 26.000],  loss: 496027.906250, mae: 2303.420166, mean_q: 7693.223633\n",
      "wrong_move\n",
      "   4289/500000: episode: 4249, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3192.000 [3192.000, 3192.000],  loss: 793224.625000, mae: 2301.464355, mean_q: 8299.837891\n",
      "wrong_move\n",
      "   4290/500000: episode: 4250, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2177205.000000, mae: 2302.277344, mean_q: 8814.401367\n",
      "wrong_move\n",
      "   4291/500000: episode: 4251, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 931037.875000, mae: 2303.474609, mean_q: 7669.569336\n",
      "wrong_move\n",
      "   4292/500000: episode: 4252, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1075614.125000, mae: 2305.294922, mean_q: 8466.716797\n",
      "wrong_move\n",
      "   4293/500000: episode: 4253, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1043794.375000, mae: 2306.409912, mean_q: 8537.032227\n",
      "wrong_move\n",
      "   4294/500000: episode: 4254, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2910.000 [2910.000, 2910.000],  loss: 2284755.000000, mae: 2306.210205, mean_q: 7934.714355\n",
      "wrong_move\n",
      "   4295/500000: episode: 4255, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6338994.000000, mae: 2305.556641, mean_q: 7873.912109\n",
      "wrong_move\n",
      "   4296/500000: episode: 4256, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 80747176.000000, mae: 2304.395752, mean_q: 7818.552246\n",
      "wrong_move\n",
      "   4297/500000: episode: 4257, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 14438771.000000, mae: 2290.177734, mean_q: 7528.883789\n",
      "wrong_move\n",
      "   4298/500000: episode: 4258, duration: 0.154s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 612.000 [612.000, 612.000],  loss: 616460.625000, mae: 2276.507324, mean_q: 7447.553711\n",
      "wrong_move\n",
      "   4299/500000: episode: 4259, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 77.000 [77.000, 77.000],  loss: 9244554.000000, mae: 2261.603271, mean_q: 7306.947754\n",
      "wrong_move\n",
      "   4300/500000: episode: 4260, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3673859.000000, mae: 2250.590576, mean_q: 7124.127930\n",
      "wrong_move\n",
      "   4301/500000: episode: 4261, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2081433.500000, mae: 2308.752441, mean_q: 6761.305664\n",
      "wrong_move\n",
      "   4303/500000: episode: 4262, duration: 0.178s, episode steps:   2, steps per second:  11, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3781134.000000, mae: 2232.216309, mean_q: 6999.491699\n",
      "wrong_move\n",
      "   4304/500000: episode: 4263, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2808.000 [2808.000, 2808.000],  loss: 796870.812500, mae: 2225.373047, mean_q: 7230.152344\n",
      "wrong_move\n",
      "   4305/500000: episode: 4264, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4094.000 [4094.000, 4094.000],  loss: 4731349.500000, mae: 2219.001709, mean_q: 6226.649902\n",
      "wrong_move\n",
      "   4306/500000: episode: 4265, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 73681880.000000, mae: 2211.012207, mean_q: 7159.080078\n",
      "wrong_move\n",
      "   4307/500000: episode: 4266, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4094.000 [4094.000, 4094.000],  loss: 851603.437500, mae: 2194.208496, mean_q: 6538.272461\n",
      "wrong_move\n",
      "   4308/500000: episode: 4267, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2880.000 [2880.000, 2880.000],  loss: 933649.937500, mae: 2208.999023, mean_q: 6303.654297\n",
      "wrong_move\n",
      "   4309/500000: episode: 4268, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6940450.000000, mae: 2176.708496, mean_q: 6531.677734\n",
      "wrong_move\n",
      "   4310/500000: episode: 4269, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 600968.062500, mae: 2169.968506, mean_q: 6310.405273\n",
      "wrong_move\n",
      "   4311/500000: episode: 4270, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 985437.250000, mae: 2167.335938, mean_q: 5763.507324\n",
      "wrong_move\n",
      "   4312/500000: episode: 4271, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 23653062.000000, mae: 2170.293701, mean_q: 6369.107422\n",
      "wrong_move\n",
      "   4313/500000: episode: 4272, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2171.000 [2171.000, 2171.000],  loss: 2148185.000000, mae: 2163.059814, mean_q: 5602.062500\n",
      "wrong_move\n",
      "   4314/500000: episode: 4273, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 579.000 [579.000, 579.000],  loss: 1900611.375000, mae: 2166.232910, mean_q: 5264.674316\n",
      "wrong_move\n",
      "   4315/500000: episode: 4274, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 582017.000000, mae: 2171.618408, mean_q: 5698.101562\n",
      "wrong_move\n",
      "   4316/500000: episode: 4275, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 4433912.000000, mae: 2177.304199, mean_q: 5763.687012\n",
      "wrong_move\n",
      "   4317/500000: episode: 4276, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7606271.000000, mae: 2182.141113, mean_q: 5642.744141\n",
      "wrong_move\n",
      "   4318/500000: episode: 4277, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1287.000 [1287.000, 1287.000],  loss: 1451020.000000, mae: 2187.412842, mean_q: 5371.822266\n",
      "wrong_move\n",
      "   4320/500000: episode: 4278, duration: 0.142s, episode steps:   2, steps per second:  14, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 811404.500000, mae: 2196.516113, mean_q: 5681.839355\n",
      "wrong_move\n",
      "   4321/500000: episode: 4279, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2299031.500000, mae: 2205.433838, mean_q: 5342.495117\n",
      "wrong_move\n",
      "   4322/500000: episode: 4280, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1004977.812500, mae: 2211.994629, mean_q: 5168.369629\n",
      "wrong_move\n",
      "   4323/500000: episode: 4281, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1195801.250000, mae: 2216.449707, mean_q: 5081.244141\n",
      "wrong_move\n",
      "   4324/500000: episode: 4282, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 961965.750000, mae: 2221.103271, mean_q: 5772.300781\n",
      "wrong_move\n",
      "   4325/500000: episode: 4283, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 426278.281250, mae: 2224.738525, mean_q: 4784.465820\n",
      "wrong_move\n",
      "   4326/500000: episode: 4284, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2230587.000000, mae: 2229.660645, mean_q: 4851.775391\n",
      "wrong_move\n",
      "   4327/500000: episode: 4285, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3230.000 [3230.000, 3230.000],  loss: 1042424.875000, mae: 2233.923340, mean_q: 5041.073730\n",
      "wrong_move\n",
      "   4328/500000: episode: 4286, duration: 0.141s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3752.000 [3752.000, 3752.000],  loss: 417878.656250, mae: 2236.635742, mean_q: 5326.540527\n",
      "wrong_move\n",
      "   4329/500000: episode: 4287, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 10652764.000000, mae: 2239.074219, mean_q: 5413.250977\n",
      "wrong_move\n",
      "   4330/500000: episode: 4288, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1064922.750000, mae: 2240.720703, mean_q: 4490.468262\n",
      "wrong_move\n",
      "   4332/500000: episode: 4289, duration: 0.134s, episode steps:   2, steps per second:  15, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 3157.000 [2722.000, 3592.000],  loss: 1943822.250000, mae: 2245.260254, mean_q: 4331.382812\n",
      "wrong_move\n",
      "   4333/500000: episode: 4290, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 591785.375000, mae: 2250.021973, mean_q: 5056.506836\n",
      "wrong_move\n",
      "   4334/500000: episode: 4291, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2341.000 [2341.000, 2341.000],  loss: 3904432.500000, mae: 2253.644531, mean_q: 5300.915527\n",
      "wrong_move\n",
      "   4335/500000: episode: 4292, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 490989.562500, mae: 2253.320312, mean_q: 4350.669434\n",
      "wrong_move\n",
      "   4336/500000: episode: 4293, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9913940.000000, mae: 2253.316406, mean_q: 4474.734863\n",
      "wrong_move\n",
      "   4337/500000: episode: 4294, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 943203.937500, mae: 2247.194336, mean_q: 4735.590332\n",
      "wrong_move\n",
      "   4338/500000: episode: 4295, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1667.000 [1667.000, 1667.000],  loss: 3468732.500000, mae: 2243.395996, mean_q: 4037.349365\n",
      "wrong_move\n",
      "   4339/500000: episode: 4296, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2975.000 [2975.000, 2975.000],  loss: 1876581.875000, mae: 2240.735596, mean_q: 4794.061523\n",
      "wrong_move\n",
      "   4340/500000: episode: 4297, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 912.000 [912.000, 912.000],  loss: 2252288.000000, mae: 2238.653320, mean_q: 3389.784668\n",
      "wrong_move\n",
      "   4341/500000: episode: 4298, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1820720.000000, mae: 2236.715820, mean_q: 4003.834473\n",
      "wrong_move\n",
      "   4342/500000: episode: 4299, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4035.000 [4035.000, 4035.000],  loss: 1977833.250000, mae: 2234.344727, mean_q: 4192.475586\n",
      "wrong_move\n",
      "   4343/500000: episode: 4300, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1975429.750000, mae: 2233.884766, mean_q: 3270.809814\n",
      "wrong_move\n",
      "   4345/500000: episode: 4301, duration: 0.128s, episode steps:   2, steps per second:  16, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 908828.875000, mae: 2240.387695, mean_q: 4068.199463\n",
      "wrong_move\n",
      "   4346/500000: episode: 4302, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 346289.937500, mae: 2241.287109, mean_q: 4161.123047\n",
      "wrong_move\n",
      "   4347/500000: episode: 4303, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1797554.250000, mae: 2242.184814, mean_q: 3868.582764\n",
      "wrong_move\n",
      "   4348/500000: episode: 4304, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 729.000 [729.000, 729.000],  loss: 5616154.500000, mae: 2241.833496, mean_q: 3840.607910\n",
      "wrong_move\n",
      "   4350/500000: episode: 4305, duration: 0.115s, episode steps:   2, steps per second:  17, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1323839.375000, mae: 2242.840576, mean_q: 3327.485840\n",
      "wrong_move\n",
      "   4351/500000: episode: 4306, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2310.000 [2310.000, 2310.000],  loss: 333882.312500, mae: 2238.913086, mean_q: 3542.944824\n",
      "wrong_move\n",
      "   4352/500000: episode: 4307, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2424134.500000, mae: 2240.034424, mean_q: 2988.407227\n",
      "wrong_move\n",
      "   4353/500000: episode: 4308, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2249.000 [2249.000, 2249.000],  loss: 1247189.000000, mae: 2240.299072, mean_q: 2834.424316\n",
      "wrong_move\n",
      "   4354/500000: episode: 4309, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2089812.000000, mae: 2247.594971, mean_q: 2628.900635\n",
      "wrong_move\n",
      "   4355/500000: episode: 4310, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: 393224.406250, mae: 2238.185547, mean_q: 3142.240723\n",
      "wrong_move\n",
      "   4356/500000: episode: 4311, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 499644.062500, mae: 2237.423828, mean_q: 3061.758789\n",
      "wrong_move\n",
      "   4357/500000: episode: 4312, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 804610.312500, mae: 2238.436279, mean_q: 2893.035400\n",
      "wrong_move\n",
      "   4358/500000: episode: 4313, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 300526.843750, mae: 2242.539062, mean_q: 3407.674316\n",
      "wrong_move\n",
      "   4359/500000: episode: 4314, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2723482.000000, mae: 2243.770020, mean_q: 2732.833008\n",
      "wrong_move\n",
      "   4360/500000: episode: 4315, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 322910.562500, mae: 2246.370361, mean_q: 2158.097168\n",
      "wrong_move\n",
      "   4361/500000: episode: 4316, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 882009.062500, mae: 2250.673340, mean_q: 2774.738525\n",
      "wrong_move\n",
      "   4362/500000: episode: 4317, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 893242.375000, mae: 2256.830566, mean_q: 3578.838623\n",
      "wrong_move\n",
      "   4363/500000: episode: 4318, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1276.000 [1276.000, 1276.000],  loss: 582651.937500, mae: 2259.461426, mean_q: 2438.057129\n",
      "wrong_move\n",
      "   4364/500000: episode: 4319, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3074.000 [3074.000, 3074.000],  loss: 1183555.000000, mae: 2265.104492, mean_q: 3007.081543\n",
      "wrong_move\n",
      "   4365/500000: episode: 4320, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 635929.812500, mae: 2271.286621, mean_q: 2680.997559\n",
      "wrong_move\n",
      "   4366/500000: episode: 4321, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1656.000 [1656.000, 1656.000],  loss: 1144067.250000, mae: 2273.628418, mean_q: 2387.580322\n",
      "wrong_move\n",
      "   4367/500000: episode: 4322, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2437.000 [2437.000, 2437.000],  loss: 2006872.750000, mae: 2276.598389, mean_q: 3261.564697\n",
      "wrong_move\n",
      "   4368/500000: episode: 4323, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2539395.750000, mae: 2275.860352, mean_q: 2606.424805\n",
      "wrong_move\n",
      "   4369/500000: episode: 4324, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 520.000 [520.000, 520.000],  loss: 1722302.000000, mae: 2277.266602, mean_q: 3448.580811\n",
      "wrong_move\n",
      "   4370/500000: episode: 4325, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 236.000 [236.000, 236.000],  loss: 921632.062500, mae: 2276.623779, mean_q: 2580.371094\n",
      "wrong_move\n",
      "   4371/500000: episode: 4326, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 864.000 [864.000, 864.000],  loss: 496769.031250, mae: 2278.446289, mean_q: 2465.461914\n",
      "wrong_move\n",
      "   4372/500000: episode: 4327, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2401.000 [2401.000, 2401.000],  loss: 737887.125000, mae: 2280.260742, mean_q: 2542.170410\n",
      "wrong_move\n",
      "   4373/500000: episode: 4328, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3797.000 [3797.000, 3797.000],  loss: 55593524.000000, mae: 2281.125000, mean_q: 3239.462646\n",
      "wrong_move\n",
      "   4374/500000: episode: 4329, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1909123.000000, mae: 2276.488281, mean_q: 3154.303467\n",
      "wrong_move\n",
      "   4375/500000: episode: 4330, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 427001.750000, mae: 2272.034424, mean_q: 3055.441650\n",
      "wrong_move\n",
      "   4376/500000: episode: 4331, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6155397.000000, mae: 2267.791748, mean_q: 3777.066406\n",
      "wrong_move\n",
      "   4377/500000: episode: 4332, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1149.000 [1149.000, 1149.000],  loss: 732049.875000, mae: 2259.162109, mean_q: 2679.014160\n",
      "wrong_move\n",
      "   4378/500000: episode: 4333, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3752.000 [3752.000, 3752.000],  loss: 4996689.500000, mae: 2254.043457, mean_q: 3281.834473\n",
      "wrong_move\n",
      "   4379/500000: episode: 4334, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 423863.500000, mae: 2252.278809, mean_q: 2777.205811\n",
      "wrong_move\n",
      "   4380/500000: episode: 4335, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3752.000 [3752.000, 3752.000],  loss: 456214.968750, mae: 2241.407227, mean_q: 2761.101562\n",
      "wrong_move\n",
      "   4381/500000: episode: 4336, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3752.000 [3752.000, 3752.000],  loss: 6905242.500000, mae: 2238.256348, mean_q: 2678.106445\n",
      "wrong_move\n",
      "   4382/500000: episode: 4337, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3752.000 [3752.000, 3752.000],  loss: 317412.968750, mae: 2239.700684, mean_q: 3101.580322\n",
      "wrong_move\n",
      "   4383/500000: episode: 4338, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2401.000 [2401.000, 2401.000],  loss: 1674751.125000, mae: 2231.375488, mean_q: 2714.500488\n",
      "wrong_move\n",
      "   4384/500000: episode: 4339, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2451.000 [2451.000, 2451.000],  loss: 5677068.000000, mae: 2231.146973, mean_q: 2839.445801\n",
      "wrong_move\n",
      "   4385/500000: episode: 4340, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 679.000 [679.000, 679.000],  loss: 7160492.000000, mae: 2231.954590, mean_q: 2298.165039\n",
      "wrong_move\n",
      "   4386/500000: episode: 4341, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1400710.750000, mae: 2232.278076, mean_q: 2245.454102\n",
      "wrong_move\n",
      "   4387/500000: episode: 4342, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1369.000 [1369.000, 1369.000],  loss: 1495106.250000, mae: 2232.848145, mean_q: 1771.252075\n",
      "wrong_move\n",
      "   4388/500000: episode: 4343, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 537617.125000, mae: 2231.342773, mean_q: 1615.903809\n",
      "wrong_move\n",
      "   4389/500000: episode: 4344, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1066.000 [1066.000, 1066.000],  loss: 867629.875000, mae: 2232.022461, mean_q: 1875.829834\n",
      "wrong_move\n",
      "   4390/500000: episode: 4345, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2835.000 [2835.000, 2835.000],  loss: 1543488.000000, mae: 2234.024902, mean_q: 2319.991211\n",
      "wrong_move\n",
      "   4391/500000: episode: 4346, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3738.000 [3738.000, 3738.000],  loss: 29335398.000000, mae: 2235.625977, mean_q: 2298.929199\n",
      "wrong_move\n",
      "   4392/500000: episode: 4347, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2835.000 [2835.000, 2835.000],  loss: 409410.437500, mae: 2233.958984, mean_q: 2196.025391\n",
      "wrong_move\n",
      "   4393/500000: episode: 4348, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: 3967830.250000, mae: 2234.641113, mean_q: 1676.708008\n",
      "wrong_move\n",
      "   4394/500000: episode: 4349, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 444581.687500, mae: 2237.697266, mean_q: 1784.112061\n",
      "wrong_move\n",
      "   4395/500000: episode: 4350, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2835.000 [2835.000, 2835.000],  loss: 460049.750000, mae: 2239.095703, mean_q: 1940.645752\n",
      "wrong_move\n",
      "   4396/500000: episode: 4351, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3601.000 [3601.000, 3601.000],  loss: 684783.625000, mae: 2239.372070, mean_q: 2258.720459\n",
      "wrong_move\n",
      "   4397/500000: episode: 4352, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 33.000 [33.000, 33.000],  loss: 1026955.500000, mae: 2239.278320, mean_q: 1853.421143\n",
      "wrong_move\n",
      "   4398/500000: episode: 4353, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2835.000 [2835.000, 2835.000],  loss: 20564410.000000, mae: 2249.455078, mean_q: 2433.299805\n",
      "wrong_move\n",
      "   4399/500000: episode: 4354, duration: 0.150s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2835.000 [2835.000, 2835.000],  loss: 580296.375000, mae: 2262.916748, mean_q: 2939.378418\n",
      "wrong_move\n",
      "   4400/500000: episode: 4355, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2835.000 [2835.000, 2835.000],  loss: 996745.000000, mae: 2272.470215, mean_q: 2441.186035\n",
      "wrong_move\n",
      "   4401/500000: episode: 4356, duration: 0.165s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2244.000 [2244.000, 2244.000],  loss: 4001603.500000, mae: 2282.506348, mean_q: 1797.572510\n",
      "wrong_move\n",
      "   4402/500000: episode: 4357, duration: 0.147s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2039.000 [2039.000, 2039.000],  loss: 1185726.500000, mae: 2289.377197, mean_q: 1898.794922\n",
      "wrong_move\n",
      "   4403/500000: episode: 4358, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2835.000 [2835.000, 2835.000],  loss: 810850.875000, mae: 2295.980469, mean_q: 2178.033447\n",
      "wrong_move\n",
      "   4404/500000: episode: 4359, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3737.000 [3737.000, 3737.000],  loss: 3187076.500000, mae: 2299.817139, mean_q: 2149.099609\n",
      "wrong_move\n",
      "   4405/500000: episode: 4360, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2577.000 [2577.000, 2577.000],  loss: 748745.437500, mae: 2304.706787, mean_q: 3345.125000\n",
      "wrong_move\n",
      "   4406/500000: episode: 4361, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1596.000 [1596.000, 1596.000],  loss: 16978344.000000, mae: 2323.419922, mean_q: 3250.641846\n",
      "wrong_move\n",
      "   4407/500000: episode: 4362, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1553.000 [1553.000, 1553.000],  loss: 672323.000000, mae: 2303.255371, mean_q: 2583.157227\n",
      "wrong_move\n",
      "   4408/500000: episode: 4363, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3236.000 [3236.000, 3236.000],  loss: 581752.562500, mae: 2323.082520, mean_q: 3108.069824\n",
      "wrong_move\n",
      "   4409/500000: episode: 4364, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3555.000 [3555.000, 3555.000],  loss: 1178441.750000, mae: 2301.451904, mean_q: 3645.980957\n",
      "wrong_move\n",
      "   4410/500000: episode: 4365, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2812.000 [2812.000, 2812.000],  loss: 7931092.000000, mae: 2313.172607, mean_q: 2611.892822\n",
      "wrong_move\n",
      "   4411/500000: episode: 4366, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2532.000 [2532.000, 2532.000],  loss: 2792569.000000, mae: 2296.150391, mean_q: 2478.616943\n",
      "wrong_move\n",
      "   4412/500000: episode: 4367, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1013.000 [1013.000, 1013.000],  loss: 51124596.000000, mae: 2293.835938, mean_q: 2847.166016\n",
      "wrong_move\n",
      "   4413/500000: episode: 4368, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1596.000 [1596.000, 1596.000],  loss: 361990.437500, mae: 2287.462402, mean_q: 2209.632080\n",
      "wrong_move\n",
      "   4414/500000: episode: 4369, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 902.000 [902.000, 902.000],  loss: 1120013.250000, mae: 2282.994141, mean_q: 2319.477051\n",
      "wrong_move\n",
      "   4415/500000: episode: 4370, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1585.000 [1585.000, 1585.000],  loss: 24572518.000000, mae: 2294.262939, mean_q: 2769.675293\n",
      "wrong_move\n",
      "   4416/500000: episode: 4371, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 207.000 [207.000, 207.000],  loss: 473638.562500, mae: 2275.055176, mean_q: 3229.868164\n",
      "wrong_move\n",
      "   4417/500000: episode: 4372, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1596.000 [1596.000, 1596.000],  loss: 2769493.250000, mae: 2267.375000, mean_q: 2119.934814\n",
      "wrong_move\n",
      "   4418/500000: episode: 4373, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 675424.625000, mae: 2280.670654, mean_q: 3108.160156\n",
      "wrong_move\n",
      "   4419/500000: episode: 4374, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2033.000 [2033.000, 2033.000],  loss: 8487598.000000, mae: 2260.253662, mean_q: 2081.396484\n",
      "wrong_move\n",
      "   4420/500000: episode: 4375, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3132.000 [3132.000, 3132.000],  loss: 1234927.500000, mae: 2258.300781, mean_q: 2764.708496\n",
      "wrong_move\n",
      "   4421/500000: episode: 4376, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1259.000 [1259.000, 1259.000],  loss: 1757271.000000, mae: 2257.568848, mean_q: 2329.522949\n",
      "wrong_move\n",
      "   4422/500000: episode: 4377, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2287.000 [2287.000, 2287.000],  loss: 627950.562500, mae: 2274.269043, mean_q: 2805.976562\n",
      "wrong_move\n",
      "   4423/500000: episode: 4378, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: 443279.093750, mae: 2264.517090, mean_q: 1680.955322\n",
      "wrong_move\n",
      "   4424/500000: episode: 4379, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3015902.000000, mae: 2269.790527, mean_q: 1553.877441\n",
      "wrong_move\n",
      "   4425/500000: episode: 4380, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1596.000 [1596.000, 1596.000],  loss: 436499.750000, mae: 2270.610840, mean_q: 1859.735596\n",
      "wrong_move\n",
      "   4426/500000: episode: 4381, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2406.000 [2406.000, 2406.000],  loss: 450606.906250, mae: 2271.757080, mean_q: 2536.513184\n",
      "wrong_move\n",
      "   4427/500000: episode: 4382, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1596.000 [1596.000, 1596.000],  loss: 324836.531250, mae: 2273.747803, mean_q: 2050.935059\n",
      "wrong_move\n",
      "   4428/500000: episode: 4383, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 950134.000000, mae: 2276.926270, mean_q: 1948.802734\n",
      "wrong_move\n",
      "   4429/500000: episode: 4384, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1596.000 [1596.000, 1596.000],  loss: 2541274.500000, mae: 2312.204346, mean_q: 1395.485474\n",
      "wrong_move\n",
      "   4430/500000: episode: 4385, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3600.000 [3600.000, 3600.000],  loss: 350594.656250, mae: 2285.493652, mean_q: 1837.677246\n",
      "wrong_move\n",
      "   4431/500000: episode: 4386, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1840.000 [1840.000, 1840.000],  loss: 1129685.750000, mae: 2288.170898, mean_q: 2331.848389\n",
      "wrong_move\n",
      "   4432/500000: episode: 4387, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1596.000 [1596.000, 1596.000],  loss: 5324372.000000, mae: 2320.851318, mean_q: 2148.956299\n",
      "wrong_move\n",
      "   4433/500000: episode: 4388, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: 17121864.000000, mae: 2293.553955, mean_q: 3194.854492\n",
      "wrong_move\n",
      "   4434/500000: episode: 4389, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1596.000 [1596.000, 1596.000],  loss: 2251443.250000, mae: 2289.284424, mean_q: 1945.983887\n",
      "wrong_move\n",
      "   4435/500000: episode: 4390, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1596.000 [1596.000, 1596.000],  loss: 2795711.000000, mae: 2293.218018, mean_q: 2576.031250\n",
      "wrong_move\n",
      "   4436/500000: episode: 4391, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 725.000 [725.000, 725.000],  loss: 931085.875000, mae: 2289.900879, mean_q: 2308.163818\n",
      "wrong_move\n",
      "   4437/500000: episode: 4392, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3393.000 [3393.000, 3393.000],  loss: 1506414.375000, mae: 2293.076172, mean_q: 1779.857056\n",
      "wrong_move\n",
      "   4438/500000: episode: 4393, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1369.000 [1369.000, 1369.000],  loss: 325347.968750, mae: 2294.454102, mean_q: 2313.323730\n",
      "wrong_move\n",
      "   4439/500000: episode: 4394, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2403.000 [2403.000, 2403.000],  loss: 734261.937500, mae: 2297.123535, mean_q: 1890.766235\n",
      "wrong_move\n",
      "   4440/500000: episode: 4395, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3914.000 [3914.000, 3914.000],  loss: 877241.875000, mae: 2299.875977, mean_q: 1654.577393\n",
      "wrong_move\n",
      "   4441/500000: episode: 4396, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1980.000 [1980.000, 1980.000],  loss: 4587200.500000, mae: 2304.527588, mean_q: 1687.182739\n",
      "wrong_move\n",
      "   4442/500000: episode: 4397, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1596.000 [1596.000, 1596.000],  loss: 739438.875000, mae: 2305.142578, mean_q: 2658.976807\n",
      "wrong_move\n",
      "   4443/500000: episode: 4398, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1207.000 [1207.000, 1207.000],  loss: 1249162.875000, mae: 2306.418701, mean_q: 1086.803467\n",
      "wrong_move\n",
      "   4445/500000: episode: 4399, duration: 0.116s, episode steps:   2, steps per second:  17, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2274.000 [463.000, 4085.000],  loss: 5171671.000000, mae: 2323.377441, mean_q: 2127.842285\n",
      "wrong_move\n",
      "   4446/500000: episode: 4400, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3199.000 [3199.000, 3199.000],  loss: 598208.562500, mae: 2305.344238, mean_q: 2860.014648\n",
      "wrong_move\n",
      "   4447/500000: episode: 4401, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1767.000 [1767.000, 1767.000],  loss: 1446624.250000, mae: 2298.944336, mean_q: 2134.155029\n",
      "wrong_move\n",
      "   4448/500000: episode: 4402, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 405.000 [405.000, 405.000],  loss: 1470328.125000, mae: 2299.516113, mean_q: 1800.750488\n",
      "wrong_move\n",
      "   4449/500000: episode: 4403, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2785.000 [2785.000, 2785.000],  loss: 958054.437500, mae: 2303.177490, mean_q: 1871.352539\n",
      "wrong_move\n",
      "   4450/500000: episode: 4404, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2746.000 [2746.000, 2746.000],  loss: 485220.437500, mae: 2305.341797, mean_q: 1452.387695\n",
      "wrong_move\n",
      "   4451/500000: episode: 4405, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 782.000 [782.000, 782.000],  loss: 823996.250000, mae: 2306.396973, mean_q: 1513.713379\n",
      "wrong_move\n",
      "   4452/500000: episode: 4406, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4085.000 [4085.000, 4085.000],  loss: 1133900.625000, mae: 2307.479492, mean_q: 1856.776367\n",
      "wrong_move\n",
      "   4453/500000: episode: 4407, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4085.000 [4085.000, 4085.000],  loss: 842995.187500, mae: 2309.045898, mean_q: 2456.427246\n",
      "wrong_move\n",
      "   4454/500000: episode: 4408, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 916.000 [916.000, 916.000],  loss: 10999727.000000, mae: 2309.202148, mean_q: 2252.963379\n",
      "wrong_move\n",
      "   4455/500000: episode: 4409, duration: 0.146s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1274.000 [1274.000, 1274.000],  loss: 7758717.000000, mae: 2312.425049, mean_q: 1945.506836\n",
      "wrong_move\n",
      "   4456/500000: episode: 4410, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 825669.000000, mae: 2306.033447, mean_q: 2186.710205\n",
      "wrong_move\n",
      "   4457/500000: episode: 4411, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4085.000 [4085.000, 4085.000],  loss: 832220.500000, mae: 2302.888184, mean_q: 2270.870117\n",
      "wrong_move\n",
      "   4458/500000: episode: 4412, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2552.000 [2552.000, 2552.000],  loss: 554171.312500, mae: 2300.066895, mean_q: 1379.951782\n",
      "wrong_move\n",
      "   4459/500000: episode: 4413, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1558.000 [1558.000, 1558.000],  loss: 11844490.000000, mae: 2358.422363, mean_q: 2018.986328\n",
      "wrong_move\n",
      "   4460/500000: episode: 4414, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3992.000 [3992.000, 3992.000],  loss: 603071.187500, mae: 2292.787109, mean_q: 1960.963257\n",
      "wrong_move\n",
      "   4461/500000: episode: 4415, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3992.000 [3992.000, 3992.000],  loss: 3443177.750000, mae: 2291.009033, mean_q: 1466.720215\n",
      "wrong_move\n",
      "   4462/500000: episode: 4416, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3992.000 [3992.000, 3992.000],  loss: 3333638.750000, mae: 2289.926270, mean_q: 1841.167603\n",
      "wrong_move\n",
      "   4463/500000: episode: 4417, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4085.000 [4085.000, 4085.000],  loss: 894489.750000, mae: 2291.856934, mean_q: 2653.415771\n",
      "wrong_move\n",
      "   4464/500000: episode: 4418, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2654.000 [2654.000, 2654.000],  loss: 1530213.625000, mae: 2292.336914, mean_q: 1844.427490\n",
      "wrong_move\n",
      "   4465/500000: episode: 4419, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3992.000 [3992.000, 3992.000],  loss: 545324.625000, mae: 2306.235107, mean_q: 1746.912842\n",
      "wrong_move\n",
      "   4466/500000: episode: 4420, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 748208.937500, mae: 2295.882324, mean_q: 2337.157227\n",
      "wrong_move\n",
      "   4467/500000: episode: 4421, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 943.000 [943.000, 943.000],  loss: 390109.093750, mae: 2296.914795, mean_q: 1890.469604\n",
      "wrong_move\n",
      "   4468/500000: episode: 4422, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2020.000 [2020.000, 2020.000],  loss: 650367.937500, mae: 2295.317871, mean_q: 1846.384399\n",
      "wrong_move\n",
      "   4469/500000: episode: 4423, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2785.000 [2785.000, 2785.000],  loss: 1803745.750000, mae: 2296.560059, mean_q: 1534.759766\n",
      "wrong_move\n",
      "   4470/500000: episode: 4424, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2630.000 [2630.000, 2630.000],  loss: 1344930.750000, mae: 2298.583496, mean_q: 1609.723999\n",
      "wrong_move\n",
      "   4471/500000: episode: 4425, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3702.000 [3702.000, 3702.000],  loss: 1424439.625000, mae: 2301.614746, mean_q: 1474.554688\n",
      "wrong_move\n",
      "   4472/500000: episode: 4426, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3992.000 [3992.000, 3992.000],  loss: 2255696.750000, mae: 2320.504883, mean_q: 2555.969238\n",
      "wrong_move\n",
      "   4473/500000: episode: 4427, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: 2029477.500000, mae: 2314.648926, mean_q: 2179.474121\n",
      "wrong_move\n",
      "   4474/500000: episode: 4428, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1915591.375000, mae: 2302.341797, mean_q: 1284.520264\n",
      "wrong_move\n",
      "   4475/500000: episode: 4429, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5417950.500000, mae: 2304.266846, mean_q: 1472.962769\n",
      "wrong_move\n",
      "   4476/500000: episode: 4430, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2965.000 [2965.000, 2965.000],  loss: 983924.000000, mae: 2306.184814, mean_q: 1182.586182\n",
      "wrong_move\n",
      "   4477/500000: episode: 4431, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3992.000 [3992.000, 3992.000],  loss: 506934.218750, mae: 2309.516602, mean_q: 1627.950684\n",
      "wrong_move\n",
      "   4478/500000: episode: 4432, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 499396.281250, mae: 2315.468750, mean_q: 1423.266113\n",
      "wrong_move\n",
      "   4479/500000: episode: 4433, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3992.000 [3992.000, 3992.000],  loss: 368791.750000, mae: 2315.708984, mean_q: 1260.556396\n",
      "wrong_move\n",
      "   4480/500000: episode: 4434, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: 964307.000000, mae: 2317.970703, mean_q: 1084.837646\n",
      "wrong_move\n",
      "   4481/500000: episode: 4435, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: 1514076.375000, mae: 2322.137207, mean_q: 1086.587402\n",
      "wrong_move\n",
      "   4482/500000: episode: 4436, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 699.000 [699.000, 699.000],  loss: 1061773.125000, mae: 2323.687500, mean_q: 2049.027344\n",
      "wrong_move\n",
      "   4483/500000: episode: 4437, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3109.000 [3109.000, 3109.000],  loss: 2995520.000000, mae: 2324.490967, mean_q: 1795.640381\n",
      "wrong_move\n",
      "   4484/500000: episode: 4438, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1980.000 [1980.000, 1980.000],  loss: 956556.375000, mae: 2327.210693, mean_q: 1508.765381\n",
      "wrong_move\n",
      "   4485/500000: episode: 4439, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2482.000 [2482.000, 2482.000],  loss: 1250618.000000, mae: 2328.429443, mean_q: 1897.549561\n",
      "wrong_move\n",
      "   4486/500000: episode: 4440, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: 5561502.500000, mae: 2331.608887, mean_q: 1456.368164\n",
      "wrong_move\n",
      "   4487/500000: episode: 4441, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 262.000 [262.000, 262.000],  loss: 862042.000000, mae: 2331.541992, mean_q: 1753.357910\n",
      "wrong_move\n",
      "   4488/500000: episode: 4442, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 475125.281250, mae: 2332.964844, mean_q: 1795.892212\n",
      "wrong_move\n",
      "   4489/500000: episode: 4443, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2922.000 [2922.000, 2922.000],  loss: 442816.812500, mae: 2332.738770, mean_q: 1329.111450\n",
      "wrong_move\n",
      "   4490/500000: episode: 4444, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: 475670.437500, mae: 2331.959229, mean_q: 924.366638\n",
      "wrong_move\n",
      "   4491/500000: episode: 4445, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: 716245568.000000, mae: 2354.063965, mean_q: 2514.806396\n",
      "wrong_move\n",
      "   4492/500000: episode: 4446, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2926.000 [2926.000, 2926.000],  loss: 669828608.000000, mae: 2357.788574, mean_q: 2839.391602\n",
      "wrong_move\n",
      "   4493/500000: episode: 4447, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3855.000 [3855.000, 3855.000],  loss: 822541.750000, mae: 2356.943848, mean_q: 1874.208740\n",
      "wrong_move\n",
      "   4494/500000: episode: 4448, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2785.000 [2785.000, 2785.000],  loss: 3485495.750000, mae: 2369.052246, mean_q: 1984.191772\n",
      "wrong_move\n",
      "   4495/500000: episode: 4449, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2654.000 [2654.000, 2654.000],  loss: 430862.250000, mae: 2381.020020, mean_q: 3212.715332\n",
      "wrong_move\n",
      "   4496/500000: episode: 4450, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2654.000 [2654.000, 2654.000],  loss: 1087958.875000, mae: 2379.115234, mean_q: 2240.236816\n",
      "wrong_move\n",
      "   4497/500000: episode: 4451, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2654.000 [2654.000, 2654.000],  loss: 610354752.000000, mae: 2388.703613, mean_q: 3114.359375\n",
      "wrong_move\n",
      "   4498/500000: episode: 4452, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 706732.750000, mae: 2390.276611, mean_q: 3501.956543\n",
      "wrong_move\n",
      "   4499/500000: episode: 4453, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1975.000 [1975.000, 1975.000],  loss: 963678.500000, mae: 2397.240723, mean_q: 3209.025879\n",
      "wrong_move\n",
      "   4500/500000: episode: 4454, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 983.000 [983.000, 983.000],  loss: 1513659.750000, mae: 2401.393555, mean_q: 1753.031494\n",
      "wrong_move\n",
      "   4501/500000: episode: 4455, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 983.000 [983.000, 983.000],  loss: 352340.375000, mae: 2403.864014, mean_q: 1915.434326\n",
      "wrong_move\n",
      "   4502/500000: episode: 4456, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1653.000 [1653.000, 1653.000],  loss: 4627987.000000, mae: 2409.887451, mean_q: 3281.072266\n",
      "wrong_move\n",
      "   4503/500000: episode: 4457, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 983.000 [983.000, 983.000],  loss: 589039.312500, mae: 2403.432129, mean_q: 2857.576904\n",
      "wrong_move\n",
      "   4504/500000: episode: 4458, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 295.000 [295.000, 295.000],  loss: 891530.750000, mae: 2401.562256, mean_q: 3170.146484\n",
      "wrong_move\n",
      "   4505/500000: episode: 4459, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3356.000 [3356.000, 3356.000],  loss: 4341331.500000, mae: 2401.369629, mean_q: 3042.108887\n",
      "wrong_move\n",
      "   4506/500000: episode: 4460, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3484.000 [3484.000, 3484.000],  loss: 707664.250000, mae: 2399.281738, mean_q: 3336.429932\n",
      "wrong_move\n",
      "   4507/500000: episode: 4461, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1549.000 [1549.000, 1549.000],  loss: 1636327.750000, mae: 2394.983398, mean_q: 3538.829590\n",
      "wrong_move\n",
      "   4508/500000: episode: 4462, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 444.000 [444.000, 444.000],  loss: 741516.875000, mae: 2392.689697, mean_q: 2488.240967\n",
      "wrong_move\n",
      "   4509/500000: episode: 4463, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1818.000 [1818.000, 1818.000],  loss: 1228376.375000, mae: 2395.041016, mean_q: 3238.591797\n",
      "wrong_move\n",
      "   4510/500000: episode: 4464, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2332.000 [2332.000, 2332.000],  loss: 1171462.500000, mae: 2391.401367, mean_q: 3329.411621\n",
      "wrong_move\n",
      "   4511/500000: episode: 4465, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1888.000 [1888.000, 1888.000],  loss: 697878.062500, mae: 2392.683105, mean_q: 3393.608887\n",
      "wrong_move\n",
      "   4512/500000: episode: 4466, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3318.000 [3318.000, 3318.000],  loss: 994820.000000, mae: 2386.124512, mean_q: 2640.765137\n",
      "wrong_move\n",
      "   4513/500000: episode: 4467, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3197.000 [3197.000, 3197.000],  loss: 473428.500000, mae: 2383.100586, mean_q: 2690.154053\n",
      "wrong_move\n",
      "   4514/500000: episode: 4468, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 202.000 [202.000, 202.000],  loss: 2877133.500000, mae: 2376.729492, mean_q: 1927.151123\n",
      "wrong_move\n",
      "   4515/500000: episode: 4469, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1185.000 [1185.000, 1185.000],  loss: 408645.750000, mae: 2372.920898, mean_q: 2517.388428\n",
      "wrong_move\n",
      "   4516/500000: episode: 4470, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1259.000 [1259.000, 1259.000],  loss: 1108819.250000, mae: 2370.377441, mean_q: 2590.003662\n",
      "wrong_move\n",
      "   4517/500000: episode: 4471, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1298.000 [1298.000, 1298.000],  loss: 436309.812500, mae: 2362.147217, mean_q: 2723.365479\n",
      "wrong_move\n",
      "   4518/500000: episode: 4472, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 31.000 [31.000, 31.000],  loss: 1691175.750000, mae: 2362.947510, mean_q: 3270.279541\n",
      "wrong_move\n",
      "   4519/500000: episode: 4473, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1185.000 [1185.000, 1185.000],  loss: 651956.750000, mae: 2353.053223, mean_q: 2921.920898\n",
      "wrong_move\n",
      "   4520/500000: episode: 4474, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 110.000 [110.000, 110.000],  loss: 2907452.000000, mae: 2351.298828, mean_q: 3411.208496\n",
      "wrong_move\n",
      "   4521/500000: episode: 4475, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1793.000 [1793.000, 1793.000],  loss: 931961.437500, mae: 2351.946533, mean_q: 4122.584961\n",
      "wrong_move\n",
      "   4522/500000: episode: 4476, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 59.000 [59.000, 59.000],  loss: 585208.750000, mae: 2343.570312, mean_q: 2397.755615\n",
      "wrong_move\n",
      "   4523/500000: episode: 4477, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 475.000 [475.000, 475.000],  loss: 1083407744.000000, mae: 2404.210938, mean_q: 3568.333740\n",
      "wrong_move\n",
      "   4524/500000: episode: 4478, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 911.000 [911.000, 911.000],  loss: 647657.000000, mae: 2357.388672, mean_q: 2497.377197\n",
      "wrong_move\n",
      "   4525/500000: episode: 4479, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3199.000 [3199.000, 3199.000],  loss: 283796.781250, mae: 2371.139404, mean_q: 1825.777100\n",
      "wrong_move\n",
      "   4526/500000: episode: 4480, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2332.000 [2332.000, 2332.000],  loss: 593272.125000, mae: 2383.643799, mean_q: 2359.929688\n",
      "wrong_move\n",
      "   4527/500000: episode: 4481, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1793.000 [1793.000, 1793.000],  loss: 561177.500000, mae: 2413.448975, mean_q: 3870.783936\n",
      "wrong_move\n",
      "   4528/500000: episode: 4482, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 202.000 [202.000, 202.000],  loss: 3149571.500000, mae: 2397.637695, mean_q: 2785.203125\n",
      "wrong_move\n",
      "   4529/500000: episode: 4483, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1403.000 [1403.000, 1403.000],  loss: 426589.500000, mae: 2397.709717, mean_q: 2299.366455\n",
      "wrong_move\n",
      "   4530/500000: episode: 4484, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 679.000 [679.000, 679.000],  loss: 362243.437500, mae: 2429.909424, mean_q: 3505.783691\n",
      "wrong_move\n",
      "   4531/500000: episode: 4485, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1158060.000000, mae: 2402.816406, mean_q: 2291.456299\n",
      "wrong_move\n",
      "   4532/500000: episode: 4486, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1185.000 [1185.000, 1185.000],  loss: 716159.062500, mae: 2403.745605, mean_q: 3056.860352\n",
      "wrong_move\n",
      "   4534/500000: episode: 4487, duration: 0.244s, episode steps:   2, steps per second:   8, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 367949.468750, mae: 2403.518311, mean_q: 3155.649414\n",
      "wrong_move\n",
      "   4535/500000: episode: 4488, duration: 0.143s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2764.000 [2764.000, 2764.000],  loss: 2152872.500000, mae: 2402.626465, mean_q: 3359.389404\n",
      "wrong_move\n",
      "   4536/500000: episode: 4489, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2916.000 [2916.000, 2916.000],  loss: 1568141.375000, mae: 2398.347656, mean_q: 2513.654297\n",
      "wrong_move\n",
      "   4537/500000: episode: 4490, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 305.000 [305.000, 305.000],  loss: 363201.718750, mae: 2393.713379, mean_q: 2174.821777\n",
      "wrong_move\n",
      "   4538/500000: episode: 4491, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3498.000 [3498.000, 3498.000],  loss: 708619.312500, mae: 2391.552734, mean_q: 3000.913574\n",
      "wrong_move\n",
      "   4539/500000: episode: 4492, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3811.000 [3811.000, 3811.000],  loss: 3121429.250000, mae: 2388.430664, mean_q: 2520.863037\n",
      "wrong_move\n",
      "   4540/500000: episode: 4493, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1505.000 [1505.000, 1505.000],  loss: 581156.125000, mae: 2386.945557, mean_q: 3609.632812\n",
      "wrong_move\n",
      "   4541/500000: episode: 4494, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3974.000 [3974.000, 3974.000],  loss: 4146889.250000, mae: 2385.663574, mean_q: 2734.895020\n",
      "wrong_move\n",
      "   4543/500000: episode: 4495, duration: 0.165s, episode steps:   2, steps per second:  12, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 3811.000 [3811.000, 3811.000],  loss: 1678532.625000, mae: 2386.107666, mean_q: 2993.289062\n",
      "wrong_move\n",
      "   4544/500000: episode: 4496, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3811.000 [3811.000, 3811.000],  loss: 735387.875000, mae: 2392.956055, mean_q: 1730.518555\n",
      "wrong_move\n",
      "   4545/500000: episode: 4497, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2785.000 [2785.000, 2785.000],  loss: 1053406.750000, mae: 2419.919922, mean_q: 3620.004150\n",
      "wrong_move\n",
      "   4546/500000: episode: 4498, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 1032080.375000, mae: 2376.505371, mean_q: 2918.166504\n",
      "wrong_move\n",
      "   4547/500000: episode: 4499, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1211.000 [1211.000, 1211.000],  loss: 1260040.750000, mae: 2372.742188, mean_q: 1470.934937\n",
      "wrong_move\n",
      "   4548/500000: episode: 4500, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2093.000 [2093.000, 2093.000],  loss: 322575.875000, mae: 2368.857422, mean_q: 1702.708618\n",
      "wrong_move\n",
      "   4549/500000: episode: 4501, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 31.000 [31.000, 31.000],  loss: 356374.343750, mae: 2371.108887, mean_q: 3475.375732\n",
      "wrong_move\n",
      "   4550/500000: episode: 4502, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2614.000 [2614.000, 2614.000],  loss: 2271600.250000, mae: 2363.469238, mean_q: 2649.395264\n",
      "wrong_move\n",
      "   4551/500000: episode: 4503, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3199.000 [3199.000, 3199.000],  loss: 1213245.375000, mae: 2358.405762, mean_q: 1833.154053\n",
      "wrong_move\n",
      "   4552/500000: episode: 4504, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3811.000 [3811.000, 3811.000],  loss: 463472.625000, mae: 2355.835449, mean_q: 2242.068359\n",
      "wrong_move\n",
      "   4553/500000: episode: 4505, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3725.000 [3725.000, 3725.000],  loss: 2564859.750000, mae: 2352.263184, mean_q: 2273.805176\n",
      "wrong_move\n",
      "   4554/500000: episode: 4506, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3459.000 [3459.000, 3459.000],  loss: 1028301.187500, mae: 2349.833740, mean_q: 1997.392212\n",
      "wrong_move\n",
      "   4555/500000: episode: 4507, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3930.000 [3930.000, 3930.000],  loss: 3154294.000000, mae: 2347.973633, mean_q: 2226.507568\n",
      "wrong_move\n",
      "   4556/500000: episode: 4508, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 877.000 [877.000, 877.000],  loss: 2289508.500000, mae: 2351.497559, mean_q: 3131.083984\n",
      "wrong_move\n",
      "   4557/500000: episode: 4509, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2340.000 [2340.000, 2340.000],  loss: 381217.500000, mae: 2371.136230, mean_q: 2344.925293\n",
      "wrong_move\n",
      "   4558/500000: episode: 4510, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 877.000 [877.000, 877.000],  loss: 1020994.562500, mae: 2348.005127, mean_q: 1487.626465\n",
      "wrong_move\n",
      "   4559/500000: episode: 4511, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 877.000 [877.000, 877.000],  loss: 372808.125000, mae: 2368.625488, mean_q: 1196.442139\n",
      "wrong_move\n",
      "   4560/500000: episode: 4512, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1249.000 [1249.000, 1249.000],  loss: 5310689.000000, mae: 2353.589844, mean_q: 2583.738281\n",
      "wrong_move\n",
      "   4561/500000: episode: 4513, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 34.000 [34.000, 34.000],  loss: 222441.125000, mae: 2354.754395, mean_q: 1629.233521\n",
      "wrong_move\n",
      "   4562/500000: episode: 4514, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3930.000 [3930.000, 3930.000],  loss: 1654656.375000, mae: 2364.339355, mean_q: 2450.236328\n",
      "wrong_move\n",
      "   4563/500000: episode: 4515, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 877.000 [877.000, 877.000],  loss: 688643.000000, mae: 2368.454590, mean_q: 1215.004639\n",
      "wrong_move\n",
      "   4564/500000: episode: 4516, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2852.000 [2852.000, 2852.000],  loss: 417163.562500, mae: 2374.501465, mean_q: 2877.715576\n",
      "wrong_move\n",
      "   4565/500000: episode: 4517, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 877.000 [877.000, 877.000],  loss: 600937.500000, mae: 2364.953613, mean_q: 1794.427368\n",
      "wrong_move\n",
      "   4566/500000: episode: 4518, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2340.000 [2340.000, 2340.000],  loss: 2417944.000000, mae: 2366.085938, mean_q: 2316.667480\n",
      "wrong_move\n",
      "   4567/500000: episode: 4519, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2340.000 [2340.000, 2340.000],  loss: 370530.875000, mae: 2372.416992, mean_q: 2946.411621\n",
      "wrong_move\n",
      "   4568/500000: episode: 4520, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 877.000 [877.000, 877.000],  loss: 1326420.625000, mae: 2366.043457, mean_q: 1189.971436\n",
      "wrong_move\n",
      "   4569/500000: episode: 4521, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 58.000 [58.000, 58.000],  loss: 331372.312500, mae: 2370.054199, mean_q: 2000.690186\n",
      "wrong_move\n",
      "   4570/500000: episode: 4522, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 113.000 [113.000, 113.000],  loss: 5530128.500000, mae: 2369.558594, mean_q: 1864.964722\n",
      "wrong_move\n",
      "   4571/500000: episode: 4523, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1551.000 [1551.000, 1551.000],  loss: 384809.687500, mae: 2371.504395, mean_q: 1955.557861\n",
      "wrong_move\n",
      "   4572/500000: episode: 4524, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 31.000 [31.000, 31.000],  loss: 875613.125000, mae: 2372.083984, mean_q: 2201.953369\n",
      "wrong_move\n",
      "   4573/500000: episode: 4525, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1607.000 [1607.000, 1607.000],  loss: 1101272.125000, mae: 2376.073730, mean_q: 2195.151855\n",
      "wrong_move\n",
      "   4574/500000: episode: 4526, duration: 0.141s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1607.000 [1607.000, 1607.000],  loss: 339736.437500, mae: 2383.570801, mean_q: 2079.326660\n",
      "wrong_move\n",
      "   4575/500000: episode: 4527, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1607.000 [1607.000, 1607.000],  loss: 976203.625000, mae: 2385.849121, mean_q: 2971.989990\n",
      "wrong_move\n",
      "   4576/500000: episode: 4528, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 626.000 [626.000, 626.000],  loss: 1121846.625000, mae: 2386.605225, mean_q: 2418.047607\n",
      "wrong_move\n",
      "   4577/500000: episode: 4529, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1607.000 [1607.000, 1607.000],  loss: 6763539.000000, mae: 2387.147949, mean_q: 1351.653320\n",
      "wrong_move\n",
      "   4578/500000: episode: 4530, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3194.000 [3194.000, 3194.000],  loss: 901295.000000, mae: 2389.261230, mean_q: 2021.927124\n",
      "wrong_move\n",
      "   4579/500000: episode: 4531, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2421.000 [2421.000, 2421.000],  loss: 900718.250000, mae: 2391.867188, mean_q: 2606.866455\n",
      "wrong_move\n",
      "   4580/500000: episode: 4532, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1607.000 [1607.000, 1607.000],  loss: 347016.750000, mae: 2387.888184, mean_q: 3263.088135\n",
      "wrong_move\n",
      "   4581/500000: episode: 4533, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3279.000 [3279.000, 3279.000],  loss: 527222.062500, mae: 2385.288086, mean_q: 799.120239\n",
      "wrong_move\n",
      "   4582/500000: episode: 4534, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1607.000 [1607.000, 1607.000],  loss: 593283.562500, mae: 2387.016602, mean_q: 1812.343262\n",
      "wrong_move\n",
      "   4583/500000: episode: 4535, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 860.000 [860.000, 860.000],  loss: 522212.406250, mae: 2387.708008, mean_q: 1658.460449\n",
      "wrong_move\n",
      "   4584/500000: episode: 4536, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2296.000 [2296.000, 2296.000],  loss: 1400715.000000, mae: 2387.404297, mean_q: 1674.359497\n",
      "wrong_move\n",
      "   4585/500000: episode: 4537, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1340.000 [1340.000, 1340.000],  loss: 921561.937500, mae: 2388.123535, mean_q: 2152.019043\n",
      "wrong_move\n",
      "   4586/500000: episode: 4538, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3072.000 [3072.000, 3072.000],  loss: 597354.375000, mae: 2387.583496, mean_q: 1104.938965\n",
      "wrong_move\n",
      "   4587/500000: episode: 4539, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1607.000 [1607.000, 1607.000],  loss: 350523.593750, mae: 2408.949463, mean_q: 2042.366455\n",
      "wrong_move\n",
      "   4588/500000: episode: 4540, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1607.000 [1607.000, 1607.000],  loss: 3337183.000000, mae: 2390.725830, mean_q: 2520.526367\n",
      "wrong_move\n",
      "   4589/500000: episode: 4541, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3142.000 [3142.000, 3142.000],  loss: 970555.625000, mae: 2393.979248, mean_q: 3722.271240\n",
      "wrong_move\n",
      "   4590/500000: episode: 4542, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2481.000 [2481.000, 2481.000],  loss: 1139298.000000, mae: 2394.135986, mean_q: 1909.593384\n",
      "wrong_move\n",
      "   4591/500000: episode: 4543, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1038.000 [1038.000, 1038.000],  loss: 323002.468750, mae: 2393.915771, mean_q: 2429.220703\n",
      "wrong_move\n",
      "   4592/500000: episode: 4544, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 690.000 [690.000, 690.000],  loss: 211537.671875, mae: 2394.724121, mean_q: 1596.870117\n",
      "wrong_move\n",
      "   4593/500000: episode: 4545, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3840.000 [3840.000, 3840.000],  loss: 472646.218750, mae: 2397.270508, mean_q: 1938.217529\n",
      "wrong_move\n",
      "   4594/500000: episode: 4546, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3266.000 [3266.000, 3266.000],  loss: 869178.500000, mae: 2405.576172, mean_q: 2475.952393\n",
      "wrong_move\n",
      "   4595/500000: episode: 4547, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1390.000 [1390.000, 1390.000],  loss: 497962.125000, mae: 2405.778320, mean_q: 2410.108887\n",
      "wrong_move\n",
      "   4596/500000: episode: 4548, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 270.000 [270.000, 270.000],  loss: 463131.187500, mae: 2409.458496, mean_q: 1027.779785\n",
      "wrong_move\n",
      "   4597/500000: episode: 4549, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3840.000 [3840.000, 3840.000],  loss: 626583.250000, mae: 2413.862305, mean_q: 1199.917114\n",
      "wrong_move\n",
      "   4598/500000: episode: 4550, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3840.000 [3840.000, 3840.000],  loss: 1060694.000000, mae: 2442.126953, mean_q: 3157.204590\n",
      "wrong_move\n",
      "   4599/500000: episode: 4551, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 660185.375000, mae: 2419.887939, mean_q: 2498.411377\n",
      "wrong_move\n",
      "   4600/500000: episode: 4552, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3840.000 [3840.000, 3840.000],  loss: 547349.937500, mae: 2419.538086, mean_q: 2165.185303\n",
      "wrong_move\n",
      "   4601/500000: episode: 4553, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3840.000 [3840.000, 3840.000],  loss: 750206.875000, mae: 2419.283691, mean_q: 1414.951416\n",
      "wrong_move\n",
      "   4602/500000: episode: 4554, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1113.000 [1113.000, 1113.000],  loss: 322362.593750, mae: 2419.451172, mean_q: 1550.912842\n",
      "wrong_move\n",
      "   4603/500000: episode: 4555, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2109.000 [2109.000, 2109.000],  loss: 632892.125000, mae: 2420.200439, mean_q: 1664.064819\n",
      "wrong_move\n",
      "   4604/500000: episode: 4556, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1182.000 [1182.000, 1182.000],  loss: 316928.593750, mae: 2420.692627, mean_q: 1271.954834\n",
      "wrong_move\n",
      "   4605/500000: episode: 4557, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3840.000 [3840.000, 3840.000],  loss: 843670.875000, mae: 2440.490234, mean_q: 2120.406006\n",
      "wrong_move\n",
      "   4606/500000: episode: 4558, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1182.000 [1182.000, 1182.000],  loss: 560857.750000, mae: 2414.855225, mean_q: 2064.072754\n",
      "wrong_move\n",
      "   4607/500000: episode: 4559, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2982.000 [2982.000, 2982.000],  loss: 605728.250000, mae: 2428.594971, mean_q: 1410.761841\n",
      "wrong_move\n",
      "   4608/500000: episode: 4560, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 491028.562500, mae: 2414.436768, mean_q: 2194.586670\n",
      "wrong_move\n",
      "   4609/500000: episode: 4561, duration: 0.161s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1855.000 [1855.000, 1855.000],  loss: 1433714.625000, mae: 2414.165039, mean_q: 1441.654175\n",
      "wrong_move\n",
      "   4610/500000: episode: 4562, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 159.000 [159.000, 159.000],  loss: 423821.531250, mae: 2440.041504, mean_q: 1335.968506\n",
      "wrong_move\n",
      "   4611/500000: episode: 4563, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 992.000 [992.000, 992.000],  loss: 1115860.000000, mae: 2417.666992, mean_q: 3075.786377\n",
      "wrong_move\n",
      "   4612/500000: episode: 4564, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1953.000 [1953.000, 1953.000],  loss: 711412.625000, mae: 2426.976562, mean_q: 1931.939209\n",
      "wrong_move\n",
      "   4613/500000: episode: 4565, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1953.000 [1953.000, 1953.000],  loss: 656207.125000, mae: 2414.946777, mean_q: 1579.489014\n",
      "wrong_move\n",
      "   4614/500000: episode: 4566, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 159.000 [159.000, 159.000],  loss: 4554914.500000, mae: 2425.405762, mean_q: 2053.157959\n",
      "wrong_move\n",
      "   4615/500000: episode: 4567, duration: 0.150s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3266.000 [3266.000, 3266.000],  loss: 364969.593750, mae: 2409.015625, mean_q: 1238.563354\n",
      "wrong_move\n",
      "   4616/500000: episode: 4568, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 159.000 [159.000, 159.000],  loss: 293598.937500, mae: 2407.013916, mean_q: 1312.454346\n",
      "wrong_move\n",
      "   4617/500000: episode: 4569, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 239.000 [239.000, 239.000],  loss: 542551.437500, mae: 2405.531250, mean_q: 1227.534180\n",
      "wrong_move\n",
      "   4618/500000: episode: 4570, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1953.000 [1953.000, 1953.000],  loss: 7417825.000000, mae: 2403.385254, mean_q: 1617.271484\n",
      "wrong_move\n",
      "   4619/500000: episode: 4571, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 992.000 [992.000, 992.000],  loss: 986127.062500, mae: 2400.346680, mean_q: 595.328369\n",
      "wrong_move\n",
      "   4620/500000: episode: 4572, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2412.000 [2412.000, 2412.000],  loss: 1766374.125000, mae: 2400.285156, mean_q: 1615.470337\n",
      "wrong_move\n",
      "   4621/500000: episode: 4573, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1953.000 [1953.000, 1953.000],  loss: 1962075.875000, mae: 2405.233398, mean_q: 1623.533081\n",
      "wrong_move\n",
      "   4622/500000: episode: 4574, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2179.000 [2179.000, 2179.000],  loss: 666238.375000, mae: 2453.070557, mean_q: 3408.374756\n",
      "wrong_move\n",
      "   4623/500000: episode: 4575, duration: 0.142s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 58.000 [58.000, 58.000],  loss: 17049304.000000, mae: 2410.196289, mean_q: 1525.677002\n",
      "wrong_move\n",
      "   4624/500000: episode: 4576, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1953.000 [1953.000, 1953.000],  loss: 1309343.250000, mae: 2416.937500, mean_q: 2302.455811\n",
      "wrong_move\n",
      "   4625/500000: episode: 4577, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3749.000 [3749.000, 3749.000],  loss: 270799.125000, mae: 2416.739258, mean_q: 1169.684937\n",
      "wrong_move\n",
      "   4626/500000: episode: 4578, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3749.000 [3749.000, 3749.000],  loss: 1406189.750000, mae: 2411.389648, mean_q: 824.013794\n",
      "wrong_move\n",
      "   4627/500000: episode: 4579, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2342.000 [2342.000, 2342.000],  loss: 1142699.000000, mae: 2411.541748, mean_q: 1430.644165\n",
      "wrong_move\n",
      "   4628/500000: episode: 4580, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1953.000 [1953.000, 1953.000],  loss: 1224753.500000, mae: 2411.500244, mean_q: 1765.742554\n",
      "wrong_move\n",
      "   4629/500000: episode: 4581, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3043.000 [3043.000, 3043.000],  loss: 430683.531250, mae: 2411.554688, mean_q: 1035.120483\n",
      "wrong_move\n",
      "   4630/500000: episode: 4582, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1888.000 [1888.000, 1888.000],  loss: 863944.000000, mae: 2431.899414, mean_q: 3061.667725\n",
      "wrong_move\n",
      "   4631/500000: episode: 4583, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1953.000 [1953.000, 1953.000],  loss: 2935363.000000, mae: 2411.798340, mean_q: 2246.212402\n",
      "wrong_move\n",
      "   4632/500000: episode: 4584, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1953.000 [1953.000, 1953.000],  loss: 758184.562500, mae: 2413.784424, mean_q: 1235.828857\n",
      "wrong_move\n",
      "   4633/500000: episode: 4585, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: 132179.203125, mae: 2413.844727, mean_q: 680.838867\n",
      "wrong_move\n",
      "   4634/500000: episode: 4586, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1403.000 [1403.000, 1403.000],  loss: 1202172.000000, mae: 2416.688965, mean_q: 1286.091187\n",
      "wrong_move\n",
      "   4635/500000: episode: 4587, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3467.000 [3467.000, 3467.000],  loss: 747185.125000, mae: 2417.999756, mean_q: 716.387390\n",
      "wrong_move\n",
      "   4636/500000: episode: 4588, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1953.000 [1953.000, 1953.000],  loss: 536167.687500, mae: 2422.162842, mean_q: 1971.792480\n",
      "wrong_move\n",
      "   4637/500000: episode: 4589, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 380.000 [380.000, 380.000],  loss: 334084.000000, mae: 2422.979980, mean_q: 1749.823975\n",
      "wrong_move\n",
      "   4638/500000: episode: 4590, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3370.000 [3370.000, 3370.000],  loss: 752680.500000, mae: 2424.627441, mean_q: 1275.919800\n",
      "wrong_move\n",
      "   4639/500000: episode: 4591, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 373.000 [373.000, 373.000],  loss: 3577204.250000, mae: 2429.976318, mean_q: 2205.654541\n",
      "wrong_move\n",
      "   4640/500000: episode: 4592, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3919.000 [3919.000, 3919.000],  loss: 1058301.875000, mae: 2426.548340, mean_q: 1130.650024\n",
      "wrong_move\n",
      "   4641/500000: episode: 4593, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2193.000 [2193.000, 2193.000],  loss: 320581.781250, mae: 2450.262695, mean_q: 2543.248047\n",
      "wrong_move\n",
      "   4642/500000: episode: 4594, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 188.000 [188.000, 188.000],  loss: 9088793.000000, mae: 2425.969238, mean_q: 1229.519043\n",
      "wrong_move\n",
      "   4643/500000: episode: 4595, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2733.000 [2733.000, 2733.000],  loss: 786461.250000, mae: 2421.618164, mean_q: 1568.380981\n",
      "wrong_move\n",
      "   4644/500000: episode: 4596, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 555.000 [555.000, 555.000],  loss: 7414054.500000, mae: 2437.093018, mean_q: 2761.541016\n",
      "wrong_move\n",
      "   4645/500000: episode: 4597, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1572.000 [1572.000, 1572.000],  loss: 584878.687500, mae: 2415.754395, mean_q: 960.784424\n",
      "wrong_move\n",
      "   4646/500000: episode: 4598, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1750.000 [1750.000, 1750.000],  loss: 317197.500000, mae: 2414.641846, mean_q: 1398.434570\n",
      "wrong_move\n",
      "   4647/500000: episode: 4599, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 555.000 [555.000, 555.000],  loss: 28790308.000000, mae: 2418.914551, mean_q: 1307.443115\n",
      "wrong_move\n",
      "   4648/500000: episode: 4600, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2371.000 [2371.000, 2371.000],  loss: 842214.625000, mae: 2417.294434, mean_q: 1217.988037\n",
      "wrong_move\n",
      "   4649/500000: episode: 4601, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1387.000 [1387.000, 1387.000],  loss: 766674.625000, mae: 2408.515137, mean_q: 628.623474\n",
      "wrong_move\n",
      "   4650/500000: episode: 4602, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3853.000 [3853.000, 3853.000],  loss: 760768.437500, mae: 2411.077148, mean_q: 1757.499023\n",
      "wrong_move\n",
      "   4651/500000: episode: 4603, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 21.000 [21.000, 21.000],  loss: 1588159.875000, mae: 2411.719727, mean_q: 713.826050\n",
      "wrong_move\n",
      "   4652/500000: episode: 4604, duration: 0.154s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2253.000 [2253.000, 2253.000],  loss: 438805.218750, mae: 2413.260986, mean_q: 890.767883\n",
      "wrong_move\n",
      "   4653/500000: episode: 4605, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 555.000 [555.000, 555.000],  loss: 810906.000000, mae: 2413.306396, mean_q: 705.768677\n",
      "wrong_move\n",
      "   4654/500000: episode: 4606, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3782.000 [3782.000, 3782.000],  loss: 1085796.375000, mae: 2414.880615, mean_q: 836.649902\n",
      "wrong_move\n",
      "   4655/500000: episode: 4607, duration: 0.141s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 158856.453125, mae: 2416.116211, mean_q: 540.520691\n",
      "wrong_move\n",
      "   4656/500000: episode: 4608, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2733.000 [2733.000, 2733.000],  loss: 181796.000000, mae: 2416.967285, mean_q: 1079.307251\n",
      "wrong_move\n",
      "   4657/500000: episode: 4609, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 478493.968750, mae: 2416.380859, mean_q: 1123.324829\n",
      "wrong_move\n",
      "   4658/500000: episode: 4610, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2733.000 [2733.000, 2733.000],  loss: 1318005.500000, mae: 2423.108887, mean_q: 1754.611328\n",
      "wrong_move\n",
      "   4659/500000: episode: 4611, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4016.000 [4016.000, 4016.000],  loss: 1099294.250000, mae: 2415.848633, mean_q: 2075.515625\n",
      "wrong_move\n",
      "   4660/500000: episode: 4612, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2342.000 [2342.000, 2342.000],  loss: 608721.000000, mae: 2416.388672, mean_q: 966.779419\n",
      "wrong_move\n",
      "   4661/500000: episode: 4613, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1894.000 [1894.000, 1894.000],  loss: 750692.187500, mae: 2414.287109, mean_q: 859.176575\n",
      "wrong_move\n",
      "   4662/500000: episode: 4614, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 188.000 [188.000, 188.000],  loss: 1230826.875000, mae: 2414.562988, mean_q: 825.790588\n",
      "wrong_move\n",
      "   4663/500000: episode: 4615, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 925.000 [925.000, 925.000],  loss: 829455.062500, mae: 2413.378418, mean_q: 910.786865\n",
      "wrong_move\n",
      "   4664/500000: episode: 4616, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1743.000 [1743.000, 1743.000],  loss: 1283753.250000, mae: 2419.046143, mean_q: 2771.238770\n",
      "wrong_move\n",
      "   4665/500000: episode: 4617, duration: 0.142s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 650.000 [650.000, 650.000],  loss: 803344.625000, mae: 2411.886230, mean_q: 1055.675781\n",
      "wrong_move\n",
      "   4666/500000: episode: 4618, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 188.000 [188.000, 188.000],  loss: 1148541.250000, mae: 2414.605957, mean_q: 1179.042114\n",
      "wrong_move\n",
      "   4667/500000: episode: 4619, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 293.000 [293.000, 293.000],  loss: 378573.812500, mae: 2411.134766, mean_q: 908.334961\n",
      "wrong_move\n",
      "   4668/500000: episode: 4620, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2481.000 [2481.000, 2481.000],  loss: 2686447.500000, mae: 2445.773438, mean_q: 3246.924072\n",
      "wrong_move\n",
      "   4669/500000: episode: 4621, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1271.000 [1271.000, 1271.000],  loss: 331066.906250, mae: 2416.857178, mean_q: 1461.633301\n",
      "wrong_move\n",
      "   4670/500000: episode: 4622, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2438.000 [2438.000, 2438.000],  loss: 2443042.750000, mae: 2420.389893, mean_q: 1288.861084\n",
      "wrong_move\n",
      "   4671/500000: episode: 4623, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: 512215.375000, mae: 2426.253662, mean_q: 989.495850\n",
      "wrong_move\n",
      "   4672/500000: episode: 4624, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2761.000 [2761.000, 2761.000],  loss: 299863.875000, mae: 2431.049072, mean_q: 940.752075\n",
      "wrong_move\n",
      "   4673/500000: episode: 4625, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 188.000 [188.000, 188.000],  loss: 15071683.000000, mae: 2488.139160, mean_q: 1081.050781\n",
      "wrong_move\n",
      "   4674/500000: episode: 4626, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1338.000 [1338.000, 1338.000],  loss: 325866.625000, mae: 2438.283691, mean_q: 1046.258179\n",
      "wrong_move\n",
      "   4675/500000: episode: 4627, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 188.000 [188.000, 188.000],  loss: 1226659.500000, mae: 2439.663574, mean_q: 436.476562\n",
      "wrong_move\n",
      "   4676/500000: episode: 4628, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1743.000 [1743.000, 1743.000],  loss: 769305.687500, mae: 2441.366699, mean_q: 696.997498\n",
      "wrong_move\n",
      "   4677/500000: episode: 4629, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 188.000 [188.000, 188.000],  loss: 394883.000000, mae: 2443.895264, mean_q: 1053.617554\n",
      "wrong_move\n",
      "   4678/500000: episode: 4630, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 188.000 [188.000, 188.000],  loss: 349290.843750, mae: 2444.024170, mean_q: 718.976074\n",
      "wrong_move\n",
      "   4679/500000: episode: 4631, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1743.000 [1743.000, 1743.000],  loss: 1074172.625000, mae: 2510.820312, mean_q: 2873.239258\n",
      "wrong_move\n",
      "   4680/500000: episode: 4632, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1743.000 [1743.000, 1743.000],  loss: 3038253.500000, mae: 2443.193359, mean_q: 1321.222900\n",
      "wrong_move\n",
      "   4681/500000: episode: 4633, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1296.000 [1296.000, 1296.000],  loss: 154680.218750, mae: 2444.554199, mean_q: 1506.657471\n",
      "wrong_move\n",
      "   4682/500000: episode: 4634, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1743.000 [1743.000, 1743.000],  loss: 875121.125000, mae: 2445.910645, mean_q: 906.894775\n",
      "wrong_move\n",
      "   4683/500000: episode: 4635, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1743.000 [1743.000, 1743.000],  loss: 737168.000000, mae: 2447.048584, mean_q: 1032.580933\n",
      "wrong_move\n",
      "   4684/500000: episode: 4636, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 635.000 [635.000, 635.000],  loss: 3327339.250000, mae: 2450.589355, mean_q: 819.708557\n",
      "wrong_move\n",
      "   4685/500000: episode: 4637, duration: 0.168s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1743.000 [1743.000, 1743.000],  loss: 587081.875000, mae: 2450.887207, mean_q: 2260.092529\n",
      "wrong_move\n",
      "   4686/500000: episode: 4638, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2278.000 [2278.000, 2278.000],  loss: 394114.531250, mae: 2450.988770, mean_q: 1881.219971\n",
      "wrong_move\n",
      "   4687/500000: episode: 4639, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1743.000 [1743.000, 1743.000],  loss: 367336.500000, mae: 2496.979004, mean_q: 930.142029\n",
      "wrong_move\n",
      "   4688/500000: episode: 4640, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1338.000 [1338.000, 1338.000],  loss: 1214214.750000, mae: 2447.320312, mean_q: 658.818481\n",
      "wrong_move\n",
      "   4689/500000: episode: 4641, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3642.000 [3642.000, 3642.000],  loss: 1196888.125000, mae: 2447.033203, mean_q: 1547.331055\n",
      "wrong_move\n",
      "   4690/500000: episode: 4642, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1338.000 [1338.000, 1338.000],  loss: 523015.062500, mae: 2457.168457, mean_q: 2037.879028\n",
      "wrong_move\n",
      "   4691/500000: episode: 4643, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3522.000 [3522.000, 3522.000],  loss: 761599.875000, mae: 2444.913574, mean_q: 1254.309082\n",
      "wrong_move\n",
      "   4692/500000: episode: 4644, duration: 0.149s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 747.000 [747.000, 747.000],  loss: 985957.125000, mae: 2449.354492, mean_q: 1560.189331\n",
      "wrong_move\n",
      "   4693/500000: episode: 4645, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1338.000 [1338.000, 1338.000],  loss: 453083.625000, mae: 2443.345459, mean_q: 1241.203125\n",
      "wrong_move\n",
      "   4694/500000: episode: 4646, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3280.000 [3280.000, 3280.000],  loss: 512599.500000, mae: 2444.080322, mean_q: 905.125610\n",
      "wrong_move\n",
      "   4695/500000: episode: 4647, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 606.000 [606.000, 606.000],  loss: 569890.250000, mae: 2444.843262, mean_q: 643.951294\n",
      "wrong_move\n",
      "   4696/500000: episode: 4648, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1876.000 [1876.000, 1876.000],  loss: 607949.000000, mae: 2449.020508, mean_q: 2411.119629\n",
      "wrong_move\n",
      "   4697/500000: episode: 4649, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3107.000 [3107.000, 3107.000],  loss: 2215792.500000, mae: 2449.411377, mean_q: 865.981934\n",
      "wrong_move\n",
      "   4698/500000: episode: 4650, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 527.000 [527.000, 527.000],  loss: 1097642.500000, mae: 2453.809082, mean_q: 762.797302\n",
      "wrong_move\n",
      "   4699/500000: episode: 4651, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: 192875.968750, mae: 2472.805664, mean_q: 1802.298218\n",
      "wrong_move\n",
      "   4700/500000: episode: 4652, duration: 0.165s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1339.000 [1339.000, 1339.000],  loss: 542176.500000, mae: 2462.860596, mean_q: 884.529785\n",
      "wrong_move\n",
      "   4701/500000: episode: 4653, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3522.000 [3522.000, 3522.000],  loss: 337477.250000, mae: 2465.149658, mean_q: 612.268433\n",
      "wrong_move\n",
      "   4702/500000: episode: 4654, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2085.000 [2085.000, 2085.000],  loss: 544016.750000, mae: 2469.786133, mean_q: 759.316589\n",
      "wrong_move\n",
      "   4703/500000: episode: 4655, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1279.000 [1279.000, 1279.000],  loss: 2987479.500000, mae: 2484.626709, mean_q: 2468.377197\n",
      "wrong_move\n",
      "   4704/500000: episode: 4656, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2349.000 [2349.000, 2349.000],  loss: 533971.250000, mae: 2479.951904, mean_q: 2071.322510\n",
      "wrong_move\n",
      "   4705/500000: episode: 4657, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1279.000 [1279.000, 1279.000],  loss: 1199674.000000, mae: 2478.041504, mean_q: 734.594849\n",
      "wrong_move\n",
      "   4706/500000: episode: 4658, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3493.000 [3493.000, 3493.000],  loss: 583938.187500, mae: 2477.040527, mean_q: 487.188751\n",
      "wrong_move\n",
      "   4707/500000: episode: 4659, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2316.000 [2316.000, 2316.000],  loss: 847838.000000, mae: 2476.731689, mean_q: 592.447998\n",
      "wrong_move\n",
      "   4708/500000: episode: 4660, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2921.000 [2921.000, 2921.000],  loss: 825790.812500, mae: 2474.037109, mean_q: 643.790405\n",
      "wrong_move\n",
      "   4710/500000: episode: 4661, duration: 0.067s, episode steps:   2, steps per second:  30, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1249.000 [426.000, 2072.000],  loss: 1226494.750000, mae: 2472.124512, mean_q: 1725.711914\n",
      "wrong_move\n",
      "   4711/500000: episode: 4662, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3365.000 [3365.000, 3365.000],  loss: 568798.375000, mae: 2467.446289, mean_q: 811.840759\n",
      "wrong_move\n",
      "   4712/500000: episode: 4663, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3704.000 [3704.000, 3704.000],  loss: 631976.750000, mae: 2463.627441, mean_q: 522.351746\n",
      "wrong_move\n",
      "   4713/500000: episode: 4664, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3768.000 [3768.000, 3768.000],  loss: 1318802.750000, mae: 2521.272461, mean_q: 2801.315186\n",
      "wrong_move\n",
      "   4714/500000: episode: 4665, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2349.000 [2349.000, 2349.000],  loss: 532369.875000, mae: 2460.716797, mean_q: 946.163147\n",
      "wrong_move\n",
      "   4715/500000: episode: 4666, duration: 0.150s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 776.000 [776.000, 776.000],  loss: 762007.437500, mae: 2458.422607, mean_q: 787.224609\n",
      "wrong_move\n",
      "   4716/500000: episode: 4667, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 421.000 [421.000, 421.000],  loss: 384782.875000, mae: 2456.026123, mean_q: 1567.930786\n",
      "wrong_move\n",
      "   4717/500000: episode: 4668, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2349.000 [2349.000, 2349.000],  loss: 381828.343750, mae: 2452.677246, mean_q: 1664.728271\n",
      "wrong_move\n",
      "   4718/500000: episode: 4669, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2349.000 [2349.000, 2349.000],  loss: 724027.687500, mae: 2449.630859, mean_q: 874.620789\n",
      "wrong_move\n",
      "   4719/500000: episode: 4670, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2349.000 [2349.000, 2349.000],  loss: 820134.000000, mae: 2447.498535, mean_q: 1633.005249\n",
      "wrong_move\n",
      "   4720/500000: episode: 4671, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2349.000 [2349.000, 2349.000],  loss: 1429937.250000, mae: 2446.861816, mean_q: 642.909485\n",
      "wrong_move\n",
      "   4721/500000: episode: 4672, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 776.000 [776.000, 776.000],  loss: 252770.578125, mae: 2450.297852, mean_q: 730.875427\n",
      "wrong_move\n",
      "   4722/500000: episode: 4673, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1523.000 [1523.000, 1523.000],  loss: 1402152.625000, mae: 2455.743652, mean_q: 439.183868\n",
      "wrong_move\n",
      "   4723/500000: episode: 4674, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2152.000 [2152.000, 2152.000],  loss: 451347.062500, mae: 2466.776367, mean_q: 2286.702637\n",
      "wrong_move\n",
      "   4724/500000: episode: 4675, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2349.000 [2349.000, 2349.000],  loss: 588148.125000, mae: 2482.784668, mean_q: 1570.824463\n",
      "wrong_move\n",
      "   4725/500000: episode: 4676, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2176.000 [2176.000, 2176.000],  loss: 200540.968750, mae: 2468.112305, mean_q: 868.129639\n",
      "wrong_move\n",
      "   4726/500000: episode: 4677, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1369.000 [1369.000, 1369.000],  loss: 3254591.500000, mae: 2470.060547, mean_q: 723.752625\n",
      "wrong_move\n",
      "   4727/500000: episode: 4678, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1246.000 [1246.000, 1246.000],  loss: 688720.125000, mae: 2475.063232, mean_q: 1283.217773\n",
      "wrong_move\n",
      "   4728/500000: episode: 4679, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3342.000 [3342.000, 3342.000],  loss: 884577.750000, mae: 2478.819824, mean_q: 1339.550781\n",
      "wrong_move\n",
      "   4729/500000: episode: 4680, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: 430975.593750, mae: 2483.528320, mean_q: 1293.971191\n",
      "wrong_move\n",
      "   4730/500000: episode: 4681, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2685.000 [2685.000, 2685.000],  loss: 363106.531250, mae: 2491.456543, mean_q: 1001.271118\n",
      "wrong_move\n",
      "   4731/500000: episode: 4682, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: 852028.250000, mae: 2497.282227, mean_q: 2321.983887\n",
      "wrong_move\n",
      "   4732/500000: episode: 4683, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 650.000 [650.000, 650.000],  loss: 489877.156250, mae: 2498.084473, mean_q: 754.213379\n",
      "wrong_move\n",
      "   4733/500000: episode: 4684, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: 1038913.750000, mae: 2500.141113, mean_q: 662.063538\n",
      "wrong_move\n",
      "   4734/500000: episode: 4685, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2761.000 [2761.000, 2761.000],  loss: 217919.656250, mae: 2502.518555, mean_q: 760.350647\n",
      "wrong_move\n",
      "   4735/500000: episode: 4686, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4093.000 [4093.000, 4093.000],  loss: 280144.625000, mae: 2504.036377, mean_q: 1560.976562\n",
      "wrong_move\n",
      "   4736/500000: episode: 4687, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 347.000 [347.000, 347.000],  loss: 5873254.000000, mae: 2502.246094, mean_q: 933.384583\n",
      "wrong_move\n",
      "   4737/500000: episode: 4688, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2857.000 [2857.000, 2857.000],  loss: 1418928.250000, mae: 2501.217529, mean_q: 1845.606323\n",
      "wrong_move\n",
      "   4738/500000: episode: 4689, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 214304.250000, mae: 2498.753418, mean_q: 874.303467\n",
      "wrong_move\n",
      "   4739/500000: episode: 4690, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3330.000 [3330.000, 3330.000],  loss: 494055.812500, mae: 2507.076660, mean_q: 1960.172363\n",
      "wrong_move\n",
      "   4740/500000: episode: 4691, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1130.000 [1130.000, 1130.000],  loss: 560591.625000, mae: 2497.271240, mean_q: 1580.584351\n",
      "wrong_move\n",
      "   4741/500000: episode: 4692, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 377140.562500, mae: 2514.662598, mean_q: 855.145264\n",
      "wrong_move\n",
      "   4742/500000: episode: 4693, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2342.000 [2342.000, 2342.000],  loss: 180349.000000, mae: 2498.349365, mean_q: 1770.084839\n",
      "wrong_move\n",
      "   4743/500000: episode: 4694, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2366.000 [2366.000, 2366.000],  loss: 690069.625000, mae: 2527.839355, mean_q: 1335.888794\n",
      "wrong_move\n",
      "   4744/500000: episode: 4695, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1638.000 [1638.000, 1638.000],  loss: 649618.375000, mae: 2501.725830, mean_q: 2117.013916\n",
      "wrong_move\n",
      "   4745/500000: episode: 4696, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 75.000 [75.000, 75.000],  loss: 473816.625000, mae: 2495.502441, mean_q: 704.862427\n",
      "wrong_move\n",
      "   4746/500000: episode: 4697, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 982.000 [982.000, 982.000],  loss: 314642.968750, mae: 2495.320068, mean_q: 915.729065\n",
      "wrong_move\n",
      "   4747/500000: episode: 4698, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2366.000 [2366.000, 2366.000],  loss: 747231.875000, mae: 2508.684326, mean_q: 2738.846924\n",
      "wrong_move\n",
      "   4748/500000: episode: 4699, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2342.000 [2342.000, 2342.000],  loss: 265890.062500, mae: 2494.836426, mean_q: 669.558411\n",
      "wrong_move\n",
      "   4749/500000: episode: 4700, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2366.000 [2366.000, 2366.000],  loss: 927203.375000, mae: 2496.501465, mean_q: 1632.185181\n",
      "wrong_move\n",
      "   4750/500000: episode: 4701, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2551.000 [2551.000, 2551.000],  loss: 357562.906250, mae: 2496.274902, mean_q: 792.598755\n",
      "wrong_move\n",
      "   4751/500000: episode: 4702, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 282544.187500, mae: 2497.824951, mean_q: 1433.655151\n",
      "wrong_move\n",
      "   4752/500000: episode: 4703, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2342.000 [2342.000, 2342.000],  loss: 2421272.250000, mae: 2499.509766, mean_q: 1272.273438\n",
      "wrong_move\n",
      "   4753/500000: episode: 4704, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3824.000 [3824.000, 3824.000],  loss: 457360.562500, mae: 2500.913574, mean_q: 1611.456299\n",
      "wrong_move\n",
      "   4754/500000: episode: 4705, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2366.000 [2366.000, 2366.000],  loss: 1440200.250000, mae: 2498.726074, mean_q: 1152.629028\n",
      "wrong_move\n",
      "   4755/500000: episode: 4706, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2422.000 [2422.000, 2422.000],  loss: 485124.000000, mae: 2495.931396, mean_q: 1001.451538\n",
      "wrong_move\n",
      "   4756/500000: episode: 4707, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2150.000 [2150.000, 2150.000],  loss: 698811.062500, mae: 2494.194580, mean_q: 1089.927856\n",
      "wrong_move\n",
      "   4757/500000: episode: 4708, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1325.000 [1325.000, 1325.000],  loss: 460880.906250, mae: 2487.737549, mean_q: 1251.537354\n",
      "wrong_move\n",
      "   4758/500000: episode: 4709, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 19.000 [19.000, 19.000],  loss: 368217.687500, mae: 2483.974609, mean_q: 1628.740234\n",
      "wrong_move\n",
      "   4759/500000: episode: 4710, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1481.000 [1481.000, 1481.000],  loss: 226138.453125, mae: 2480.019043, mean_q: 710.806946\n",
      "wrong_move\n",
      "   4760/500000: episode: 4711, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2366.000 [2366.000, 2366.000],  loss: 654628.500000, mae: 2477.797607, mean_q: 748.032227\n",
      "wrong_move\n",
      "   4761/500000: episode: 4712, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1390.000 [1390.000, 1390.000],  loss: 424529.437500, mae: 2474.297852, mean_q: 933.075439\n",
      "wrong_move\n",
      "   4762/500000: episode: 4713, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2366.000 [2366.000, 2366.000],  loss: 710119.562500, mae: 2471.501953, mean_q: 724.660156\n",
      "wrong_move\n",
      "   4763/500000: episode: 4714, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3815.000 [3815.000, 3815.000],  loss: 809604.000000, mae: 2470.155273, mean_q: 743.940063\n",
      "wrong_move\n",
      "   4764/500000: episode: 4715, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1832.000 [1832.000, 1832.000],  loss: 251929.078125, mae: 2468.901855, mean_q: 700.384583\n",
      "wrong_move\n",
      "   4765/500000: episode: 4716, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 964208.625000, mae: 2468.637207, mean_q: 794.021790\n",
      "wrong_move\n",
      "   4766/500000: episode: 4717, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2071.000 [2071.000, 2071.000],  loss: 435673.656250, mae: 2469.211182, mean_q: 989.432556\n",
      "wrong_move\n",
      "   4767/500000: episode: 4718, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3648.000 [3648.000, 3648.000],  loss: 775449.437500, mae: 2470.506836, mean_q: 910.834045\n",
      "wrong_move\n",
      "   4768/500000: episode: 4719, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2366.000 [2366.000, 2366.000],  loss: 1510196.750000, mae: 2467.644531, mean_q: 1211.686279\n",
      "wrong_move\n",
      "   4769/500000: episode: 4720, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2366.000 [2366.000, 2366.000],  loss: 189062.203125, mae: 2466.374512, mean_q: 2365.063965\n",
      "wrong_move\n",
      "   4770/500000: episode: 4721, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2366.000 [2366.000, 2366.000],  loss: 553366.875000, mae: 2460.920898, mean_q: 952.199463\n",
      "wrong_move\n",
      "   4771/500000: episode: 4722, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2754.000 [2754.000, 2754.000],  loss: 213126.531250, mae: 2459.839111, mean_q: 995.322571\n",
      "wrong_move\n",
      "   4772/500000: episode: 4723, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 274.000 [274.000, 274.000],  loss: 7891755.500000, mae: 2460.705811, mean_q: 1275.187744\n",
      "wrong_move\n",
      "   4773/500000: episode: 4724, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2366.000 [2366.000, 2366.000],  loss: 2893187.750000, mae: 2459.303955, mean_q: 1195.327881\n",
      "wrong_move\n",
      "   4774/500000: episode: 4725, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2072.000 [2072.000, 2072.000],  loss: 1330627.625000, mae: 2456.453613, mean_q: 592.930359\n",
      "wrong_move\n",
      "   4775/500000: episode: 4726, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2366.000 [2366.000, 2366.000],  loss: 599313.500000, mae: 2454.420410, mean_q: 840.420166\n",
      "wrong_move\n",
      "   4776/500000: episode: 4727, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2366.000 [2366.000, 2366.000],  loss: 948262.562500, mae: 2457.503418, mean_q: 1631.983398\n",
      "wrong_move\n",
      "   4778/500000: episode: 4728, duration: 0.109s, episode steps:   2, steps per second:  18, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2791.000 [2072.000, 3510.000],  loss: 728631.187500, mae: 2482.741455, mean_q: 1876.480347\n",
      "wrong_move\n",
      "   4779/500000: episode: 4729, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2072.000 [2072.000, 2072.000],  loss: 486542.187500, mae: 2451.986572, mean_q: 941.531250\n",
      "wrong_move\n",
      "   4780/500000: episode: 4730, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2072.000 [2072.000, 2072.000],  loss: 560058.000000, mae: 2453.995605, mean_q: 696.654602\n",
      "wrong_move\n",
      "   4781/500000: episode: 4731, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1770.000 [1770.000, 1770.000],  loss: 128951.226562, mae: 2460.116699, mean_q: 1918.362183\n",
      "wrong_move\n",
      "   4782/500000: episode: 4732, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2933.000 [2933.000, 2933.000],  loss: 805682.062500, mae: 2464.004395, mean_q: 723.264404\n",
      "wrong_move\n",
      "   4783/500000: episode: 4733, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2072.000 [2072.000, 2072.000],  loss: 499923.250000, mae: 2470.934082, mean_q: 1143.252686\n",
      "wrong_move\n",
      "   4784/500000: episode: 4734, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2072.000 [2072.000, 2072.000],  loss: 961981.500000, mae: 2477.398438, mean_q: 720.287354\n",
      "wrong_move\n",
      "   4785/500000: episode: 4735, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2072.000 [2072.000, 2072.000],  loss: 503296.812500, mae: 2483.630615, mean_q: 658.692383\n",
      "wrong_move\n",
      "   4786/500000: episode: 4736, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2967.000 [2967.000, 2967.000],  loss: 1533225.750000, mae: 2487.985596, mean_q: 803.127441\n",
      "wrong_move\n",
      "   4787/500000: episode: 4737, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3815.000 [3815.000, 3815.000],  loss: 1584106.500000, mae: 2489.383301, mean_q: 320.043365\n",
      "wrong_move\n",
      "   4788/500000: episode: 4738, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2815.000 [2815.000, 2815.000],  loss: 338954.312500, mae: 2490.286377, mean_q: 755.296753\n",
      "wrong_move\n",
      "   4789/500000: episode: 4739, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1130.000 [1130.000, 1130.000],  loss: 392021.093750, mae: 2511.416504, mean_q: 2740.825928\n",
      "wrong_move\n",
      "   4790/500000: episode: 4740, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 479.000 [479.000, 479.000],  loss: 5645398.500000, mae: 2493.001221, mean_q: 564.190369\n",
      "wrong_move\n",
      "   4791/500000: episode: 4741, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1698.000 [1698.000, 1698.000],  loss: 756015.437500, mae: 2487.450195, mean_q: 1155.583862\n",
      "wrong_move\n",
      "   4792/500000: episode: 4742, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2912.000 [2912.000, 2912.000],  loss: 449741.218750, mae: 2486.144043, mean_q: 494.733124\n",
      "wrong_move\n",
      "   4793/500000: episode: 4743, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1304.000 [1304.000, 1304.000],  loss: 923005.375000, mae: 2500.909180, mean_q: 462.471191\n",
      "wrong_move\n",
      "   4794/500000: episode: 4744, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2211.000 [2211.000, 2211.000],  loss: 1009317.312500, mae: 2525.245117, mean_q: 733.022278\n",
      "wrong_move\n",
      "   4795/500000: episode: 4745, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1844.000 [1844.000, 1844.000],  loss: 1209572.250000, mae: 2542.628906, mean_q: 617.875671\n",
      "wrong_move\n",
      "   4796/500000: episode: 4746, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1393.000 [1393.000, 1393.000],  loss: 888210.500000, mae: 2592.854004, mean_q: 3160.495605\n",
      "wrong_move\n",
      "   4797/500000: episode: 4747, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2754.000 [2754.000, 2754.000],  loss: 526349.437500, mae: 2567.322754, mean_q: 1284.137817\n",
      "wrong_move\n",
      "   4798/500000: episode: 4748, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2485.000 [2485.000, 2485.000],  loss: 729672.125000, mae: 2573.875000, mean_q: 729.745117\n",
      "wrong_move\n",
      "   4799/500000: episode: 4749, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2754.000 [2754.000, 2754.000],  loss: 2187016.750000, mae: 2576.935547, mean_q: 1275.960693\n",
      "wrong_move\n",
      "   4800/500000: episode: 4750, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2754.000 [2754.000, 2754.000],  loss: 783051.500000, mae: 2574.944336, mean_q: 1032.221680\n",
      "wrong_move\n",
      "   4801/500000: episode: 4751, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2754.000 [2754.000, 2754.000],  loss: 1526220.875000, mae: 2571.655273, mean_q: 1619.123413\n",
      "wrong_move\n",
      "   4802/500000: episode: 4752, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2014.000 [2014.000, 2014.000],  loss: 1288200.625000, mae: 2567.143311, mean_q: 2282.573242\n",
      "wrong_move\n",
      "   4803/500000: episode: 4753, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2014.000 [2014.000, 2014.000],  loss: 1698589.000000, mae: 2568.982422, mean_q: 1151.379517\n",
      "wrong_move\n",
      "   4804/500000: episode: 4754, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2014.000 [2014.000, 2014.000],  loss: 19373394.000000, mae: 2549.457520, mean_q: 1885.511108\n",
      "wrong_move\n",
      "   4805/500000: episode: 4755, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2014.000 [2014.000, 2014.000],  loss: 148508.156250, mae: 2541.737061, mean_q: 1636.334595\n",
      "wrong_move\n",
      "   4806/500000: episode: 4756, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2014.000 [2014.000, 2014.000],  loss: 598054.312500, mae: 2534.614258, mean_q: 1195.205322\n",
      "wrong_move\n",
      "   4807/500000: episode: 4757, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 334.000 [334.000, 334.000],  loss: 1393705.500000, mae: 2532.841797, mean_q: 3239.736084\n",
      "wrong_move\n",
      "   4808/500000: episode: 4758, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 123.000 [123.000, 123.000],  loss: 1050727.125000, mae: 2526.216553, mean_q: 1526.630249\n",
      "wrong_move\n",
      "   4809/500000: episode: 4759, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 664981.562500, mae: 2519.145752, mean_q: 1409.865479\n",
      "wrong_move\n",
      "   4810/500000: episode: 4760, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2014.000 [2014.000, 2014.000],  loss: 566429.625000, mae: 2515.040527, mean_q: 1504.353394\n",
      "wrong_move\n",
      "   4811/500000: episode: 4761, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2014.000 [2014.000, 2014.000],  loss: 580041.312500, mae: 2539.765625, mean_q: 2970.287598\n",
      "wrong_move\n",
      "   4812/500000: episode: 4762, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1212.000 [1212.000, 1212.000],  loss: 258450.125000, mae: 2510.231445, mean_q: 1972.245972\n",
      "wrong_move\n",
      "   4813/500000: episode: 4763, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2342.000 [2342.000, 2342.000],  loss: 377067.343750, mae: 2509.886230, mean_q: 1634.444092\n",
      "wrong_move\n",
      "   4814/500000: episode: 4764, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1085.000 [1085.000, 1085.000],  loss: 304375.406250, mae: 2511.850830, mean_q: 2316.096924\n",
      "wrong_move\n",
      "   4815/500000: episode: 4765, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2953.000 [2953.000, 2953.000],  loss: 777900.500000, mae: 2511.485352, mean_q: 1248.993042\n",
      "wrong_move\n",
      "   4816/500000: episode: 4766, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1027889.937500, mae: 2532.835449, mean_q: 2322.967285\n",
      "wrong_move\n",
      "   4817/500000: episode: 4767, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3249.000 [3249.000, 3249.000],  loss: 429718.625000, mae: 2512.769531, mean_q: 1300.546997\n",
      "wrong_move\n",
      "   4818/500000: episode: 4768, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1318001.500000, mae: 2514.577393, mean_q: 1937.633057\n",
      "wrong_move\n",
      "   4819/500000: episode: 4769, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: 622062.437500, mae: 2517.162109, mean_q: 1941.458130\n",
      "wrong_move\n",
      "   4820/500000: episode: 4770, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2197.000 [2197.000, 2197.000],  loss: 851308.250000, mae: 2516.159668, mean_q: 885.634277\n",
      "wrong_move\n",
      "   4821/500000: episode: 4771, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 348568.468750, mae: 2519.399170, mean_q: 1818.854736\n",
      "wrong_move\n",
      "   4822/500000: episode: 4772, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2014.000 [2014.000, 2014.000],  loss: 718414.250000, mae: 2519.447021, mean_q: 852.203491\n",
      "wrong_move\n",
      "   4823/500000: episode: 4773, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2014.000 [2014.000, 2014.000],  loss: 573977.125000, mae: 2520.516846, mean_q: 1183.774048\n",
      "wrong_move\n",
      "   4824/500000: episode: 4774, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2014.000 [2014.000, 2014.000],  loss: 417998.062500, mae: 2521.537109, mean_q: 1384.958008\n",
      "wrong_move\n",
      "   4825/500000: episode: 4775, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2627.000 [2627.000, 2627.000],  loss: 1960895.250000, mae: 2525.094727, mean_q: 1544.244995\n",
      "wrong_move\n",
      "   4826/500000: episode: 4776, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3726.000 [3726.000, 3726.000],  loss: 824669.875000, mae: 2525.891113, mean_q: 1973.579346\n",
      "wrong_move\n",
      "   4827/500000: episode: 4777, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3726.000 [3726.000, 3726.000],  loss: 1064343.875000, mae: 2527.274170, mean_q: 1750.340820\n",
      "wrong_move\n",
      "   4828/500000: episode: 4778, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3726.000 [3726.000, 3726.000],  loss: 1206983.000000, mae: 2533.872070, mean_q: 1311.542114\n",
      "wrong_move\n",
      "   4829/500000: episode: 4779, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 955.000 [955.000, 955.000],  loss: 1222678.625000, mae: 2525.938965, mean_q: 1526.401245\n",
      "wrong_move\n",
      "   4830/500000: episode: 4780, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3726.000 [3726.000, 3726.000],  loss: 139011.796875, mae: 2527.593262, mean_q: 1978.862671\n",
      "wrong_move\n",
      "   4831/500000: episode: 4781, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3726.000 [3726.000, 3726.000],  loss: 352452.812500, mae: 2589.198242, mean_q: 3779.261963\n",
      "wrong_move\n",
      "   4832/500000: episode: 4782, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3246.000 [3246.000, 3246.000],  loss: 471153.375000, mae: 2531.439453, mean_q: 1314.963623\n",
      "wrong_move\n",
      "   4833/500000: episode: 4783, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3726.000 [3726.000, 3726.000],  loss: 638373.750000, mae: 2536.551270, mean_q: 2123.318359\n",
      "wrong_move\n",
      "   4834/500000: episode: 4784, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3726.000 [3726.000, 3726.000],  loss: 630839.187500, mae: 2540.470947, mean_q: 1235.222534\n",
      "wrong_move\n",
      "   4835/500000: episode: 4785, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3726.000 [3726.000, 3726.000],  loss: 408844.718750, mae: 2544.114502, mean_q: 983.894287\n",
      "wrong_move\n",
      "   4836/500000: episode: 4786, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2877.000 [2877.000, 2877.000],  loss: 837378.875000, mae: 2547.124512, mean_q: 1336.977783\n",
      "wrong_move\n",
      "   4837/500000: episode: 4787, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3598.000 [3598.000, 3598.000],  loss: 381242.187500, mae: 2548.040771, mean_q: 1036.423218\n",
      "wrong_move\n",
      "   4838/500000: episode: 4788, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2826.000 [2826.000, 2826.000],  loss: 437043.593750, mae: 2568.270508, mean_q: 4092.134766\n",
      "wrong_move\n",
      "   4839/500000: episode: 4789, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3726.000 [3726.000, 3726.000],  loss: 296208.875000, mae: 2547.538086, mean_q: 1127.799438\n",
      "wrong_move\n",
      "   4840/500000: episode: 4790, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1908.000 [1908.000, 1908.000],  loss: 384724.437500, mae: 2548.975098, mean_q: 2844.481445\n",
      "wrong_move\n",
      "   4841/500000: episode: 4791, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 953.000 [953.000, 953.000],  loss: 531093.812500, mae: 2542.959961, mean_q: 855.273682\n",
      "wrong_move\n",
      "   4842/500000: episode: 4792, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2858.000 [2858.000, 2858.000],  loss: 1436724.500000, mae: 2540.592773, mean_q: 972.670776\n",
      "wrong_move\n",
      "   4843/500000: episode: 4793, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 953.000 [953.000, 953.000],  loss: 1711748.500000, mae: 2541.102539, mean_q: 2731.008789\n",
      "wrong_move\n",
      "   4844/500000: episode: 4794, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 953.000 [953.000, 953.000],  loss: 536070.625000, mae: 2536.042969, mean_q: 1100.589844\n",
      "wrong_move\n",
      "   4845/500000: episode: 4795, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3139.000 [3139.000, 3139.000],  loss: 781339.250000, mae: 2532.844727, mean_q: 1689.942505\n",
      "wrong_move\n",
      "   4846/500000: episode: 4796, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3271.000 [3271.000, 3271.000],  loss: 818055.312500, mae: 2530.228027, mean_q: 1301.825439\n",
      "wrong_move\n",
      "   4847/500000: episode: 4797, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 953.000 [953.000, 953.000],  loss: 672699.375000, mae: 2529.811523, mean_q: 1319.129272\n",
      "wrong_move\n",
      "   4848/500000: episode: 4798, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 953.000 [953.000, 953.000],  loss: 532127.875000, mae: 2530.811279, mean_q: 1777.637695\n",
      "wrong_move\n",
      "   4849/500000: episode: 4799, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 953.000 [953.000, 953.000],  loss: 651957.312500, mae: 2529.984131, mean_q: 968.425171\n",
      "wrong_move\n",
      "   4850/500000: episode: 4800, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3734.000 [3734.000, 3734.000],  loss: 644601.875000, mae: 2533.713867, mean_q: 3046.809082\n",
      "wrong_move\n",
      "   4851/500000: episode: 4801, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 781571.562500, mae: 2534.231201, mean_q: 3253.212891\n",
      "wrong_move\n",
      "   4852/500000: episode: 4802, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3512.000 [3512.000, 3512.000],  loss: 680245.625000, mae: 2531.154297, mean_q: 1070.623901\n",
      "wrong_move\n",
      "   4853/500000: episode: 4803, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3206.000 [3206.000, 3206.000],  loss: 429631.750000, mae: 2531.820801, mean_q: 1080.934814\n",
      "wrong_move\n",
      "   4854/500000: episode: 4804, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3859.000 [3859.000, 3859.000],  loss: 918914.437500, mae: 2533.772949, mean_q: 1539.859497\n",
      "wrong_move\n",
      "   4855/500000: episode: 4805, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2731.000 [2731.000, 2731.000],  loss: 309032.375000, mae: 2535.293457, mean_q: 740.771851\n",
      "wrong_move\n",
      "   4856/500000: episode: 4806, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1685.000 [1685.000, 1685.000],  loss: 458891.656250, mae: 2540.419189, mean_q: 3487.471191\n",
      "wrong_move\n",
      "   4858/500000: episode: 4807, duration: 0.176s, episode steps:   2, steps per second:  11, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2447.500 [1761.000, 3134.000],  loss: 905761.312500, mae: 2539.483398, mean_q: 1103.397949\n",
      "wrong_move\n",
      "   4859/500000: episode: 4808, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 777.000 [777.000, 777.000],  loss: 360863.687500, mae: 2541.930176, mean_q: 1505.375732\n",
      "wrong_move\n",
      "   4860/500000: episode: 4809, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 625510.562500, mae: 2592.216309, mean_q: 2867.858154\n",
      "wrong_move\n",
      "   4861/500000: episode: 4810, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1685.000 [1685.000, 1685.000],  loss: 2147461.750000, mae: 2539.804199, mean_q: 395.777283\n",
      "wrong_move\n",
      "   4862/500000: episode: 4811, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 619.000 [619.000, 619.000],  loss: 568065.000000, mae: 2538.552246, mean_q: 789.756226\n",
      "wrong_move\n",
      "   4863/500000: episode: 4812, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 175.000 [175.000, 175.000],  loss: 996949.812500, mae: 2536.951172, mean_q: 1316.651123\n",
      "wrong_move\n",
      "   4864/500000: episode: 4813, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3183.000 [3183.000, 3183.000],  loss: 247101.562500, mae: 2536.986084, mean_q: 998.432190\n",
      "wrong_move\n",
      "   4865/500000: episode: 4814, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3518.000 [3518.000, 3518.000],  loss: 348827.875000, mae: 2538.149902, mean_q: 691.744507\n",
      "wrong_move\n",
      "   4866/500000: episode: 4815, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2706.000 [2706.000, 2706.000],  loss: 427107.125000, mae: 2540.229004, mean_q: 980.327515\n",
      "wrong_move\n",
      "   4867/500000: episode: 4816, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1685.000 [1685.000, 1685.000],  loss: 1345968.250000, mae: 2626.400391, mean_q: 3938.623291\n",
      "wrong_move\n",
      "   4868/500000: episode: 4817, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 74.000 [74.000, 74.000],  loss: 322779.093750, mae: 2545.631104, mean_q: 973.181152\n",
      "wrong_move\n",
      "   4869/500000: episode: 4818, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1685.000 [1685.000, 1685.000],  loss: 296809.093750, mae: 2548.936768, mean_q: 1286.342285\n",
      "wrong_move\n",
      "   4870/500000: episode: 4819, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 955.000 [955.000, 955.000],  loss: 4272643.000000, mae: 2548.996582, mean_q: 791.631958\n",
      "wrong_move\n",
      "   4871/500000: episode: 4820, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1685.000 [1685.000, 1685.000],  loss: 18665170944.000000, mae: 2719.443359, mean_q: 4160.064941\n",
      "wrong_move\n",
      "   4872/500000: episode: 4821, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1036.000 [1036.000, 1036.000],  loss: 866527.875000, mae: 2594.250488, mean_q: 1383.096680\n",
      "wrong_move\n",
      "   4873/500000: episode: 4822, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 245.000 [245.000, 245.000],  loss: 646033.000000, mae: 2630.500488, mean_q: 2183.687012\n",
      "wrong_move\n",
      "   4874/500000: episode: 4823, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 660.000 [660.000, 660.000],  loss: 611224.875000, mae: 2670.572754, mean_q: 1889.587280\n",
      "wrong_move\n",
      "   4875/500000: episode: 4824, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1827.000 [1827.000, 1827.000],  loss: 802421.125000, mae: 2678.498047, mean_q: 1931.973145\n",
      "wrong_move\n",
      "   4876/500000: episode: 4825, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2894.000 [2894.000, 2894.000],  loss: 2768685.250000, mae: 2690.980469, mean_q: 2047.387939\n",
      "wrong_move\n",
      "   4877/500000: episode: 4826, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1036.000 [1036.000, 1036.000],  loss: 6647569.500000, mae: 2697.435547, mean_q: 2411.357910\n",
      "wrong_move\n",
      "   4878/500000: episode: 4827, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 544.000 [544.000, 544.000],  loss: 1395101.750000, mae: 2700.573242, mean_q: 2263.485107\n",
      "wrong_move\n",
      "   4879/500000: episode: 4828, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1852.000 [1852.000, 1852.000],  loss: 192053.218750, mae: 2694.083008, mean_q: 2694.509521\n",
      "wrong_move\n",
      "   4880/500000: episode: 4829, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 833.000 [833.000, 833.000],  loss: 208420.000000, mae: 2695.808105, mean_q: 3184.740967\n",
      "wrong_move\n",
      "   4881/500000: episode: 4830, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1036.000 [1036.000, 1036.000],  loss: 793323.250000, mae: 2676.201172, mean_q: 2547.927979\n",
      "wrong_move\n",
      "   4882/500000: episode: 4831, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2894.000 [2894.000, 2894.000],  loss: 8039793.000000, mae: 2666.424316, mean_q: 2761.468506\n",
      "wrong_move\n",
      "   4883/500000: episode: 4832, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1036.000 [1036.000, 1036.000],  loss: 488128.187500, mae: 2652.308838, mean_q: 3014.781006\n",
      "wrong_move\n",
      "   4884/500000: episode: 4833, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4049.000 [4049.000, 4049.000],  loss: 384958.437500, mae: 2638.240723, mean_q: 2751.346436\n",
      "wrong_move\n",
      "   4885/500000: episode: 4834, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1055.000 [1055.000, 1055.000],  loss: 494577.656250, mae: 2626.097168, mean_q: 2757.730469\n",
      "wrong_move\n",
      "   4886/500000: episode: 4835, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2476.000 [2476.000, 2476.000],  loss: 255753.843750, mae: 2613.482910, mean_q: 2466.801025\n",
      "wrong_move\n",
      "   4887/500000: episode: 4836, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1036.000 [1036.000, 1036.000],  loss: 1120104.250000, mae: 2603.187012, mean_q: 2893.062744\n",
      "wrong_move\n",
      "   4888/500000: episode: 4837, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4038.000 [4038.000, 4038.000],  loss: 307678.750000, mae: 2594.288330, mean_q: 3428.627441\n",
      "wrong_move\n",
      "   4890/500000: episode: 4838, duration: 0.166s, episode steps:   2, steps per second:  12, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1290911.375000, mae: 2623.458008, mean_q: 3939.811279\n",
      "wrong_move\n",
      "   4891/500000: episode: 4839, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 621.000 [621.000, 621.000],  loss: 3594193.750000, mae: 2576.919189, mean_q: 2725.967285\n",
      "wrong_move\n",
      "   4892/500000: episode: 4840, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1036.000 [1036.000, 1036.000],  loss: 363085.687500, mae: 2569.822754, mean_q: 2680.971924\n",
      "wrong_move\n",
      "   4893/500000: episode: 4841, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 319.000 [319.000, 319.000],  loss: 322183.875000, mae: 2563.402832, mean_q: 2697.315186\n",
      "wrong_move\n",
      "   4894/500000: episode: 4842, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1036.000 [1036.000, 1036.000],  loss: 1873567.750000, mae: 2558.759033, mean_q: 2096.361816\n",
      "wrong_move\n",
      "   4895/500000: episode: 4843, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2602.000 [2602.000, 2602.000],  loss: 2392639.250000, mae: 2556.614990, mean_q: 2181.602051\n",
      "wrong_move\n",
      "   4896/500000: episode: 4844, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3875.000 [3875.000, 3875.000],  loss: 872795.375000, mae: 2557.985352, mean_q: 3826.166748\n",
      "wrong_move\n",
      "   4897/500000: episode: 4845, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2894.000 [2894.000, 2894.000],  loss: 322133.125000, mae: 2562.931641, mean_q: 4336.780762\n",
      "wrong_move\n",
      "   4898/500000: episode: 4846, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2894.000 [2894.000, 2894.000],  loss: 6095337.000000, mae: 2560.515381, mean_q: 3378.416992\n",
      "wrong_move\n",
      "   4899/500000: episode: 4847, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2563.000 [2563.000, 2563.000],  loss: 708070.312500, mae: 2556.819824, mean_q: 2094.822998\n",
      "wrong_move\n",
      "   4900/500000: episode: 4848, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 74.000 [74.000, 74.000],  loss: 2031371.750000, mae: 2554.865723, mean_q: 1901.096924\n",
      "wrong_move\n",
      "   4901/500000: episode: 4849, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2952.000 [2952.000, 2952.000],  loss: 1355368.500000, mae: 2553.729736, mean_q: 3125.697754\n",
      "wrong_move\n",
      "   4902/500000: episode: 4850, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 319.000 [319.000, 319.000],  loss: 2188066.500000, mae: 2551.050049, mean_q: 2303.172607\n",
      "wrong_move\n",
      "   4903/500000: episode: 4851, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1917.000 [1917.000, 1917.000],  loss: 2390165.750000, mae: 2548.890137, mean_q: 2510.668945\n",
      "wrong_move\n",
      "   4904/500000: episode: 4852, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 749.000 [749.000, 749.000],  loss: 1489406.625000, mae: 2544.031982, mean_q: 2289.961670\n",
      "wrong_move\n",
      "   4905/500000: episode: 4853, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1036.000 [1036.000, 1036.000],  loss: 331194.125000, mae: 2541.526367, mean_q: 4114.352051\n",
      "wrong_move\n",
      "   4906/500000: episode: 4854, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1761.000 [1761.000, 1761.000],  loss: 464527.687500, mae: 2531.787354, mean_q: 2574.101318\n",
      "wrong_move\n",
      "   4907/500000: episode: 4855, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1761.000 [1761.000, 1761.000],  loss: 380695.625000, mae: 2530.097168, mean_q: 2134.229980\n",
      "wrong_move\n",
      "   4908/500000: episode: 4856, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1761.000 [1761.000, 1761.000],  loss: 258786.250000, mae: 2531.221191, mean_q: 2608.970459\n",
      "wrong_move\n",
      "   4909/500000: episode: 4857, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1761.000 [1761.000, 1761.000],  loss: 3941944.250000, mae: 2533.883057, mean_q: 3799.647217\n",
      "wrong_move\n",
      "   4910/500000: episode: 4858, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1761.000 [1761.000, 1761.000],  loss: 1564137.000000, mae: 2537.119141, mean_q: 3835.567383\n",
      "wrong_move\n",
      "   4911/500000: episode: 4859, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 579.000 [579.000, 579.000],  loss: 950730.187500, mae: 2533.518311, mean_q: 2683.645508\n",
      "wrong_move\n",
      "   4912/500000: episode: 4860, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1158.000 [1158.000, 1158.000],  loss: 1844456.250000, mae: 2535.060791, mean_q: 2824.191406\n",
      "wrong_move\n",
      "   4913/500000: episode: 4861, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 344.000 [344.000, 344.000],  loss: 2089166.750000, mae: 2537.496582, mean_q: 3558.866455\n",
      "wrong_move\n",
      "   4914/500000: episode: 4862, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1761.000 [1761.000, 1761.000],  loss: 764216.687500, mae: 2541.099854, mean_q: 3383.550781\n",
      "wrong_move\n",
      "   4915/500000: episode: 4863, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3370.000 [3370.000, 3370.000],  loss: 1828031.250000, mae: 2540.203125, mean_q: 2091.058594\n",
      "wrong_move\n",
      "   4916/500000: episode: 4864, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1761.000 [1761.000, 1761.000],  loss: 3675868.750000, mae: 2541.123535, mean_q: 1675.888794\n",
      "wrong_move\n",
      "   4917/500000: episode: 4865, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3924.000 [3924.000, 3924.000],  loss: 1274687.750000, mae: 2542.720215, mean_q: 2375.039307\n",
      "wrong_move\n",
      "   4918/500000: episode: 4866, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 763168.562500, mae: 2597.426270, mean_q: 3413.521240\n",
      "wrong_move\n",
      "   4919/500000: episode: 4867, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 161.000 [161.000, 161.000],  loss: 927791.125000, mae: 2641.664062, mean_q: 3115.721680\n",
      "wrong_move\n",
      "   4920/500000: episode: 4868, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 463.000 [463.000, 463.000],  loss: 4833674.000000, mae: 2716.654053, mean_q: 4428.498535\n",
      "wrong_move\n",
      "   4922/500000: episode: 4869, duration: 0.170s, episode steps:   2, steps per second:  12, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 463.000 [463.000, 463.000],  loss: 933728.750000, mae: 2812.241211, mean_q: 4843.388184\n",
      "wrong_move\n",
      "   4923/500000: episode: 4870, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 463.000 [463.000, 463.000],  loss: 1297170.000000, mae: 2915.023926, mean_q: 7328.826660\n",
      "wrong_move\n",
      "   4924/500000: episode: 4871, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1225.000 [1225.000, 1225.000],  loss: 1184387.500000, mae: 2919.469971, mean_q: 7346.398926\n",
      "wrong_move\n",
      "   4925/500000: episode: 4872, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 463.000 [463.000, 463.000],  loss: 4959232.500000, mae: 2945.546387, mean_q: 8338.234375\n",
      "wrong_move\n",
      "   4926/500000: episode: 4873, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 463.000 [463.000, 463.000],  loss: 18713932.000000, mae: 2960.101807, mean_q: 8614.685547\n",
      "wrong_move\n",
      "   4927/500000: episode: 4874, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 463.000 [463.000, 463.000],  loss: 855450.187500, mae: 3022.199219, mean_q: 9612.449219\n",
      "wrong_move\n",
      "   4928/500000: episode: 4875, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3598.000 [3598.000, 3598.000],  loss: 5583355.000000, mae: 2970.907227, mean_q: 9555.767578\n",
      "wrong_move\n",
      "   4930/500000: episode: 4876, duration: 0.065s, episode steps:   2, steps per second:  31, episode reward: -5041.000, mean reward: -2520.500 [-5000.000, -41.000], mean action: 463.000 [463.000, 463.000],  loss: 2206503.000000, mae: 2972.913818, mean_q: 9979.772461\n",
      "wrong_move\n",
      "   4931/500000: episode: 4877, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2136.000 [2136.000, 2136.000],  loss: 753785.500000, mae: 2966.304688, mean_q: 10290.983398\n",
      "wrong_move\n",
      "   4932/500000: episode: 4878, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4051.000 [4051.000, 4051.000],  loss: 573679.875000, mae: 2956.200439, mean_q: 10483.384766\n",
      "wrong_move\n",
      "   4933/500000: episode: 4879, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1564.000 [1564.000, 1564.000],  loss: 3594166.000000, mae: 2942.522461, mean_q: 10953.592773\n",
      "wrong_move\n",
      "   4934/500000: episode: 4880, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 463.000 [463.000, 463.000],  loss: 13261671.000000, mae: 2927.450928, mean_q: 10655.181641\n",
      "wrong_move\n",
      "   4935/500000: episode: 4881, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 463.000 [463.000, 463.000],  loss: 13262483.000000, mae: 2909.030273, mean_q: 10729.650391\n",
      "wrong_move\n",
      "   4936/500000: episode: 4882, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2007.000 [2007.000, 2007.000],  loss: 7744242.500000, mae: 2881.096191, mean_q: 10655.648438\n",
      "wrong_move\n",
      "   4937/500000: episode: 4883, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 463.000 [463.000, 463.000],  loss: 7419937.500000, mae: 2856.960449, mean_q: 10523.199219\n",
      "wrong_move\n",
      "   4938/500000: episode: 4884, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 463.000 [463.000, 463.000],  loss: 787477.125000, mae: 2833.356445, mean_q: 10418.181641\n",
      "wrong_move\n",
      "   4939/500000: episode: 4885, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 463.000 [463.000, 463.000],  loss: 9721573.000000, mae: 2811.889893, mean_q: 10346.093750\n",
      "wrong_move\n",
      "   4940/500000: episode: 4886, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 463.000 [463.000, 463.000],  loss: 3437284.000000, mae: 2789.690186, mean_q: 10203.732422\n",
      "wrong_move\n",
      "   4941/500000: episode: 4887, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 463.000 [463.000, 463.000],  loss: 5435505.000000, mae: 2774.739014, mean_q: 10082.987305\n",
      "wrong_move\n",
      "   4942/500000: episode: 4888, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3629.000 [3629.000, 3629.000],  loss: 702522.500000, mae: 2750.576660, mean_q: 10044.323242\n",
      "wrong_move\n",
      "   4943/500000: episode: 4889, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 463.000 [463.000, 463.000],  loss: 431285.281250, mae: 2734.428711, mean_q: 9914.522461\n",
      "wrong_move\n",
      "   4944/500000: episode: 4890, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3843.000 [3843.000, 3843.000],  loss: 15114198.000000, mae: 2721.140625, mean_q: 9744.895508\n",
      "wrong_move\n",
      "   4945/500000: episode: 4891, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 463.000 [463.000, 463.000],  loss: 1092682.000000, mae: 2706.668701, mean_q: 9618.156250\n",
      "wrong_move\n",
      "   4946/500000: episode: 4892, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3843.000 [3843.000, 3843.000],  loss: 32468970.000000, mae: 2694.531738, mean_q: 9620.221680\n",
      "wrong_move\n",
      "   4947/500000: episode: 4893, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3172773.500000, mae: 2681.490723, mean_q: 9366.587891\n",
      "wrong_move\n",
      "   4948/500000: episode: 4894, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3890.000 [3890.000, 3890.000],  loss: 6477624.500000, mae: 2670.392090, mean_q: 9270.384766\n",
      "wrong_move\n",
      "   4949/500000: episode: 4895, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2684.000 [2684.000, 2684.000],  loss: 9454957.000000, mae: 2659.810059, mean_q: 9094.029297\n",
      "wrong_move\n",
      "   4950/500000: episode: 4896, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3710.000 [3710.000, 3710.000],  loss: 1525381.750000, mae: 2649.735596, mean_q: 8999.856445\n",
      "wrong_move\n",
      "   4951/500000: episode: 4897, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3729.000 [3729.000, 3729.000],  loss: 2775777.250000, mae: 2640.537109, mean_q: 9238.018555\n",
      "wrong_move\n",
      "   4952/500000: episode: 4898, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1004965.625000, mae: 2631.609863, mean_q: 8739.907227\n",
      "wrong_move\n",
      "   4954/500000: episode: 4899, duration: 0.176s, episode steps:   2, steps per second:  11, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7746526.000000, mae: 2621.901855, mean_q: 8760.484375\n",
      "wrong_move\n",
      "   4955/500000: episode: 4900, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 666.000 [666.000, 666.000],  loss: 1058276.500000, mae: 2612.318848, mean_q: 8502.214844\n",
      "wrong_move\n",
      "   4956/500000: episode: 4901, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2007.000 [2007.000, 2007.000],  loss: 8483714.000000, mae: 2616.744385, mean_q: 8918.667969\n",
      "wrong_move\n",
      "   4957/500000: episode: 4902, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 914.000 [914.000, 914.000],  loss: 15014838.000000, mae: 2598.972168, mean_q: 8394.355469\n",
      "wrong_move\n",
      "   4958/500000: episode: 4903, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3843.000 [3843.000, 3843.000],  loss: 689503.000000, mae: 2595.009521, mean_q: 8505.246094\n",
      "wrong_move\n",
      "   4960/500000: episode: 4904, duration: 0.108s, episode steps:   2, steps per second:  18, episode reward: -4981.000, mean reward: -2490.500 [-5000.000, 19.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3586341.000000, mae: 2610.198730, mean_q: 8520.111328\n",
      "wrong_move\n",
      "   4961/500000: episode: 4905, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1141475.500000, mae: 2593.481445, mean_q: 8272.366211\n",
      "wrong_move\n",
      "   4962/500000: episode: 4906, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2007.000 [2007.000, 2007.000],  loss: 982079.375000, mae: 2575.012939, mean_q: 7696.077637\n",
      "wrong_move\n",
      "   4963/500000: episode: 4907, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2204.000 [2204.000, 2204.000],  loss: 5951420.500000, mae: 2575.827148, mean_q: 8046.152344\n",
      "wrong_move\n",
      "   4964/500000: episode: 4908, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1881.000 [1881.000, 1881.000],  loss: 1168101.375000, mae: 2576.000000, mean_q: 8248.434570\n",
      "wrong_move\n",
      "   4965/500000: episode: 4909, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 970.000 [970.000, 970.000],  loss: 5035624.000000, mae: 2560.331055, mean_q: 8091.817871\n",
      "wrong_move\n",
      "   4966/500000: episode: 4910, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4044.000 [4044.000, 4044.000],  loss: 60266412.000000, mae: 2562.402832, mean_q: 7512.317871\n",
      "wrong_move\n",
      "   4967/500000: episode: 4911, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5444382.000000, mae: 2596.270996, mean_q: 7734.935547\n",
      "wrong_move\n",
      "   4968/500000: episode: 4912, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3869145.250000, mae: 2556.003174, mean_q: 7158.900391\n",
      "wrong_move\n",
      "   4969/500000: episode: 4913, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2723658.500000, mae: 2545.591797, mean_q: 7122.338867\n",
      "wrong_move\n",
      "   4970/500000: episode: 4914, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1089612.750000, mae: 2529.482666, mean_q: 7539.762207\n",
      "wrong_move\n",
      "   4971/500000: episode: 4915, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 940725.375000, mae: 2527.195068, mean_q: 7236.016602\n",
      "wrong_move\n",
      "   4972/500000: episode: 4916, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 45185008.000000, mae: 2538.591797, mean_q: 6887.492188\n",
      "wrong_move\n",
      "   4973/500000: episode: 4917, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9184882.000000, mae: 2530.381348, mean_q: 6778.866211\n",
      "wrong_move\n",
      "   4974/500000: episode: 4918, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2660.000 [2660.000, 2660.000],  loss: 640304.437500, mae: 2521.544922, mean_q: 6882.333496\n",
      "wrong_move\n",
      "   4975/500000: episode: 4919, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4298132.500000, mae: 2519.407715, mean_q: 6727.327637\n",
      "wrong_move\n",
      "   4976/500000: episode: 4920, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1565064.000000, mae: 2513.503906, mean_q: 6499.255859\n",
      "wrong_move\n",
      "   4977/500000: episode: 4921, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4391481.000000, mae: 2509.307861, mean_q: 6538.030762\n",
      "wrong_move\n",
      "   4978/500000: episode: 4922, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2396.000 [2396.000, 2396.000],  loss: 5526529.000000, mae: 2507.764160, mean_q: 6489.683105\n",
      "wrong_move\n",
      "   4979/500000: episode: 4923, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1918183.375000, mae: 2507.301758, mean_q: 6610.184570\n",
      "wrong_move\n",
      "   4980/500000: episode: 4924, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 477266.843750, mae: 2510.274414, mean_q: 7026.326660\n",
      "wrong_move\n",
      "   4981/500000: episode: 4925, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 914.000 [914.000, 914.000],  loss: 1590001.625000, mae: 2534.605957, mean_q: 6971.638672\n",
      "wrong_move\n",
      "   4982/500000: episode: 4926, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 366.000 [366.000, 366.000],  loss: 1035689.375000, mae: 2512.115479, mean_q: 6039.694336\n",
      "wrong_move\n",
      "   4983/500000: episode: 4927, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 13680935.000000, mae: 2516.035400, mean_q: 6096.780273\n",
      "wrong_move\n",
      "   4984/500000: episode: 4928, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1291924.750000, mae: 2514.258789, mean_q: 5643.275879\n",
      "wrong_move\n",
      "   4985/500000: episode: 4929, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 793444.437500, mae: 2513.014893, mean_q: 6343.380859\n",
      "wrong_move\n",
      "   4986/500000: episode: 4930, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2378.000 [2378.000, 2378.000],  loss: 9799823.000000, mae: 2510.933105, mean_q: 5790.686523\n",
      "wrong_move\n",
      "   4987/500000: episode: 4931, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 989319.312500, mae: 2516.720215, mean_q: 5474.098633\n",
      "wrong_move\n",
      "   4988/500000: episode: 4932, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 12629633.000000, mae: 2516.308105, mean_q: 6086.813965\n",
      "wrong_move\n",
      "   4989/500000: episode: 4933, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2768047.000000, mae: 2514.920654, mean_q: 5813.390137\n",
      "wrong_move\n",
      "   4990/500000: episode: 4934, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2114.000 [2114.000, 2114.000],  loss: 2181565.750000, mae: 2507.101074, mean_q: 5216.578125\n",
      "wrong_move\n",
      "   4991/500000: episode: 4935, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 13562896.000000, mae: 2507.576660, mean_q: 4753.542480\n",
      "wrong_move\n",
      "   4992/500000: episode: 4936, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2356684.000000, mae: 2508.768799, mean_q: 5097.465820\n",
      "wrong_move\n",
      "   4993/500000: episode: 4937, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2775.000 [2775.000, 2775.000],  loss: 645100.750000, mae: 2512.715576, mean_q: 5397.557617\n",
      "wrong_move\n",
      "   4994/500000: episode: 4938, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1428.000 [1428.000, 1428.000],  loss: 1387412.000000, mae: 2514.863281, mean_q: 4887.040039\n",
      "wrong_move\n",
      "   4995/500000: episode: 4939, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 587.000 [587.000, 587.000],  loss: 16764116.000000, mae: 2518.295898, mean_q: 4520.530273\n",
      "wrong_move\n",
      "   4996/500000: episode: 4940, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3023.000 [3023.000, 3023.000],  loss: 193561472.000000, mae: 2530.202393, mean_q: 5642.654297\n",
      "wrong_move\n",
      "   4997/500000: episode: 4941, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7183673.000000, mae: 2531.840332, mean_q: 5091.111328\n",
      "wrong_move\n",
      "   4998/500000: episode: 4942, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1311.000 [1311.000, 1311.000],  loss: 966844.000000, mae: 2537.319824, mean_q: 4545.086426\n",
      "wrong_move\n",
      "   4999/500000: episode: 4943, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2281283.750000, mae: 2506.692383, mean_q: 4201.762695\n",
      "wrong_move\n",
      "   5000/500000: episode: 4944, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2208098.000000, mae: 2506.399414, mean_q: 4673.629395\n",
      "wrong_move\n",
      "   5001/500000: episode: 4945, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2161.000 [2161.000, 2161.000],  loss: 2666519.500000, mae: 2554.004150, mean_q: 5230.371094\n",
      "wrong_move\n",
      "   5002/500000: episode: 4946, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1168.000 [1168.000, 1168.000],  loss: 1987599.500000, mae: 2507.039551, mean_q: 3790.286621\n",
      "wrong_move\n",
      "   5003/500000: episode: 4947, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1984.000 [1984.000, 1984.000],  loss: 1332625.750000, mae: 2594.910400, mean_q: 6527.479492\n",
      "wrong_move\n",
      "   5004/500000: episode: 4948, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 105.000 [105.000, 105.000],  loss: 1469547.500000, mae: 2509.317627, mean_q: 3583.168457\n",
      "wrong_move\n",
      "   5005/500000: episode: 4949, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2052.000 [2052.000, 2052.000],  loss: 6081761.000000, mae: 2553.455078, mean_q: 4862.026855\n",
      "wrong_move\n",
      "   5006/500000: episode: 4950, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3480.000 [3480.000, 3480.000],  loss: 742349.687500, mae: 2545.332520, mean_q: 5502.270996\n",
      "wrong_move\n",
      "   5007/500000: episode: 4951, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2007.000 [2007.000, 2007.000],  loss: 626291.500000, mae: 2524.486084, mean_q: 4740.323242\n",
      "wrong_move\n",
      "   5008/500000: episode: 4952, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1747.000 [1747.000, 1747.000],  loss: 1438322.500000, mae: 2566.649658, mean_q: 3916.262207\n",
      "wrong_move\n",
      "   5009/500000: episode: 4953, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1515.000 [1515.000, 1515.000],  loss: 2439804.750000, mae: 2590.192139, mean_q: 5398.953613\n",
      "wrong_move\n",
      "   5010/500000: episode: 4954, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5849267.500000, mae: 2622.197754, mean_q: 5521.365723\n",
      "wrong_move\n",
      "   5011/500000: episode: 4955, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3802400.000000, mae: 2695.923828, mean_q: 6105.074219\n",
      "wrong_move\n",
      "   5012/500000: episode: 4956, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3639.000 [3639.000, 3639.000],  loss: 2243594.500000, mae: 2694.392090, mean_q: 6403.696289\n",
      "wrong_move\n",
      "   5013/500000: episode: 4957, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2587204.750000, mae: 2686.068848, mean_q: 6439.028320\n",
      "wrong_move\n",
      "   5014/500000: episode: 4958, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 486867.718750, mae: 2697.274414, mean_q: 6817.277344\n",
      "wrong_move\n",
      "   5015/500000: episode: 4959, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3650202.250000, mae: 2735.714844, mean_q: 7731.756836\n",
      "wrong_move\n",
      "   5016/500000: episode: 4960, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1925.000 [1925.000, 1925.000],  loss: 18110830.000000, mae: 2709.885986, mean_q: 6859.241211\n",
      "wrong_move\n",
      "   5017/500000: episode: 4961, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2374109.250000, mae: 2710.943848, mean_q: 7679.858398\n",
      "wrong_move\n",
      "   5018/500000: episode: 4962, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 419049.500000, mae: 2711.387207, mean_q: 7341.399902\n",
      "wrong_move\n",
      "   5019/500000: episode: 4963, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3595.000 [3595.000, 3595.000],  loss: 874013.687500, mae: 2765.957764, mean_q: 8564.776367\n",
      "wrong_move\n",
      "   5020/500000: episode: 4964, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1237914.750000, mae: 2705.129150, mean_q: 7400.810059\n",
      "wrong_move\n",
      "   5021/500000: episode: 4965, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3713.000 [3713.000, 3713.000],  loss: 1322360.250000, mae: 2772.626221, mean_q: 8361.040039\n",
      "wrong_move\n",
      "   5022/500000: episode: 4966, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2544.000 [2544.000, 2544.000],  loss: 3949144.000000, mae: 2808.228027, mean_q: 8047.149902\n",
      "wrong_move\n",
      "   5023/500000: episode: 4967, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1515.000 [1515.000, 1515.000],  loss: 1846952.000000, mae: 2690.258301, mean_q: 7551.657227\n",
      "wrong_move\n",
      "   5024/500000: episode: 4968, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 15349876.000000, mae: 2712.079102, mean_q: 7507.862793\n",
      "wrong_move\n",
      "   5025/500000: episode: 4969, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1981.000 [1981.000, 1981.000],  loss: 2937473.000000, mae: 2679.400879, mean_q: 7348.924316\n",
      "wrong_move\n",
      "   5026/500000: episode: 4970, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1500.000 [1500.000, 1500.000],  loss: 3146612.000000, mae: 2695.593018, mean_q: 7169.259277\n",
      "wrong_move\n",
      "   5028/500000: episode: 4971, duration: 0.132s, episode steps:   2, steps per second:  15, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2311634.000000, mae: 2671.343018, mean_q: 7939.268555\n",
      "wrong_move\n",
      "   5029/500000: episode: 4972, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1732.000 [1732.000, 1732.000],  loss: 2238083.500000, mae: 2659.199219, mean_q: 7602.961914\n",
      "wrong_move\n",
      "   5030/500000: episode: 4973, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4523518.500000, mae: 2654.075684, mean_q: 7142.039062\n",
      "wrong_move\n",
      "   5031/500000: episode: 4974, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8490194.000000, mae: 2652.414062, mean_q: 7771.311523\n",
      "wrong_move\n",
      "   5034/500000: episode: 4975, duration: 0.182s, episode steps:   3, steps per second:  17, episode reward: -4982.000, mean reward: -1660.667 [-5000.000,  9.000], mean action: 2938.333 [2722.000, 3371.000],  loss: 4308659.000000, mae: 2638.338867, mean_q: 7221.235840\n",
      "wrong_move\n",
      "   5035/500000: episode: 4976, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3088097.500000, mae: 2832.942383, mean_q: 7856.308105\n",
      "wrong_move\n",
      "   5036/500000: episode: 4977, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 14379762.000000, mae: 2624.452637, mean_q: 7056.565430\n",
      "wrong_move\n",
      "   5037/500000: episode: 4978, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 30429188.000000, mae: 2629.434570, mean_q: 6389.425781\n",
      "wrong_move\n",
      "   5038/500000: episode: 4979, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 728929.000000, mae: 2605.025635, mean_q: 6597.261719\n",
      "wrong_move\n",
      "   5039/500000: episode: 4980, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11289575.000000, mae: 2607.243896, mean_q: 6948.270508\n",
      "wrong_move\n",
      "   5040/500000: episode: 4981, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 933726.437500, mae: 2598.657227, mean_q: 6309.399414\n",
      "wrong_move\n",
      "   5041/500000: episode: 4982, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4346193.000000, mae: 2584.314941, mean_q: 6715.608398\n",
      "wrong_move\n",
      "   5042/500000: episode: 4983, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2916442.750000, mae: 2646.569824, mean_q: 7943.682617\n",
      "wrong_move\n",
      "   5043/500000: episode: 4984, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1190.000 [1190.000, 1190.000],  loss: 3099285.500000, mae: 2574.093506, mean_q: 6091.510254\n",
      "wrong_move\n",
      "   5044/500000: episode: 4985, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1040763.250000, mae: 2671.377441, mean_q: 7265.859863\n",
      "wrong_move\n",
      "   5045/500000: episode: 4986, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1629.000 [1629.000, 1629.000],  loss: 2154751.000000, mae: 2574.628418, mean_q: 6896.951172\n",
      "wrong_move\n",
      "   5046/500000: episode: 4987, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3271.000 [3271.000, 3271.000],  loss: 2029287.000000, mae: 2568.098633, mean_q: 5884.284180\n",
      "wrong_move\n",
      "   5047/500000: episode: 4988, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3743.000 [3743.000, 3743.000],  loss: 3159066.000000, mae: 2590.272461, mean_q: 7439.341797\n",
      "wrong_move\n",
      "   5048/500000: episode: 4989, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2706623.750000, mae: 2625.433105, mean_q: 7041.326172\n",
      "wrong_move\n",
      "   5049/500000: episode: 4990, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1250.000 [1250.000, 1250.000],  loss: 2324027.500000, mae: 2564.156738, mean_q: 5910.031738\n",
      "wrong_move\n",
      "   5050/500000: episode: 4991, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1544.000 [1544.000, 1544.000],  loss: 1809890.625000, mae: 2563.500977, mean_q: 6125.293945\n",
      "wrong_move\n",
      "   5051/500000: episode: 4992, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 179.000 [179.000, 179.000],  loss: 488034.125000, mae: 2623.192139, mean_q: 5604.716797\n",
      "wrong_move\n",
      "   5052/500000: episode: 4993, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5837472.000000, mae: 2565.301758, mean_q: 6134.671875\n",
      "wrong_move\n",
      "   5053/500000: episode: 4994, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 745.000 [745.000, 745.000],  loss: 2300729.500000, mae: 2560.391357, mean_q: 5817.505859\n",
      "wrong_move\n",
      "   5054/500000: episode: 4995, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 392.000 [392.000, 392.000],  loss: 443053.312500, mae: 2560.417480, mean_q: 5836.125488\n",
      "wrong_move\n",
      "   5055/500000: episode: 4996, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2191971.000000, mae: 2564.989258, mean_q: 6795.328613\n",
      "wrong_move\n",
      "   5056/500000: episode: 4997, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3066.000 [3066.000, 3066.000],  loss: 7177621.500000, mae: 2568.642578, mean_q: 5896.434082\n",
      "wrong_move\n",
      "   5058/500000: episode: 4998, duration: 0.083s, episode steps:   2, steps per second:  24, episode reward: -4951.000, mean reward: -2475.500 [-5000.000, 49.000], mean action: 3347.500 [2722.000, 3973.000],  loss: 1603582.500000, mae: 2578.170898, mean_q: 6489.232422\n",
      "wrong_move\n",
      "   5059/500000: episode: 4999, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3788.000 [3788.000, 3788.000],  loss: 708713.062500, mae: 2567.151855, mean_q: 7047.659180\n",
      "wrong_move\n",
      "   5060/500000: episode: 5000, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2099616.500000, mae: 2609.137207, mean_q: 6322.306152\n",
      "wrong_move\n",
      "   5061/500000: episode: 5001, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2010498.125000, mae: 2566.039062, mean_q: 6140.405762\n",
      "wrong_move\n",
      "   5062/500000: episode: 5002, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 32375326.000000, mae: 2568.228516, mean_q: 6180.349609\n",
      "wrong_move\n",
      "   5063/500000: episode: 5003, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1791885.375000, mae: 2582.005859, mean_q: 5578.958984\n",
      "wrong_move\n",
      "   5064/500000: episode: 5004, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1586.000 [1586.000, 1586.000],  loss: 4788460.000000, mae: 2568.654785, mean_q: 5754.438477\n",
      "wrong_move\n",
      "   5065/500000: episode: 5005, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3564.000 [3564.000, 3564.000],  loss: 783390.500000, mae: 2559.884277, mean_q: 5792.539551\n",
      "wrong_move\n",
      "   5066/500000: episode: 5006, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1629.000 [1629.000, 1629.000],  loss: 2688098.750000, mae: 2582.917480, mean_q: 7066.168945\n",
      "wrong_move\n",
      "   5067/500000: episode: 5007, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 195.000 [195.000, 195.000],  loss: 6194006.000000, mae: 2587.356201, mean_q: 5243.261230\n",
      "wrong_move\n",
      "   5068/500000: episode: 5008, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 797570.500000, mae: 2572.352051, mean_q: 6173.902344\n",
      "wrong_move\n",
      "   5069/500000: episode: 5009, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3706931.000000, mae: 2569.222656, mean_q: 5320.319824\n",
      "wrong_move\n",
      "   5070/500000: episode: 5010, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1514594.625000, mae: 2624.541016, mean_q: 6774.090332\n",
      "wrong_move\n",
      "   5071/500000: episode: 5011, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2154097.750000, mae: 2554.465820, mean_q: 4760.444336\n",
      "wrong_move\n",
      "   5072/500000: episode: 5012, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1921.000 [1921.000, 1921.000],  loss: 6971069.000000, mae: 2531.867676, mean_q: 4905.681152\n",
      "wrong_move\n",
      "   5073/500000: episode: 5013, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2444.000 [2444.000, 2444.000],  loss: 3796869.250000, mae: 2517.493408, mean_q: 4909.149414\n",
      "wrong_move\n",
      "   5074/500000: episode: 5014, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2842.000 [2842.000, 2842.000],  loss: 1161465.750000, mae: 2518.206055, mean_q: 5916.031738\n",
      "wrong_move\n",
      "   5075/500000: episode: 5015, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 278.000 [278.000, 278.000],  loss: 966463.375000, mae: 2690.926270, mean_q: 7830.006836\n",
      "wrong_move\n",
      "   5076/500000: episode: 5016, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1643.000 [1643.000, 1643.000],  loss: 3773964.750000, mae: 2539.897461, mean_q: 3778.547852\n",
      "wrong_move\n",
      "   5077/500000: episode: 5017, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1587855.500000, mae: 2512.479980, mean_q: 4199.668457\n",
      "wrong_move\n",
      "   5078/500000: episode: 5018, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1931377.000000, mae: 2642.476562, mean_q: 7128.545898\n",
      "wrong_move\n",
      "   5079/500000: episode: 5019, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1000.000 [1000.000, 1000.000],  loss: 1531657.125000, mae: 2548.252441, mean_q: 5402.874023\n",
      "wrong_move\n",
      "   5080/500000: episode: 5020, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3096.000 [3096.000, 3096.000],  loss: 4855501.500000, mae: 2534.621582, mean_q: 4539.119141\n",
      "wrong_move\n",
      "   5081/500000: episode: 5021, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1740505.375000, mae: 2552.388672, mean_q: 5732.362793\n",
      "wrong_move\n",
      "   5082/500000: episode: 5022, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 20820544.000000, mae: 2563.742676, mean_q: 4070.398926\n",
      "wrong_move\n",
      "   5083/500000: episode: 5023, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3166.000 [3166.000, 3166.000],  loss: 2298163.250000, mae: 2614.002441, mean_q: 6914.879395\n",
      "wrong_move\n",
      "   5084/500000: episode: 5024, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1631.000 [1631.000, 1631.000],  loss: 2828894.500000, mae: 2526.675781, mean_q: 3528.084961\n",
      "wrong_move\n",
      "   5085/500000: episode: 5025, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1643.000 [1643.000, 1643.000],  loss: 561622.875000, mae: 2677.579102, mean_q: 6110.755859\n",
      "wrong_move\n",
      "   5086/500000: episode: 5026, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1643.000 [1643.000, 1643.000],  loss: 73006952.000000, mae: 2535.847656, mean_q: 4602.612305\n",
      "wrong_move\n",
      "   5087/500000: episode: 5027, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2369.000 [2369.000, 2369.000],  loss: 11155887.000000, mae: 2565.956055, mean_q: 5360.431641\n",
      "wrong_move\n",
      "   5088/500000: episode: 5028, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 968.000 [968.000, 968.000],  loss: 7970398.500000, mae: 2560.475098, mean_q: 7294.545410\n",
      "wrong_move\n",
      "   5089/500000: episode: 5029, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1556.000 [1556.000, 1556.000],  loss: 3580408.000000, mae: 2557.157715, mean_q: 5267.647461\n",
      "wrong_move\n",
      "   5090/500000: episode: 5030, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3311.000 [3311.000, 3311.000],  loss: 5718024.500000, mae: 2612.196777, mean_q: 6721.760742\n",
      "wrong_move\n",
      "   5091/500000: episode: 5031, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1556.000 [1556.000, 1556.000],  loss: 3199921.750000, mae: 2665.299316, mean_q: 4959.317383\n",
      "wrong_move\n",
      "   5092/500000: episode: 5032, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1556.000 [1556.000, 1556.000],  loss: 4756651.500000, mae: 2590.032715, mean_q: 6318.421875\n",
      "wrong_move\n",
      "   5093/500000: episode: 5033, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1556.000 [1556.000, 1556.000],  loss: 1518773.250000, mae: 2608.687500, mean_q: 5142.242676\n",
      "wrong_move\n",
      "   5094/500000: episode: 5034, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1978.000 [1978.000, 1978.000],  loss: 2454533.750000, mae: 2584.597656, mean_q: 5474.342773\n",
      "wrong_move\n",
      "   5095/500000: episode: 5035, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2826.000 [2826.000, 2826.000],  loss: 2021090.250000, mae: 2559.978027, mean_q: 4555.290039\n",
      "wrong_move\n",
      "   5096/500000: episode: 5036, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 736.000 [736.000, 736.000],  loss: 1769413.625000, mae: 2560.434082, mean_q: 5688.127930\n",
      "wrong_move\n",
      "   5097/500000: episode: 5037, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1556.000 [1556.000, 1556.000],  loss: 1445141.000000, mae: 2571.498047, mean_q: 6033.940430\n",
      "wrong_move\n",
      "   5098/500000: episode: 5038, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1513.000 [1513.000, 1513.000],  loss: 637136.500000, mae: 2562.326416, mean_q: 4083.165039\n",
      "wrong_move\n",
      "   5099/500000: episode: 5039, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1556.000 [1556.000, 1556.000],  loss: 1476151.500000, mae: 2566.413818, mean_q: 5299.240234\n",
      "wrong_move\n",
      "   5100/500000: episode: 5040, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 456.000 [456.000, 456.000],  loss: 2056710.500000, mae: 2566.931152, mean_q: 5713.555664\n",
      "wrong_move\n",
      "   5101/500000: episode: 5041, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1556.000 [1556.000, 1556.000],  loss: 2527067.000000, mae: 2574.528076, mean_q: 5152.199219\n",
      "wrong_move\n",
      "   5102/500000: episode: 5042, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1556.000 [1556.000, 1556.000],  loss: 18781932.000000, mae: 2570.617920, mean_q: 4742.625488\n",
      "wrong_move\n",
      "   5103/500000: episode: 5043, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1541.000 [1541.000, 1541.000],  loss: 2663445.500000, mae: 2565.697754, mean_q: 3944.223877\n",
      "wrong_move\n",
      "   5104/500000: episode: 5044, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 736.000 [736.000, 736.000],  loss: 500557.968750, mae: 2565.473145, mean_q: 3996.757812\n",
      "wrong_move\n",
      "   5105/500000: episode: 5045, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 10086904.000000, mae: 2566.769531, mean_q: 5312.867188\n",
      "wrong_move\n",
      "   5106/500000: episode: 5046, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1125.000 [1125.000, 1125.000],  loss: 335058.062500, mae: 2647.347412, mean_q: 4403.231445\n",
      "wrong_move\n",
      "   5107/500000: episode: 5047, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1395.000 [1395.000, 1395.000],  loss: 1266765.250000, mae: 2578.460449, mean_q: 5996.120117\n",
      "wrong_move\n",
      "   5108/500000: episode: 5048, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 105.000 [105.000, 105.000],  loss: 18287302.000000, mae: 2569.029053, mean_q: 4481.691406\n",
      "wrong_move\n",
      "   5110/500000: episode: 5049, duration: 0.087s, episode steps:   2, steps per second:  23, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2231.000 [1091.000, 3371.000],  loss: 2784350.750000, mae: 2576.761719, mean_q: 4991.540527\n",
      "wrong_move\n",
      "   5111/500000: episode: 5050, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 2145185.000000, mae: 2571.777344, mean_q: 4507.548828\n",
      "wrong_move\n",
      "   5112/500000: episode: 5051, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2471.000 [2471.000, 2471.000],  loss: 11372478.000000, mae: 2596.917969, mean_q: 4540.173828\n",
      "wrong_move\n",
      "   5113/500000: episode: 5052, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 16170080.000000, mae: 2633.458008, mean_q: 5003.847656\n",
      "wrong_move\n",
      "   5114/500000: episode: 5053, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1499.000 [1499.000, 1499.000],  loss: 869227.812500, mae: 2577.433594, mean_q: 5813.288086\n",
      "wrong_move\n",
      "   5115/500000: episode: 5054, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 969.000 [969.000, 969.000],  loss: 3162874.500000, mae: 2555.125488, mean_q: 4145.390625\n",
      "wrong_move\n",
      "   5116/500000: episode: 5055, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3906.000 [3906.000, 3906.000],  loss: 16537692.000000, mae: 2600.421631, mean_q: 4526.041016\n",
      "wrong_move\n",
      "   5117/500000: episode: 5056, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 2370124.500000, mae: 2552.281250, mean_q: 5411.399902\n",
      "wrong_move\n",
      "   5118/500000: episode: 5057, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1321.000 [1321.000, 1321.000],  loss: 138272736.000000, mae: 2550.026611, mean_q: 6796.648438\n",
      "wrong_move\n",
      "   5119/500000: episode: 5058, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 807509.625000, mae: 2541.025635, mean_q: 4084.761963\n",
      "wrong_move\n",
      "   5120/500000: episode: 5059, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 3376319.500000, mae: 2539.352539, mean_q: 5644.346191\n",
      "wrong_move\n",
      "   5121/500000: episode: 5060, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 36208768.000000, mae: 2535.812256, mean_q: 5873.628906\n",
      "wrong_move\n",
      "   5122/500000: episode: 5061, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 667233.687500, mae: 2541.445312, mean_q: 4250.519531\n",
      "   5123/500000: episode: 5062, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: 909.000, mean reward: 909.000 [909.000, 909.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 945867.000000, mae: 2538.223633, mean_q: 4402.472168\n",
      "wrong_move\n",
      "   5124/500000: episode: 5063, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2900.000 [2900.000, 2900.000],  loss: 2582131.750000, mae: 2533.869629, mean_q: 3763.688477\n",
      "wrong_move\n",
      "   5125/500000: episode: 5064, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 584397.875000, mae: 2534.554688, mean_q: 5325.073730\n",
      "wrong_move\n",
      "   5126/500000: episode: 5065, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 1219600.375000, mae: 2561.759521, mean_q: 8888.502930\n",
      "wrong_move\n",
      "   5127/500000: episode: 5066, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 1435636.000000, mae: 2556.364258, mean_q: 7736.442871\n",
      "wrong_move\n",
      "   5128/500000: episode: 5067, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 1529964.000000, mae: 2540.912842, mean_q: 5499.453125\n",
      "wrong_move\n",
      "   5130/500000: episode: 5068, duration: 0.079s, episode steps:   2, steps per second:  25, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 3529.500 [3371.000, 3688.000],  loss: 5658390.500000, mae: 2551.270508, mean_q: 5738.775879\n",
      "wrong_move\n",
      "   5131/500000: episode: 5069, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 1446060.250000, mae: 2672.753906, mean_q: 9087.006836\n",
      "wrong_move\n",
      "   5132/500000: episode: 5070, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3715.000 [3715.000, 3715.000],  loss: 841373.437500, mae: 2536.212891, mean_q: 5504.711914\n",
      "wrong_move\n",
      "   5133/500000: episode: 5071, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 913013.250000, mae: 2537.761475, mean_q: 4772.806641\n",
      "wrong_move\n",
      "   5134/500000: episode: 5072, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 673772.500000, mae: 2537.255371, mean_q: 5307.642090\n",
      "wrong_move\n",
      "   5135/500000: episode: 5073, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1330.000 [1330.000, 1330.000],  loss: 270617.843750, mae: 2553.267090, mean_q: 5472.039062\n",
      "wrong_move\n",
      "   5136/500000: episode: 5074, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3426.000 [3426.000, 3426.000],  loss: 12056979.000000, mae: 2598.004150, mean_q: 5200.298340\n",
      "wrong_move\n",
      "   5137/500000: episode: 5075, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 1074806.375000, mae: 2553.293945, mean_q: 7775.388672\n",
      "wrong_move\n",
      "   5138/500000: episode: 5076, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 764.000 [764.000, 764.000],  loss: 109632496.000000, mae: 2545.149902, mean_q: 6957.878906\n",
      "wrong_move\n",
      "   5139/500000: episode: 5077, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 236073.937500, mae: 2611.362549, mean_q: 8645.303711\n",
      "wrong_move\n",
      "   5140/500000: episode: 5078, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 3141168.750000, mae: 2543.903564, mean_q: 4896.412109\n",
      "wrong_move\n",
      "   5141/500000: episode: 5079, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 1280299.500000, mae: 2547.554199, mean_q: 6588.741211\n",
      "wrong_move\n",
      "   5142/500000: episode: 5080, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2173.000 [2173.000, 2173.000],  loss: 1893650.000000, mae: 2603.447021, mean_q: 9867.059570\n",
      "wrong_move\n",
      "   5143/500000: episode: 5081, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 1628829.250000, mae: 2623.750488, mean_q: 8537.258789\n",
      "wrong_move\n",
      "   5144/500000: episode: 5082, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1006.000 [1006.000, 1006.000],  loss: 11159509.000000, mae: 2558.797119, mean_q: 5522.206055\n",
      "wrong_move\n",
      "   5145/500000: episode: 5083, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3155.000 [3155.000, 3155.000],  loss: 1753982.500000, mae: 2595.339355, mean_q: 6022.547363\n",
      "wrong_move\n",
      "   5146/500000: episode: 5084, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1069.000 [1069.000, 1069.000],  loss: 705050.500000, mae: 2597.448242, mean_q: 8017.511230\n",
      "wrong_move\n",
      "   5147/500000: episode: 5085, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 1982822.875000, mae: 2554.440186, mean_q: 4887.462402\n",
      "wrong_move\n",
      "   5148/500000: episode: 5086, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 2917866.750000, mae: 2556.306885, mean_q: 4308.202148\n",
      "wrong_move\n",
      "   5149/500000: episode: 5087, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 14015253.000000, mae: 2805.287842, mean_q: 10233.193359\n",
      "wrong_move\n",
      "   5150/500000: episode: 5088, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 978241.750000, mae: 2558.841309, mean_q: 4477.859863\n",
      "wrong_move\n",
      "   5151/500000: episode: 5089, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1028538.500000, mae: 2562.725830, mean_q: 5128.424805\n",
      "wrong_move\n",
      "   5152/500000: episode: 5090, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2546.000 [2546.000, 2546.000],  loss: 2558764.250000, mae: 2652.246094, mean_q: 7444.464355\n",
      "wrong_move\n",
      "   5153/500000: episode: 5091, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 914.000 [914.000, 914.000],  loss: 1439553.875000, mae: 2564.855469, mean_q: 4231.081055\n",
      "wrong_move\n",
      "   5155/500000: episode: 5092, duration: 0.070s, episode steps:   2, steps per second:  29, episode reward: -4091.000, mean reward: -2045.500 [-5000.000, 909.000], mean action: 2377.500 [1384.000, 3371.000],  loss: 2805209.250000, mae: 2579.236328, mean_q: 6855.119141\n",
      "wrong_move\n",
      "   5156/500000: episode: 5093, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 949792.000000, mae: 2573.040039, mean_q: 4667.140137\n",
      "wrong_move\n",
      "   5157/500000: episode: 5094, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 20942996.000000, mae: 2625.708252, mean_q: 8054.381836\n",
      "wrong_move\n",
      "   5158/500000: episode: 5095, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 4846153.000000, mae: 2588.078125, mean_q: 4678.527832\n",
      "wrong_move\n",
      "   5159/500000: episode: 5096, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 358882.343750, mae: 2578.809570, mean_q: 4828.501953\n",
      "wrong_move\n",
      "   5160/500000: episode: 5097, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 419259.812500, mae: 2569.624023, mean_q: 5806.269043\n",
      "wrong_move\n",
      "   5161/500000: episode: 5098, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 1372391.250000, mae: 2705.667725, mean_q: 7785.742676\n",
      "wrong_move\n",
      "   5162/500000: episode: 5099, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 2462069.000000, mae: 2577.286133, mean_q: 4824.126953\n",
      "wrong_move\n",
      "   5163/500000: episode: 5100, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 974091.312500, mae: 2604.112793, mean_q: 6773.328613\n",
      "wrong_move\n",
      "   5164/500000: episode: 5101, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 1477978.625000, mae: 2570.565918, mean_q: 4628.473633\n",
      "wrong_move\n",
      "   5165/500000: episode: 5102, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1346747.750000, mae: 2582.473633, mean_q: 5774.884277\n",
      "wrong_move\n",
      "   5166/500000: episode: 5103, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 11673428.000000, mae: 2658.501221, mean_q: 5204.316406\n",
      "wrong_move\n",
      "   5167/500000: episode: 5104, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 2498985.750000, mae: 2571.086670, mean_q: 4513.967773\n",
      "wrong_move\n",
      "   5168/500000: episode: 5105, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 1624471.750000, mae: 2570.140381, mean_q: 4085.688965\n",
      "wrong_move\n",
      "   5169/500000: episode: 5106, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1399.000 [1399.000, 1399.000],  loss: 910036.375000, mae: 2568.227539, mean_q: 3504.831299\n",
      "wrong_move\n",
      "   5170/500000: episode: 5107, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 1510872.375000, mae: 2583.578369, mean_q: 4402.187988\n",
      "wrong_move\n",
      "   5171/500000: episode: 5108, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 2018573.250000, mae: 2573.730469, mean_q: 3643.478027\n",
      "wrong_move\n",
      "   5172/500000: episode: 5109, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2003405.375000, mae: 2568.912109, mean_q: 4121.599121\n",
      "wrong_move\n",
      "   5174/500000: episode: 5110, duration: 0.080s, episode steps:   2, steps per second:  25, episode reward: -4951.000, mean reward: -2475.500 [-5000.000, 49.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 3729551.000000, mae: 2580.919189, mean_q: 2777.002930\n",
      "wrong_move\n",
      "   5175/500000: episode: 5111, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 995583.000000, mae: 2599.938232, mean_q: 3970.289551\n",
      "wrong_move\n",
      "   5176/500000: episode: 5112, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 4564545.500000, mae: 2575.483643, mean_q: 2786.920410\n",
      "wrong_move\n",
      "   5177/500000: episode: 5113, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 960249.250000, mae: 2579.111328, mean_q: 2933.545898\n",
      "wrong_move\n",
      "   5178/500000: episode: 5114, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 2472374.250000, mae: 2575.119629, mean_q: 3015.819824\n",
      "wrong_move\n",
      "   5179/500000: episode: 5115, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1308.000 [1308.000, 1308.000],  loss: 1117105.250000, mae: 2583.389160, mean_q: 3335.174805\n",
      "wrong_move\n",
      "   5180/500000: episode: 5116, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 1834251.375000, mae: 2595.019775, mean_q: 3928.408936\n",
      "wrong_move\n",
      "   5181/500000: episode: 5117, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 1791401.875000, mae: 2579.752930, mean_q: 3074.324707\n",
      "wrong_move\n",
      "   5182/500000: episode: 5118, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 1321829.125000, mae: 2580.786133, mean_q: 2793.633789\n",
      "wrong_move\n",
      "   5183/500000: episode: 5119, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1270.000 [1270.000, 1270.000],  loss: 2334022.500000, mae: 2679.079102, mean_q: 6768.298828\n",
      "wrong_move\n",
      "   5185/500000: episode: 5120, duration: 0.128s, episode steps:   2, steps per second:  16, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 3630.500 [3371.000, 3890.000],  loss: 1298369.500000, mae: 2604.496582, mean_q: 3769.159180\n",
      "wrong_move\n",
      "   5186/500000: episode: 5121, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 3280125.000000, mae: 2722.944824, mean_q: 9188.871094\n",
      "wrong_move\n",
      "   5187/500000: episode: 5122, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 2320659.000000, mae: 2596.984131, mean_q: 3912.922363\n",
      "wrong_move\n",
      "   5188/500000: episode: 5123, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2433.000 [2433.000, 2433.000],  loss: 827321.437500, mae: 2594.582520, mean_q: 1888.586670\n",
      "wrong_move\n",
      "   5189/500000: episode: 5124, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2709.000 [2709.000, 2709.000],  loss: 524625.187500, mae: 2596.355469, mean_q: 2867.981689\n",
      "wrong_move\n",
      "   5190/500000: episode: 5125, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 632.000 [632.000, 632.000],  loss: 1035910.937500, mae: 2705.247314, mean_q: 5655.265137\n",
      "wrong_move\n",
      "   5191/500000: episode: 5126, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1318.000 [1318.000, 1318.000],  loss: 844079.375000, mae: 2596.943359, mean_q: 2221.651855\n",
      "wrong_move\n",
      "   5192/500000: episode: 5127, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 632.000 [632.000, 632.000],  loss: 1983661.500000, mae: 2595.680176, mean_q: 1672.460815\n",
      "wrong_move\n",
      "   5193/500000: episode: 5128, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 632.000 [632.000, 632.000],  loss: 443963.250000, mae: 2620.061035, mean_q: 4464.928711\n",
      "wrong_move\n",
      "   5194/500000: episode: 5129, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 632.000 [632.000, 632.000],  loss: 22969548.000000, mae: 2597.193359, mean_q: 2628.238770\n",
      "wrong_move\n",
      "   5195/500000: episode: 5130, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4750692.000000, mae: 2596.164307, mean_q: 2066.076172\n",
      "wrong_move\n",
      "   5196/500000: episode: 5131, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 632.000 [632.000, 632.000],  loss: 604238.437500, mae: 2596.679688, mean_q: 2669.602051\n",
      "wrong_move\n",
      "   5197/500000: episode: 5132, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 632.000 [632.000, 632.000],  loss: 4229334.000000, mae: 2595.479492, mean_q: 2363.368652\n",
      "wrong_move\n",
      "   5198/500000: episode: 5133, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 712.000 [712.000, 712.000],  loss: 2037843.250000, mae: 2608.536133, mean_q: 3458.984863\n",
      "wrong_move\n",
      "   5199/500000: episode: 5134, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2334.000 [2334.000, 2334.000],  loss: 2858085.750000, mae: 2607.228516, mean_q: 1819.277954\n",
      "wrong_move\n",
      "   5200/500000: episode: 5135, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2456.000 [2456.000, 2456.000],  loss: 281661.750000, mae: 2592.401855, mean_q: 1961.322998\n",
      "wrong_move\n",
      "   5201/500000: episode: 5136, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 804661.125000, mae: 2743.207275, mean_q: 4829.172852\n",
      "wrong_move\n",
      "   5202/500000: episode: 5137, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1057.000 [1057.000, 1057.000],  loss: 2570344.000000, mae: 2608.176758, mean_q: 2273.871582\n",
      "wrong_move\n",
      "   5203/500000: episode: 5138, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1591.000 [1591.000, 1591.000],  loss: 4123124.000000, mae: 2784.860840, mean_q: 5499.995605\n",
      "wrong_move\n",
      "   5204/500000: episode: 5139, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 699.000 [699.000, 699.000],  loss: 874794.812500, mae: 2596.804199, mean_q: 2124.314453\n",
      "wrong_move\n",
      "   5205/500000: episode: 5140, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1057.000 [1057.000, 1057.000],  loss: 37000264.000000, mae: 2599.701172, mean_q: 2439.417236\n",
      "wrong_move\n",
      "   5206/500000: episode: 5141, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1318.000 [1318.000, 1318.000],  loss: 197893040.000000, mae: 2645.912598, mean_q: 6618.412109\n",
      "wrong_move\n",
      "   5207/500000: episode: 5142, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 926234.750000, mae: 2608.155029, mean_q: 1884.445801\n",
      "wrong_move\n",
      "   5208/500000: episode: 5143, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3039.000 [3039.000, 3039.000],  loss: 1741361.375000, mae: 2595.304199, mean_q: 1979.508301\n",
      "wrong_move\n",
      "   5209/500000: episode: 5144, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3893.000 [3893.000, 3893.000],  loss: 2911625.250000, mae: 2594.984375, mean_q: 1885.727295\n",
      "wrong_move\n",
      "   5210/500000: episode: 5145, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1057.000 [1057.000, 1057.000],  loss: 500705.937500, mae: 2595.474609, mean_q: 1467.695557\n",
      "wrong_move\n",
      "   5211/500000: episode: 5146, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2668.000 [2668.000, 2668.000],  loss: 981371.312500, mae: 2604.499756, mean_q: 1685.230103\n",
      "wrong_move\n",
      "   5212/500000: episode: 5147, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2789.000 [2789.000, 2789.000],  loss: 300235680.000000, mae: 2694.987549, mean_q: 4817.230957\n",
      "wrong_move\n",
      "   5213/500000: episode: 5148, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2951.000 [2951.000, 2951.000],  loss: 1538167.000000, mae: 2602.186035, mean_q: 2218.885986\n",
      "wrong_move\n",
      "   5214/500000: episode: 5149, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1308.000 [1308.000, 1308.000],  loss: 865634.187500, mae: 2600.545898, mean_q: 2931.843750\n",
      "wrong_move\n",
      "   5215/500000: episode: 5150, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2456.000 [2456.000, 2456.000],  loss: 1033558.562500, mae: 2619.319580, mean_q: 3433.150635\n",
      "wrong_move\n",
      "   5216/500000: episode: 5151, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 386.000 [386.000, 386.000],  loss: 3170891.500000, mae: 2610.597168, mean_q: 2755.754639\n",
      "wrong_move\n",
      "   5217/500000: episode: 5152, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3681.000 [3681.000, 3681.000],  loss: 938336.375000, mae: 2611.115234, mean_q: 1577.635132\n",
      "wrong_move\n",
      "   5218/500000: episode: 5153, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 405.000 [405.000, 405.000],  loss: 2137299.750000, mae: 2703.204590, mean_q: 4857.160156\n",
      "wrong_move\n",
      "   5219/500000: episode: 5154, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 905.000 [905.000, 905.000],  loss: 948922.937500, mae: 2618.560791, mean_q: 1598.099365\n",
      "wrong_move\n",
      "   5220/500000: episode: 5155, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 678.000 [678.000, 678.000],  loss: 1049061.500000, mae: 2641.357666, mean_q: 2408.499268\n",
      "wrong_move\n",
      "   5221/500000: episode: 5156, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3039.000 [3039.000, 3039.000],  loss: 1520167.250000, mae: 2631.956543, mean_q: 3132.568604\n",
      "wrong_move\n",
      "   5222/500000: episode: 5157, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4047.000 [4047.000, 4047.000],  loss: 1182502.500000, mae: 2622.695068, mean_q: 1873.041504\n",
      "wrong_move\n",
      "   5223/500000: episode: 5158, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2837.000 [2837.000, 2837.000],  loss: 7855714.000000, mae: 2741.915283, mean_q: 3180.011230\n",
      "wrong_move\n",
      "   5224/500000: episode: 5159, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 905.000 [905.000, 905.000],  loss: 543910.500000, mae: 2687.653809, mean_q: 5106.971680\n",
      "wrong_move\n",
      "   5225/500000: episode: 5160, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 905.000 [905.000, 905.000],  loss: 1047057.375000, mae: 2627.943359, mean_q: 2723.785400\n",
      "wrong_move\n",
      "   5226/500000: episode: 5161, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1979.000 [1979.000, 1979.000],  loss: 1063138.750000, mae: 2627.839844, mean_q: 2025.485596\n",
      "wrong_move\n",
      "   5227/500000: episode: 5162, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2370.000 [2370.000, 2370.000],  loss: 12531246.000000, mae: 2631.590088, mean_q: 2286.796875\n",
      "wrong_move\n",
      "   5229/500000: episode: 5163, duration: 0.124s, episode steps:   2, steps per second:  16, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 3572.500 [3252.000, 3893.000],  loss: 1249632.500000, mae: 2637.783691, mean_q: 2090.514160\n",
      "wrong_move\n",
      "   5230/500000: episode: 5164, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2236.000 [2236.000, 2236.000],  loss: 809791.750000, mae: 2887.937744, mean_q: 4268.971191\n",
      "wrong_move\n",
      "   5231/500000: episode: 5165, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 768.000 [768.000, 768.000],  loss: 2412522.500000, mae: 2641.036377, mean_q: 1738.423828\n",
      "wrong_move\n",
      "   5232/500000: episode: 5166, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2180.000 [2180.000, 2180.000],  loss: 437723.156250, mae: 2633.374756, mean_q: 977.010132\n",
      "wrong_move\n",
      "   5233/500000: episode: 5167, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1979.000 [1979.000, 1979.000],  loss: 4159960.750000, mae: 2650.496338, mean_q: 3882.180908\n",
      "wrong_move\n",
      "   5234/500000: episode: 5168, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 1250510.250000, mae: 2638.367920, mean_q: 2234.021973\n",
      "wrong_move\n",
      "   5235/500000: episode: 5169, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1215.000 [1215.000, 1215.000],  loss: 3480398.750000, mae: 2644.822510, mean_q: 1536.403931\n",
      "wrong_move\n",
      "   5236/500000: episode: 5170, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 1851328.750000, mae: 2639.000000, mean_q: 1914.581787\n",
      "wrong_move\n",
      "   5237/500000: episode: 5171, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3039.000 [3039.000, 3039.000],  loss: 531699.250000, mae: 2652.470947, mean_q: 3220.844482\n",
      "wrong_move\n",
      "   5238/500000: episode: 5172, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3039.000 [3039.000, 3039.000],  loss: 1805275.750000, mae: 2745.367676, mean_q: 2699.206787\n",
      "wrong_move\n",
      "   5239/500000: episode: 5173, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 959.000 [959.000, 959.000],  loss: 1589882.000000, mae: 2652.748047, mean_q: 1820.569702\n",
      "wrong_move\n",
      "   5240/500000: episode: 5174, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 696.000 [696.000, 696.000],  loss: 616961.625000, mae: 2645.446777, mean_q: 2269.596924\n",
      "wrong_move\n",
      "   5241/500000: episode: 5175, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 531979.250000, mae: 2654.250244, mean_q: 3389.735840\n",
      "wrong_move\n",
      "   5242/500000: episode: 5176, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3039.000 [3039.000, 3039.000],  loss: 942507.000000, mae: 2646.880127, mean_q: 2289.643311\n",
      "wrong_move\n",
      "   5243/500000: episode: 5177, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3039.000 [3039.000, 3039.000],  loss: 1070056.000000, mae: 2654.930176, mean_q: 2971.941895\n",
      "wrong_move\n",
      "   5244/500000: episode: 5178, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3039.000 [3039.000, 3039.000],  loss: 873652.625000, mae: 2795.922607, mean_q: 3767.627930\n",
      "wrong_move\n",
      "   5245/500000: episode: 5179, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 15399841.000000, mae: 2683.307129, mean_q: 1934.340820\n",
      "wrong_move\n",
      "   5246/500000: episode: 5180, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1186.000 [1186.000, 1186.000],  loss: 3895564.500000, mae: 2645.506592, mean_q: 1391.374756\n",
      "wrong_move\n",
      "   5247/500000: episode: 5181, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1853062.000000, mae: 2645.417236, mean_q: 1091.828003\n",
      "wrong_move\n",
      "   5248/500000: episode: 5182, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1828.000 [1828.000, 1828.000],  loss: 2616929.250000, mae: 2654.804443, mean_q: 2344.407227\n",
      "wrong_move\n",
      "   5249/500000: episode: 5183, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3039.000 [3039.000, 3039.000],  loss: 277212.375000, mae: 2642.823486, mean_q: 1301.112061\n",
      "wrong_move\n",
      "   5250/500000: episode: 5184, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3039.000 [3039.000, 3039.000],  loss: 887869.500000, mae: 2644.218018, mean_q: 1894.285645\n",
      "wrong_move\n",
      "   5251/500000: episode: 5185, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 3933121.000000, mae: 2641.658203, mean_q: 1505.402588\n",
      "wrong_move\n",
      "   5252/500000: episode: 5186, duration: 0.147s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1673.000 [1673.000, 1673.000],  loss: 1980291.250000, mae: 2640.992188, mean_q: 1568.012085\n",
      "wrong_move\n",
      "   5253/500000: episode: 5187, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2680.000 [2680.000, 2680.000],  loss: 936733.625000, mae: 2664.648682, mean_q: 3184.097168\n",
      "wrong_move\n",
      "   5254/500000: episode: 5188, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3039.000 [3039.000, 3039.000],  loss: 3335610.750000, mae: 2666.793457, mean_q: 3439.294189\n",
      "wrong_move\n",
      "   5255/500000: episode: 5189, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3039.000 [3039.000, 3039.000],  loss: 919075.125000, mae: 2661.881348, mean_q: 1344.367676\n",
      "wrong_move\n",
      "   5256/500000: episode: 5190, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1620.000 [1620.000, 1620.000],  loss: 1159160.250000, mae: 2700.229980, mean_q: 5036.258301\n",
      "wrong_move\n",
      "   5257/500000: episode: 5191, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3039.000 [3039.000, 3039.000],  loss: 923182.375000, mae: 2642.077637, mean_q: 1890.291138\n",
      "wrong_move\n",
      "   5258/500000: episode: 5192, duration: 0.149s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2138.000 [2138.000, 2138.000],  loss: 3590779.750000, mae: 2651.293945, mean_q: 1514.846558\n",
      "wrong_move\n",
      "   5259/500000: episode: 5193, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 516124.812500, mae: 2638.624268, mean_q: 1009.439026\n",
      "wrong_move\n",
      "   5260/500000: episode: 5194, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3543.000 [3543.000, 3543.000],  loss: 477929.375000, mae: 2638.373047, mean_q: 931.369629\n",
      "wrong_move\n",
      "   5261/500000: episode: 5195, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 148.000 [148.000, 148.000],  loss: 1582022.500000, mae: 2712.877930, mean_q: 3350.915771\n",
      "wrong_move\n",
      "   5262/500000: episode: 5196, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 4328623.000000, mae: 2752.894043, mean_q: 3707.335449\n",
      "wrong_move\n",
      "   5263/500000: episode: 5197, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3062.000 [3062.000, 3062.000],  loss: 74907544.000000, mae: 2729.105469, mean_q: 2418.885254\n",
      "wrong_move\n",
      "   5264/500000: episode: 5198, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3846.000 [3846.000, 3846.000],  loss: 487501.125000, mae: 2738.325928, mean_q: 4176.978516\n",
      "wrong_move\n",
      "   5265/500000: episode: 5199, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2789.000 [2789.000, 2789.000],  loss: 576399.375000, mae: 2715.483154, mean_q: 2655.608154\n",
      "wrong_move\n",
      "   5266/500000: episode: 5200, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3062.000 [3062.000, 3062.000],  loss: 2093710.625000, mae: 2714.167480, mean_q: 1850.341797\n",
      "wrong_move\n",
      "   5267/500000: episode: 5201, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1921.000 [1921.000, 1921.000],  loss: 1773858.250000, mae: 2733.505371, mean_q: 5086.654297\n",
      "wrong_move\n",
      "   5268/500000: episode: 5202, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3062.000 [3062.000, 3062.000],  loss: 2793092.250000, mae: 2729.985596, mean_q: 3090.850098\n",
      "wrong_move\n",
      "   5269/500000: episode: 5203, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3062.000 [3062.000, 3062.000],  loss: 558937.687500, mae: 2714.213867, mean_q: 2622.622314\n",
      "wrong_move\n",
      "   5270/500000: episode: 5204, duration: 0.143s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1351.000 [1351.000, 1351.000],  loss: 609777.437500, mae: 2802.718262, mean_q: 4969.944336\n",
      "wrong_move\n",
      "   5271/500000: episode: 5205, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2435.000 [2435.000, 2435.000],  loss: 2672041.000000, mae: 2715.156250, mean_q: 2656.170410\n",
      "wrong_move\n",
      "   5272/500000: episode: 5206, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3062.000 [3062.000, 3062.000],  loss: 601129.250000, mae: 2774.883301, mean_q: 6555.409180\n",
      "wrong_move\n",
      "   5273/500000: episode: 5207, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1779.000 [1779.000, 1779.000],  loss: 952416.875000, mae: 2713.270264, mean_q: 2452.091309\n",
      "wrong_move\n",
      "   5274/500000: episode: 5208, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3793675.000000, mae: 2721.713379, mean_q: 5576.264160\n",
      "wrong_move\n",
      "   5275/500000: episode: 5209, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3062.000 [3062.000, 3062.000],  loss: 955984.500000, mae: 2713.362061, mean_q: 2596.968262\n",
      "wrong_move\n",
      "   5276/500000: episode: 5210, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3062.000 [3062.000, 3062.000],  loss: 2109271.500000, mae: 2708.608154, mean_q: 3394.059326\n",
      "wrong_move\n",
      "   5277/500000: episode: 5211, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3062.000 [3062.000, 3062.000],  loss: 1023368.750000, mae: 2795.535156, mean_q: 6353.002930\n",
      "wrong_move\n",
      "   5278/500000: episode: 5212, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1397.000 [1397.000, 1397.000],  loss: 1359652.000000, mae: 2709.559814, mean_q: 4258.878906\n",
      "wrong_move\n",
      "   5279/500000: episode: 5213, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2691.000 [2691.000, 2691.000],  loss: 1375420.125000, mae: 2707.130859, mean_q: 3459.805664\n",
      "wrong_move\n",
      "   5280/500000: episode: 5214, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2458.000 [2458.000, 2458.000],  loss: 5671320.000000, mae: 2709.104492, mean_q: 4374.540039\n",
      "wrong_move\n",
      "   5281/500000: episode: 5215, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3062.000 [3062.000, 3062.000],  loss: 835067.000000, mae: 2875.764160, mean_q: 6344.917969\n",
      "wrong_move\n",
      "   5282/500000: episode: 5216, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3062.000 [3062.000, 3062.000],  loss: 1325005.375000, mae: 2758.286133, mean_q: 6037.261719\n",
      "wrong_move\n",
      "   5283/500000: episode: 5217, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 448882.562500, mae: 2691.917969, mean_q: 3024.741211\n",
      "wrong_move\n",
      "   5284/500000: episode: 5218, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1725.000 [1725.000, 1725.000],  loss: 4729366.500000, mae: 2689.686523, mean_q: 2706.926025\n",
      "wrong_move\n",
      "   5285/500000: episode: 5219, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1308.000 [1308.000, 1308.000],  loss: 7923424.000000, mae: 2684.265137, mean_q: 3180.587402\n",
      "wrong_move\n",
      "   5286/500000: episode: 5220, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3452.000 [3452.000, 3452.000],  loss: 3243436.750000, mae: 2709.615723, mean_q: 2479.902588\n",
      "wrong_move\n",
      "   5287/500000: episode: 5221, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1020.000 [1020.000, 1020.000],  loss: 1386762.875000, mae: 2717.525879, mean_q: 3071.019043\n",
      "wrong_move\n",
      "   5288/500000: episode: 5222, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3062.000 [3062.000, 3062.000],  loss: 984111.687500, mae: 2682.613770, mean_q: 5032.151367\n",
      "wrong_move\n",
      "   5289/500000: episode: 5223, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3062.000 [3062.000, 3062.000],  loss: 1206250.750000, mae: 2717.826660, mean_q: 5446.008301\n",
      "wrong_move\n",
      "   5290/500000: episode: 5224, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2973.000 [2973.000, 2973.000],  loss: 1705173.000000, mae: 2666.551025, mean_q: 3396.474854\n",
      "wrong_move\n",
      "   5291/500000: episode: 5225, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3062.000 [3062.000, 3062.000],  loss: 7268896.500000, mae: 2681.578125, mean_q: 2910.640137\n",
      "wrong_move\n",
      "   5292/500000: episode: 5226, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2460.000 [2460.000, 2460.000],  loss: 17979632.000000, mae: 2656.364258, mean_q: 2571.062988\n",
      "wrong_move\n",
      "   5293/500000: episode: 5227, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2681.000 [2681.000, 2681.000],  loss: 6468852.000000, mae: 2665.381348, mean_q: 2534.010742\n",
      "wrong_move\n",
      "   5294/500000: episode: 5228, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3062.000 [3062.000, 3062.000],  loss: 2466750.000000, mae: 2647.230957, mean_q: 2338.664795\n",
      "wrong_move\n",
      "   5295/500000: episode: 5229, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 851.000 [851.000, 851.000],  loss: 992945.562500, mae: 2649.946533, mean_q: 3307.377197\n",
      "wrong_move\n",
      "   5296/500000: episode: 5230, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3717.000 [3717.000, 3717.000],  loss: 900962.812500, mae: 2639.762451, mean_q: 2806.213135\n",
      "wrong_move\n",
      "   5297/500000: episode: 5231, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3062.000 [3062.000, 3062.000],  loss: 2174822.000000, mae: 2633.625244, mean_q: 2178.905518\n",
      "wrong_move\n",
      "   5299/500000: episode: 5232, duration: 0.073s, episode steps:   2, steps per second:  28, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2547318.250000, mae: 2641.436279, mean_q: 3002.163818\n",
      "wrong_move\n",
      "   5300/500000: episode: 5233, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3686.000 [3686.000, 3686.000],  loss: 1678999.500000, mae: 2625.506836, mean_q: 2263.015625\n",
      "wrong_move\n",
      "   5301/500000: episode: 5234, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3133.000 [3133.000, 3133.000],  loss: 463773.031250, mae: 2680.013672, mean_q: 2252.576172\n",
      "wrong_move\n",
      "   5302/500000: episode: 5235, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2458.000 [2458.000, 2458.000],  loss: 1322978.625000, mae: 2640.286133, mean_q: 3194.386719\n",
      "wrong_move\n",
      "   5303/500000: episode: 5236, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2393869.250000, mae: 2622.486328, mean_q: 2627.247314\n",
      "wrong_move\n",
      "   5304/500000: episode: 5237, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 879358.125000, mae: 2622.825684, mean_q: 1937.058594\n",
      "wrong_move\n",
      "   5305/500000: episode: 5238, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2789.000 [2789.000, 2789.000],  loss: 631777.250000, mae: 2628.969971, mean_q: 3104.224121\n",
      "wrong_move\n",
      "   5306/500000: episode: 5239, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2489.000 [2489.000, 2489.000],  loss: 1292387.875000, mae: 2634.987061, mean_q: 3389.428223\n",
      "wrong_move\n",
      "   5307/500000: episode: 5240, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 512098.343750, mae: 2633.114746, mean_q: 2574.562500\n",
      "wrong_move\n",
      "   5308/500000: episode: 5241, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 865.000 [865.000, 865.000],  loss: 5602263.000000, mae: 2695.837891, mean_q: 2245.524902\n",
      "wrong_move\n",
      "   5309/500000: episode: 5242, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3700.000 [3700.000, 3700.000],  loss: 773707.312500, mae: 2635.532471, mean_q: 1833.250244\n",
      "wrong_move\n",
      "   5310/500000: episode: 5243, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3062.000 [3062.000, 3062.000],  loss: 1478858.500000, mae: 2792.938477, mean_q: 7052.136230\n",
      "wrong_move\n",
      "   5311/500000: episode: 5244, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2789.000 [2789.000, 2789.000],  loss: 351689.343750, mae: 2646.732178, mean_q: 2837.741943\n",
      "wrong_move\n",
      "   5312/500000: episode: 5245, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 683.000 [683.000, 683.000],  loss: 2499905.500000, mae: 2645.255859, mean_q: 2988.565918\n",
      "wrong_move\n",
      "   5313/500000: episode: 5246, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1341.000 [1341.000, 1341.000],  loss: 1114412.250000, mae: 2754.007324, mean_q: 5684.136719\n",
      "wrong_move\n",
      "   5314/500000: episode: 5247, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3062.000 [3062.000, 3062.000],  loss: 8170677.500000, mae: 2652.468262, mean_q: 1979.954590\n",
      "wrong_move\n",
      "   5315/500000: episode: 5248, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 851.000 [851.000, 851.000],  loss: 577426.125000, mae: 2679.685547, mean_q: 2048.994629\n",
      "wrong_move\n",
      "   5316/500000: episode: 5249, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2170.000 [2170.000, 2170.000],  loss: 1200108.625000, mae: 2656.208252, mean_q: 2703.303711\n",
      "wrong_move\n",
      "   5317/500000: episode: 5250, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 129.000 [129.000, 129.000],  loss: 1116799.000000, mae: 2656.375977, mean_q: 2798.993652\n",
      "wrong_move\n",
      "   5318/500000: episode: 5251, duration: 0.145s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 259108.156250, mae: 2716.884766, mean_q: 4312.041992\n",
      "wrong_move\n",
      "   5319/500000: episode: 5252, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2216.000 [2216.000, 2216.000],  loss: 23602018.000000, mae: 2655.925781, mean_q: 2419.116211\n",
      "wrong_move\n",
      "   5320/500000: episode: 5253, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2105.000 [2105.000, 2105.000],  loss: 1077311.375000, mae: 2655.114990, mean_q: 2587.032715\n",
      "wrong_move\n",
      "   5322/500000: episode: 5254, duration: 0.110s, episode steps:   2, steps per second:  18, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 129.000 [129.000, 129.000],  loss: 745436.500000, mae: 2651.691650, mean_q: 1837.706665\n",
      "wrong_move\n",
      "   5323/500000: episode: 5255, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1921.000 [1921.000, 1921.000],  loss: 386255.406250, mae: 2656.584473, mean_q: 1991.079102\n",
      "wrong_move\n",
      "   5324/500000: episode: 5256, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 1317314.250000, mae: 2650.666504, mean_q: 2324.447021\n",
      "wrong_move\n",
      "   5325/500000: episode: 5257, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2552.000 [2552.000, 2552.000],  loss: 649629.250000, mae: 2653.398438, mean_q: 3075.270996\n",
      "wrong_move\n",
      "   5326/500000: episode: 5258, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 877020.500000, mae: 2652.962158, mean_q: 2340.142334\n",
      "wrong_move\n",
      "   5327/500000: episode: 5259, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2873.000 [2873.000, 2873.000],  loss: 6535762.500000, mae: 2657.604248, mean_q: 2331.471680\n",
      "wrong_move\n",
      "   5328/500000: episode: 5260, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 828337.250000, mae: 2659.159180, mean_q: 2746.399170\n",
      "wrong_move\n",
      "   5329/500000: episode: 5261, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2751.000 [2751.000, 2751.000],  loss: 796041.625000, mae: 2694.781494, mean_q: 3856.425537\n",
      "wrong_move\n",
      "   5330/500000: episode: 5262, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 452.000 [452.000, 452.000],  loss: 6628123.500000, mae: 2659.630859, mean_q: 2715.499268\n",
      "wrong_move\n",
      "   5331/500000: episode: 5263, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 625277.187500, mae: 2679.603271, mean_q: 3236.840576\n",
      "wrong_move\n",
      "   5332/500000: episode: 5264, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3934.000 [3934.000, 3934.000],  loss: 2153273.500000, mae: 2667.996826, mean_q: 2283.389648\n",
      "wrong_move\n",
      "   5333/500000: episode: 5265, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3651.000 [3651.000, 3651.000],  loss: 3059584.000000, mae: 2655.629150, mean_q: 2573.899902\n",
      "wrong_move\n",
      "   5334/500000: episode: 5266, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 783076.625000, mae: 2673.402100, mean_q: 2916.814697\n",
      "wrong_move\n",
      "   5335/500000: episode: 5267, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3750.000 [3750.000, 3750.000],  loss: 1076451.000000, mae: 2680.692627, mean_q: 2707.987305\n",
      "wrong_move\n",
      "   5336/500000: episode: 5268, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1271.000 [1271.000, 1271.000],  loss: 2988687.000000, mae: 2653.505615, mean_q: 2086.423828\n",
      "wrong_move\n",
      "   5337/500000: episode: 5269, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 91.000 [91.000, 91.000],  loss: 3236555.500000, mae: 2668.608887, mean_q: 1476.962646\n",
      "wrong_move\n",
      "   5338/500000: episode: 5270, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 770689.125000, mae: 2650.923828, mean_q: 1778.650513\n",
      "wrong_move\n",
      "   5339/500000: episode: 5271, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 377.000 [377.000, 377.000],  loss: 93774200.000000, mae: 2663.251953, mean_q: 3094.767822\n",
      "wrong_move\n",
      "   5340/500000: episode: 5272, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 479377.250000, mae: 2653.876953, mean_q: 2288.029541\n",
      "wrong_move\n",
      "   5341/500000: episode: 5273, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 1175782.500000, mae: 2819.058838, mean_q: 5407.476074\n",
      "wrong_move\n",
      "   5342/500000: episode: 5274, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 541622.625000, mae: 2670.510254, mean_q: 4099.115234\n",
      "wrong_move\n",
      "   5343/500000: episode: 5275, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3934.000 [3934.000, 3934.000],  loss: 852072.000000, mae: 2814.990967, mean_q: 5591.106445\n",
      "wrong_move\n",
      "   5344/500000: episode: 5276, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 380094.812500, mae: 2661.948730, mean_q: 1981.329468\n",
      "wrong_move\n",
      "   5345/500000: episode: 5277, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1338.000 [1338.000, 1338.000],  loss: 932794.500000, mae: 2717.039062, mean_q: 4267.450195\n",
      "wrong_move\n",
      "   5346/500000: episode: 5278, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 1685401.250000, mae: 2704.841064, mean_q: 4020.340820\n",
      "wrong_move\n",
      "   5347/500000: episode: 5279, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1308.000 [1308.000, 1308.000],  loss: 370600.156250, mae: 2663.764648, mean_q: 2006.110352\n",
      "wrong_move\n",
      "   5348/500000: episode: 5280, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 37008808.000000, mae: 2665.395508, mean_q: 2152.686035\n",
      "wrong_move\n",
      "   5349/500000: episode: 5281, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 322.000 [322.000, 322.000],  loss: 2975910.500000, mae: 2664.686523, mean_q: 2040.178589\n",
      "wrong_move\n",
      "   5350/500000: episode: 5282, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 1030183.312500, mae: 2669.358398, mean_q: 3686.344482\n",
      "wrong_move\n",
      "   5351/500000: episode: 5283, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 823079.875000, mae: 2892.830566, mean_q: 6082.363770\n",
      "wrong_move\n",
      "   5352/500000: episode: 5284, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3651.000 [3651.000, 3651.000],  loss: 1246497.125000, mae: 2681.977539, mean_q: 2693.088867\n",
      "wrong_move\n",
      "   5353/500000: episode: 5285, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3577.000 [3577.000, 3577.000],  loss: 1002794.875000, mae: 2755.969238, mean_q: 5883.496582\n",
      "wrong_move\n",
      "   5354/500000: episode: 5286, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 386.000 [386.000, 386.000],  loss: 1882817.250000, mae: 2662.007812, mean_q: 1612.849609\n",
      "wrong_move\n",
      "   5355/500000: episode: 5287, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1181.000 [1181.000, 1181.000],  loss: 6689448.000000, mae: 2664.962402, mean_q: 2248.488770\n",
      "wrong_move\n",
      "   5356/500000: episode: 5288, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 725082.687500, mae: 2683.022949, mean_q: 1753.304688\n",
      "wrong_move\n",
      "   5357/500000: episode: 5289, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 698.000 [698.000, 698.000],  loss: 674662.062500, mae: 2668.820068, mean_q: 1208.510254\n",
      "wrong_move\n",
      "   5358/500000: episode: 5290, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 698.000 [698.000, 698.000],  loss: 803814.812500, mae: 2673.372070, mean_q: 1458.570068\n",
      "wrong_move\n",
      "   5359/500000: episode: 5291, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2724.000 [2724.000, 2724.000],  loss: 38648352.000000, mae: 2716.399414, mean_q: 2233.691162\n",
      "wrong_move\n",
      "   5360/500000: episode: 5292, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1020.000 [1020.000, 1020.000],  loss: 548083.937500, mae: 2672.294922, mean_q: 1583.361084\n",
      "wrong_move\n",
      "   5361/500000: episode: 5293, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2370.000 [2370.000, 2370.000],  loss: 534535.437500, mae: 2673.331787, mean_q: 1966.785400\n",
      "wrong_move\n",
      "   5362/500000: episode: 5294, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1020.000 [1020.000, 1020.000],  loss: 736791.250000, mae: 2672.047363, mean_q: 1789.170288\n",
      "wrong_move\n",
      "   5363/500000: episode: 5295, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 322.000 [322.000, 322.000],  loss: 287412.062500, mae: 2691.354492, mean_q: 1734.015625\n",
      "wrong_move\n",
      "   5364/500000: episode: 5296, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2355.000 [2355.000, 2355.000],  loss: 1675708.000000, mae: 2676.857422, mean_q: 2768.784668\n",
      "wrong_move\n",
      "   5365/500000: episode: 5297, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 698.000 [698.000, 698.000],  loss: 66269618176.000000, mae: 2870.506348, mean_q: 6365.813477\n",
      "wrong_move\n",
      "   5366/500000: episode: 5298, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3860.000 [3860.000, 3860.000],  loss: 1994553.375000, mae: 2833.108887, mean_q: 6163.524902\n",
      "wrong_move\n",
      "   5367/500000: episode: 5299, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 217.000 [217.000, 217.000],  loss: 13550380.000000, mae: 2807.684570, mean_q: 2185.926758\n",
      "wrong_move\n",
      "   5368/500000: episode: 5300, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 96274976.000000, mae: 2768.796875, mean_q: 3470.177002\n",
      "wrong_move\n",
      "   5369/500000: episode: 5301, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1162636.250000, mae: 2780.549316, mean_q: 3487.514648\n",
      "wrong_move\n",
      "   5370/500000: episode: 5302, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2598.000 [2598.000, 2598.000],  loss: 919801.000000, mae: 2820.841553, mean_q: 4496.285645\n",
      "wrong_move\n",
      "   5371/500000: episode: 5303, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5600758.000000, mae: 2816.121582, mean_q: 3833.974609\n",
      "wrong_move\n",
      "   5372/500000: episode: 5304, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 883.000 [883.000, 883.000],  loss: 632784.625000, mae: 2810.250977, mean_q: 4215.013672\n",
      "wrong_move\n",
      "   5373/500000: episode: 5305, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3978.000 [3978.000, 3978.000],  loss: 41020000.000000, mae: 2814.235840, mean_q: 4828.683594\n",
      "wrong_move\n",
      "   5374/500000: episode: 5306, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 871.000 [871.000, 871.000],  loss: 263410400.000000, mae: 2878.923828, mean_q: 7065.020508\n",
      "wrong_move\n",
      "   5375/500000: episode: 5307, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1578909.000000, mae: 2838.380859, mean_q: 4546.677246\n",
      "wrong_move\n",
      "   5376/500000: episode: 5308, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4844544.000000, mae: 2828.652344, mean_q: 6431.625977\n",
      "wrong_move\n",
      "   5377/500000: episode: 5309, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1517.000 [1517.000, 1517.000],  loss: 922574.437500, mae: 2797.043701, mean_q: 3950.611328\n",
      "wrong_move\n",
      "   5378/500000: episode: 5310, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 217.000 [217.000, 217.000],  loss: 3862624.750000, mae: 2791.280762, mean_q: 4649.661133\n",
      "wrong_move\n",
      "   5379/500000: episode: 5311, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17147492.000000, mae: 2784.144531, mean_q: 4353.590332\n",
      "wrong_move\n",
      "   5380/500000: episode: 5312, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4793782.500000, mae: 2780.211426, mean_q: 5792.575195\n",
      "wrong_move\n",
      "   5381/500000: episode: 5313, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5447312.000000, mae: 2770.215088, mean_q: 4379.687500\n",
      "wrong_move\n",
      "   5382/500000: episode: 5314, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1217.000 [1217.000, 1217.000],  loss: 4098618.750000, mae: 2764.021973, mean_q: 4452.975586\n",
      "wrong_move\n",
      "   5383/500000: episode: 5315, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1792823.500000, mae: 2764.094238, mean_q: 4829.806152\n",
      "wrong_move\n",
      "   5384/500000: episode: 5316, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 551.000 [551.000, 551.000],  loss: 17203574.000000, mae: 2751.621582, mean_q: 3771.437500\n",
      "wrong_move\n",
      "   5385/500000: episode: 5317, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1784814.000000, mae: 2814.720459, mean_q: 6976.572754\n",
      "wrong_move\n",
      "   5386/500000: episode: 5318, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3595763.500000, mae: 2740.909180, mean_q: 4772.904297\n",
      "wrong_move\n",
      "   5387/500000: episode: 5319, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2934.000 [2934.000, 2934.000],  loss: 9314075.000000, mae: 2735.966797, mean_q: 4096.214844\n",
      "wrong_move\n",
      "   5388/500000: episode: 5320, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 551.000 [551.000, 551.000],  loss: 2209346.500000, mae: 2731.934082, mean_q: 3959.405762\n",
      "wrong_move\n",
      "   5389/500000: episode: 5321, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2418.000 [2418.000, 2418.000],  loss: 418731.812500, mae: 2726.459473, mean_q: 3980.328857\n",
      "wrong_move\n",
      "   5390/500000: episode: 5322, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3656.000 [3656.000, 3656.000],  loss: 717288.625000, mae: 2723.046387, mean_q: 3657.682129\n",
      "wrong_move\n",
      "   5391/500000: episode: 5323, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5523552.000000, mae: 2720.991211, mean_q: 4004.685303\n",
      "wrong_move\n",
      "   5392/500000: episode: 5324, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 697.000 [697.000, 697.000],  loss: 1635666.375000, mae: 2821.304199, mean_q: 6864.466797\n",
      "wrong_move\n",
      "   5393/500000: episode: 5325, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3315.000 [3315.000, 3315.000],  loss: 5268673.000000, mae: 2715.426025, mean_q: 3844.430908\n",
      "wrong_move\n",
      "   5394/500000: episode: 5326, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 697.000 [697.000, 697.000],  loss: 1769155.250000, mae: 2718.125244, mean_q: 5352.674805\n",
      "wrong_move\n",
      "   5395/500000: episode: 5327, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 697.000 [697.000, 697.000],  loss: 7317787.000000, mae: 2707.974121, mean_q: 3907.186035\n",
      "wrong_move\n",
      "   5396/500000: episode: 5328, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 697.000 [697.000, 697.000],  loss: 10530661.000000, mae: 2706.645508, mean_q: 2851.891602\n",
      "wrong_move\n",
      "   5397/500000: episode: 5329, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2793.000 [2793.000, 2793.000],  loss: 145698720.000000, mae: 2705.811035, mean_q: 5284.074219\n",
      "wrong_move\n",
      "   5398/500000: episode: 5330, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 697.000 [697.000, 697.000],  loss: 1817355.250000, mae: 2695.777832, mean_q: 3494.679443\n",
      "wrong_move\n",
      "   5399/500000: episode: 5331, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 118.000 [118.000, 118.000],  loss: 3063910.750000, mae: 2757.879150, mean_q: 3842.936768\n",
      "wrong_move\n",
      "   5400/500000: episode: 5332, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 697.000 [697.000, 697.000],  loss: 965298.250000, mae: 2686.542480, mean_q: 3525.293945\n",
      "wrong_move\n",
      "   5401/500000: episode: 5333, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1891.000 [1891.000, 1891.000],  loss: 1276356.125000, mae: 2682.841309, mean_q: 3063.863281\n",
      "wrong_move\n",
      "   5402/500000: episode: 5334, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 697.000 [697.000, 697.000],  loss: 2154177.000000, mae: 2737.021729, mean_q: 6405.137207\n",
      "wrong_move\n",
      "   5403/500000: episode: 5335, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 697.000 [697.000, 697.000],  loss: 464791.156250, mae: 2683.338867, mean_q: 4669.920898\n",
      "wrong_move\n",
      "   5404/500000: episode: 5336, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 697.000 [697.000, 697.000],  loss: 586384.125000, mae: 2681.466064, mean_q: 2776.123535\n",
      "wrong_move\n",
      "   5405/500000: episode: 5337, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 697.000 [697.000, 697.000],  loss: 1000517.625000, mae: 2681.804199, mean_q: 2473.284668\n",
      "wrong_move\n",
      "   5406/500000: episode: 5338, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1351.000 [1351.000, 1351.000],  loss: 1147143.500000, mae: 2682.198486, mean_q: 2743.243652\n",
      "wrong_move\n",
      "   5407/500000: episode: 5339, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1561.000 [1561.000, 1561.000],  loss: 5887698.000000, mae: 2683.376709, mean_q: 3034.404053\n",
      "wrong_move\n",
      "   5408/500000: episode: 5340, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 127.000 [127.000, 127.000],  loss: 9992678.000000, mae: 2683.677979, mean_q: 3455.607422\n",
      "wrong_move\n",
      "   5409/500000: episode: 5341, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3177.000 [3177.000, 3177.000],  loss: 1412099.500000, mae: 2683.285645, mean_q: 3085.089111\n",
      "wrong_move\n",
      "   5410/500000: episode: 5342, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2598.000 [2598.000, 2598.000],  loss: 1151742.500000, mae: 2746.405273, mean_q: 4598.839355\n",
      "wrong_move\n",
      "   5411/500000: episode: 5343, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 124.000 [124.000, 124.000],  loss: 760212.750000, mae: 2678.914062, mean_q: 2768.529297\n",
      "wrong_move\n",
      "   5412/500000: episode: 5344, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2934.000 [2934.000, 2934.000],  loss: 1354410.000000, mae: 2712.565674, mean_q: 3817.917480\n",
      "wrong_move\n",
      "   5413/500000: episode: 5345, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4032.000 [4032.000, 4032.000],  loss: 798712.062500, mae: 2679.134521, mean_q: 3537.609131\n",
      "wrong_move\n",
      "   5414/500000: episode: 5346, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 886.000 [886.000, 886.000],  loss: 2559679.000000, mae: 2670.116211, mean_q: 2891.290527\n",
      "wrong_move\n",
      "   5415/500000: episode: 5347, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3893.000 [3893.000, 3893.000],  loss: 2269989.750000, mae: 2668.223633, mean_q: 3024.940430\n",
      "wrong_move\n",
      "   5417/500000: episode: 5348, duration: 0.099s, episode steps:   2, steps per second:  20, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2405.000 [917.000, 3893.000],  loss: 5726261.000000, mae: 2672.349609, mean_q: 3490.766113\n",
      "wrong_move\n",
      "   5418/500000: episode: 5349, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 788.000 [788.000, 788.000],  loss: 1162189.375000, mae: 2678.007324, mean_q: 3948.202881\n",
      "wrong_move\n",
      "   5419/500000: episode: 5350, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3893.000 [3893.000, 3893.000],  loss: 980398.125000, mae: 2671.271973, mean_q: 2356.075684\n",
      "wrong_move\n",
      "   5420/500000: episode: 5351, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1467.000 [1467.000, 1467.000],  loss: 1087437.500000, mae: 2676.144531, mean_q: 3270.512451\n",
      "wrong_move\n",
      "   5421/500000: episode: 5352, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3893.000 [3893.000, 3893.000],  loss: 473972.375000, mae: 2681.507080, mean_q: 2998.466064\n",
      "wrong_move\n",
      "   5422/500000: episode: 5353, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1435962.250000, mae: 2691.895020, mean_q: 3817.341553\n",
      "wrong_move\n",
      "   5423/500000: episode: 5354, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 788.000 [788.000, 788.000],  loss: 663536.000000, mae: 2718.561523, mean_q: 3326.464355\n",
      "wrong_move\n",
      "   5424/500000: episode: 5355, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3893.000 [3893.000, 3893.000],  loss: 1330136.375000, mae: 2742.612793, mean_q: 4152.216797\n",
      "wrong_move\n",
      "   5425/500000: episode: 5356, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 604838.000000, mae: 2762.270508, mean_q: 5610.038574\n",
      "wrong_move\n",
      "   5426/500000: episode: 5357, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 772.000 [772.000, 772.000],  loss: 3884295.500000, mae: 2774.128418, mean_q: 5004.741211\n",
      "wrong_move\n",
      "   5427/500000: episode: 5358, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1985204.250000, mae: 2846.444336, mean_q: 6560.011230\n",
      "wrong_move\n",
      "   5428/500000: episode: 5359, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 800205.750000, mae: 2793.699707, mean_q: 4483.004883\n",
      "wrong_move\n",
      "   5429/500000: episode: 5360, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 921232.687500, mae: 2905.939453, mean_q: 6528.094727\n",
      "wrong_move\n",
      "   5430/500000: episode: 5361, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3615.000 [3615.000, 3615.000],  loss: 2693570.000000, mae: 2806.660156, mean_q: 6253.090820\n",
      "wrong_move\n",
      "   5431/500000: episode: 5362, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11323173.000000, mae: 2807.979980, mean_q: 5418.406738\n",
      "wrong_move\n",
      "   5432/500000: episode: 5363, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 268087056.000000, mae: 2840.977051, mean_q: 6561.007324\n",
      "wrong_move\n",
      "   5433/500000: episode: 5364, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1112146.250000, mae: 2801.354492, mean_q: 5150.650879\n",
      "wrong_move\n",
      "   5434/500000: episode: 5365, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 26198464.000000, mae: 2795.059326, mean_q: 5362.724609\n",
      "wrong_move\n",
      "   5435/500000: episode: 5366, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1118.000 [1118.000, 1118.000],  loss: 5360228.000000, mae: 2823.635742, mean_q: 5564.761719\n",
      "wrong_move\n",
      "   5436/500000: episode: 5367, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8668364.000000, mae: 2785.241699, mean_q: 6171.406250\n",
      "wrong_move\n",
      "   5437/500000: episode: 5368, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 671851.750000, mae: 2773.722168, mean_q: 5742.915527\n",
      "wrong_move\n",
      "   5438/500000: episode: 5369, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3871462.500000, mae: 2766.809570, mean_q: 5832.406738\n",
      "wrong_move\n",
      "   5439/500000: episode: 5370, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1893.000 [1893.000, 1893.000],  loss: 611715.625000, mae: 2780.965820, mean_q: 6936.182617\n",
      "wrong_move\n",
      "   5440/500000: episode: 5371, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1849.000 [1849.000, 1849.000],  loss: 2844473.750000, mae: 2755.009277, mean_q: 5527.270020\n",
      "wrong_move\n",
      "   5441/500000: episode: 5372, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2766.000 [2766.000, 2766.000],  loss: 1064746.250000, mae: 2752.669922, mean_q: 6371.035645\n",
      "wrong_move\n",
      "   5442/500000: episode: 5373, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2259558.500000, mae: 2868.849609, mean_q: 7544.504883\n",
      "wrong_move\n",
      "   5443/500000: episode: 5374, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1452521.750000, mae: 2740.316650, mean_q: 6072.327148\n",
      "wrong_move\n",
      "   5444/500000: episode: 5375, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1626.000 [1626.000, 1626.000],  loss: 650109.812500, mae: 2733.392578, mean_q: 5307.583008\n",
      "wrong_move\n",
      "   5445/500000: episode: 5376, duration: 0.065s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1863387.875000, mae: 2731.704102, mean_q: 5358.851562\n",
      "wrong_move\n",
      "   5446/500000: episode: 5377, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 22751402.000000, mae: 2754.406738, mean_q: 5512.738281\n",
      "wrong_move\n",
      "   5447/500000: episode: 5378, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6474502.000000, mae: 2723.519531, mean_q: 5271.272461\n",
      "wrong_move\n",
      "   5448/500000: episode: 5379, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2235683.250000, mae: 2719.321777, mean_q: 4740.135742\n",
      "wrong_move\n",
      "   5449/500000: episode: 5380, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 905094.812500, mae: 2723.547363, mean_q: 6312.706543\n",
      "wrong_move\n",
      "   5450/500000: episode: 5381, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2577258.250000, mae: 2713.417480, mean_q: 5167.711914\n",
      "wrong_move\n",
      "   5451/500000: episode: 5382, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3811.000 [3811.000, 3811.000],  loss: 5185774.500000, mae: 2714.513672, mean_q: 6076.583496\n",
      "wrong_move\n",
      "   5452/500000: episode: 5383, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2490228.500000, mae: 2710.450684, mean_q: 4871.123047\n",
      "wrong_move\n",
      "   5453/500000: episode: 5384, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 20836228.000000, mae: 2712.935059, mean_q: 5706.156250\n",
      "wrong_move\n",
      "   5454/500000: episode: 5385, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2.000 [2.000, 2.000],  loss: 15576544.000000, mae: 2734.469971, mean_q: 5958.453613\n",
      "wrong_move\n",
      "   5455/500000: episode: 5386, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 15997351.000000, mae: 2705.867676, mean_q: 5044.082031\n",
      "wrong_move\n",
      "   5456/500000: episode: 5387, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 975916.250000, mae: 2700.887207, mean_q: 4592.205078\n",
      "wrong_move\n",
      "   5457/500000: episode: 5388, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3423888.000000, mae: 2699.731201, mean_q: 4808.572266\n",
      "wrong_move\n",
      "   5458/500000: episode: 5389, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 399.000 [399.000, 399.000],  loss: 570890.875000, mae: 2698.407715, mean_q: 4905.578125\n",
      "wrong_move\n",
      "   5459/500000: episode: 5390, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 811168.500000, mae: 2705.404297, mean_q: 5122.810547\n",
      "wrong_move\n",
      "   5460/500000: episode: 5391, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1758.000 [1758.000, 1758.000],  loss: 1924423.750000, mae: 2699.325684, mean_q: 4904.093750\n",
      "wrong_move\n",
      "   5461/500000: episode: 5392, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 261.000 [261.000, 261.000],  loss: 13560433.000000, mae: 2697.041016, mean_q: 4600.416504\n",
      "wrong_move\n",
      "   5462/500000: episode: 5393, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 838671.625000, mae: 2696.692383, mean_q: 4173.746094\n",
      "wrong_move\n",
      "   5463/500000: episode: 5394, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3773.000 [3773.000, 3773.000],  loss: 5864064.000000, mae: 2817.453125, mean_q: 7264.169922\n",
      "wrong_move\n",
      "   5464/500000: episode: 5395, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3813.000 [3813.000, 3813.000],  loss: 54807844.000000, mae: 2699.723145, mean_q: 4629.150879\n",
      "wrong_move\n",
      "   5465/500000: episode: 5396, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4842821.500000, mae: 2699.671387, mean_q: 4366.432129\n",
      "wrong_move\n",
      "   5466/500000: episode: 5397, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1832.000 [1832.000, 1832.000],  loss: 929360.750000, mae: 2729.358154, mean_q: 7543.126953\n",
      "wrong_move\n",
      "   5467/500000: episode: 5398, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1379.000 [1379.000, 1379.000],  loss: 5665692.000000, mae: 2692.259766, mean_q: 3996.343262\n",
      "wrong_move\n",
      "   5468/500000: episode: 5399, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1379.000 [1379.000, 1379.000],  loss: 11987741.000000, mae: 2690.562988, mean_q: 4914.658691\n",
      "wrong_move\n",
      "   5469/500000: episode: 5400, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2828.000 [2828.000, 2828.000],  loss: 2959242.500000, mae: 2688.890381, mean_q: 4106.592773\n",
      "wrong_move\n",
      "   5470/500000: episode: 5401, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2186.000 [2186.000, 2186.000],  loss: 2201806.500000, mae: 2685.332520, mean_q: 4348.282715\n",
      "wrong_move\n",
      "   5471/500000: episode: 5402, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3549.000 [3549.000, 3549.000],  loss: 1578061.000000, mae: 2682.544434, mean_q: 4090.356689\n",
      "wrong_move\n",
      "   5472/500000: episode: 5403, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1379.000 [1379.000, 1379.000],  loss: 1741795.875000, mae: 2720.349121, mean_q: 4610.412109\n",
      "wrong_move\n",
      "   5473/500000: episode: 5404, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 446624.718750, mae: 2682.302002, mean_q: 3928.498779\n",
      "wrong_move\n",
      "   5474/500000: episode: 5405, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1379.000 [1379.000, 1379.000],  loss: 1219046.000000, mae: 2682.803467, mean_q: 3473.227783\n",
      "wrong_move\n",
      "   5475/500000: episode: 5406, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1234.000 [1234.000, 1234.000],  loss: 2128407.000000, mae: 2798.089844, mean_q: 7449.051758\n",
      "wrong_move\n",
      "   5476/500000: episode: 5407, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1030.000 [1030.000, 1030.000],  loss: 5687902.000000, mae: 2693.360596, mean_q: 4717.340332\n",
      "wrong_move\n",
      "   5477/500000: episode: 5408, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1379.000 [1379.000, 1379.000],  loss: 812163.500000, mae: 2686.596680, mean_q: 3990.972656\n",
      "wrong_move\n",
      "   5478/500000: episode: 5409, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1379.000 [1379.000, 1379.000],  loss: 1380226.375000, mae: 2692.325684, mean_q: 5228.759766\n",
      "wrong_move\n",
      "   5479/500000: episode: 5410, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1234.000 [1234.000, 1234.000],  loss: 2478267.000000, mae: 2690.855469, mean_q: 4681.071289\n",
      "wrong_move\n",
      "   5480/500000: episode: 5411, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1358.000 [1358.000, 1358.000],  loss: 18288708.000000, mae: 2691.535156, mean_q: 3894.103027\n",
      "wrong_move\n",
      "   5481/500000: episode: 5412, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1997.000 [1997.000, 1997.000],  loss: 79286840.000000, mae: 2691.773926, mean_q: 4489.573242\n",
      "wrong_move\n",
      "   5482/500000: episode: 5413, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1358.000 [1358.000, 1358.000],  loss: 511361.062500, mae: 2689.849365, mean_q: 3458.565430\n",
      "wrong_move\n",
      "   5483/500000: episode: 5414, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3483.000 [3483.000, 3483.000],  loss: 1393785.750000, mae: 2687.172852, mean_q: 2904.829102\n",
      "wrong_move\n",
      "   5484/500000: episode: 5415, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 263.000 [263.000, 263.000],  loss: 5319357.000000, mae: 2737.062500, mean_q: 3553.808594\n",
      "wrong_move\n",
      "   5485/500000: episode: 5416, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 788.000 [788.000, 788.000],  loss: 5886652.500000, mae: 2686.715088, mean_q: 4336.152344\n",
      "wrong_move\n",
      "   5486/500000: episode: 5417, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1358.000 [1358.000, 1358.000],  loss: 2366543.000000, mae: 2683.339111, mean_q: 4350.110352\n",
      "wrong_move\n",
      "   5487/500000: episode: 5418, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1358.000 [1358.000, 1358.000],  loss: 614038.250000, mae: 2674.406250, mean_q: 3869.834473\n",
      "wrong_move\n",
      "   5488/500000: episode: 5419, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1358.000 [1358.000, 1358.000],  loss: 1316070.500000, mae: 2789.310547, mean_q: 6733.583008\n",
      "wrong_move\n",
      "   5489/500000: episode: 5420, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1769.000 [1769.000, 1769.000],  loss: 911478.937500, mae: 2673.653320, mean_q: 3389.642822\n",
      "wrong_move\n",
      "   5490/500000: episode: 5421, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1358.000 [1358.000, 1358.000],  loss: 1645529.500000, mae: 2688.251953, mean_q: 4854.752930\n",
      "wrong_move\n",
      "   5491/500000: episode: 5422, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 263.000 [263.000, 263.000],  loss: 41526124.000000, mae: 2675.952148, mean_q: 3232.389160\n",
      "wrong_move\n",
      "   5492/500000: episode: 5423, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 788.000 [788.000, 788.000],  loss: 1933066.875000, mae: 2663.903564, mean_q: 3611.263916\n",
      "wrong_move\n",
      "   5493/500000: episode: 5424, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4553474.000000, mae: 2662.520020, mean_q: 3391.542969\n",
      "wrong_move\n",
      "   5494/500000: episode: 5425, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4027.000 [4027.000, 4027.000],  loss: 5952723.500000, mae: 2657.145996, mean_q: 2805.612549\n",
      "wrong_move\n",
      "   5495/500000: episode: 5426, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1358.000 [1358.000, 1358.000],  loss: 1478964.250000, mae: 2832.163086, mean_q: 2896.348633\n",
      "wrong_move\n",
      "   5496/500000: episode: 5427, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1358.000 [1358.000, 1358.000],  loss: 5844313.000000, mae: 2720.727051, mean_q: 3635.530273\n",
      "wrong_move\n",
      "   5497/500000: episode: 5428, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1358.000 [1358.000, 1358.000],  loss: 2302922.000000, mae: 2654.164551, mean_q: 2567.173340\n",
      "wrong_move\n",
      "   5498/500000: episode: 5429, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2978.000 [2978.000, 2978.000],  loss: 775773.062500, mae: 2668.271484, mean_q: 3219.642578\n",
      "wrong_move\n",
      "   5499/500000: episode: 5430, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 1122480.625000, mae: 2725.131592, mean_q: 4472.262207\n",
      "wrong_move\n",
      "   5500/500000: episode: 5431, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1011.000 [1011.000, 1011.000],  loss: 1223984.000000, mae: 2664.000977, mean_q: 2774.889160\n",
      "wrong_move\n",
      "   5501/500000: episode: 5432, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 466.000 [466.000, 466.000],  loss: 4289587.500000, mae: 2657.255859, mean_q: 3241.866699\n",
      "wrong_move\n",
      "   5503/500000: episode: 5433, duration: 0.168s, episode steps:   2, steps per second:  12, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2661.500 [2601.000, 2722.000],  loss: 4192871.500000, mae: 2700.708008, mean_q: 3197.921875\n",
      "wrong_move\n",
      "   5504/500000: episode: 5434, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3022.000 [3022.000, 3022.000],  loss: 991708.187500, mae: 2663.183105, mean_q: 2413.531250\n",
      "wrong_move\n",
      "   5505/500000: episode: 5435, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 466.000 [466.000, 466.000],  loss: 1167433.000000, mae: 2678.201660, mean_q: 4095.289307\n",
      "wrong_move\n",
      "   5506/500000: episode: 5436, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 466.000 [466.000, 466.000],  loss: 26125410.000000, mae: 2682.955078, mean_q: 2505.748535\n",
      "wrong_move\n",
      "   5507/500000: episode: 5437, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 466.000 [466.000, 466.000],  loss: 163529952.000000, mae: 2694.125732, mean_q: 3359.716797\n",
      "wrong_move\n",
      "   5508/500000: episode: 5438, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 466.000 [466.000, 466.000],  loss: 13169274.000000, mae: 2682.761719, mean_q: 2925.662842\n",
      "wrong_move\n",
      "   5509/500000: episode: 5439, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1707.000 [1707.000, 1707.000],  loss: 1780059.500000, mae: 2730.583740, mean_q: 4045.633057\n",
      "wrong_move\n",
      "   5510/500000: episode: 5440, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4027.000 [4027.000, 4027.000],  loss: 10601354.000000, mae: 2681.934082, mean_q: 2721.994141\n",
      "wrong_move\n",
      "   5511/500000: episode: 5441, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 466.000 [466.000, 466.000],  loss: 4045088.000000, mae: 2723.130371, mean_q: 2897.701660\n",
      "wrong_move\n",
      "   5512/500000: episode: 5442, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2233.000 [2233.000, 2233.000],  loss: 5832000.000000, mae: 2720.099365, mean_q: 4946.458984\n",
      "wrong_move\n",
      "   5513/500000: episode: 5443, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2978.000 [2978.000, 2978.000],  loss: 2533611.500000, mae: 2679.890625, mean_q: 2252.161621\n",
      "wrong_move\n",
      "   5514/500000: episode: 5444, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 931.000 [931.000, 931.000],  loss: 1598750.125000, mae: 2728.039551, mean_q: 2648.503418\n",
      "wrong_move\n",
      "   5516/500000: episode: 5445, duration: 0.249s, episode steps:   2, steps per second:   8, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1342.500 [192.000, 2493.000],  loss: 5290025.000000, mae: 2712.909180, mean_q: 3280.199707\n",
      "wrong_move\n",
      "   5517/500000: episode: 5446, duration: 0.143s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 1568999.000000, mae: 2683.730957, mean_q: 3206.105469\n",
      "wrong_move\n",
      "   5518/500000: episode: 5447, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 466.000 [466.000, 466.000],  loss: 1410234.250000, mae: 2691.488281, mean_q: 2864.130371\n",
      "wrong_move\n",
      "   5519/500000: episode: 5448, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1949.000 [1949.000, 1949.000],  loss: 1575070.625000, mae: 2688.976562, mean_q: 2517.789551\n",
      "wrong_move\n",
      "   5520/500000: episode: 5449, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 487472.500000, mae: 2693.485840, mean_q: 2506.378418\n",
      "wrong_move\n",
      "   5521/500000: episode: 5450, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1319.000 [1319.000, 1319.000],  loss: 523187.843750, mae: 2697.861328, mean_q: 3226.541504\n",
      "wrong_move\n",
      "   5522/500000: episode: 5451, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 54.000 [54.000, 54.000],  loss: 3365111.500000, mae: 2693.206787, mean_q: 2272.393555\n",
      "wrong_move\n",
      "   5523/500000: episode: 5452, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1583.000 [1583.000, 1583.000],  loss: 1223396.500000, mae: 2717.596191, mean_q: 5328.262207\n",
      "wrong_move\n",
      "   5524/500000: episode: 5453, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 1091428.000000, mae: 2712.205566, mean_q: 3494.144531\n",
      "wrong_move\n",
      "   5525/500000: episode: 5454, duration: 0.156s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 1074154.500000, mae: 2726.418945, mean_q: 2917.846680\n",
      "wrong_move\n",
      "   5526/500000: episode: 5455, duration: 0.147s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3047.000 [3047.000, 3047.000],  loss: 1834192.500000, mae: 2699.787598, mean_q: 3913.761719\n",
      "wrong_move\n",
      "   5527/500000: episode: 5456, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 2007746.375000, mae: 2699.439453, mean_q: 3120.385498\n",
      "wrong_move\n",
      "   5528/500000: episode: 5457, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 5390676.000000, mae: 2701.367432, mean_q: 3297.925537\n",
      "wrong_move\n",
      "   5529/500000: episode: 5458, duration: 0.153s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 1582312.625000, mae: 2701.542969, mean_q: 3785.621826\n",
      "wrong_move\n",
      "   5530/500000: episode: 5459, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 5026977.000000, mae: 2733.078125, mean_q: 4230.758789\n",
      "wrong_move\n",
      "   5531/500000: episode: 5460, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1445.000 [1445.000, 1445.000],  loss: 1423764.750000, mae: 2737.523682, mean_q: 3445.327148\n",
      "wrong_move\n",
      "   5532/500000: episode: 5461, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 5889807.500000, mae: 2704.878174, mean_q: 4395.485352\n",
      "wrong_move\n",
      "   5533/500000: episode: 5462, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2390.000 [2390.000, 2390.000],  loss: 401487.625000, mae: 2724.160645, mean_q: 3871.524902\n",
      "wrong_move\n",
      "   5534/500000: episode: 5463, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 147115760.000000, mae: 2718.909668, mean_q: 4871.861328\n",
      "wrong_move\n",
      "   5535/500000: episode: 5464, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2948.000 [2948.000, 2948.000],  loss: 5479371.000000, mae: 2694.592285, mean_q: 2258.561035\n",
      "wrong_move\n",
      "   5536/500000: episode: 5465, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 902133.250000, mae: 2741.134766, mean_q: 4333.521973\n",
      "wrong_move\n",
      "   5537/500000: episode: 5466, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 410.000 [410.000, 410.000],  loss: 712014.875000, mae: 2689.129883, mean_q: 2866.783936\n",
      "wrong_move\n",
      "   5538/500000: episode: 5467, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 952839.125000, mae: 2687.762451, mean_q: 2940.149414\n",
      "wrong_move\n",
      "   5539/500000: episode: 5468, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 192.000 [192.000, 192.000],  loss: 6074208.500000, mae: 2690.241211, mean_q: 2735.257812\n",
      "wrong_move\n",
      "   5540/500000: episode: 5469, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 611.000 [611.000, 611.000],  loss: 668558.250000, mae: 2689.188721, mean_q: 2811.907227\n",
      "wrong_move\n",
      "   5541/500000: episode: 5470, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1362.000 [1362.000, 1362.000],  loss: 2046413.500000, mae: 2689.878906, mean_q: 2256.854736\n",
      "wrong_move\n",
      "   5542/500000: episode: 5471, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 974.000 [974.000, 974.000],  loss: 3316490.500000, mae: 2754.356445, mean_q: 2493.668457\n",
      "wrong_move\n",
      "   5543/500000: episode: 5472, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2988.000 [2988.000, 2988.000],  loss: 1646249.250000, mae: 2692.455078, mean_q: 1568.823730\n",
      "wrong_move\n",
      "   5544/500000: episode: 5473, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3425.000 [3425.000, 3425.000],  loss: 670817.062500, mae: 2698.102539, mean_q: 1909.931152\n",
      "wrong_move\n",
      "   5545/500000: episode: 5474, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2334508.500000, mae: 2694.360352, mean_q: 1696.088257\n",
      "wrong_move\n",
      "   5546/500000: episode: 5475, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3285.000 [3285.000, 3285.000],  loss: 836338.500000, mae: 2736.845215, mean_q: 2851.757324\n",
      "wrong_move\n",
      "   5547/500000: episode: 5476, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3285.000 [3285.000, 3285.000],  loss: 1135040.625000, mae: 2749.028564, mean_q: 4388.206543\n",
      "wrong_move\n",
      "   5548/500000: episode: 5477, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 153.000 [153.000, 153.000],  loss: 535048.875000, mae: 2695.618408, mean_q: 2398.049805\n",
      "wrong_move\n",
      "   5549/500000: episode: 5478, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3285.000 [3285.000, 3285.000],  loss: 5539757.500000, mae: 2695.123535, mean_q: 2120.972900\n",
      "wrong_move\n",
      "   5550/500000: episode: 5479, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 224.000 [224.000, 224.000],  loss: 1115962.875000, mae: 2726.260254, mean_q: 2682.491699\n",
      "wrong_move\n",
      "   5551/500000: episode: 5480, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3801.000 [3801.000, 3801.000],  loss: 53245936.000000, mae: 2704.958496, mean_q: 4225.249023\n",
      "wrong_move\n",
      "   5552/500000: episode: 5481, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 558.000 [558.000, 558.000],  loss: 458160.312500, mae: 2695.547852, mean_q: 3333.345459\n",
      "wrong_move\n",
      "   5553/500000: episode: 5482, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 636.000 [636.000, 636.000],  loss: 1959903.250000, mae: 2703.833496, mean_q: 3722.764893\n",
      "wrong_move\n",
      "   5554/500000: episode: 5483, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 810.000 [810.000, 810.000],  loss: 1329928.500000, mae: 2745.970459, mean_q: 4071.028320\n",
      "wrong_move\n",
      "   5555/500000: episode: 5484, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1018.000 [1018.000, 1018.000],  loss: 1164022.750000, mae: 2700.462402, mean_q: 1960.874023\n",
      "wrong_move\n",
      "   5556/500000: episode: 5485, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 810.000 [810.000, 810.000],  loss: 3988736.500000, mae: 2709.229492, mean_q: 1916.630005\n",
      "wrong_move\n",
      "   5557/500000: episode: 5486, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3294161.500000, mae: 2732.483154, mean_q: 4787.327148\n",
      "wrong_move\n",
      "   5558/500000: episode: 5487, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 810.000 [810.000, 810.000],  loss: 1574356.500000, mae: 2695.196533, mean_q: 2065.627197\n",
      "wrong_move\n",
      "   5559/500000: episode: 5488, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1491.000 [1491.000, 1491.000],  loss: 1883273.250000, mae: 2696.323975, mean_q: 1701.951660\n",
      "wrong_move\n",
      "   5560/500000: episode: 5489, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 522044.187500, mae: 2696.007812, mean_q: 2986.763428\n",
      "wrong_move\n",
      "   5561/500000: episode: 5490, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3285.000 [3285.000, 3285.000],  loss: 4448064.000000, mae: 2695.965088, mean_q: 2199.965820\n",
      "wrong_move\n",
      "   5562/500000: episode: 5491, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 974.000 [974.000, 974.000],  loss: 7510887.500000, mae: 2699.403809, mean_q: 2614.701416\n",
      "wrong_move\n",
      "   5563/500000: episode: 5492, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 611.000 [611.000, 611.000],  loss: 841076.812500, mae: 2703.979980, mean_q: 3046.207031\n",
      "wrong_move\n",
      "   5564/500000: episode: 5493, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4034.000 [4034.000, 4034.000],  loss: 1672273.000000, mae: 2697.249512, mean_q: 1822.742188\n",
      "wrong_move\n",
      "   5565/500000: episode: 5494, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 1860152.250000, mae: 2732.510010, mean_q: 2872.756348\n",
      "wrong_move\n",
      "   5566/500000: episode: 5495, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2634.000 [2634.000, 2634.000],  loss: 1673400.375000, mae: 2708.161377, mean_q: 1970.688477\n",
      "wrong_move\n",
      "   5567/500000: episode: 5496, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1392.000 [1392.000, 1392.000],  loss: 1119264.250000, mae: 2701.003662, mean_q: 2113.796387\n",
      "wrong_move\n",
      "   5568/500000: episode: 5497, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3725.000 [3725.000, 3725.000],  loss: 6699153.500000, mae: 2708.856445, mean_q: 2950.095459\n",
      "wrong_move\n",
      "   5569/500000: episode: 5498, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1920.000 [1920.000, 1920.000],  loss: 2803218.000000, mae: 2703.821045, mean_q: 2665.592529\n",
      "wrong_move\n",
      "   5571/500000: episode: 5499, duration: 0.088s, episode steps:   2, steps per second:  23, episode reward: -4941.000, mean reward: -2470.500 [-5000.000, 59.000], mean action: 2389.000 [2056.000, 2722.000],  loss: 7357712.000000, mae: 2707.516602, mean_q: 2466.475830\n",
      "wrong_move\n",
      "   5572/500000: episode: 5500, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 161.000 [161.000, 161.000],  loss: 1062156.125000, mae: 2720.643799, mean_q: 4848.365723\n",
      "wrong_move\n",
      "   5573/500000: episode: 5501, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1753.000 [1753.000, 1753.000],  loss: 979514.625000, mae: 2714.193604, mean_q: 2538.319824\n",
      "wrong_move\n",
      "   5574/500000: episode: 5502, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1920.000 [1920.000, 1920.000],  loss: 654552.125000, mae: 2707.904541, mean_q: 2499.001953\n",
      "wrong_move\n",
      "   5575/500000: episode: 5503, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 731993.875000, mae: 2715.432617, mean_q: 1825.376587\n",
      "wrong_move\n",
      "   5576/500000: episode: 5504, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1853.000 [1853.000, 1853.000],  loss: 836888.250000, mae: 2727.717285, mean_q: 3924.393066\n",
      "wrong_move\n",
      "   5577/500000: episode: 5505, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2968.000 [2968.000, 2968.000],  loss: 827237.062500, mae: 2714.007080, mean_q: 2688.023926\n",
      "wrong_move\n",
      "   5578/500000: episode: 5506, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 974.000 [974.000, 974.000],  loss: 1508289.375000, mae: 2724.168213, mean_q: 2203.644775\n",
      "wrong_move\n",
      "   5579/500000: episode: 5507, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1920.000 [1920.000, 1920.000],  loss: 396641.187500, mae: 2713.232666, mean_q: 2784.094971\n",
      "wrong_move\n",
      "   5580/500000: episode: 5508, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 974.000 [974.000, 974.000],  loss: 2581129.750000, mae: 2717.080078, mean_q: 2350.524170\n",
      "wrong_move\n",
      "   5581/500000: episode: 5509, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 399.000 [399.000, 399.000],  loss: 2987285.000000, mae: 2728.787598, mean_q: 2502.349609\n",
      "wrong_move\n",
      "   5582/500000: episode: 5510, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2095.000 [2095.000, 2095.000],  loss: 2857935.500000, mae: 2724.441895, mean_q: 2441.355469\n",
      "wrong_move\n",
      "   5583/500000: episode: 5511, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 364.000 [364.000, 364.000],  loss: 1319399.250000, mae: 2723.429932, mean_q: 1678.111572\n",
      "wrong_move\n",
      "   5584/500000: episode: 5512, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3711.000 [3711.000, 3711.000],  loss: 1274525.750000, mae: 2736.734375, mean_q: 3774.604492\n",
      "wrong_move\n",
      "   5585/500000: episode: 5513, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4058.000 [4058.000, 4058.000],  loss: 1319715.250000, mae: 2735.270508, mean_q: 2150.316650\n",
      "wrong_move\n",
      "   5586/500000: episode: 5514, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 399.000 [399.000, 399.000],  loss: 2934505.750000, mae: 2730.937012, mean_q: 2367.974609\n",
      "wrong_move\n",
      "   5587/500000: episode: 5515, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 399.000 [399.000, 399.000],  loss: 1397448.250000, mae: 2733.188232, mean_q: 1080.022461\n",
      "wrong_move\n",
      "   5588/500000: episode: 5516, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 399.000 [399.000, 399.000],  loss: 1687152.250000, mae: 2763.617188, mean_q: 3943.754150\n",
      "wrong_move\n",
      "   5589/500000: episode: 5517, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 106.000 [106.000, 106.000],  loss: 7342672.500000, mae: 2739.991211, mean_q: 1817.109131\n",
      "wrong_move\n",
      "   5590/500000: episode: 5518, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3022.000 [3022.000, 3022.000],  loss: 2220054.500000, mae: 2741.080078, mean_q: 2330.948975\n",
      "wrong_move\n",
      "   5591/500000: episode: 5519, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 364.000 [364.000, 364.000],  loss: 2589047.000000, mae: 2742.183105, mean_q: 1853.904175\n",
      "wrong_move\n",
      "   5592/500000: episode: 5520, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 643.000 [643.000, 643.000],  loss: 186533536.000000, mae: 2745.030762, mean_q: 2153.668945\n",
      "wrong_move\n",
      "   5593/500000: episode: 5521, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2178.000 [2178.000, 2178.000],  loss: 891859.500000, mae: 2747.234863, mean_q: 2051.735107\n",
      "wrong_move\n",
      "   5594/500000: episode: 5522, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2178.000 [2178.000, 2178.000],  loss: 452790.718750, mae: 2745.733643, mean_q: 1390.762939\n",
      "wrong_move\n",
      "   5595/500000: episode: 5523, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2178.000 [2178.000, 2178.000],  loss: 7295522.000000, mae: 2751.958984, mean_q: 2361.937744\n",
      "wrong_move\n",
      "   5596/500000: episode: 5524, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3558.000 [3558.000, 3558.000],  loss: 576697.625000, mae: 2749.280273, mean_q: 2987.775391\n",
      "wrong_move\n",
      "   5597/500000: episode: 5525, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 359.000 [359.000, 359.000],  loss: 1491264.500000, mae: 2747.040771, mean_q: 1761.020630\n",
      "wrong_move\n",
      "   5598/500000: episode: 5526, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3990.000 [3990.000, 3990.000],  loss: 484640.125000, mae: 2748.447266, mean_q: 2259.562988\n",
      "wrong_move\n",
      "   5599/500000: episode: 5527, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1901.000 [1901.000, 1901.000],  loss: 11161811.000000, mae: 2753.227051, mean_q: 3009.352051\n",
      "wrong_move\n",
      "   5600/500000: episode: 5528, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 359.000 [359.000, 359.000],  loss: 918026.000000, mae: 2748.110352, mean_q: 2067.191162\n",
      "wrong_move\n",
      "   5601/500000: episode: 5529, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2178.000 [2178.000, 2178.000],  loss: 3155512.750000, mae: 2746.900146, mean_q: 2810.781250\n",
      "wrong_move\n",
      "   5602/500000: episode: 5530, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1392.000 [1392.000, 1392.000],  loss: 979766.000000, mae: 2748.024902, mean_q: 2316.093506\n",
      "wrong_move\n",
      "   5603/500000: episode: 5531, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3990.000 [3990.000, 3990.000],  loss: 1438345.500000, mae: 2747.621094, mean_q: 2335.035645\n",
      "wrong_move\n",
      "   5604/500000: episode: 5532, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1347.000 [1347.000, 1347.000],  loss: 2880158.000000, mae: 2753.594727, mean_q: 1847.792847\n",
      "wrong_move\n",
      "   5605/500000: episode: 5533, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 382.000 [382.000, 382.000],  loss: 934880.625000, mae: 2750.094482, mean_q: 2590.427246\n",
      "wrong_move\n",
      "   5606/500000: episode: 5534, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2153.000 [2153.000, 2153.000],  loss: 786272.125000, mae: 2796.593506, mean_q: 5726.400391\n",
      "wrong_move\n",
      "   5607/500000: episode: 5535, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 1187632.250000, mae: 2754.667969, mean_q: 1305.238770\n",
      "wrong_move\n",
      "   5608/500000: episode: 5536, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 359.000 [359.000, 359.000],  loss: 676828.875000, mae: 2765.798828, mean_q: 2093.257324\n",
      "wrong_move\n",
      "   5609/500000: episode: 5537, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 291.000 [291.000, 291.000],  loss: 914462.625000, mae: 2759.513672, mean_q: 2031.313477\n",
      "wrong_move\n",
      "   5610/500000: episode: 5538, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 12.000 [12.000, 12.000],  loss: 4476397.000000, mae: 2784.489258, mean_q: 2024.452148\n",
      "wrong_move\n",
      "   5611/500000: episode: 5539, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 359.000 [359.000, 359.000],  loss: 768592.625000, mae: 2754.044922, mean_q: 1909.789551\n",
      "wrong_move\n",
      "   5612/500000: episode: 5540, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1392.000 [1392.000, 1392.000],  loss: 4647060.000000, mae: 2760.023438, mean_q: 2220.376709\n",
      "wrong_move\n",
      "   5613/500000: episode: 5541, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2535.000 [2535.000, 2535.000],  loss: 1219265.250000, mae: 2754.104980, mean_q: 1885.598877\n",
      "wrong_move\n",
      "   5614/500000: episode: 5542, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3951.000 [3951.000, 3951.000],  loss: 1173427.375000, mae: 2755.495361, mean_q: 1639.093140\n",
      "wrong_move\n",
      "   5615/500000: episode: 5543, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 762.000 [762.000, 762.000],  loss: 998242.250000, mae: 2752.869141, mean_q: 1127.410767\n",
      "wrong_move\n",
      "   5616/500000: episode: 5544, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 359.000 [359.000, 359.000],  loss: 942692.437500, mae: 2753.419189, mean_q: 2334.872314\n",
      "wrong_move\n",
      "   5617/500000: episode: 5545, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3217.000 [3217.000, 3217.000],  loss: 47285048.000000, mae: 2815.122070, mean_q: 4600.418457\n",
      "wrong_move\n",
      "   5618/500000: episode: 5546, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1292636.375000, mae: 2752.796875, mean_q: 2589.989746\n",
      "wrong_move\n",
      "   5619/500000: episode: 5547, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4012.000 [4012.000, 4012.000],  loss: 2480700.000000, mae: 2751.832031, mean_q: 1845.951538\n",
      "wrong_move\n",
      "   5620/500000: episode: 5548, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2178.000 [2178.000, 2178.000],  loss: 951700.500000, mae: 2778.681641, mean_q: 2440.402588\n",
      "wrong_move\n",
      "   5621/500000: episode: 5549, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 800.000 [800.000, 800.000],  loss: 1132365.000000, mae: 2749.466309, mean_q: 1325.245239\n",
      "wrong_move\n",
      "   5622/500000: episode: 5550, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2178.000 [2178.000, 2178.000],  loss: 1378742.250000, mae: 2748.613770, mean_q: 1404.384766\n",
      "wrong_move\n",
      "   5623/500000: episode: 5551, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3872.000 [3872.000, 3872.000],  loss: 3037184.000000, mae: 2756.359375, mean_q: 2013.559692\n",
      "wrong_move\n",
      "   5624/500000: episode: 5552, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1253.000 [1253.000, 1253.000],  loss: 699427.125000, mae: 2749.802734, mean_q: 2256.850830\n",
      "wrong_move\n",
      "   5625/500000: episode: 5553, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 359.000 [359.000, 359.000],  loss: 16368246.000000, mae: 2751.224365, mean_q: 2603.394531\n",
      "wrong_move\n",
      "   5626/500000: episode: 5554, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 359.000 [359.000, 359.000],  loss: 2074007.250000, mae: 2758.830566, mean_q: 2828.708496\n",
      "wrong_move\n",
      "   5627/500000: episode: 5555, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 359.000 [359.000, 359.000],  loss: 831535.312500, mae: 2753.501221, mean_q: 1767.482422\n",
      "wrong_move\n",
      "   5628/500000: episode: 5556, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 359.000 [359.000, 359.000],  loss: 992429.437500, mae: 2757.767578, mean_q: 1630.070801\n",
      "wrong_move\n",
      "   5629/500000: episode: 5557, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 359.000 [359.000, 359.000],  loss: 13124454.000000, mae: 2757.658936, mean_q: 2203.941406\n",
      "wrong_move\n",
      "   5630/500000: episode: 5558, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 359.000 [359.000, 359.000],  loss: 2937096.250000, mae: 2758.357178, mean_q: 1359.784180\n",
      "wrong_move\n",
      "   5631/500000: episode: 5559, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 359.000 [359.000, 359.000],  loss: 2187527.000000, mae: 2761.638672, mean_q: 2577.861816\n",
      "wrong_move\n",
      "   5632/500000: episode: 5560, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3304.000 [3304.000, 3304.000],  loss: 1340356.250000, mae: 2771.089355, mean_q: 2334.250977\n",
      "wrong_move\n",
      "   5633/500000: episode: 5561, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2153.000 [2153.000, 2153.000],  loss: 1707136.000000, mae: 2790.255615, mean_q: 2391.735596\n",
      "wrong_move\n",
      "   5634/500000: episode: 5562, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4027.000 [4027.000, 4027.000],  loss: 1016354.125000, mae: 2772.453857, mean_q: 2613.527100\n",
      "wrong_move\n",
      "   5635/500000: episode: 5563, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1490.000 [1490.000, 1490.000],  loss: 1076109.250000, mae: 2768.988525, mean_q: 2394.236084\n",
      "wrong_move\n",
      "   5636/500000: episode: 5564, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3571.000 [3571.000, 3571.000],  loss: 2254464.250000, mae: 2771.008789, mean_q: 2420.043213\n",
      "wrong_move\n",
      "   5637/500000: episode: 5565, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3448.000 [3448.000, 3448.000],  loss: 460836.187500, mae: 2781.478516, mean_q: 2028.462524\n",
      "wrong_move\n",
      "   5638/500000: episode: 5566, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1490.000 [1490.000, 1490.000],  loss: 1253863.375000, mae: 2771.782227, mean_q: 1889.123657\n",
      "wrong_move\n",
      "   5639/500000: episode: 5567, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1137.000 [1137.000, 1137.000],  loss: 1123616.000000, mae: 2778.785645, mean_q: 3478.081787\n",
      "wrong_move\n",
      "   5640/500000: episode: 5568, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 368.000 [368.000, 368.000],  loss: 1713520.000000, mae: 2774.982422, mean_q: 1682.711182\n",
      "wrong_move\n",
      "   5641/500000: episode: 5569, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2064.000 [2064.000, 2064.000],  loss: 748557.125000, mae: 2779.380371, mean_q: 2081.050781\n",
      "wrong_move\n",
      "   5642/500000: episode: 5570, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3799.000 [3799.000, 3799.000],  loss: 94258728.000000, mae: 2793.064941, mean_q: 3340.582031\n",
      "wrong_move\n",
      "   5643/500000: episode: 5571, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 602.000 [602.000, 602.000],  loss: 790164.500000, mae: 2805.149902, mean_q: 2099.498047\n",
      "wrong_move\n",
      "   5644/500000: episode: 5572, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1490.000 [1490.000, 1490.000],  loss: 1776027.500000, mae: 2775.069580, mean_q: 2029.052734\n",
      "wrong_move\n",
      "   5645/500000: episode: 5573, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 762.000 [762.000, 762.000],  loss: 1315381.375000, mae: 2776.150879, mean_q: 2469.431152\n",
      "wrong_move\n",
      "   5646/500000: episode: 5574, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3686.000 [3686.000, 3686.000],  loss: 663280.812500, mae: 2797.822754, mean_q: 2256.677734\n",
      "wrong_move\n",
      "   5647/500000: episode: 5575, duration: 0.044s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2505.000 [2505.000, 2505.000],  loss: 909636.875000, mae: 2787.312500, mean_q: 2177.424805\n",
      "wrong_move\n",
      "   5648/500000: episode: 5576, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1316.000 [1316.000, 1316.000],  loss: 2239294.750000, mae: 2777.035645, mean_q: 2068.857910\n",
      "wrong_move\n",
      "   5649/500000: episode: 5577, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2501.000 [2501.000, 2501.000],  loss: 1046494.187500, mae: 2774.832520, mean_q: 1061.683228\n",
      "wrong_move\n",
      "   5650/500000: episode: 5578, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3387.000 [3387.000, 3387.000],  loss: 517492.750000, mae: 2776.922852, mean_q: 1771.846924\n",
      "wrong_move\n",
      "   5651/500000: episode: 5579, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3387.000 [3387.000, 3387.000],  loss: 889633.750000, mae: 2774.774902, mean_q: 1311.079712\n",
      "wrong_move\n",
      "   5652/500000: episode: 5580, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3387.000 [3387.000, 3387.000],  loss: 1032474.812500, mae: 2780.958496, mean_q: 1406.057617\n",
      "wrong_move\n",
      "   5653/500000: episode: 5581, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3387.000 [3387.000, 3387.000],  loss: 2867700.500000, mae: 2776.168213, mean_q: 1607.628052\n",
      "wrong_move\n",
      "   5654/500000: episode: 5582, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3387.000 [3387.000, 3387.000],  loss: 1943905.875000, mae: 2796.268555, mean_q: 2687.684570\n",
      "wrong_move\n",
      "   5655/500000: episode: 5583, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 677.000 [677.000, 677.000],  loss: 239065743360.000000, mae: 3052.265625, mean_q: 7615.989258\n",
      "wrong_move\n",
      "   5656/500000: episode: 5584, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1600.000 [1600.000, 1600.000],  loss: 12658189.000000, mae: 2802.517334, mean_q: 2366.437988\n",
      "wrong_move\n",
      "   5657/500000: episode: 5585, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1600.000 [1600.000, 1600.000],  loss: 2204517.000000, mae: 2834.815674, mean_q: 3275.462891\n",
      "wrong_move\n",
      "   5658/500000: episode: 5586, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1828.000 [1828.000, 1828.000],  loss: 2511167.500000, mae: 2858.127930, mean_q: 4643.097656\n",
      "wrong_move\n",
      "   5659/500000: episode: 5587, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1081455.875000, mae: 2877.492676, mean_q: 4271.299805\n",
      "wrong_move\n",
      "   5660/500000: episode: 5588, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: 7631713.500000, mae: 2918.472412, mean_q: 6695.086426\n",
      "wrong_move\n",
      "   5661/500000: episode: 5589, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6574279.000000, mae: 3020.541260, mean_q: 8662.523438\n",
      "wrong_move\n",
      "   5662/500000: episode: 5590, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4359272.000000, mae: 2979.018311, mean_q: 7406.740234\n",
      "wrong_move\n",
      "   5663/500000: episode: 5591, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2562.000 [2562.000, 2562.000],  loss: 2750241.500000, mae: 2992.531006, mean_q: 9183.426758\n",
      "wrong_move\n",
      "   5664/500000: episode: 5592, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 334.000 [334.000, 334.000],  loss: 67530576.000000, mae: 3006.116699, mean_q: 8782.193359\n",
      "wrong_move\n",
      "   5665/500000: episode: 5593, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1811.000 [1811.000, 1811.000],  loss: 11733535.000000, mae: 3108.899414, mean_q: 10157.271484\n",
      "wrong_move\n",
      "   5666/500000: episode: 5594, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 316245.281250, mae: 3018.108398, mean_q: 9685.604492\n",
      "wrong_move\n",
      "   5667/500000: episode: 5595, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 13889590.000000, mae: 3021.966797, mean_q: 10187.251953\n",
      "wrong_move\n",
      "   5668/500000: episode: 5596, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1583334.500000, mae: 3021.031738, mean_q: 9959.429688\n",
      "wrong_move\n",
      "   5669/500000: episode: 5597, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2110.000 [2110.000, 2110.000],  loss: 773402.000000, mae: 3020.657227, mean_q: 10516.561523\n",
      "wrong_move\n",
      "   5670/500000: episode: 5598, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2568337.500000, mae: 3020.199707, mean_q: 10534.155273\n",
      "wrong_move\n",
      "   5671/500000: episode: 5599, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1838.000 [1838.000, 1838.000],  loss: 617978.375000, mae: 3015.084717, mean_q: 10261.615234\n",
      "wrong_move\n",
      "   5672/500000: episode: 5600, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1656633.500000, mae: 3010.590332, mean_q: 10855.072266\n",
      "wrong_move\n",
      "   5673/500000: episode: 5601, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1656.000 [1656.000, 1656.000],  loss: 407534496.000000, mae: 3028.973633, mean_q: 11421.927734\n",
      "wrong_move\n",
      "   5674/500000: episode: 5602, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3913055.250000, mae: 3041.371338, mean_q: 11928.039062\n",
      "wrong_move\n",
      "   5675/500000: episode: 5603, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3718.000 [3718.000, 3718.000],  loss: 1629323.500000, mae: 2987.801758, mean_q: 10235.258789\n",
      "wrong_move\n",
      "   5676/500000: episode: 5604, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2325.000 [2325.000, 2325.000],  loss: 737469.125000, mae: 2976.447510, mean_q: 10590.880859\n",
      "wrong_move\n",
      "   5677/500000: episode: 5605, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4083.000 [4083.000, 4083.000],  loss: 2156208.500000, mae: 2965.618896, mean_q: 11055.516602\n",
      "wrong_move\n",
      "   5678/500000: episode: 5606, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3212.000 [3212.000, 3212.000],  loss: 15303446.000000, mae: 2955.086670, mean_q: 10080.861328\n",
      "wrong_move\n",
      "   5679/500000: episode: 5607, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1751330.625000, mae: 2976.227539, mean_q: 11015.985352\n",
      "wrong_move\n",
      "   5680/500000: episode: 5608, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2088.000 [2088.000, 2088.000],  loss: 70039328.000000, mae: 2939.721680, mean_q: 9895.898438\n",
      "wrong_move\n",
      "   5681/500000: episode: 5609, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 262.000 [262.000, 262.000],  loss: 3746410.000000, mae: 2931.488281, mean_q: 10006.996094\n",
      "wrong_move\n",
      "   5682/500000: episode: 5610, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1411031.000000, mae: 2927.394531, mean_q: 10162.607422\n",
      "wrong_move\n",
      "   5683/500000: episode: 5611, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1103.000 [1103.000, 1103.000],  loss: 871114.875000, mae: 2919.138672, mean_q: 10234.281250\n",
      "wrong_move\n",
      "   5684/500000: episode: 5612, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1696176.875000, mae: 2907.805664, mean_q: 9600.583008\n",
      "wrong_move\n",
      "   5685/500000: episode: 5613, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 738071.250000, mae: 2929.187256, mean_q: 9182.974609\n",
      "wrong_move\n",
      "   5687/500000: episode: 5614, duration: 0.222s, episode steps:   2, steps per second:   9, episode reward: -4981.000, mean reward: -2490.500 [-5000.000, 19.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 35193496.000000, mae: 2894.618652, mean_q: 9634.865234\n",
      "wrong_move\n",
      "   5688/500000: episode: 5615, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8406864.000000, mae: 2884.382812, mean_q: 9269.708984\n",
      "wrong_move\n",
      "   5689/500000: episode: 5616, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2858113.250000, mae: 2878.105469, mean_q: 9682.800781\n",
      "wrong_move\n",
      "   5690/500000: episode: 5617, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 592.000 [592.000, 592.000],  loss: 1684402.750000, mae: 2875.811768, mean_q: 9996.548828\n",
      "wrong_move\n",
      "   5691/500000: episode: 5618, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6370806.500000, mae: 2907.028564, mean_q: 8825.334961\n",
      "wrong_move\n",
      "   5692/500000: episode: 5619, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 105.000 [105.000, 105.000],  loss: 5847544.000000, mae: 2863.376465, mean_q: 9119.072266\n",
      "wrong_move\n",
      "   5693/500000: episode: 5620, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 727.000 [727.000, 727.000],  loss: 28296604.000000, mae: 2861.319336, mean_q: 9730.225586\n",
      "wrong_move\n",
      "   5694/500000: episode: 5621, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3235252.500000, mae: 2945.103271, mean_q: 10233.021484\n",
      "wrong_move\n",
      "   5695/500000: episode: 5622, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 596.000 [596.000, 596.000],  loss: 2276513.750000, mae: 2854.219238, mean_q: 9147.309570\n",
      "wrong_move\n",
      "   5696/500000: episode: 5623, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2134.000 [2134.000, 2134.000],  loss: 3820947.250000, mae: 2999.291992, mean_q: 9895.689453\n",
      "wrong_move\n",
      "   5697/500000: episode: 5624, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1843.000 [1843.000, 1843.000],  loss: 5231364.000000, mae: 2851.459961, mean_q: 8923.525391\n",
      "wrong_move\n",
      "   5698/500000: episode: 5625, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2075687.125000, mae: 2856.391846, mean_q: 9684.097656\n",
      "wrong_move\n",
      "   5699/500000: episode: 5626, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 301720480.000000, mae: 2865.564453, mean_q: 7983.120605\n",
      "wrong_move\n",
      "   5700/500000: episode: 5627, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 752548.750000, mae: 2844.999512, mean_q: 8240.538086\n",
      "wrong_move\n",
      "   5701/500000: episode: 5628, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 486.000 [486.000, 486.000],  loss: 17676274.000000, mae: 2824.745605, mean_q: 8546.666992\n",
      "wrong_move\n",
      "   5702/500000: episode: 5629, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2248413.250000, mae: 2820.364502, mean_q: 9194.064453\n",
      "wrong_move\n",
      "   5703/500000: episode: 5630, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 908138.625000, mae: 2818.276367, mean_q: 7624.880371\n",
      "wrong_move\n",
      "   5704/500000: episode: 5631, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3171.000 [3171.000, 3171.000],  loss: 15474834.000000, mae: 2811.200684, mean_q: 8832.637695\n",
      "wrong_move\n",
      "   5705/500000: episode: 5632, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: 31707848.000000, mae: 2807.484375, mean_q: 8159.451660\n",
      "wrong_move\n",
      "   5706/500000: episode: 5633, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1217224.625000, mae: 2829.428711, mean_q: 7768.036133\n",
      "wrong_move\n",
      "   5707/500000: episode: 5634, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3177163.000000, mae: 2802.522949, mean_q: 8108.943359\n",
      "wrong_move\n",
      "   5708/500000: episode: 5635, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3932.000 [3932.000, 3932.000],  loss: 1064936.875000, mae: 2801.709229, mean_q: 7599.276855\n",
      "wrong_move\n",
      "   5709/500000: episode: 5636, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6765621.500000, mae: 2801.100098, mean_q: 7636.450195\n",
      "wrong_move\n",
      "   5710/500000: episode: 5637, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2614.000 [2614.000, 2614.000],  loss: 40666220.000000, mae: 2798.674805, mean_q: 7369.696289\n",
      "wrong_move\n",
      "   5711/500000: episode: 5638, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2341235.250000, mae: 2796.478516, mean_q: 7434.275391\n",
      "wrong_move\n",
      "   5712/500000: episode: 5639, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6668567.000000, mae: 2817.192871, mean_q: 7440.660645\n",
      "wrong_move\n",
      "   5713/500000: episode: 5640, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2360.000 [2360.000, 2360.000],  loss: 152141296.000000, mae: 2797.380859, mean_q: 8028.193359\n",
      "wrong_move\n",
      "   5714/500000: episode: 5641, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2028.000 [2028.000, 2028.000],  loss: 6805200.000000, mae: 2792.571533, mean_q: 6893.066406\n",
      "wrong_move\n",
      "   5715/500000: episode: 5642, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3932.000 [3932.000, 3932.000],  loss: 5105704.000000, mae: 2791.960938, mean_q: 6924.173828\n",
      "wrong_move\n",
      "   5716/500000: episode: 5643, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2206.000 [2206.000, 2206.000],  loss: 19968488.000000, mae: 2791.281250, mean_q: 6883.555664\n",
      "wrong_move\n",
      "   5717/500000: episode: 5644, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1653.000 [1653.000, 1653.000],  loss: 198422944.000000, mae: 2791.695801, mean_q: 7213.734375\n",
      "wrong_move\n",
      "   5718/500000: episode: 5645, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2962.000 [2962.000, 2962.000],  loss: 5402291.500000, mae: 2792.483887, mean_q: 7985.347656\n",
      "wrong_move\n",
      "   5720/500000: episode: 5646, duration: 0.109s, episode steps:   2, steps per second:  18, episode reward: -5901.000, mean reward: -2950.500 [-5000.000, -901.000], mean action: 2362.500 [1604.000, 3121.000],  loss: 2544107.000000, mae: 2829.778809, mean_q: 6731.107422\n",
      "wrong_move\n",
      "   5721/500000: episode: 5647, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3932.000 [3932.000, 3932.000],  loss: 2458700.500000, mae: 2788.139648, mean_q: 6895.300781\n",
      "wrong_move\n",
      "   5722/500000: episode: 5648, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3932.000 [3932.000, 3932.000],  loss: 40602476.000000, mae: 2790.053955, mean_q: 7631.866211\n",
      "wrong_move\n",
      "   5723/500000: episode: 5649, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1600261.000000, mae: 2818.048340, mean_q: 7384.484863\n",
      "wrong_move\n",
      "   5724/500000: episode: 5650, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3718.000 [3718.000, 3718.000],  loss: 9003160.000000, mae: 2790.653320, mean_q: 6848.858398\n",
      "wrong_move\n",
      "   5725/500000: episode: 5651, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 787.000 [787.000, 787.000],  loss: 372348354560.000000, mae: 2908.735352, mean_q: 8849.589844\n",
      "wrong_move\n",
      "   5726/500000: episode: 5652, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2541.000 [2541.000, 2541.000],  loss: 18886268.000000, mae: 2840.658203, mean_q: 7907.091309\n",
      "wrong_move\n",
      "   5727/500000: episode: 5653, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 646721.312500, mae: 2861.733887, mean_q: 8427.642578\n",
      "wrong_move\n",
      "   5728/500000: episode: 5654, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1997.000 [1997.000, 1997.000],  loss: 1379289.625000, mae: 2940.962891, mean_q: 11369.076172\n",
      "wrong_move\n",
      "   5729/500000: episode: 5655, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1251.000 [1251.000, 1251.000],  loss: 17210330.000000, mae: 2917.499756, mean_q: 10966.171875\n",
      "wrong_move\n",
      "   5730/500000: episode: 5656, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 50675280.000000, mae: 2923.583496, mean_q: 11136.951172\n",
      "wrong_move\n",
      "   5731/500000: episode: 5657, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 25569368.000000, mae: 2943.253906, mean_q: 11954.603516\n",
      "wrong_move\n",
      "   5732/500000: episode: 5658, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7881236.500000, mae: 2941.020752, mean_q: 11258.029297\n",
      "wrong_move\n",
      "   5733/500000: episode: 5659, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3199.000 [3199.000, 3199.000],  loss: 746026688.000000, mae: 3038.147217, mean_q: 13447.646484\n",
      "wrong_move\n",
      "   5734/500000: episode: 5660, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11235699.000000, mae: 2948.498047, mean_q: 11331.068359\n",
      "wrong_move\n",
      "   5735/500000: episode: 5661, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 957318.750000, mae: 2950.908691, mean_q: 12221.065430\n",
      "wrong_move\n",
      "   5737/500000: episode: 5662, duration: 0.218s, episode steps:   2, steps per second:   9, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17884328.000000, mae: 2958.886719, mean_q: 12343.582031\n",
      "wrong_move\n",
      "   5738/500000: episode: 5663, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1632706.625000, mae: 2952.062012, mean_q: 12238.177734\n",
      "wrong_move\n",
      "   5739/500000: episode: 5664, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 155.000 [155.000, 155.000],  loss: 428928368640.000000, mae: 3157.604004, mean_q: 13961.127930\n",
      "wrong_move\n",
      "   5740/500000: episode: 5665, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1466517.250000, mae: 2996.662842, mean_q: 13536.130859\n",
      "wrong_move\n",
      "   5741/500000: episode: 5666, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9302927.000000, mae: 3013.838623, mean_q: 14181.787109\n",
      "wrong_move\n",
      "   5742/500000: episode: 5667, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 13163515.000000, mae: 3033.650391, mean_q: 15469.291992\n",
      "wrong_move\n",
      "   5743/500000: episode: 5668, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 474.000 [474.000, 474.000],  loss: 23546840.000000, mae: 3049.583008, mean_q: 16403.183594\n",
      "wrong_move\n",
      "   5744/500000: episode: 5669, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 181584656.000000, mae: 3129.311035, mean_q: 16330.027344\n",
      "wrong_move\n",
      "   5745/500000: episode: 5670, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 18104176.000000, mae: 3069.148926, mean_q: 16834.882812\n",
      "wrong_move\n",
      "   5746/500000: episode: 5671, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 10851919.000000, mae: 3084.457031, mean_q: 17888.464844\n",
      "wrong_move\n",
      "   5747/500000: episode: 5672, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1298.000 [1298.000, 1298.000],  loss: 206985281536.000000, mae: 3323.078125, mean_q: 19420.025391\n",
      "wrong_move\n",
      "   5748/500000: episode: 5673, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1360.000 [1360.000, 1360.000],  loss: 2268261.500000, mae: 3123.238037, mean_q: 18962.824219\n",
      "wrong_move\n",
      "   5749/500000: episode: 5674, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3783.000 [3783.000, 3783.000],  loss: 13769540.000000, mae: 3128.989746, mean_q: 20610.718750\n",
      "wrong_move\n",
      "   5750/500000: episode: 5675, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 30114660.000000, mae: 3138.224121, mean_q: 20944.453125\n",
      "wrong_move\n",
      "   5752/500000: episode: 5676, duration: 0.075s, episode steps:   2, steps per second:  27, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 1838.000 [954.000, 2722.000],  loss: 24925596.000000, mae: 3148.598145, mean_q: 21305.843750\n",
      "wrong_move\n",
      "   5753/500000: episode: 5677, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2506.000 [2506.000, 2506.000],  loss: 5102812.500000, mae: 3154.377441, mean_q: 21706.851562\n",
      "wrong_move\n",
      "   5754/500000: episode: 5678, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2506.000 [2506.000, 2506.000],  loss: 30168704.000000, mae: 3157.365479, mean_q: 22366.427734\n",
      "wrong_move\n",
      "   5755/500000: episode: 5679, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2506.000 [2506.000, 2506.000],  loss: 49526740.000000, mae: 3158.568359, mean_q: 22286.546875\n",
      "wrong_move\n",
      "   5756/500000: episode: 5680, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 463842144.000000, mae: 3191.748535, mean_q: 23066.482422\n",
      "wrong_move\n",
      "   5757/500000: episode: 5681, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1993.000 [1993.000, 1993.000],  loss: 32868600.000000, mae: 3156.778809, mean_q: 23151.343750\n",
      "wrong_move\n",
      "   5758/500000: episode: 5682, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1442.000 [1442.000, 1442.000],  loss: 61363656.000000, mae: 3143.060059, mean_q: 23362.445312\n",
      "wrong_move\n",
      "   5759/500000: episode: 5683, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11467311.000000, mae: 3130.805176, mean_q: 22826.804688\n",
      "wrong_move\n",
      "   5760/500000: episode: 5684, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2506.000 [2506.000, 2506.000],  loss: 33337392.000000, mae: 3119.422852, mean_q: 22572.654297\n",
      "wrong_move\n",
      "   5761/500000: episode: 5685, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2076.000 [2076.000, 2076.000],  loss: 1407974.750000, mae: 3111.943359, mean_q: 23093.400391\n",
      "wrong_move\n",
      "   5762/500000: episode: 5686, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5127378.500000, mae: 3106.935059, mean_q: 22455.111328\n",
      "wrong_move\n",
      "   5763/500000: episode: 5687, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1848.000 [1848.000, 1848.000],  loss: 9584675.000000, mae: 3088.362305, mean_q: 22499.781250\n",
      "wrong_move\n",
      "   5764/500000: episode: 5688, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 21122692.000000, mae: 3077.770508, mean_q: 22551.888672\n",
      "wrong_move\n",
      "   5765/500000: episode: 5689, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 580.000 [580.000, 580.000],  loss: 20734744.000000, mae: 3067.745850, mean_q: 22627.710938\n",
      "wrong_move\n",
      "   5766/500000: episode: 5690, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 415380768.000000, mae: 3120.611328, mean_q: 22477.386719\n",
      "wrong_move\n",
      "   5767/500000: episode: 5691, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 104937968.000000, mae: 3047.897461, mean_q: 21973.894531\n",
      "wrong_move\n",
      "   5768/500000: episode: 5692, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1940.000 [1940.000, 1940.000],  loss: 567448.375000, mae: 3061.375488, mean_q: 22295.228516\n",
      "wrong_move\n",
      "   5769/500000: episode: 5693, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1487.000 [1487.000, 1487.000],  loss: 32707792.000000, mae: 3062.345459, mean_q: 23057.013672\n",
      "wrong_move\n",
      "   5770/500000: episode: 5694, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 779715.875000, mae: 3072.078613, mean_q: 23960.839844\n",
      "wrong_move\n",
      "   5771/500000: episode: 5695, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11456895.000000, mae: 3338.585449, mean_q: 25300.886719\n",
      "wrong_move\n",
      "   5772/500000: episode: 5696, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3111.000 [3111.000, 3111.000],  loss: 7980343.000000, mae: 3080.970703, mean_q: 24501.003906\n",
      "wrong_move\n",
      "   5773/500000: episode: 5697, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2545.000 [2545.000, 2545.000],  loss: 39719960.000000, mae: 3083.970215, mean_q: 24781.640625\n",
      "wrong_move\n",
      "   5774/500000: episode: 5698, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3493.000 [3493.000, 3493.000],  loss: 37474564.000000, mae: 3083.145508, mean_q: 24979.494141\n",
      "wrong_move\n",
      "   5775/500000: episode: 5699, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 394.000 [394.000, 394.000],  loss: 4623175.500000, mae: 3086.509277, mean_q: 25104.183594\n",
      "wrong_move\n",
      "   5776/500000: episode: 5700, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 72910568.000000, mae: 3085.002686, mean_q: 26002.234375\n",
      "wrong_move\n",
      "   5777/500000: episode: 5701, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 37655896.000000, mae: 3073.910889, mean_q: 25722.453125\n",
      "wrong_move\n",
      "   5778/500000: episode: 5702, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 33103664.000000, mae: 3067.958496, mean_q: 25193.410156\n",
      "wrong_move\n",
      "   5779/500000: episode: 5703, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 662.000 [662.000, 662.000],  loss: 13966052.000000, mae: 3310.963867, mean_q: 25657.572266\n",
      "wrong_move\n",
      "   5780/500000: episode: 5704, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11743444.000000, mae: 3058.354736, mean_q: 25119.986328\n",
      "wrong_move\n",
      "   5781/500000: episode: 5705, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 851.000 [851.000, 851.000],  loss: 12799532.000000, mae: 3059.139893, mean_q: 25041.074219\n",
      "wrong_move\n",
      "   5782/500000: episode: 5706, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1507.000 [1507.000, 1507.000],  loss: 11074261.000000, mae: 3046.810303, mean_q: 25145.443359\n",
      "wrong_move\n",
      "   5783/500000: episode: 5707, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 19428306.000000, mae: 3039.950928, mean_q: 24854.308594\n",
      "wrong_move\n",
      "   5785/500000: episode: 5708, duration: 0.066s, episode steps:   2, steps per second:  30, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 3031.000 [2722.000, 3340.000],  loss: 52745052.000000, mae: 3049.573975, mean_q: 25534.804688\n",
      "wrong_move\n",
      "   5786/500000: episode: 5709, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 138.000 [138.000, 138.000],  loss: 1475862.750000, mae: 3057.949951, mean_q: 24529.812500\n",
      "wrong_move\n",
      "   5787/500000: episode: 5710, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 28780876.000000, mae: 3017.660156, mean_q: 24566.210938\n",
      "wrong_move\n",
      "   5788/500000: episode: 5711, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1821.000 [1821.000, 1821.000],  loss: 18313872.000000, mae: 3016.647461, mean_q: 24516.238281\n",
      "wrong_move\n",
      "   5789/500000: episode: 5712, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 31346812.000000, mae: 3008.760986, mean_q: 24445.328125\n",
      "wrong_move\n",
      "   5790/500000: episode: 5713, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1244.000 [1244.000, 1244.000],  loss: 38670812.000000, mae: 3017.030762, mean_q: 24949.873047\n",
      "wrong_move\n",
      "   5791/500000: episode: 5714, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17365662.000000, mae: 3322.415527, mean_q: 24522.136719\n",
      "wrong_move\n",
      "   5792/500000: episode: 5715, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 19695764.000000, mae: 3016.458984, mean_q: 24231.333984\n",
      "wrong_move\n",
      "   5793/500000: episode: 5716, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6766275.500000, mae: 2986.123291, mean_q: 23780.777344\n",
      "wrong_move\n",
      "   5794/500000: episode: 5717, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2764.000 [2764.000, 2764.000],  loss: 5523644.000000, mae: 2980.945557, mean_q: 23334.457031\n",
      "wrong_move\n",
      "   5795/500000: episode: 5718, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3297.000 [3297.000, 3297.000],  loss: 55216232.000000, mae: 2976.134766, mean_q: 23197.593750\n",
      "wrong_move\n",
      "   5796/500000: episode: 5719, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9871830.000000, mae: 2976.178711, mean_q: 23263.410156\n",
      "wrong_move\n",
      "   5797/500000: episode: 5720, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 26507654.000000, mae: 2965.304688, mean_q: 22895.894531\n",
      "wrong_move\n",
      "   5798/500000: episode: 5721, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 10355705.000000, mae: 2965.188965, mean_q: 24219.082031\n",
      "wrong_move\n",
      "   5799/500000: episode: 5722, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 985508.750000, mae: 2957.646240, mean_q: 22611.027344\n",
      "wrong_move\n",
      "   5800/500000: episode: 5723, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2807.000 [2807.000, 2807.000],  loss: 21743652.000000, mae: 2955.750732, mean_q: 22506.757812\n",
      "wrong_move\n",
      "   5801/500000: episode: 5724, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3623.000 [3623.000, 3623.000],  loss: 5944135.500000, mae: 2950.843750, mean_q: 22363.789062\n",
      "wrong_move\n",
      "   5802/500000: episode: 5725, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 33671120.000000, mae: 2951.918945, mean_q: 22272.025391\n",
      "wrong_move\n",
      "   5803/500000: episode: 5726, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9633838.000000, mae: 2942.850586, mean_q: 22309.658203\n",
      "wrong_move\n",
      "   5804/500000: episode: 5727, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 955.000 [955.000, 955.000],  loss: 9825802.000000, mae: 2937.893799, mean_q: 22042.621094\n",
      "wrong_move\n",
      "   5805/500000: episode: 5728, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 31798528.000000, mae: 2935.393555, mean_q: 22760.378906\n",
      "wrong_move\n",
      "   5806/500000: episode: 5729, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 13886033.000000, mae: 2929.903564, mean_q: 21735.251953\n",
      "wrong_move\n",
      "   5807/500000: episode: 5730, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 29712004.000000, mae: 2926.520996, mean_q: 21575.519531\n",
      "wrong_move\n",
      "   5808/500000: episode: 5731, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1993.000 [1993.000, 1993.000],  loss: 72800240.000000, mae: 2923.741211, mean_q: 21364.810547\n",
      "wrong_move\n",
      "   5809/500000: episode: 5732, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 32617140.000000, mae: 2919.992432, mean_q: 21208.587891\n",
      "wrong_move\n",
      "   5810/500000: episode: 5733, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17494894.000000, mae: 2928.868164, mean_q: 21500.683594\n",
      "wrong_move\n",
      "   5811/500000: episode: 5734, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3640.000 [3640.000, 3640.000],  loss: 483363.500000, mae: 2951.435791, mean_q: 20964.910156\n",
      "wrong_move\n",
      "   5812/500000: episode: 5735, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1301.000 [1301.000, 1301.000],  loss: 41010424.000000, mae: 2911.430664, mean_q: 21362.046875\n",
      "wrong_move\n",
      "   5813/500000: episode: 5736, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 232789856.000000, mae: 2910.885742, mean_q: 20577.615234\n",
      "wrong_move\n",
      "   5814/500000: episode: 5737, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2751.000 [2751.000, 2751.000],  loss: 22430404.000000, mae: 2903.831543, mean_q: 20724.578125\n",
      "wrong_move\n",
      "   5815/500000: episode: 5738, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 72683840.000000, mae: 2981.132568, mean_q: 20730.269531\n",
      "wrong_move\n",
      "   5816/500000: episode: 5739, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 23844260.000000, mae: 2896.440918, mean_q: 20016.777344\n",
      "wrong_move\n",
      "   5817/500000: episode: 5740, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3700991.250000, mae: 2893.878174, mean_q: 20226.679688\n",
      "wrong_move\n",
      "   5818/500000: episode: 5741, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 52573080.000000, mae: 2890.326416, mean_q: 20954.564453\n",
      "wrong_move\n",
      "   5819/500000: episode: 5742, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17442586.000000, mae: 2886.189453, mean_q: 19920.400391\n",
      "wrong_move\n",
      "   5821/500000: episode: 5743, duration: 0.152s, episode steps:   2, steps per second:  13, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 10640369.000000, mae: 2882.221924, mean_q: 19308.003906\n",
      "wrong_move\n",
      "   5823/500000: episode: 5744, duration: 0.131s, episode steps:   2, steps per second:  15, episode reward: -4941.000, mean reward: -2470.500 [-5000.000, 59.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 35004632.000000, mae: 2941.492188, mean_q: 20311.917969\n",
      "wrong_move\n",
      "   5824/500000: episode: 5745, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 14131618.000000, mae: 2893.405762, mean_q: 19905.742188\n",
      "wrong_move\n",
      "   5825/500000: episode: 5746, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2521.000 [2521.000, 2521.000],  loss: 828628.125000, mae: 2925.452637, mean_q: 19551.404297\n",
      "wrong_move\n",
      "   5826/500000: episode: 5747, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2959.000 [2959.000, 2959.000],  loss: 83264880.000000, mae: 2879.585938, mean_q: 19517.101562\n",
      "wrong_move\n",
      "   5827/500000: episode: 5748, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17429960.000000, mae: 2872.713867, mean_q: 19181.279297\n",
      "wrong_move\n",
      "   5828/500000: episode: 5749, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1079.000 [1079.000, 1079.000],  loss: 32393352.000000, mae: 2872.328613, mean_q: 18708.468750\n",
      "wrong_move\n",
      "   5829/500000: episode: 5750, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9822762.000000, mae: 2872.714111, mean_q: 19235.515625\n",
      "wrong_move\n",
      "   5830/500000: episode: 5751, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 60320224.000000, mae: 2868.905273, mean_q: 18099.585938\n",
      "wrong_move\n",
      "   5831/500000: episode: 5752, duration: 0.143s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1429300.000000, mae: 2868.388672, mean_q: 18255.392578\n",
      "wrong_move\n",
      "   5832/500000: episode: 5753, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 878.000 [878.000, 878.000],  loss: 1792786.750000, mae: 2919.781250, mean_q: 19350.955078\n",
      "wrong_move\n",
      "   5833/500000: episode: 5754, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1399007.750000, mae: 2919.208984, mean_q: 19300.464844\n",
      "wrong_move\n",
      "   5834/500000: episode: 5755, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1869.000 [1869.000, 1869.000],  loss: 19691482.000000, mae: 2865.512207, mean_q: 18529.876953\n",
      "wrong_move\n",
      "   5835/500000: episode: 5756, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1191537.875000, mae: 2863.354492, mean_q: 17429.962891\n",
      "wrong_move\n",
      "   5836/500000: episode: 5757, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 14204308.000000, mae: 2860.701416, mean_q: 17627.947266\n",
      "wrong_move\n",
      "   5837/500000: episode: 5758, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 41519656.000000, mae: 2860.799805, mean_q: 17028.468750\n",
      "wrong_move\n",
      "   5839/500000: episode: 5759, duration: 0.145s, episode steps:   2, steps per second:  14, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2306.500 [1891.000, 2722.000],  loss: 11516926.000000, mae: 2875.796143, mean_q: 18001.304688\n",
      "wrong_move\n",
      "   5840/500000: episode: 5760, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8900204.000000, mae: 2878.592041, mean_q: 17562.929688\n",
      "wrong_move\n",
      "   5841/500000: episode: 5761, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9550442.000000, mae: 2854.067139, mean_q: 16867.859375\n",
      "wrong_move\n",
      "   5842/500000: episode: 5762, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 81998928.000000, mae: 2851.466797, mean_q: 17037.632812\n",
      "wrong_move\n",
      "   5843/500000: episode: 5763, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8523863.000000, mae: 2847.341309, mean_q: 16382.021484\n",
      "wrong_move\n",
      "   5844/500000: episode: 5764, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2502.000 [2502.000, 2502.000],  loss: 3116682.250000, mae: 2846.796875, mean_q: 17237.693359\n",
      "wrong_move\n",
      "   5845/500000: episode: 5765, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 19900798.000000, mae: 2845.951416, mean_q: 16528.121094\n",
      "wrong_move\n",
      "   5846/500000: episode: 5766, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 12890771.000000, mae: 2852.921875, mean_q: 16860.578125\n",
      "wrong_move\n",
      "   5847/500000: episode: 5767, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2943900.000000, mae: 2900.403320, mean_q: 15911.869141\n",
      "wrong_move\n",
      "   5848/500000: episode: 5768, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7166783.000000, mae: 2841.442871, mean_q: 16637.269531\n",
      "wrong_move\n",
      "   5849/500000: episode: 5769, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 19047282.000000, mae: 2839.648926, mean_q: 16152.095703\n",
      "wrong_move\n",
      "   5850/500000: episode: 5770, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1604.000 [1604.000, 1604.000],  loss: 31095872.000000, mae: 2843.400879, mean_q: 17024.125000\n",
      "wrong_move\n",
      "   5851/500000: episode: 5771, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3136.000 [3136.000, 3136.000],  loss: 21611522.000000, mae: 2841.359863, mean_q: 15701.658203\n",
      "wrong_move\n",
      "   5852/500000: episode: 5772, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 45549384.000000, mae: 2834.796387, mean_q: 15446.245117\n",
      "wrong_move\n",
      "   5853/500000: episode: 5773, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 16046522.000000, mae: 2832.043213, mean_q: 15697.526367\n",
      "wrong_move\n",
      "   5854/500000: episode: 5774, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3947.000 [3947.000, 3947.000],  loss: 5598768.000000, mae: 2832.642822, mean_q: 15506.098633\n",
      "wrong_move\n",
      "   5855/500000: episode: 5775, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1604.000 [1604.000, 1604.000],  loss: 15324927.000000, mae: 2829.263428, mean_q: 14864.146484\n",
      "wrong_move\n",
      "   5857/500000: episode: 5776, duration: 0.161s, episode steps:   2, steps per second:  12, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2815.500 [2722.000, 2909.000],  loss: 19070748.000000, mae: 2829.097168, mean_q: 15398.417969\n",
      "wrong_move\n",
      "   5858/500000: episode: 5777, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2959.000 [2959.000, 2959.000],  loss: 13741435.000000, mae: 2827.385986, mean_q: 15398.921875\n",
      "wrong_move\n",
      "   5860/500000: episode: 5778, duration: 0.216s, episode steps:   2, steps per second:   9, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 1555.000 [388.000, 2722.000],  loss: 36013984.000000, mae: 2829.032227, mean_q: 15276.015625\n",
      "wrong_move\n",
      "   5861/500000: episode: 5779, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1604.000 [1604.000, 1604.000],  loss: 44046292.000000, mae: 2825.499512, mean_q: 14827.400391\n",
      "wrong_move\n",
      "   5862/500000: episode: 5780, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 20661892.000000, mae: 2823.018555, mean_q: 13943.056641\n",
      "wrong_move\n",
      "   5863/500000: episode: 5781, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 97561128.000000, mae: 2830.741699, mean_q: 14614.492188\n",
      "wrong_move\n",
      "   5864/500000: episode: 5782, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1604.000 [1604.000, 1604.000],  loss: 39118896.000000, mae: 2824.925293, mean_q: 14719.998047\n",
      "wrong_move\n",
      "   5865/500000: episode: 5783, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11949939.000000, mae: 2822.781738, mean_q: 13929.948242\n",
      "wrong_move\n",
      "   5866/500000: episode: 5784, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 253331008.000000, mae: 2826.924805, mean_q: 14253.719727\n",
      "wrong_move\n",
      "   5867/500000: episode: 5785, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 15635404.000000, mae: 2833.488281, mean_q: 15684.726562\n",
      "wrong_move\n",
      "   5868/500000: episode: 5786, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1604.000 [1604.000, 1604.000],  loss: 94978864.000000, mae: 2879.092773, mean_q: 13430.048828\n",
      "wrong_move\n",
      "   5869/500000: episode: 5787, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1051308.625000, mae: 2820.470459, mean_q: 14206.796875\n",
      "wrong_move\n",
      "   5870/500000: episode: 5788, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3130.000 [3130.000, 3130.000],  loss: 2154008.000000, mae: 2819.223877, mean_q: 13591.796875\n",
      "wrong_move\n",
      "   5871/500000: episode: 5789, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11873507.000000, mae: 2819.069336, mean_q: 13529.296875\n",
      "wrong_move\n",
      "   5872/500000: episode: 5790, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 30444732.000000, mae: 2818.394287, mean_q: 13364.913086\n",
      "wrong_move\n",
      "   5873/500000: episode: 5791, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1763.000 [1763.000, 1763.000],  loss: 577550.187500, mae: 2817.764160, mean_q: 13171.976562\n",
      "wrong_move\n",
      "   5874/500000: episode: 5792, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1331624.875000, mae: 2815.816650, mean_q: 12908.833984\n",
      "wrong_move\n",
      "   5875/500000: episode: 5793, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3567907.500000, mae: 2815.897217, mean_q: 13134.613281\n",
      "wrong_move\n",
      "   5876/500000: episode: 5794, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2336.000 [2336.000, 2336.000],  loss: 32158720.000000, mae: 2816.547119, mean_q: 13271.863281\n",
      "wrong_move\n",
      "   5877/500000: episode: 5795, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 764282.750000, mae: 2820.567871, mean_q: 12016.713867\n",
      "wrong_move\n",
      "   5878/500000: episode: 5796, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1743.000 [1743.000, 1743.000],  loss: 24096556.000000, mae: 2816.244873, mean_q: 12723.195312\n",
      "wrong_move\n",
      "   5879/500000: episode: 5797, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1893477.500000, mae: 2815.929199, mean_q: 12955.752930\n",
      "wrong_move\n",
      "   5880/500000: episode: 5798, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3023.000 [3023.000, 3023.000],  loss: 34351260.000000, mae: 2819.636230, mean_q: 12904.443359\n",
      "wrong_move\n",
      "   5881/500000: episode: 5799, duration: 0.142s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 13616258.000000, mae: 2853.509277, mean_q: 12836.895508\n",
      "wrong_move\n",
      "   5882/500000: episode: 5800, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2527584.000000, mae: 2815.947266, mean_q: 12850.842773\n",
      "wrong_move\n",
      "   5883/500000: episode: 5801, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 12634518.000000, mae: 2815.567383, mean_q: 12074.413086\n",
      "wrong_move\n",
      "   5884/500000: episode: 5802, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7280575.000000, mae: 2818.929199, mean_q: 12991.358398\n",
      "wrong_move\n",
      "   5885/500000: episode: 5803, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1604.000 [1604.000, 1604.000],  loss: 10035299.000000, mae: 2814.830566, mean_q: 11968.433594\n",
      "wrong_move\n",
      "   5886/500000: episode: 5804, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 889.000 [889.000, 889.000],  loss: 43624368.000000, mae: 2890.834961, mean_q: 14234.837891\n",
      "wrong_move\n",
      "   5887/500000: episode: 5805, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2504.000 [2504.000, 2504.000],  loss: 7226086.500000, mae: 2876.169189, mean_q: 15397.486328\n",
      "wrong_move\n",
      "   5888/500000: episode: 5806, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3588413.000000, mae: 2816.690430, mean_q: 13267.697266\n",
      "wrong_move\n",
      "   5889/500000: episode: 5807, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1604.000 [1604.000, 1604.000],  loss: 8656347.000000, mae: 2838.663086, mean_q: 12848.369141\n",
      "wrong_move\n",
      "   5890/500000: episode: 5808, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7517510.000000, mae: 2812.436523, mean_q: 10840.801758\n",
      "wrong_move\n",
      "   5891/500000: episode: 5809, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1604.000 [1604.000, 1604.000],  loss: 18672748.000000, mae: 2811.441650, mean_q: 11283.514648\n",
      "wrong_move\n",
      "   5892/500000: episode: 5810, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1604.000 [1604.000, 1604.000],  loss: 3875201.500000, mae: 2810.265137, mean_q: 10685.326172\n",
      "wrong_move\n",
      "   5893/500000: episode: 5811, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11925426.000000, mae: 2812.187256, mean_q: 10736.546875\n",
      "wrong_move\n",
      "   5894/500000: episode: 5812, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 92716608.000000, mae: 2816.093262, mean_q: 12203.810547\n",
      "wrong_move\n",
      "   5895/500000: episode: 5813, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 719.000 [719.000, 719.000],  loss: 7394711.000000, mae: 2811.332520, mean_q: 11138.428711\n",
      "wrong_move\n",
      "   5896/500000: episode: 5814, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 28905838.000000, mae: 2813.199219, mean_q: 10762.798828\n",
      "wrong_move\n",
      "   5897/500000: episode: 5815, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: 1826067.625000, mae: 2811.493408, mean_q: 11267.382812\n",
      "wrong_move\n",
      "   5898/500000: episode: 5816, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1157.000 [1157.000, 1157.000],  loss: 15102560.000000, mae: 2819.255859, mean_q: 11112.685547\n",
      "wrong_move\n",
      "   5899/500000: episode: 5817, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3306.000 [3306.000, 3306.000],  loss: 13155223.000000, mae: 2820.009766, mean_q: 12341.442383\n",
      "wrong_move\n",
      "   5900/500000: episode: 5818, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17540096.000000, mae: 2820.673828, mean_q: 11285.296875\n",
      "wrong_move\n",
      "   5901/500000: episode: 5819, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1776.000 [1776.000, 1776.000],  loss: 7439806.500000, mae: 2828.068604, mean_q: 12734.336914\n",
      "wrong_move\n",
      "   5902/500000: episode: 5820, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1117.000 [1117.000, 1117.000],  loss: 44751724.000000, mae: 2853.041992, mean_q: 13612.866211\n",
      "wrong_move\n",
      "   5903/500000: episode: 5821, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17309602.000000, mae: 2850.713867, mean_q: 13401.312500\n",
      "wrong_move\n",
      "   5904/500000: episode: 5822, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2707.000 [2707.000, 2707.000],  loss: 10762871.000000, mae: 2838.711426, mean_q: 12762.675781\n",
      "wrong_move\n",
      "   5905/500000: episode: 5823, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 20306456.000000, mae: 2841.938965, mean_q: 12745.763672\n",
      "wrong_move\n",
      "   5906/500000: episode: 5824, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 21354720.000000, mae: 2844.829590, mean_q: 13315.895508\n",
      "wrong_move\n",
      "   5907/500000: episode: 5825, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 42257284.000000, mae: 2858.552734, mean_q: 14566.358398\n",
      "wrong_move\n",
      "   5908/500000: episode: 5826, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 13077614.000000, mae: 2851.247559, mean_q: 14243.001953\n",
      "wrong_move\n",
      "   5909/500000: episode: 5827, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1589.000 [1589.000, 1589.000],  loss: 9049618.000000, mae: 2852.205322, mean_q: 14575.375000\n",
      "wrong_move\n",
      "   5910/500000: episode: 5828, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4838709.000000, mae: 2893.612793, mean_q: 14104.158203\n",
      "wrong_move\n",
      "   5911/500000: episode: 5829, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4271210.500000, mae: 2850.041504, mean_q: 13313.604492\n",
      "wrong_move\n",
      "   5912/500000: episode: 5830, duration: 0.152s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6646480.500000, mae: 2852.978516, mean_q: 14084.062500\n",
      "wrong_move\n",
      "   5913/500000: episode: 5831, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5429195.500000, mae: 2903.799072, mean_q: 15016.952148\n",
      "wrong_move\n",
      "   5914/500000: episode: 5832, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7610510.000000, mae: 2852.713135, mean_q: 13979.421875\n",
      "wrong_move\n",
      "   5915/500000: episode: 5833, duration: 0.141s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5292969.000000, mae: 2967.787842, mean_q: 15445.288086\n",
      "wrong_move\n",
      "   5916/500000: episode: 5834, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17667536.000000, mae: 2847.035645, mean_q: 12714.128906\n",
      "wrong_move\n",
      "   5917/500000: episode: 5835, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 997.000 [997.000, 997.000],  loss: 46097016.000000, mae: 2844.929688, mean_q: 13890.504883\n",
      "wrong_move\n",
      "   5918/500000: episode: 5836, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 13176189.000000, mae: 2855.620605, mean_q: 13063.038086\n",
      "wrong_move\n",
      "   5919/500000: episode: 5837, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 53201552.000000, mae: 2868.756104, mean_q: 12924.085938\n",
      "wrong_move\n",
      "   5920/500000: episode: 5838, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 152.000 [152.000, 152.000],  loss: 3085552.000000, mae: 2843.026367, mean_q: 13140.399414\n",
      "wrong_move\n",
      "   5921/500000: episode: 5839, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9283612.000000, mae: 2844.929688, mean_q: 14169.878906\n",
      "wrong_move\n",
      "   5922/500000: episode: 5840, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 18381878.000000, mae: 2863.150879, mean_q: 12409.866211\n",
      "wrong_move\n",
      "   5923/500000: episode: 5841, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 712.000 [712.000, 712.000],  loss: 5273835.000000, mae: 2846.034180, mean_q: 14554.632812\n",
      "wrong_move\n",
      "   5924/500000: episode: 5842, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 21622936.000000, mae: 2848.340820, mean_q: 12360.226562\n",
      "wrong_move\n",
      "   5925/500000: episode: 5843, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3328.000 [3328.000, 3328.000],  loss: 24165988.000000, mae: 2845.050293, mean_q: 13285.797852\n",
      "wrong_move\n",
      "   5926/500000: episode: 5844, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3792955.000000, mae: 2963.616211, mean_q: 15395.683594\n",
      "wrong_move\n",
      "   5927/500000: episode: 5845, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 26786274.000000, mae: 2842.829590, mean_q: 12153.595703\n",
      "wrong_move\n",
      "   5928/500000: episode: 5846, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5613391.000000, mae: 2844.536621, mean_q: 13118.978516\n",
      "wrong_move\n",
      "   5929/500000: episode: 5847, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9628252.000000, mae: 2868.324219, mean_q: 12934.636719\n",
      "wrong_move\n",
      "   5930/500000: episode: 5848, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 18240970.000000, mae: 2841.897949, mean_q: 11864.949219\n",
      "wrong_move\n",
      "   5931/500000: episode: 5849, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7480755.000000, mae: 2841.377197, mean_q: 12250.552734\n",
      "wrong_move\n",
      "   5932/500000: episode: 5850, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1024.000 [1024.000, 1024.000],  loss: 5048718.500000, mae: 2841.803711, mean_q: 12277.796875\n",
      "wrong_move\n",
      "   5933/500000: episode: 5851, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8869400.000000, mae: 2906.372803, mean_q: 12805.932617\n",
      "wrong_move\n",
      "   5934/500000: episode: 5852, duration: 0.143s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 55467660.000000, mae: 2847.512939, mean_q: 13271.839844\n",
      "wrong_move\n",
      "   5935/500000: episode: 5853, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3555314.000000, mae: 2842.107422, mean_q: 12540.057617\n",
      "wrong_move\n",
      "   5936/500000: episode: 5854, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 395.000 [395.000, 395.000],  loss: 8966454.000000, mae: 2836.673828, mean_q: 12104.145508\n",
      "wrong_move\n",
      "   5937/500000: episode: 5855, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 50995344.000000, mae: 2903.748047, mean_q: 11478.414062\n",
      "wrong_move\n",
      "   5938/500000: episode: 5856, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8771225.000000, mae: 2840.462891, mean_q: 13489.660156\n",
      "wrong_move\n",
      "   5939/500000: episode: 5857, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6263633.000000, mae: 2844.864258, mean_q: 12457.806641\n",
      "wrong_move\n",
      "   5940/500000: episode: 5858, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 20031880.000000, mae: 2834.919678, mean_q: 12455.236328\n",
      "wrong_move\n",
      "   5941/500000: episode: 5859, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 137956448.000000, mae: 2832.667969, mean_q: 11744.062500\n",
      "wrong_move\n",
      "   5942/500000: episode: 5860, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4216949.500000, mae: 2831.830078, mean_q: 11717.647461\n",
      "wrong_move\n",
      "   5943/500000: episode: 5861, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 72953984.000000, mae: 2834.794434, mean_q: 10816.684570\n",
      "wrong_move\n",
      "   5944/500000: episode: 5862, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8585106.000000, mae: 2876.109863, mean_q: 10756.908203\n",
      "wrong_move\n",
      "   5945/500000: episode: 5863, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2272.000 [2272.000, 2272.000],  loss: 82610624.000000, mae: 2829.276123, mean_q: 10950.398438\n",
      "wrong_move\n",
      "   5946/500000: episode: 5864, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2145.000 [2145.000, 2145.000],  loss: 8859880.000000, mae: 2830.849854, mean_q: 12440.906250\n",
      "wrong_move\n",
      "   5947/500000: episode: 5865, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 48206880.000000, mae: 2828.884277, mean_q: 9790.067383\n",
      "wrong_move\n",
      "   5948/500000: episode: 5866, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 14638784.000000, mae: 2824.312012, mean_q: 9982.983398\n",
      "wrong_move\n",
      "   5949/500000: episode: 5867, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2103.000 [2103.000, 2103.000],  loss: 18857982.000000, mae: 2823.081543, mean_q: 9554.702148\n",
      "wrong_move\n",
      "   5950/500000: episode: 5868, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3102.000 [3102.000, 3102.000],  loss: 21300170.000000, mae: 2825.164062, mean_q: 11251.221680\n",
      "wrong_move\n",
      "   5951/500000: episode: 5869, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1324663.250000, mae: 2823.067871, mean_q: 9276.540039\n",
      "wrong_move\n",
      "   5952/500000: episode: 5870, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9087151.000000, mae: 2821.966797, mean_q: 9515.237305\n",
      "wrong_move\n",
      "   5953/500000: episode: 5871, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 752.000 [752.000, 752.000],  loss: 14794714.000000, mae: 2825.568359, mean_q: 10309.021484\n",
      "wrong_move\n",
      "   5954/500000: episode: 5872, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 560.000 [560.000, 560.000],  loss: 5079908.500000, mae: 2823.461426, mean_q: 8890.871094\n",
      "wrong_move\n",
      "   5955/500000: episode: 5873, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 14757124.000000, mae: 2825.125488, mean_q: 9694.094727\n",
      "wrong_move\n",
      "   5956/500000: episode: 5874, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3946035.250000, mae: 2826.262207, mean_q: 9534.617188\n",
      "wrong_move\n",
      "   5957/500000: episode: 5875, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4048.000 [4048.000, 4048.000],  loss: 1331449.750000, mae: 2824.515381, mean_q: 8895.057617\n",
      "wrong_move\n",
      "   5958/500000: episode: 5876, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 28141828.000000, mae: 2826.951416, mean_q: 10346.083008\n",
      "wrong_move\n",
      "   5959/500000: episode: 5877, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2939163.000000, mae: 2843.308594, mean_q: 9718.322266\n",
      "wrong_move\n",
      "   5960/500000: episode: 5878, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 72.000 [72.000, 72.000],  loss: 2200027.250000, mae: 2824.558594, mean_q: 8768.039062\n",
      "wrong_move\n",
      "   5961/500000: episode: 5879, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 72.000 [72.000, 72.000],  loss: 3554951.250000, mae: 2828.725586, mean_q: 9195.013672\n",
      "wrong_move\n",
      "   5962/500000: episode: 5880, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3263.000 [3263.000, 3263.000],  loss: 13409604.000000, mae: 2825.307373, mean_q: 8704.848633\n",
      "wrong_move\n",
      "   5963/500000: episode: 5881, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 72.000 [72.000, 72.000],  loss: 23735810.000000, mae: 2825.970459, mean_q: 8325.857422\n",
      "wrong_move\n",
      "   5964/500000: episode: 5882, duration: 0.156s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 872.000 [872.000, 872.000],  loss: 21340356.000000, mae: 2849.714355, mean_q: 10116.313477\n",
      "wrong_move\n",
      "   5965/500000: episode: 5883, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4048.000 [4048.000, 4048.000],  loss: 4450564.500000, mae: 2830.467285, mean_q: 8423.650391\n",
      "wrong_move\n",
      "   5966/500000: episode: 5884, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3523477.250000, mae: 2831.837158, mean_q: 8257.959961\n",
      "wrong_move\n",
      "   5967/500000: episode: 5885, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 13942917.000000, mae: 2831.979736, mean_q: 9118.662109\n",
      "wrong_move\n",
      "   5968/500000: episode: 5886, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 72.000 [72.000, 72.000],  loss: 17534510.000000, mae: 2837.683594, mean_q: 9434.044922\n",
      "wrong_move\n",
      "   5969/500000: episode: 5887, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 26153828.000000, mae: 2836.856445, mean_q: 9141.701172\n",
      "wrong_move\n",
      "   5970/500000: episode: 5888, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 14503515.000000, mae: 2834.940918, mean_q: 8857.440430\n",
      "wrong_move\n",
      "   5971/500000: episode: 5889, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 72.000 [72.000, 72.000],  loss: 38937748.000000, mae: 2845.853760, mean_q: 8961.505859\n",
      "wrong_move\n",
      "   5972/500000: episode: 5890, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1230.000 [1230.000, 1230.000],  loss: 2395478.500000, mae: 2841.119629, mean_q: 7562.660156\n",
      "wrong_move\n",
      "   5973/500000: episode: 5891, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8993729.000000, mae: 2864.761475, mean_q: 8506.398438\n",
      "wrong_move\n",
      "   5974/500000: episode: 5892, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 12974876.000000, mae: 2838.237549, mean_q: 6991.792969\n",
      "wrong_move\n",
      "   5975/500000: episode: 5893, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: 1681279.750000, mae: 2844.272949, mean_q: 8221.450195\n",
      "wrong_move\n",
      "   5976/500000: episode: 5894, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 72.000 [72.000, 72.000],  loss: 358357824.000000, mae: 2841.803711, mean_q: 6731.493164\n",
      "wrong_move\n",
      "   5977/500000: episode: 5895, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 72.000 [72.000, 72.000],  loss: 38430364.000000, mae: 2842.815918, mean_q: 7402.009277\n",
      "wrong_move\n",
      "   5978/500000: episode: 5896, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 44.000 [44.000, 44.000],  loss: 4611316.500000, mae: 2843.545410, mean_q: 7498.028809\n",
      "wrong_move\n",
      "   5979/500000: episode: 5897, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1470.000 [1470.000, 1470.000],  loss: 1194283.500000, mae: 2845.562012, mean_q: 8958.417969\n",
      "wrong_move\n",
      "   5980/500000: episode: 5898, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1839.000 [1839.000, 1839.000],  loss: 11503905.000000, mae: 2847.859863, mean_q: 8586.375000\n",
      "wrong_move\n",
      "   5981/500000: episode: 5899, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 72.000 [72.000, 72.000],  loss: 18792652.000000, mae: 2849.789795, mean_q: 8897.458984\n",
      "wrong_move\n",
      "   5982/500000: episode: 5900, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3589888.750000, mae: 2848.640625, mean_q: 7221.160645\n",
      "wrong_move\n",
      "   5983/500000: episode: 5901, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 147797040.000000, mae: 2914.216553, mean_q: 8078.760742\n",
      "wrong_move\n",
      "   5984/500000: episode: 5902, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2221.000 [2221.000, 2221.000],  loss: 8702348.000000, mae: 2873.907227, mean_q: 6990.173340\n",
      "wrong_move\n",
      "   5985/500000: episode: 5903, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 72.000 [72.000, 72.000],  loss: 18079500.000000, mae: 2851.306885, mean_q: 6755.217773\n",
      "wrong_move\n",
      "   5986/500000: episode: 5904, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 72.000 [72.000, 72.000],  loss: 905107.750000, mae: 2852.907227, mean_q: 7426.927734\n",
      "wrong_move\n",
      "   5987/500000: episode: 5905, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 435.000 [435.000, 435.000],  loss: 19857584.000000, mae: 2854.023682, mean_q: 7046.186035\n",
      "wrong_move\n",
      "   5988/500000: episode: 5906, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 72.000 [72.000, 72.000],  loss: 7810111.000000, mae: 2856.106934, mean_q: 7466.736816\n",
      "wrong_move\n",
      "   5989/500000: episode: 5907, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 72.000 [72.000, 72.000],  loss: 1470738.750000, mae: 2866.097168, mean_q: 8111.391113\n",
      "wrong_move\n",
      "   5990/500000: episode: 5908, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3336.000 [3336.000, 3336.000],  loss: 6377065.000000, mae: 2857.913086, mean_q: 6671.949219\n",
      "wrong_move\n",
      "   5991/500000: episode: 5909, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3664.000 [3664.000, 3664.000],  loss: 44735528.000000, mae: 3032.050781, mean_q: 11709.901367\n",
      "wrong_move\n",
      "   5992/500000: episode: 5910, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3664.000 [3664.000, 3664.000],  loss: 1746244.875000, mae: 2872.502686, mean_q: 9034.856445\n",
      "wrong_move\n",
      "   5993/500000: episode: 5911, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3885.000 [3885.000, 3885.000],  loss: 543675.500000, mae: 2858.700195, mean_q: 6604.279297\n",
      "wrong_move\n",
      "   5994/500000: episode: 5912, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1708.000 [1708.000, 1708.000],  loss: 6329113.000000, mae: 2889.432129, mean_q: 7790.462891\n",
      "wrong_move\n",
      "   5995/500000: episode: 5913, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1094.000 [1094.000, 1094.000],  loss: 18798874.000000, mae: 2891.295654, mean_q: 7279.080078\n",
      "wrong_move\n",
      "   5996/500000: episode: 5914, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 44.000 [44.000, 44.000],  loss: 13250294.000000, mae: 2864.877686, mean_q: 7753.699707\n",
      "wrong_move\n",
      "   5997/500000: episode: 5915, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2043230.000000, mae: 2866.873047, mean_q: 8530.946289\n",
      "wrong_move\n",
      "   5998/500000: episode: 5916, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 24689182.000000, mae: 2872.850342, mean_q: 9610.191406\n",
      "wrong_move\n",
      "   5999/500000: episode: 5917, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 16722618.000000, mae: 2871.652344, mean_q: 9227.400391\n",
      "wrong_move\n",
      "   6000/500000: episode: 5918, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2654.000 [2654.000, 2654.000],  loss: 48945892.000000, mae: 2874.872070, mean_q: 9714.145508\n",
      "wrong_move\n",
      "   6001/500000: episode: 5919, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1974011.875000, mae: 2875.832275, mean_q: 9960.781250\n",
      "wrong_move\n",
      "   6002/500000: episode: 5920, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 436.000 [436.000, 436.000],  loss: 24122016.000000, mae: 2880.526611, mean_q: 10811.185547\n",
      "wrong_move\n",
      "   6003/500000: episode: 5921, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 771268.937500, mae: 2885.581543, mean_q: 10980.291016\n",
      "wrong_move\n",
      "   6004/500000: episode: 5922, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4150785.500000, mae: 2882.074463, mean_q: 11098.015625\n",
      "wrong_move\n",
      "   6005/500000: episode: 5923, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 62803612.000000, mae: 2883.185547, mean_q: 9862.029297\n",
      "wrong_move\n",
      "   6006/500000: episode: 5924, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4202149.000000, mae: 2887.519775, mean_q: 10280.526367\n",
      "wrong_move\n",
      "   6007/500000: episode: 5925, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 16259018.000000, mae: 2887.675293, mean_q: 9991.843750\n",
      "wrong_move\n",
      "   6008/500000: episode: 5926, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 458.000 [458.000, 458.000],  loss: 2442407.750000, mae: 3133.116455, mean_q: 15154.009766\n",
      "wrong_move\n",
      "   6009/500000: episode: 5927, duration: 0.153s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2845046.000000, mae: 2892.774902, mean_q: 13459.895508\n",
      "wrong_move\n",
      "   6010/500000: episode: 5928, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 48096148.000000, mae: 2887.202881, mean_q: 9912.071289\n",
      "wrong_move\n",
      "   6011/500000: episode: 5929, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11287339.000000, mae: 2913.408936, mean_q: 10977.935547\n",
      "wrong_move\n",
      "   6012/500000: episode: 5930, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 803.000 [803.000, 803.000],  loss: 316617312.000000, mae: 2893.002930, mean_q: 11603.034180\n",
      "wrong_move\n",
      "   6013/500000: episode: 5931, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 28374974.000000, mae: 2886.622559, mean_q: 9052.271484\n",
      "wrong_move\n",
      "   6014/500000: episode: 5932, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 12077559.000000, mae: 2887.212158, mean_q: 9054.853516\n",
      "wrong_move\n",
      "   6015/500000: episode: 5933, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 33141938.000000, mae: 2889.221436, mean_q: 10706.122070\n",
      "wrong_move\n",
      "   6016/500000: episode: 5934, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2374754.500000, mae: 2888.241699, mean_q: 9488.408203\n",
      "wrong_move\n",
      "   6017/500000: episode: 5935, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1758.000 [1758.000, 1758.000],  loss: 2790784.750000, mae: 2896.222168, mean_q: 10774.254883\n",
      "wrong_move\n",
      "   6018/500000: episode: 5936, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2172.000 [2172.000, 2172.000],  loss: 59863020.000000, mae: 2899.716309, mean_q: 12242.536133\n",
      "wrong_move\n",
      "   6019/500000: episode: 5937, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6629631.000000, mae: 2898.293457, mean_q: 11004.620117\n",
      "wrong_move\n",
      "   6020/500000: episode: 5938, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3775089.500000, mae: 2903.776367, mean_q: 11780.523438\n",
      "wrong_move\n",
      "   6021/500000: episode: 5939, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11699076.000000, mae: 2938.801025, mean_q: 12437.639648\n",
      "wrong_move\n",
      "   6022/500000: episode: 5940, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 13419147.000000, mae: 2911.152344, mean_q: 12358.630859\n",
      "wrong_move\n",
      "   6023/500000: episode: 5941, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 26678826.000000, mae: 2911.530518, mean_q: 13869.345703\n",
      "wrong_move\n",
      "   6024/500000: episode: 5942, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7331683.000000, mae: 2908.979492, mean_q: 12565.493164\n",
      "wrong_move\n",
      "   6025/500000: episode: 5943, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2924.000 [2924.000, 2924.000],  loss: 10180669.000000, mae: 3077.224121, mean_q: 14067.408203\n",
      "wrong_move\n",
      "   6027/500000: episode: 5944, duration: 0.125s, episode steps:   2, steps per second:  16, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11744768.000000, mae: 2943.744141, mean_q: 11916.636719\n",
      "wrong_move\n",
      "   6028/500000: episode: 5945, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 33682712.000000, mae: 2908.494141, mean_q: 13106.885742\n",
      "wrong_move\n",
      "   6030/500000: episode: 5946, duration: 0.265s, episode steps:   2, steps per second:   8, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1383263.750000, mae: 2901.937744, mean_q: 12350.729492\n",
      "wrong_move\n",
      "   6031/500000: episode: 5947, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1185560.625000, mae: 2895.496826, mean_q: 11226.539062\n",
      "wrong_move\n",
      "   6032/500000: episode: 5948, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1754934.500000, mae: 2957.805176, mean_q: 11924.404297\n",
      "wrong_move\n",
      "   6033/500000: episode: 5949, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3489.000 [3489.000, 3489.000],  loss: 27070012.000000, mae: 2898.583496, mean_q: 11600.712891\n",
      "wrong_move\n",
      "   6034/500000: episode: 5950, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2789736.500000, mae: 2891.655029, mean_q: 11822.892578\n",
      "wrong_move\n",
      "   6035/500000: episode: 5951, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1873.000 [1873.000, 1873.000],  loss: 17326096.000000, mae: 2890.985840, mean_q: 12682.947266\n",
      "wrong_move\n",
      "   6036/500000: episode: 5952, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9922986.000000, mae: 2891.797607, mean_q: 11229.794922\n",
      "wrong_move\n",
      "   6037/500000: episode: 5953, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 33642732.000000, mae: 2894.685791, mean_q: 10970.412109\n",
      "wrong_move\n",
      "   6039/500000: episode: 5954, duration: 0.153s, episode steps:   2, steps per second:  13, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 1761.000 [800.000, 2722.000],  loss: 28254142.000000, mae: 2917.963867, mean_q: 13364.681641\n",
      "wrong_move\n",
      "   6040/500000: episode: 5955, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 944.000 [944.000, 944.000],  loss: 29812122.000000, mae: 2891.712158, mean_q: 10528.311523\n",
      "wrong_move\n",
      "   6041/500000: episode: 5956, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1863174.500000, mae: 3042.007812, mean_q: 16037.275391\n",
      "wrong_move\n",
      "   6042/500000: episode: 5957, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3308.000 [3308.000, 3308.000],  loss: 13367550.000000, mae: 2904.770020, mean_q: 12307.558594\n",
      "wrong_move\n",
      "   6044/500000: episode: 5958, duration: 0.140s, episode steps:   2, steps per second:  14, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5558693.000000, mae: 3035.903320, mean_q: 12826.704102\n",
      "wrong_move\n",
      "   6045/500000: episode: 5959, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 2834709.250000, mae: 2896.562256, mean_q: 10865.939453\n",
      "wrong_move\n",
      "   6046/500000: episode: 5960, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1392.000 [1392.000, 1392.000],  loss: 25168364.000000, mae: 2882.776611, mean_q: 10805.951172\n",
      "wrong_move\n",
      "   6047/500000: episode: 5961, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2160.000 [2160.000, 2160.000],  loss: 1463577728.000000, mae: 2888.969727, mean_q: 12029.601562\n",
      "wrong_move\n",
      "   6048/500000: episode: 5962, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 140592064.000000, mae: 2904.659912, mean_q: 11699.133789\n",
      "wrong_move\n",
      "   6049/500000: episode: 5963, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 14838754.000000, mae: 3000.463379, mean_q: 14957.812500\n",
      "wrong_move\n",
      "   6050/500000: episode: 5964, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 16713774.000000, mae: 2878.809082, mean_q: 10848.693359\n",
      "wrong_move\n",
      "   6051/500000: episode: 5965, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 746324.750000, mae: 2877.592285, mean_q: 9932.796875\n",
      "wrong_move\n",
      "   6052/500000: episode: 5966, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1703.000 [1703.000, 1703.000],  loss: 6136710.500000, mae: 2878.117188, mean_q: 10032.721680\n",
      "wrong_move\n",
      "   6053/500000: episode: 5967, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 154295504.000000, mae: 2902.224609, mean_q: 11161.526367\n",
      "wrong_move\n",
      "   6054/500000: episode: 5968, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3640.000 [3640.000, 3640.000],  loss: 17841970.000000, mae: 2878.127441, mean_q: 9519.591797\n",
      "wrong_move\n",
      "   6055/500000: episode: 5969, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 312815712.000000, mae: 2885.242188, mean_q: 11244.790039\n",
      "wrong_move\n",
      "   6056/500000: episode: 5970, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 43838304.000000, mae: 2878.522461, mean_q: 10465.806641\n",
      "wrong_move\n",
      "   6057/500000: episode: 5971, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 330.000 [330.000, 330.000],  loss: 13802276.000000, mae: 2880.065918, mean_q: 9654.982422\n",
      "wrong_move\n",
      "   6058/500000: episode: 5972, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2160.000 [2160.000, 2160.000],  loss: 13070048.000000, mae: 2879.478027, mean_q: 9961.480469\n",
      "wrong_move\n",
      "   6059/500000: episode: 5973, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2831.000 [2831.000, 2831.000],  loss: 20003320.000000, mae: 2878.557373, mean_q: 10392.167969\n",
      "wrong_move\n",
      "   6060/500000: episode: 5974, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 19142808.000000, mae: 2952.164795, mean_q: 9327.517578\n",
      "wrong_move\n",
      "   6061/500000: episode: 5975, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3249.000 [3249.000, 3249.000],  loss: 1705578.000000, mae: 2881.154053, mean_q: 10017.788086\n",
      "wrong_move\n",
      "   6062/500000: episode: 5976, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 624.000 [624.000, 624.000],  loss: 7581239.000000, mae: 2878.213379, mean_q: 10037.472656\n",
      "wrong_move\n",
      "   6063/500000: episode: 5977, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11219340.000000, mae: 2918.103027, mean_q: 9650.234375\n",
      "wrong_move\n",
      "   6064/500000: episode: 5978, duration: 0.156s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 634.000 [634.000, 634.000],  loss: 56664620.000000, mae: 2935.798096, mean_q: 9435.293945\n",
      "wrong_move\n",
      "   6065/500000: episode: 5979, duration: 0.142s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3352.000 [3352.000, 3352.000],  loss: 11159620.000000, mae: 2878.566895, mean_q: 9271.087891\n",
      "wrong_move\n",
      "   6066/500000: episode: 5980, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 28557242.000000, mae: 2889.298828, mean_q: 9399.805664\n",
      "wrong_move\n",
      "   6068/500000: episode: 5981, duration: 0.102s, episode steps:   2, steps per second:  20, episode reward: -4951.000, mean reward: -2475.500 [-5000.000, 49.000], mean action: 731.000 [731.000, 731.000],  loss: 14217169.000000, mae: 2931.190918, mean_q: 10927.125000\n",
      "wrong_move\n",
      "   6069/500000: episode: 5982, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 983223.000000, mae: 2879.177979, mean_q: 9096.343750\n",
      "wrong_move\n",
      "   6070/500000: episode: 5983, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9610716.000000, mae: 2879.285156, mean_q: 8419.695312\n",
      "wrong_move\n",
      "   6071/500000: episode: 5984, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3747.000 [3747.000, 3747.000],  loss: 4774032.500000, mae: 2879.569336, mean_q: 8725.597656\n",
      "wrong_move\n",
      "   6072/500000: episode: 5985, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5688457.000000, mae: 2880.326660, mean_q: 8983.770508\n",
      "wrong_move\n",
      "   6073/500000: episode: 5986, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3172.000 [3172.000, 3172.000],  loss: 4419363.500000, mae: 2890.006836, mean_q: 8673.932617\n",
      "wrong_move\n",
      "   6074/500000: episode: 5987, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 34570260.000000, mae: 2890.751953, mean_q: 10551.617188\n",
      "wrong_move\n",
      "   6075/500000: episode: 5988, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2357.000 [2357.000, 2357.000],  loss: 3370717.500000, mae: 2893.878418, mean_q: 9592.311523\n",
      "wrong_move\n",
      "   6076/500000: episode: 5989, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 12212070.000000, mae: 2887.649414, mean_q: 9729.963867\n",
      "wrong_move\n",
      "   6077/500000: episode: 5990, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 15190156.000000, mae: 2885.991211, mean_q: 8960.915039\n",
      "wrong_move\n",
      "   6078/500000: episode: 5991, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 25300190.000000, mae: 2895.225830, mean_q: 9029.522461\n",
      "wrong_move\n",
      "   6080/500000: episode: 5992, duration: 0.113s, episode steps:   2, steps per second:  18, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 731.000 [731.000, 731.000],  loss: 18706216.000000, mae: 2889.585449, mean_q: 9890.285156\n",
      "wrong_move\n",
      "   6081/500000: episode: 5993, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3809148.000000, mae: 2883.179688, mean_q: 8605.341797\n",
      "wrong_move\n",
      "   6082/500000: episode: 5994, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2874.000 [2874.000, 2874.000],  loss: 20352670.000000, mae: 2896.365234, mean_q: 9742.599609\n",
      "wrong_move\n",
      "   6084/500000: episode: 5995, duration: 0.097s, episode steps:   2, steps per second:  21, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 25248400.000000, mae: 2885.240234, mean_q: 9166.039062\n",
      "wrong_move\n",
      "   6086/500000: episode: 5996, duration: 0.214s, episode steps:   2, steps per second:   9, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 731.000 [731.000, 731.000],  loss: 3045723.500000, mae: 2888.115723, mean_q: 10093.400391\n",
      "wrong_move\n",
      "   6087/500000: episode: 5997, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 12430603.000000, mae: 2884.821289, mean_q: 8966.619141\n",
      "wrong_move\n",
      "   6088/500000: episode: 5998, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 994.000 [994.000, 994.000],  loss: 2968238.750000, mae: 2884.189453, mean_q: 9065.255859\n",
      "wrong_move\n",
      "   6089/500000: episode: 5999, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 41111668.000000, mae: 2890.560791, mean_q: 9720.357422\n",
      "wrong_move\n",
      "   6090/500000: episode: 6000, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 297.000 [297.000, 297.000],  loss: 1065733.000000, mae: 2890.252686, mean_q: 8230.507812\n",
      "wrong_move\n",
      "   6091/500000: episode: 6001, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 19526858.000000, mae: 2886.154541, mean_q: 8811.401367\n",
      "wrong_move\n",
      "   6092/500000: episode: 6002, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 38644520.000000, mae: 2883.844727, mean_q: 8458.347656\n",
      "wrong_move\n",
      "   6094/500000: episode: 6003, duration: 0.068s, episode steps:   2, steps per second:  29, episode reward: -5021.000, mean reward: -2510.500 [-5000.000, -21.000], mean action: 1094.500 [731.000, 1458.000],  loss: 38841196.000000, mae: 2879.608398, mean_q: 7808.899902\n",
      "wrong_move\n",
      "   6095/500000: episode: 6004, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 7842253.500000, mae: 2882.414551, mean_q: 8679.388672\n",
      "wrong_move\n",
      "   6097/500000: episode: 6005, duration: 0.133s, episode steps:   2, steps per second:  15, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 731.000 [731.000, 731.000],  loss: 9203573.000000, mae: 2918.848145, mean_q: 9323.851562\n",
      "wrong_move\n",
      "   6098/500000: episode: 6006, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 108114360.000000, mae: 2892.128418, mean_q: 9817.868164\n",
      "wrong_move\n",
      "   6099/500000: episode: 6007, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 10086360.000000, mae: 2884.731445, mean_q: 8530.945312\n",
      "wrong_move\n",
      "   6100/500000: episode: 6008, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5569478.000000, mae: 2882.390625, mean_q: 8116.751465\n",
      "wrong_move\n",
      "   6101/500000: episode: 6009, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 3870809.000000, mae: 2878.971680, mean_q: 7240.053711\n",
      "wrong_move\n",
      "   6102/500000: episode: 6010, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 15865590.000000, mae: 2882.927734, mean_q: 6797.700195\n",
      "wrong_move\n",
      "   6103/500000: episode: 6011, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3226.000 [3226.000, 3226.000],  loss: 3726313.250000, mae: 2882.436768, mean_q: 7761.608887\n",
      "wrong_move\n",
      "   6104/500000: episode: 6012, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 16248851.000000, mae: 2879.644043, mean_q: 7777.649414\n",
      "wrong_move\n",
      "   6105/500000: episode: 6013, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 866038.500000, mae: 2884.757324, mean_q: 8400.260742\n",
      "wrong_move\n",
      "   6106/500000: episode: 6014, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1692640.000000, mae: 2893.653076, mean_q: 9131.343750\n",
      "wrong_move\n",
      "   6107/500000: episode: 6015, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 22185782.000000, mae: 2881.968506, mean_q: 7799.359375\n",
      "wrong_move\n",
      "   6108/500000: episode: 6016, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 5232184.500000, mae: 2880.747559, mean_q: 7268.436523\n",
      "wrong_move\n",
      "   6109/500000: episode: 6017, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 11044002.000000, mae: 2883.314941, mean_q: 8248.089844\n",
      "wrong_move\n",
      "   6110/500000: episode: 6018, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 15349359.000000, mae: 2910.724121, mean_q: 9549.696289\n",
      "wrong_move\n",
      "   6111/500000: episode: 6019, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 3820283.000000, mae: 2887.210205, mean_q: 9636.881836\n",
      "wrong_move\n",
      "   6112/500000: episode: 6020, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 228.000 [228.000, 228.000],  loss: 155401764864.000000, mae: 3295.878418, mean_q: 17623.634766\n",
      "wrong_move\n",
      "   6113/500000: episode: 6021, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 7024901.000000, mae: 2888.358398, mean_q: 7921.542969\n",
      "wrong_move\n",
      "   6114/500000: episode: 6022, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3490.000 [3490.000, 3490.000],  loss: 37113992.000000, mae: 2903.031250, mean_q: 9142.302734\n",
      "wrong_move\n",
      "   6116/500000: episode: 6023, duration: 0.070s, episode steps:   2, steps per second:  29, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 731.000 [731.000, 731.000],  loss: 104959568.000000, mae: 2901.778809, mean_q: 9872.533203\n",
      "wrong_move\n",
      "   6117/500000: episode: 6024, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 10720534.000000, mae: 2941.535156, mean_q: 11438.517578\n",
      "wrong_move\n",
      "   6118/500000: episode: 6025, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 211.000 [211.000, 211.000],  loss: 3937779.500000, mae: 2912.002930, mean_q: 10534.669922\n",
      "wrong_move\n",
      "   6119/500000: episode: 6026, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 15951051.000000, mae: 2897.471191, mean_q: 10530.978516\n",
      "wrong_move\n",
      "   6121/500000: episode: 6027, duration: 0.091s, episode steps:   2, steps per second:  22, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 731.000 [731.000, 731.000],  loss: 41877336.000000, mae: 2901.824219, mean_q: 9487.873047\n",
      "wrong_move\n",
      "   6122/500000: episode: 6028, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 1307555.000000, mae: 2988.676758, mean_q: 14044.975586\n",
      "wrong_move\n",
      "   6123/500000: episode: 6029, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3212.000 [3212.000, 3212.000],  loss: 9474061.000000, mae: 2896.979004, mean_q: 9793.825195\n",
      "wrong_move\n",
      "   6124/500000: episode: 6030, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 62348700.000000, mae: 2898.327637, mean_q: 9645.231445\n",
      "wrong_move\n",
      "   6125/500000: episode: 6031, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 1754834.750000, mae: 2895.558594, mean_q: 9210.850586\n",
      "wrong_move\n",
      "   6126/500000: episode: 6032, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2605.000 [2605.000, 2605.000],  loss: 16804674.000000, mae: 2896.191895, mean_q: 9057.046875\n",
      "wrong_move\n",
      "   6127/500000: episode: 6033, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 119602968.000000, mae: 2993.182373, mean_q: 9778.763672\n",
      "wrong_move\n",
      "   6128/500000: episode: 6034, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3434.000 [3434.000, 3434.000],  loss: 3535413.000000, mae: 2900.366699, mean_q: 10919.523438\n",
      "wrong_move\n",
      "   6129/500000: episode: 6035, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 26882528.000000, mae: 2902.237793, mean_q: 10627.240234\n",
      "wrong_move\n",
      "   6130/500000: episode: 6036, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 5841660.000000, mae: 2898.170166, mean_q: 9905.644531\n",
      "wrong_move\n",
      "   6131/500000: episode: 6037, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 3499305.250000, mae: 2954.852295, mean_q: 10456.507812\n",
      "wrong_move\n",
      "   6132/500000: episode: 6038, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2723202.000000, mae: 2912.699707, mean_q: 12852.327148\n",
      "wrong_move\n",
      "   6134/500000: episode: 6039, duration: 0.133s, episode steps:   2, steps per second:  15, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 731.000 [731.000, 731.000],  loss: 10664837.000000, mae: 2898.916748, mean_q: 10028.699219\n",
      "wrong_move\n",
      "   6136/500000: episode: 6040, duration: 0.184s, episode steps:   2, steps per second:  11, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 915.500 [731.000, 1100.000],  loss: 5589025.500000, mae: 2897.221924, mean_q: 9162.912109\n",
      "wrong_move\n",
      "   6137/500000: episode: 6041, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 12681171.000000, mae: 2900.725342, mean_q: 10558.154297\n",
      "wrong_move\n",
      "   6138/500000: episode: 6042, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 9995012.000000, mae: 2898.746582, mean_q: 9823.776367\n",
      "wrong_move\n",
      "   6139/500000: episode: 6043, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 740.000 [740.000, 740.000],  loss: 13056579.000000, mae: 2979.052734, mean_q: 8778.324219\n",
      "wrong_move\n",
      "   6140/500000: episode: 6044, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 23784484.000000, mae: 2906.964844, mean_q: 10872.853516\n",
      "wrong_move\n",
      "   6141/500000: episode: 6045, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2388.000 [2388.000, 2388.000],  loss: 1432711.000000, mae: 2950.202881, mean_q: 9215.705078\n",
      "wrong_move\n",
      "   6142/500000: episode: 6046, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1152240.500000, mae: 2899.976562, mean_q: 7939.331055\n",
      "wrong_move\n",
      "   6143/500000: episode: 6047, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3695.000 [3695.000, 3695.000],  loss: 1809318.125000, mae: 2903.691162, mean_q: 10180.216797\n",
      "wrong_move\n",
      "   6144/500000: episode: 6048, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 2430279.750000, mae: 3227.915039, mean_q: 14967.080078\n",
      "wrong_move\n",
      "   6145/500000: episode: 6049, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3066.000 [3066.000, 3066.000],  loss: 8769658.000000, mae: 2904.410645, mean_q: 8606.252930\n",
      "wrong_move\n",
      "   6146/500000: episode: 6050, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 20824928.000000, mae: 3005.201660, mean_q: 10007.324219\n",
      "wrong_move\n",
      "   6147/500000: episode: 6051, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6713694.500000, mae: 2906.701172, mean_q: 9703.797852\n",
      "wrong_move\n",
      "   6148/500000: episode: 6052, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 46.000 [46.000, 46.000],  loss: 3622916.250000, mae: 2904.653320, mean_q: 9242.341797\n",
      "wrong_move\n",
      "   6149/500000: episode: 6053, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 30672914.000000, mae: 2903.199707, mean_q: 7006.719727\n",
      "wrong_move\n",
      "   6150/500000: episode: 6054, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1045.000 [1045.000, 1045.000],  loss: 2689898.000000, mae: 2903.748535, mean_q: 7460.400391\n",
      "wrong_move\n",
      "   6151/500000: episode: 6055, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4004.000 [4004.000, 4004.000],  loss: 4067181.500000, mae: 2905.843262, mean_q: 8909.719727\n",
      "wrong_move\n",
      "   6152/500000: episode: 6056, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 6.000 [6.000, 6.000],  loss: 26396076.000000, mae: 2910.014160, mean_q: 8633.322266\n",
      "wrong_move\n",
      "   6153/500000: episode: 6057, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2676.000 [2676.000, 2676.000],  loss: 19596784.000000, mae: 2924.208252, mean_q: 7715.631836\n",
      "wrong_move\n",
      "   6154/500000: episode: 6058, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 318.000 [318.000, 318.000],  loss: 23755188.000000, mae: 2935.609131, mean_q: 10078.808594\n",
      "wrong_move\n",
      "   6155/500000: episode: 6059, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1133.000 [1133.000, 1133.000],  loss: 124136320.000000, mae: 2947.229736, mean_q: 9884.349609\n",
      "wrong_move\n",
      "   6156/500000: episode: 6060, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3795.000 [3795.000, 3795.000],  loss: 7421948.500000, mae: 2910.312256, mean_q: 7768.625977\n",
      "wrong_move\n",
      "   6157/500000: episode: 6061, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1298.000 [1298.000, 1298.000],  loss: 24761342.000000, mae: 2913.170654, mean_q: 8817.807617\n",
      "wrong_move\n",
      "   6158/500000: episode: 6062, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1513.000 [1513.000, 1513.000],  loss: 2563542.250000, mae: 2915.511963, mean_q: 8190.217285\n",
      "wrong_move\n",
      "   6159/500000: episode: 6063, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4065.000 [4065.000, 4065.000],  loss: 8840316.000000, mae: 3028.826172, mean_q: 8701.751953\n",
      "wrong_move\n",
      "   6160/500000: episode: 6064, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9594805.000000, mae: 2920.955566, mean_q: 10064.702148\n",
      "wrong_move\n",
      "   6161/500000: episode: 6065, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 31935440.000000, mae: 2921.286865, mean_q: 9153.769531\n",
      "wrong_move\n",
      "   6162/500000: episode: 6066, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3396.000 [3396.000, 3396.000],  loss: 4250658.000000, mae: 2923.172363, mean_q: 7728.216797\n",
      "wrong_move\n",
      "   6163/500000: episode: 6067, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3795.000 [3795.000, 3795.000],  loss: 7241098.000000, mae: 2926.206299, mean_q: 7737.275879\n",
      "wrong_move\n",
      "   6164/500000: episode: 6068, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3917.000 [3917.000, 3917.000],  loss: 3578963.750000, mae: 2926.124268, mean_q: 8534.067383\n",
      "wrong_move\n",
      "   6165/500000: episode: 6069, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2923.000 [2923.000, 2923.000],  loss: 18758404.000000, mae: 2960.729492, mean_q: 9705.615234\n",
      "wrong_move\n",
      "   6166/500000: episode: 6070, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 214.000 [214.000, 214.000],  loss: 15009669.000000, mae: 2930.769775, mean_q: 9200.750000\n",
      "wrong_move\n",
      "   6167/500000: episode: 6071, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3795.000 [3795.000, 3795.000],  loss: 2108440.500000, mae: 2935.910645, mean_q: 8510.548828\n",
      "wrong_move\n",
      "   6168/500000: episode: 6072, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3795.000 [3795.000, 3795.000],  loss: 9476450.000000, mae: 2928.997070, mean_q: 6937.096191\n",
      "wrong_move\n",
      "   6169/500000: episode: 6073, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3795.000 [3795.000, 3795.000],  loss: 8785568.000000, mae: 2930.207031, mean_q: 7152.023438\n",
      "wrong_move\n",
      "   6170/500000: episode: 6074, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3795.000 [3795.000, 3795.000],  loss: 2738913.000000, mae: 2931.191895, mean_q: 7541.674805\n",
      "wrong_move\n",
      "   6171/500000: episode: 6075, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3651.000 [3651.000, 3651.000],  loss: 23468148.000000, mae: 2933.156738, mean_q: 7917.257324\n",
      "wrong_move\n",
      "   6172/500000: episode: 6076, duration: 0.150s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3795.000 [3795.000, 3795.000],  loss: 3562069.500000, mae: 2940.266113, mean_q: 9819.081055\n",
      "wrong_move\n",
      "   6173/500000: episode: 6077, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3325.000 [3325.000, 3325.000],  loss: 16896584.000000, mae: 2932.947266, mean_q: 7001.760742\n",
      "wrong_move\n",
      "   6174/500000: episode: 6078, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 298.000 [298.000, 298.000],  loss: 7618468.000000, mae: 2938.291992, mean_q: 8350.241211\n",
      "wrong_move\n",
      "   6175/500000: episode: 6079, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3795.000 [3795.000, 3795.000],  loss: 470618080.000000, mae: 2954.484375, mean_q: 8326.837891\n",
      "wrong_move\n",
      "   6176/500000: episode: 6080, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2923.000 [2923.000, 2923.000],  loss: 8120054.000000, mae: 2936.416260, mean_q: 7494.874023\n",
      "wrong_move\n",
      "   6177/500000: episode: 6081, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3795.000 [3795.000, 3795.000],  loss: 6814451.000000, mae: 2937.772461, mean_q: 7647.299316\n",
      "wrong_move\n",
      "   6178/500000: episode: 6082, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 22935584.000000, mae: 2949.509277, mean_q: 7727.700195\n",
      "wrong_move\n",
      "   6179/500000: episode: 6083, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3795.000 [3795.000, 3795.000],  loss: 6982409.500000, mae: 2936.691650, mean_q: 5966.116699\n",
      "wrong_move\n",
      "   6180/500000: episode: 6084, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 28410326.000000, mae: 2938.804199, mean_q: 7401.224609\n",
      "wrong_move\n",
      "   6181/500000: episode: 6085, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3795.000 [3795.000, 3795.000],  loss: 5596977.000000, mae: 2962.820801, mean_q: 7621.610352\n",
      "wrong_move\n",
      "   6182/500000: episode: 6086, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3795.000 [3795.000, 3795.000],  loss: 9379632.000000, mae: 2950.704590, mean_q: 7001.643555\n",
      "wrong_move\n",
      "   6183/500000: episode: 6087, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2670.000 [2670.000, 2670.000],  loss: 16774214.000000, mae: 2937.946777, mean_q: 6424.598633\n",
      "wrong_move\n",
      "   6184/500000: episode: 6088, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3795.000 [3795.000, 3795.000],  loss: 3051507.500000, mae: 2937.240723, mean_q: 5336.069336\n",
      "wrong_move\n",
      "   6185/500000: episode: 6089, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1798.000 [1798.000, 1798.000],  loss: 3357795.500000, mae: 2941.305664, mean_q: 7845.635742\n",
      "wrong_move\n",
      "   6186/500000: episode: 6090, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1552.000 [1552.000, 1552.000],  loss: 5189160.000000, mae: 2955.095703, mean_q: 7832.401855\n",
      "wrong_move\n",
      "   6187/500000: episode: 6091, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2676.000 [2676.000, 2676.000],  loss: 7556766.500000, mae: 3742.400879, mean_q: 17479.472656\n",
      "wrong_move\n",
      "   6188/500000: episode: 6092, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 21561764.000000, mae: 3695.656738, mean_q: 18279.220703\n",
      "wrong_move\n",
      "   6189/500000: episode: 6093, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 563666.125000, mae: 2940.778564, mean_q: 6818.590820\n",
      "wrong_move\n",
      "   6190/500000: episode: 6094, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1496.000 [1496.000, 1496.000],  loss: 6704387.000000, mae: 3085.348633, mean_q: 10044.267578\n",
      "wrong_move\n",
      "   6191/500000: episode: 6095, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1798.000 [1798.000, 1798.000],  loss: 22101548.000000, mae: 2963.828613, mean_q: 7830.128418\n",
      "wrong_move\n",
      "   6192/500000: episode: 6096, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6724028.000000, mae: 2940.357910, mean_q: 6436.845703\n",
      "wrong_move\n",
      "   6193/500000: episode: 6097, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2772.000 [2772.000, 2772.000],  loss: 6949380.000000, mae: 2942.891602, mean_q: 7100.856934\n",
      "wrong_move\n",
      "   6194/500000: episode: 6098, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2341.000 [2341.000, 2341.000],  loss: 4193190.000000, mae: 2940.204590, mean_q: 7376.181641\n",
      "wrong_move\n",
      "   6195/500000: episode: 6099, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 1415575.750000, mae: 2940.335205, mean_q: 6794.387207\n",
      "wrong_move\n",
      "   6196/500000: episode: 6100, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17350028.000000, mae: 2954.421875, mean_q: 8277.351562\n",
      "wrong_move\n",
      "   6197/500000: episode: 6101, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7136035.000000, mae: 2939.933594, mean_q: 5742.913574\n",
      "wrong_move\n",
      "   6198/500000: episode: 6102, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3942.000 [3942.000, 3942.000],  loss: 1917635.500000, mae: 2963.253906, mean_q: 8204.513672\n",
      "wrong_move\n",
      "   6199/500000: episode: 6103, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3027.000 [3027.000, 3027.000],  loss: 11795913.000000, mae: 3047.553955, mean_q: 11517.289062\n",
      "wrong_move\n",
      "   6200/500000: episode: 6104, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1654.000 [1654.000, 1654.000],  loss: 3268703.000000, mae: 2964.536621, mean_q: 6513.390625\n",
      "wrong_move\n",
      "   6201/500000: episode: 6105, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 534.000 [534.000, 534.000],  loss: 12295956.000000, mae: 2942.254883, mean_q: 5021.662109\n",
      "wrong_move\n",
      "   6202/500000: episode: 6106, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1740.000 [1740.000, 1740.000],  loss: 2187237.250000, mae: 3142.032715, mean_q: 11205.404297\n",
      "wrong_move\n",
      "   6203/500000: episode: 6107, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: 20538696.000000, mae: 2945.682861, mean_q: 6112.224609\n",
      "wrong_move\n",
      "   6204/500000: episode: 6108, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3027.000 [3027.000, 3027.000],  loss: 14299470.000000, mae: 2948.930908, mean_q: 7174.677246\n",
      "wrong_move\n",
      "   6205/500000: episode: 6109, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1133.000 [1133.000, 1133.000],  loss: 21627404.000000, mae: 2952.007812, mean_q: 8019.514648\n",
      "wrong_move\n",
      "   6206/500000: episode: 6110, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1133.000 [1133.000, 1133.000],  loss: 4766903.000000, mae: 2948.207031, mean_q: 5892.779785\n",
      "wrong_move\n",
      "   6207/500000: episode: 6111, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3027.000 [3027.000, 3027.000],  loss: 3268309.750000, mae: 2951.682373, mean_q: 7915.072266\n",
      "wrong_move\n",
      "   6208/500000: episode: 6112, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3396.000 [3396.000, 3396.000],  loss: 143696805888.000000, mae: 3370.952637, mean_q: 16344.432617\n",
      "wrong_move\n",
      "   6209/500000: episode: 6113, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1133.000 [1133.000, 1133.000],  loss: 10697332.000000, mae: 2947.462402, mean_q: 5685.709961\n",
      "wrong_move\n",
      "   6210/500000: episode: 6114, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1133.000 [1133.000, 1133.000],  loss: 4034060.500000, mae: 3186.649902, mean_q: 13559.111328\n",
      "wrong_move\n",
      "   6211/500000: episode: 6115, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1133.000 [1133.000, 1133.000],  loss: 21813274.000000, mae: 2947.613770, mean_q: 8026.420410\n",
      "wrong_move\n",
      "   6212/500000: episode: 6116, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1133.000 [1133.000, 1133.000],  loss: 6905041.500000, mae: 2943.607178, mean_q: 7123.306641\n",
      "wrong_move\n",
      "   6213/500000: episode: 6117, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7562818.000000, mae: 2990.677246, mean_q: 9260.062500\n",
      "wrong_move\n",
      "   6214/500000: episode: 6118, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1918.000 [1918.000, 1918.000],  loss: 8972432.000000, mae: 2941.327637, mean_q: 7655.196289\n",
      "wrong_move\n",
      "   6215/500000: episode: 6119, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 478.000 [478.000, 478.000],  loss: 4600101.500000, mae: 2940.690430, mean_q: 7329.886719\n",
      "wrong_move\n",
      "   6216/500000: episode: 6120, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: 202850560.000000, mae: 2973.555176, mean_q: 10027.291992\n",
      "wrong_move\n",
      "   6217/500000: episode: 6121, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 170.000 [170.000, 170.000],  loss: 4011501.750000, mae: 2944.500488, mean_q: 8838.111328\n",
      "wrong_move\n",
      "   6218/500000: episode: 6122, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1133.000 [1133.000, 1133.000],  loss: 15174301.000000, mae: 2952.396973, mean_q: 10496.965820\n",
      "wrong_move\n",
      "   6219/500000: episode: 6123, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3870.000 [3870.000, 3870.000],  loss: 11871850.000000, mae: 2938.279785, mean_q: 8875.371094\n",
      "wrong_move\n",
      "   6220/500000: episode: 6124, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1133.000 [1133.000, 1133.000],  loss: 66431168.000000, mae: 3103.519531, mean_q: 9971.672852\n",
      "wrong_move\n",
      "   6221/500000: episode: 6125, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1133.000 [1133.000, 1133.000],  loss: 16403099.000000, mae: 2939.720703, mean_q: 8752.324219\n",
      "wrong_move\n",
      "   6222/500000: episode: 6126, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1133.000 [1133.000, 1133.000],  loss: 15652049.000000, mae: 2946.127441, mean_q: 9013.087891\n",
      "wrong_move\n",
      "   6223/500000: episode: 6127, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1859.000 [1859.000, 1859.000],  loss: 34495844.000000, mae: 2942.481445, mean_q: 9955.933594\n",
      "wrong_move\n",
      "   6224/500000: episode: 6128, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1133.000 [1133.000, 1133.000],  loss: 31816900.000000, mae: 2941.854492, mean_q: 9016.260742\n",
      "wrong_move\n",
      "   6225/500000: episode: 6129, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3291.000 [3291.000, 3291.000],  loss: 5028633.500000, mae: 2941.573242, mean_q: 8508.783203\n",
      "wrong_move\n",
      "   6226/500000: episode: 6130, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3807.000 [3807.000, 3807.000],  loss: 933013.187500, mae: 2941.244385, mean_q: 8585.218750\n",
      "wrong_move\n",
      "   6227/500000: episode: 6131, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 704.000 [704.000, 704.000],  loss: 3475228.000000, mae: 2945.960205, mean_q: 9184.264648\n",
      "wrong_move\n",
      "   6228/500000: episode: 6132, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 704.000 [704.000, 704.000],  loss: 5915199.000000, mae: 2950.420410, mean_q: 8450.136719\n",
      "wrong_move\n",
      "   6229/500000: episode: 6133, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2059.000 [2059.000, 2059.000],  loss: 7477920.000000, mae: 2963.491211, mean_q: 9172.554688\n",
      "wrong_move\n",
      "   6230/500000: episode: 6134, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 704.000 [704.000, 704.000],  loss: 2162661.250000, mae: 2942.349121, mean_q: 8934.577148\n",
      "wrong_move\n",
      "   6231/500000: episode: 6135, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9055883.000000, mae: 2943.557129, mean_q: 8309.759766\n",
      "wrong_move\n",
      "   6232/500000: episode: 6136, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4044969.500000, mae: 2956.143555, mean_q: 9432.980469\n",
      "wrong_move\n",
      "   6233/500000: episode: 6137, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3396.000 [3396.000, 3396.000],  loss: 9017391.000000, mae: 3033.828125, mean_q: 12941.186523\n",
      "wrong_move\n",
      "   6234/500000: episode: 6138, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 704.000 [704.000, 704.000],  loss: 831094528.000000, mae: 3012.038086, mean_q: 10369.056641\n",
      "wrong_move\n",
      "   6235/500000: episode: 6139, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 704.000 [704.000, 704.000],  loss: 1856118.750000, mae: 2987.781738, mean_q: 11983.145508\n",
      "wrong_move\n",
      "   6236/500000: episode: 6140, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 704.000 [704.000, 704.000],  loss: 7064920.500000, mae: 2943.695312, mean_q: 7864.949219\n",
      "wrong_move\n",
      "   6237/500000: episode: 6141, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3883.000 [3883.000, 3883.000],  loss: 31039262.000000, mae: 2955.001465, mean_q: 10034.051758\n",
      "wrong_move\n",
      "   6238/500000: episode: 6142, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1854.000 [1854.000, 1854.000],  loss: 37419584.000000, mae: 2946.189209, mean_q: 8086.510742\n",
      "wrong_move\n",
      "   6239/500000: episode: 6143, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2728.000 [2728.000, 2728.000],  loss: 10299361.000000, mae: 2946.816895, mean_q: 8026.727051\n",
      "wrong_move\n",
      "   6240/500000: episode: 6144, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1854.000 [1854.000, 1854.000],  loss: 8104568.000000, mae: 2951.524902, mean_q: 8963.835938\n",
      "wrong_move\n",
      "   6241/500000: episode: 6145, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3191.000 [3191.000, 3191.000],  loss: 22081796.000000, mae: 2955.035156, mean_q: 8886.128906\n",
      "wrong_move\n",
      "   6242/500000: episode: 6146, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2283.000 [2283.000, 2283.000],  loss: 7796857.500000, mae: 2952.212646, mean_q: 9456.421875\n",
      "wrong_move\n",
      "   6243/500000: episode: 6147, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 346.000 [346.000, 346.000],  loss: 1209683.625000, mae: 2961.936035, mean_q: 8035.129883\n",
      "wrong_move\n",
      "   6244/500000: episode: 6148, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1854.000 [1854.000, 1854.000],  loss: 10834440.000000, mae: 2954.541504, mean_q: 7977.064453\n",
      "wrong_move\n",
      "   6245/500000: episode: 6149, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1854.000 [1854.000, 1854.000],  loss: 1290539.625000, mae: 2955.017578, mean_q: 7861.086914\n",
      "wrong_move\n",
      "   6246/500000: episode: 6150, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1854.000 [1854.000, 1854.000],  loss: 12418804.000000, mae: 2959.124512, mean_q: 6924.135742\n",
      "wrong_move\n",
      "   6247/500000: episode: 6151, duration: 0.145s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2504.000 [2504.000, 2504.000],  loss: 11765088.000000, mae: 2981.033691, mean_q: 7245.725586\n",
      "wrong_move\n",
      "   6248/500000: episode: 6152, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2807.000 [2807.000, 2807.000],  loss: 22847506.000000, mae: 2991.408447, mean_q: 7612.454102\n",
      "wrong_move\n",
      "   6249/500000: episode: 6153, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1854.000 [1854.000, 1854.000],  loss: 18788202.000000, mae: 2961.437500, mean_q: 7593.970703\n",
      "wrong_move\n",
      "   6250/500000: episode: 6154, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2677.000 [2677.000, 2677.000],  loss: 9998316.000000, mae: 2964.764404, mean_q: 8820.789062\n",
      "wrong_move\n",
      "   6251/500000: episode: 6155, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1953.000 [1953.000, 1953.000],  loss: 1972652.125000, mae: 2963.111328, mean_q: 7310.412598\n",
      "wrong_move\n",
      "   6252/500000: episode: 6156, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1854.000 [1854.000, 1854.000],  loss: 1830441.625000, mae: 2980.408936, mean_q: 9948.676758\n",
      "wrong_move\n",
      "   6253/500000: episode: 6157, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3180.000 [3180.000, 3180.000],  loss: 27385594.000000, mae: 2964.386719, mean_q: 7023.349121\n",
      "wrong_move\n",
      "   6254/500000: episode: 6158, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 53.000 [53.000, 53.000],  loss: 9106557.000000, mae: 2964.422363, mean_q: 7565.504883\n",
      "wrong_move\n",
      "   6255/500000: episode: 6159, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1854.000 [1854.000, 1854.000],  loss: 4153875.000000, mae: 2964.755859, mean_q: 8102.319336\n",
      "wrong_move\n",
      "   6256/500000: episode: 6160, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 27083878.000000, mae: 2963.980225, mean_q: 6464.917480\n",
      "wrong_move\n",
      "   6257/500000: episode: 6161, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 643.000 [643.000, 643.000],  loss: 728397.750000, mae: 2963.496826, mean_q: 6737.507324\n",
      "wrong_move\n",
      "   6258/500000: episode: 6162, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 546.000 [546.000, 546.000],  loss: 34923668.000000, mae: 2967.961426, mean_q: 7396.731934\n",
      "wrong_move\n",
      "   6259/500000: episode: 6163, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 537.000 [537.000, 537.000],  loss: 13854166.000000, mae: 2980.855713, mean_q: 8922.275391\n",
      "wrong_move\n",
      "   6260/500000: episode: 6164, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2330.000 [2330.000, 2330.000],  loss: 1344949.500000, mae: 2963.014404, mean_q: 6756.425293\n",
      "wrong_move\n",
      "   6261/500000: episode: 6165, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1291718.875000, mae: 2964.962402, mean_q: 7226.251953\n",
      "wrong_move\n",
      "   6262/500000: episode: 6166, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2996.000 [2996.000, 2996.000],  loss: 2957753.000000, mae: 2964.677490, mean_q: 6460.288574\n",
      "wrong_move\n",
      "   6263/500000: episode: 6167, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1252.000 [1252.000, 1252.000],  loss: 7977397.000000, mae: 2970.114014, mean_q: 7488.008789\n",
      "wrong_move\n",
      "   6264/500000: episode: 6168, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1252.000 [1252.000, 1252.000],  loss: 10106596.000000, mae: 2965.001953, mean_q: 5901.601562\n",
      "wrong_move\n",
      "   6265/500000: episode: 6169, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1252.000 [1252.000, 1252.000],  loss: 8333363.500000, mae: 2969.105469, mean_q: 8197.250000\n",
      "wrong_move\n",
      "   6266/500000: episode: 6170, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 927.000 [927.000, 927.000],  loss: 2352392.000000, mae: 2971.869629, mean_q: 7365.944336\n",
      "wrong_move\n",
      "   6267/500000: episode: 6171, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 86.000 [86.000, 86.000],  loss: 1499299.375000, mae: 2974.232910, mean_q: 7212.124512\n",
      "wrong_move\n",
      "   6268/500000: episode: 6172, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 927.000 [927.000, 927.000],  loss: 4218619.000000, mae: 2969.215820, mean_q: 7572.634766\n",
      "wrong_move\n",
      "   6269/500000: episode: 6173, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1744.000 [1744.000, 1744.000],  loss: 4631639.500000, mae: 2975.372070, mean_q: 6306.101562\n",
      "wrong_move\n",
      "   6270/500000: episode: 6174, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 36578664.000000, mae: 2989.516113, mean_q: 6220.112793\n",
      "wrong_move\n",
      "   6271/500000: episode: 6175, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2053.000 [2053.000, 2053.000],  loss: 2093438.750000, mae: 2967.832520, mean_q: 5747.544922\n",
      "wrong_move\n",
      "   6272/500000: episode: 6176, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1252.000 [1252.000, 1252.000],  loss: 2082689.000000, mae: 2968.230957, mean_q: 6095.708496\n",
      "wrong_move\n",
      "   6273/500000: episode: 6177, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2542.000 [2542.000, 2542.000],  loss: 16534513.000000, mae: 2972.998535, mean_q: 6275.059570\n",
      "wrong_move\n",
      "   6274/500000: episode: 6178, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 873.000 [873.000, 873.000],  loss: 3010766.250000, mae: 3081.356445, mean_q: 9874.128906\n",
      "wrong_move\n",
      "   6275/500000: episode: 6179, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1252.000 [1252.000, 1252.000],  loss: 4068839.500000, mae: 2977.282227, mean_q: 6497.771484\n",
      "wrong_move\n",
      "   6276/500000: episode: 6180, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7191640.000000, mae: 2968.334473, mean_q: 7164.173828\n",
      "wrong_move\n",
      "   6277/500000: episode: 6181, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 12904538.000000, mae: 2969.456055, mean_q: 6989.125000\n",
      "wrong_move\n",
      "   6278/500000: episode: 6182, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 30247848.000000, mae: 2968.410156, mean_q: 6987.168945\n",
      "wrong_move\n",
      "   6279/500000: episode: 6183, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1252.000 [1252.000, 1252.000],  loss: 3138956.500000, mae: 2968.788086, mean_q: 6732.457031\n",
      "wrong_move\n",
      "   6280/500000: episode: 6184, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2165.000 [2165.000, 2165.000],  loss: 1893040.750000, mae: 2971.216797, mean_q: 6372.486816\n",
      "wrong_move\n",
      "   6281/500000: episode: 6185, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3713.000 [3713.000, 3713.000],  loss: 1879965.875000, mae: 2970.079346, mean_q: 6439.791504\n",
      "wrong_move\n",
      "   6282/500000: episode: 6186, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 663.000 [663.000, 663.000],  loss: 46374484.000000, mae: 2973.641602, mean_q: 6835.700195\n",
      "wrong_move\n",
      "   6283/500000: episode: 6187, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 46.000 [46.000, 46.000],  loss: 12879368.000000, mae: 2970.427490, mean_q: 5456.706055\n",
      "wrong_move\n",
      "   6284/500000: episode: 6188, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 663.000 [663.000, 663.000],  loss: 13928913.000000, mae: 3185.741211, mean_q: 14117.346680\n",
      "wrong_move\n",
      "   6285/500000: episode: 6189, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 663.000 [663.000, 663.000],  loss: 6981634.000000, mae: 2971.232910, mean_q: 5380.958496\n",
      "wrong_move\n",
      "   6286/500000: episode: 6190, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2322.000 [2322.000, 2322.000],  loss: 13197232.000000, mae: 2973.125977, mean_q: 6430.250977\n",
      "wrong_move\n",
      "   6287/500000: episode: 6191, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 663.000 [663.000, 663.000],  loss: 367188704.000000, mae: 3021.275879, mean_q: 6240.638184\n",
      "wrong_move\n",
      "   6288/500000: episode: 6192, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3649.000 [3649.000, 3649.000],  loss: 8265126.000000, mae: 2972.922363, mean_q: 4987.218750\n",
      "wrong_move\n",
      "   6289/500000: episode: 6193, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 89.000 [89.000, 89.000],  loss: 2116089.250000, mae: 2973.800293, mean_q: 6313.606934\n",
      "wrong_move\n",
      "   6290/500000: episode: 6194, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1742.000 [1742.000, 1742.000],  loss: 11061422.000000, mae: 2975.350098, mean_q: 6952.465820\n",
      "wrong_move\n",
      "   6291/500000: episode: 6195, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3905.000 [3905.000, 3905.000],  loss: 1342307.000000, mae: 2973.348877, mean_q: 5682.729004\n",
      "wrong_move\n",
      "   6292/500000: episode: 6196, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3559.000 [3559.000, 3559.000],  loss: 2988253.000000, mae: 2973.332520, mean_q: 4346.692383\n",
      "wrong_move\n",
      "   6293/500000: episode: 6197, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 578.000 [578.000, 578.000],  loss: 16728935.000000, mae: 2976.392578, mean_q: 6792.957520\n",
      "wrong_move\n",
      "   6294/500000: episode: 6198, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3559.000 [3559.000, 3559.000],  loss: 10341394.000000, mae: 2974.876953, mean_q: 4839.687012\n",
      "wrong_move\n",
      "   6295/500000: episode: 6199, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3559.000 [3559.000, 3559.000],  loss: 16570350.000000, mae: 2976.466309, mean_q: 5613.433594\n",
      "wrong_move\n",
      "   6296/500000: episode: 6200, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3559.000 [3559.000, 3559.000],  loss: 8726992.000000, mae: 2975.681396, mean_q: 4634.430664\n",
      "wrong_move\n",
      "   6297/500000: episode: 6201, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2025.000 [2025.000, 2025.000],  loss: 15593150.000000, mae: 2978.187012, mean_q: 6089.729980\n",
      "wrong_move\n",
      "   6298/500000: episode: 6202, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7261730.000000, mae: 2978.185059, mean_q: 6394.796387\n",
      "wrong_move\n",
      "   6299/500000: episode: 6203, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3733.000 [3733.000, 3733.000],  loss: 7783908.000000, mae: 2977.552246, mean_q: 4760.172363\n",
      "wrong_move\n",
      "   6300/500000: episode: 6204, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 991.000 [991.000, 991.000],  loss: 3384880.000000, mae: 2978.205566, mean_q: 5584.758789\n",
      "wrong_move\n",
      "   6301/500000: episode: 6205, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3502.000 [3502.000, 3502.000],  loss: 99549296.000000, mae: 2993.241211, mean_q: 5446.397461\n",
      "wrong_move\n",
      "   6302/500000: episode: 6206, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3866.000 [3866.000, 3866.000],  loss: 53225628.000000, mae: 2981.461426, mean_q: 5694.190430\n",
      "wrong_move\n",
      "   6303/500000: episode: 6207, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2768.000 [2768.000, 2768.000],  loss: 760381.062500, mae: 2978.006348, mean_q: 6468.570801\n",
      "wrong_move\n",
      "   6304/500000: episode: 6208, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 367.000 [367.000, 367.000],  loss: 569341376.000000, mae: 3225.325684, mean_q: 8653.384766\n",
      "wrong_move\n",
      "   6305/500000: episode: 6209, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2349110.250000, mae: 2986.510742, mean_q: 5535.884766\n",
      "wrong_move\n",
      "   6306/500000: episode: 6210, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2062.000 [2062.000, 2062.000],  loss: 7858449.000000, mae: 2980.552979, mean_q: 5660.263672\n",
      "wrong_move\n",
      "   6307/500000: episode: 6211, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 367.000 [367.000, 367.000],  loss: 19904816.000000, mae: 2980.736328, mean_q: 4669.918945\n",
      "wrong_move\n",
      "   6308/500000: episode: 6212, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3649.000 [3649.000, 3649.000],  loss: 6326889.000000, mae: 2982.951172, mean_q: 5248.961914\n",
      "wrong_move\n",
      "   6309/500000: episode: 6213, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 15566148.000000, mae: 2986.632324, mean_q: 5949.888672\n",
      "wrong_move\n",
      "   6310/500000: episode: 6214, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 2913699.500000, mae: 2984.414551, mean_q: 5094.128418\n",
      "wrong_move\n",
      "   6311/500000: episode: 6215, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3733.000 [3733.000, 3733.000],  loss: 6494084.500000, mae: 2984.417969, mean_q: 5414.164551\n",
      "wrong_move\n",
      "   6312/500000: episode: 6216, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 24.000 [24.000, 24.000],  loss: 9356701.000000, mae: 2990.739258, mean_q: 6030.520508\n",
      "wrong_move\n",
      "   6313/500000: episode: 6217, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 873.000 [873.000, 873.000],  loss: 4044958.500000, mae: 2986.359863, mean_q: 5523.600098\n",
      "wrong_move\n",
      "   6314/500000: episode: 6218, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4063.000 [4063.000, 4063.000],  loss: 4816022.500000, mae: 2992.955566, mean_q: 6040.926758\n",
      "wrong_move\n",
      "   6315/500000: episode: 6219, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3733.000 [3733.000, 3733.000],  loss: 3432480.500000, mae: 2987.117920, mean_q: 5617.691406\n",
      "wrong_move\n",
      "   6316/500000: episode: 6220, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2102.000 [2102.000, 2102.000],  loss: 982873.375000, mae: 2984.693848, mean_q: 4705.905273\n",
      "wrong_move\n",
      "   6317/500000: episode: 6221, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3733.000 [3733.000, 3733.000],  loss: 20187678.000000, mae: 3066.292480, mean_q: 4881.182617\n",
      "wrong_move\n",
      "   6318/500000: episode: 6222, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 89.000 [89.000, 89.000],  loss: 11912558.000000, mae: 2986.159180, mean_q: 5875.420898\n",
      "wrong_move\n",
      "   6319/500000: episode: 6223, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3802.000 [3802.000, 3802.000],  loss: 5260221.000000, mae: 2985.738281, mean_q: 5308.384277\n",
      "wrong_move\n",
      "   6320/500000: episode: 6224, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2818.000 [2818.000, 2818.000],  loss: 6619677.500000, mae: 2984.853516, mean_q: 4458.479492\n",
      "wrong_move\n",
      "   6321/500000: episode: 6225, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2895.000 [2895.000, 2895.000],  loss: 4218552.500000, mae: 2985.368164, mean_q: 4495.869141\n",
      "wrong_move\n",
      "   6322/500000: episode: 6226, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2956.000 [2956.000, 2956.000],  loss: 849654.500000, mae: 2985.880859, mean_q: 5773.298828\n",
      "wrong_move\n",
      "   6323/500000: episode: 6227, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3733.000 [3733.000, 3733.000],  loss: 3973222.000000, mae: 2986.516113, mean_q: 5426.440918\n",
      "wrong_move\n",
      "   6324/500000: episode: 6228, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4082.000 [4082.000, 4082.000],  loss: 2974739.750000, mae: 2984.400146, mean_q: 3166.907959\n",
      "wrong_move\n",
      "   6325/500000: episode: 6229, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3802.000 [3802.000, 3802.000],  loss: 2291834.500000, mae: 2986.138428, mean_q: 4455.817383\n",
      "wrong_move\n",
      "   6326/500000: episode: 6230, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2903.000 [2903.000, 2903.000],  loss: 4226150.000000, mae: 2987.315674, mean_q: 4990.250488\n",
      "wrong_move\n",
      "   6327/500000: episode: 6231, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2903.000 [2903.000, 2903.000],  loss: 6535656.000000, mae: 2987.161621, mean_q: 4264.778809\n",
      "wrong_move\n",
      "   6328/500000: episode: 6232, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3975.000 [3975.000, 3975.000],  loss: 3675644.500000, mae: 2988.541016, mean_q: 5312.610840\n",
      "wrong_move\n",
      "   6329/500000: episode: 6233, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2903.000 [2903.000, 2903.000],  loss: 1456671.000000, mae: 2986.660645, mean_q: 4442.863281\n",
      "wrong_move\n",
      "   6330/500000: episode: 6234, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2102.000 [2102.000, 2102.000],  loss: 2129540.000000, mae: 2997.319580, mean_q: 4999.815430\n",
      "wrong_move\n",
      "   6331/500000: episode: 6235, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2903.000 [2903.000, 2903.000],  loss: 2385615.500000, mae: 2985.839844, mean_q: 3923.982422\n",
      "wrong_move\n",
      "   6332/500000: episode: 6236, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3946.000 [3946.000, 3946.000],  loss: 14522030.000000, mae: 2987.510010, mean_q: 5171.446289\n",
      "wrong_move\n",
      "   6333/500000: episode: 6237, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 266.000 [266.000, 266.000],  loss: 16345586.000000, mae: 2987.866211, mean_q: 4750.754883\n",
      "wrong_move\n",
      "   6334/500000: episode: 6238, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2903.000 [2903.000, 2903.000],  loss: 4480773.000000, mae: 2986.347168, mean_q: 4359.166016\n",
      "wrong_move\n",
      "   6335/500000: episode: 6239, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2102.000 [2102.000, 2102.000],  loss: 10022263.000000, mae: 2986.083008, mean_q: 3858.130859\n",
      "wrong_move\n",
      "   6336/500000: episode: 6240, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3511113.500000, mae: 2988.426270, mean_q: 6453.395020\n",
      "wrong_move\n",
      "   6337/500000: episode: 6241, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 26028092.000000, mae: 2989.378418, mean_q: 6466.042969\n",
      "wrong_move\n",
      "   6338/500000: episode: 6242, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1144.000 [1144.000, 1144.000],  loss: 4409196.000000, mae: 2987.707031, mean_q: 5335.746582\n",
      "wrong_move\n",
      "   6339/500000: episode: 6243, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1879.000 [1879.000, 1879.000],  loss: 14540060.000000, mae: 3000.507324, mean_q: 6035.504883\n",
      "wrong_move\n",
      "   6340/500000: episode: 6244, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 575.000 [575.000, 575.000],  loss: 2472636.750000, mae: 2987.036133, mean_q: 5029.433105\n",
      "wrong_move\n",
      "   6341/500000: episode: 6245, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3715.000 [3715.000, 3715.000],  loss: 5340526.000000, mae: 2986.235840, mean_q: 3277.172852\n",
      "wrong_move\n",
      "   6342/500000: episode: 6246, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 12059481.000000, mae: 2992.560547, mean_q: 4551.766602\n",
      "wrong_move\n",
      "   6343/500000: episode: 6247, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1902.000 [1902.000, 1902.000],  loss: 12724502.000000, mae: 2988.720703, mean_q: 5177.523438\n",
      "wrong_move\n",
      "   6344/500000: episode: 6248, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1003.000 [1003.000, 1003.000],  loss: 8017593.000000, mae: 2988.733643, mean_q: 4414.650391\n",
      "wrong_move\n",
      "   6345/500000: episode: 6249, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3649.000 [3649.000, 3649.000],  loss: 5868218.500000, mae: 2989.458008, mean_q: 3741.335938\n",
      "wrong_move\n",
      "   6346/500000: episode: 6250, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: 4145102.000000, mae: 3012.588135, mean_q: 4742.606445\n",
      "wrong_move\n",
      "   6347/500000: episode: 6251, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: 6908925.500000, mae: 2991.125488, mean_q: 3952.642578\n",
      "wrong_move\n",
      "   6348/500000: episode: 6252, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: 21172816.000000, mae: 2993.144775, mean_q: 5013.887695\n",
      "wrong_move\n",
      "   6349/500000: episode: 6253, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8752288.000000, mae: 2993.842285, mean_q: 4181.963867\n",
      "wrong_move\n",
      "   6350/500000: episode: 6254, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2607.000 [2607.000, 2607.000],  loss: 3837721.000000, mae: 2997.360840, mean_q: 4866.366211\n",
      "wrong_move\n",
      "   6351/500000: episode: 6255, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: 4250897.000000, mae: 2995.862793, mean_q: 3721.749268\n",
      "wrong_move\n",
      "   6352/500000: episode: 6256, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2228.000 [2228.000, 2228.000],  loss: 2773169.500000, mae: 2996.554688, mean_q: 3705.405273\n",
      "wrong_move\n",
      "   6353/500000: episode: 6257, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2161.000 [2161.000, 2161.000],  loss: 12656464.000000, mae: 2997.950928, mean_q: 3946.676270\n",
      "wrong_move\n",
      "   6354/500000: episode: 6258, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 758.000 [758.000, 758.000],  loss: 11614456.000000, mae: 2999.201172, mean_q: 4041.605469\n",
      "wrong_move\n",
      "   6355/500000: episode: 6259, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: 1883036.125000, mae: 3000.016602, mean_q: 4587.144531\n",
      "wrong_move\n",
      "   6356/500000: episode: 6260, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: 12309238.000000, mae: 3000.788574, mean_q: 4356.579590\n",
      "wrong_move\n",
      "   6357/500000: episode: 6261, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: 17691456.000000, mae: 3006.198975, mean_q: 3830.987305\n",
      "wrong_move\n",
      "   6358/500000: episode: 6262, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3165.000 [3165.000, 3165.000],  loss: 5137540.000000, mae: 3004.183105, mean_q: 4498.997070\n",
      "wrong_move\n",
      "   6359/500000: episode: 6263, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 496.000 [496.000, 496.000],  loss: 1782556.500000, mae: 3003.386963, mean_q: 5120.058594\n",
      "wrong_move\n",
      "   6360/500000: episode: 6264, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: 9253945.000000, mae: 3006.703125, mean_q: 4818.750000\n",
      "wrong_move\n",
      "   6361/500000: episode: 6265, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1270.000 [1270.000, 1270.000],  loss: 6346730.500000, mae: 3013.166504, mean_q: 3906.925537\n",
      "wrong_move\n",
      "   6362/500000: episode: 6266, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: 2289950.000000, mae: 3002.198975, mean_q: 3441.265137\n",
      "wrong_move\n",
      "   6363/500000: episode: 6267, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: 8050821.000000, mae: 3003.660645, mean_q: 4375.325684\n",
      "wrong_move\n",
      "   6364/500000: episode: 6268, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 161.000 [161.000, 161.000],  loss: 9873879.000000, mae: 3007.165283, mean_q: 2991.507568\n",
      "wrong_move\n",
      "   6365/500000: episode: 6269, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: 4232183.000000, mae: 3004.150391, mean_q: 4011.668701\n",
      "wrong_move\n",
      "   6366/500000: episode: 6270, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: 276414496.000000, mae: 3014.697021, mean_q: 7352.505859\n",
      "wrong_move\n",
      "   6367/500000: episode: 6271, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: 1280936.250000, mae: 3004.382324, mean_q: 2728.356934\n",
      "wrong_move\n",
      "   6368/500000: episode: 6272, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1709.000 [1709.000, 1709.000],  loss: 8197191.500000, mae: 3014.433594, mean_q: 4386.880859\n",
      "wrong_move\n",
      "   6369/500000: episode: 6273, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2012.000 [2012.000, 2012.000],  loss: 1915754.250000, mae: 3012.787354, mean_q: 4907.833008\n",
      "wrong_move\n",
      "   6370/500000: episode: 6274, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1084.000 [1084.000, 1084.000],  loss: 2751837.500000, mae: 3026.805420, mean_q: 4031.632812\n",
      "wrong_move\n",
      "   6371/500000: episode: 6275, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1302.000 [1302.000, 1302.000],  loss: 2419431.750000, mae: 3010.877197, mean_q: 4789.853516\n",
      "wrong_move\n",
      "   6372/500000: episode: 6276, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2394.000 [2394.000, 2394.000],  loss: 8454118.000000, mae: 3011.390625, mean_q: 4019.246582\n",
      "wrong_move\n",
      "   6373/500000: episode: 6277, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2391.000 [2391.000, 2391.000],  loss: 18921448.000000, mae: 3011.890381, mean_q: 3851.679688\n",
      "wrong_move\n",
      "   6374/500000: episode: 6278, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1366.000 [1366.000, 1366.000],  loss: 1729982.125000, mae: 3011.302002, mean_q: 2134.012695\n",
      "wrong_move\n",
      "   6375/500000: episode: 6279, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3067.000 [3067.000, 3067.000],  loss: 11235833.000000, mae: 3014.340820, mean_q: 4103.871094\n",
      "wrong_move\n",
      "   6376/500000: episode: 6280, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1512.000 [1512.000, 1512.000],  loss: 3346147.000000, mae: 3016.615234, mean_q: 4650.589844\n",
      "wrong_move\n",
      "   6377/500000: episode: 6281, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3067.000 [3067.000, 3067.000],  loss: 3414006.250000, mae: 3024.498047, mean_q: 4314.898438\n",
      "wrong_move\n",
      "   6378/500000: episode: 6282, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3067.000 [3067.000, 3067.000],  loss: 11532836.000000, mae: 3014.837891, mean_q: 2996.419922\n",
      "wrong_move\n",
      "   6379/500000: episode: 6283, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2708.000 [2708.000, 2708.000],  loss: 5359903.000000, mae: 3015.458740, mean_q: 3836.562500\n",
      "wrong_move\n",
      "   6380/500000: episode: 6284, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3067.000 [3067.000, 3067.000],  loss: 1875637.625000, mae: 3023.807617, mean_q: 3488.580322\n",
      "wrong_move\n",
      "   6381/500000: episode: 6285, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1481.000 [1481.000, 1481.000],  loss: 11214837.000000, mae: 3018.148926, mean_q: 4316.829102\n",
      "wrong_move\n",
      "   6382/500000: episode: 6286, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3065.000 [3065.000, 3065.000],  loss: 8879685.000000, mae: 3018.889160, mean_q: 4048.598145\n",
      "wrong_move\n",
      "   6383/500000: episode: 6287, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1553.000 [1553.000, 1553.000],  loss: 7021751.000000, mae: 3036.178711, mean_q: 4690.060547\n",
      "wrong_move\n",
      "   6384/500000: episode: 6288, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1932.000 [1932.000, 1932.000],  loss: 24706586.000000, mae: 3023.840332, mean_q: 5446.772461\n",
      "wrong_move\n",
      "   6385/500000: episode: 6289, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: 2633057.750000, mae: 3024.010254, mean_q: 4460.298340\n",
      "wrong_move\n",
      "   6386/500000: episode: 6290, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3040.000 [3040.000, 3040.000],  loss: 1087308.500000, mae: 3024.583008, mean_q: 3212.767090\n",
      "wrong_move\n",
      "   6387/500000: episode: 6291, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3065.000 [3065.000, 3065.000],  loss: 22207950.000000, mae: 3031.828125, mean_q: 4009.536133\n",
      "wrong_move\n",
      "   6388/500000: episode: 6292, duration: 0.158s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 821.000 [821.000, 821.000],  loss: 3886406.500000, mae: 3029.667969, mean_q: 4216.728027\n",
      "wrong_move\n",
      "   6389/500000: episode: 6293, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3065.000 [3065.000, 3065.000],  loss: 6365493.500000, mae: 3032.711914, mean_q: 3580.492188\n",
      "wrong_move\n",
      "   6390/500000: episode: 6294, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3065.000 [3065.000, 3065.000],  loss: 1157829.000000, mae: 3032.736328, mean_q: 4320.971680\n",
      "wrong_move\n",
      "   6391/500000: episode: 6295, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3917.000 [3917.000, 3917.000],  loss: 8850985.000000, mae: 3033.055176, mean_q: 4456.033203\n",
      "wrong_move\n",
      "   6392/500000: episode: 6296, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1273.000 [1273.000, 1273.000],  loss: 7772297.000000, mae: 3034.041504, mean_q: 3438.318604\n",
      "wrong_move\n",
      "   6393/500000: episode: 6297, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 6490739.000000, mae: 3034.869873, mean_q: 3614.000488\n",
      "wrong_move\n",
      "   6394/500000: episode: 6298, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 821.000 [821.000, 821.000],  loss: 1686286.625000, mae: 3039.026611, mean_q: 3985.043457\n",
      "wrong_move\n",
      "   6395/500000: episode: 6299, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1823424.750000, mae: 3036.794922, mean_q: 3139.763428\n",
      "wrong_move\n",
      "   6396/500000: episode: 6300, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 821.000 [821.000, 821.000],  loss: 927368.750000, mae: 3037.565430, mean_q: 2995.270264\n",
      "wrong_move\n",
      "   6397/500000: episode: 6301, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3061.000 [3061.000, 3061.000],  loss: 2327951.750000, mae: 3039.677490, mean_q: 3533.660156\n",
      "wrong_move\n",
      "   6398/500000: episode: 6302, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2542.000 [2542.000, 2542.000],  loss: 15165046.000000, mae: 3056.906738, mean_q: 5846.182129\n",
      "wrong_move\n",
      "   6399/500000: episode: 6303, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1942.000 [1942.000, 1942.000],  loss: 10986314.000000, mae: 3040.527832, mean_q: 3410.574219\n",
      "wrong_move\n",
      "   6401/500000: episode: 6304, duration: 0.167s, episode steps:   2, steps per second:  12, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 1609.000 [156.000, 3062.000],  loss: 5410331.000000, mae: 3043.877441, mean_q: 3948.992676\n",
      "wrong_move\n",
      "   6402/500000: episode: 6305, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3070.000 [3070.000, 3070.000],  loss: 3833058.000000, mae: 3211.822754, mean_q: 11989.480469\n",
      "wrong_move\n",
      "   6403/500000: episode: 6306, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3630.000 [3630.000, 3630.000],  loss: 1097513.875000, mae: 3043.402588, mean_q: 3245.375977\n",
      "wrong_move\n",
      "   6404/500000: episode: 6307, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3630.000 [3630.000, 3630.000],  loss: 5456228.000000, mae: 3045.135986, mean_q: 4740.870117\n",
      "wrong_move\n",
      "   6405/500000: episode: 6308, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3374.000 [3374.000, 3374.000],  loss: 1667093.500000, mae: 3044.195312, mean_q: 3637.534668\n",
      "wrong_move\n",
      "   6406/500000: episode: 6309, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1971.000 [1971.000, 1971.000],  loss: 3697551.250000, mae: 3048.187988, mean_q: 3868.622070\n",
      "wrong_move\n",
      "   6407/500000: episode: 6310, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2542.000 [2542.000, 2542.000],  loss: 6855973.000000, mae: 3054.210449, mean_q: 3610.929688\n",
      "wrong_move\n",
      "   6408/500000: episode: 6311, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: 5967012.500000, mae: 3046.478760, mean_q: 3470.564697\n",
      "wrong_move\n",
      "   6409/500000: episode: 6312, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1558.000 [1558.000, 1558.000],  loss: 16000113.000000, mae: 3049.129883, mean_q: 4102.678711\n",
      "wrong_move\n",
      "   6410/500000: episode: 6313, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 537.000 [537.000, 537.000],  loss: 2802750.000000, mae: 3049.247314, mean_q: 4216.895508\n",
      "wrong_move\n",
      "   6411/500000: episode: 6314, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 537.000 [537.000, 537.000],  loss: 18763612.000000, mae: 3049.088135, mean_q: 4977.723145\n",
      "wrong_move\n",
      "   6412/500000: episode: 6315, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 906.000 [906.000, 906.000],  loss: 1862593.875000, mae: 3051.735352, mean_q: 4700.982422\n",
      "wrong_move\n",
      "   6413/500000: episode: 6316, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 985.000 [985.000, 985.000],  loss: 542067.125000, mae: 3047.985352, mean_q: 2845.467285\n",
      "wrong_move\n",
      "   6414/500000: episode: 6317, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3917.000 [3917.000, 3917.000],  loss: 6367081.000000, mae: 3049.838135, mean_q: 2846.606934\n",
      "wrong_move\n",
      "   6415/500000: episode: 6318, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 985.000 [985.000, 985.000],  loss: 2134608.500000, mae: 3051.698730, mean_q: 4158.689453\n",
      "wrong_move\n",
      "   6416/500000: episode: 6319, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3585.000 [3585.000, 3585.000],  loss: 4824561.500000, mae: 3050.256836, mean_q: 3113.396973\n",
      "wrong_move\n",
      "   6417/500000: episode: 6320, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3213.000 [3213.000, 3213.000],  loss: 1368976.250000, mae: 3052.889160, mean_q: 3914.033203\n",
      "wrong_move\n",
      "   6418/500000: episode: 6321, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1800.000 [1800.000, 1800.000],  loss: 1820254.000000, mae: 3051.306396, mean_q: 2783.910889\n",
      "wrong_move\n",
      "   6419/500000: episode: 6322, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1016.000 [1016.000, 1016.000],  loss: 8831764.000000, mae: 3052.582275, mean_q: 3470.192383\n",
      "wrong_move\n",
      "   6420/500000: episode: 6323, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3070.000 [3070.000, 3070.000],  loss: 21229168.000000, mae: 3148.736328, mean_q: 8629.757812\n",
      "wrong_move\n",
      "   6421/500000: episode: 6324, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3509.000 [3509.000, 3509.000],  loss: 4068232.500000, mae: 3059.541992, mean_q: 4455.119141\n",
      "wrong_move\n",
      "   6423/500000: episode: 6325, duration: 0.133s, episode steps:   2, steps per second:  15, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 1350.000 [1350.000, 1350.000],  loss: 10666604.000000, mae: 3053.705811, mean_q: 5027.177734\n",
      "wrong_move\n",
      "   6424/500000: episode: 6326, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 24.000 [24.000, 24.000],  loss: 1794434.000000, mae: 3053.053467, mean_q: 2920.310791\n",
      "wrong_move\n",
      "   6425/500000: episode: 6327, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3070.000 [3070.000, 3070.000],  loss: 4622772.000000, mae: 3053.604248, mean_q: 2552.805664\n",
      "wrong_move\n",
      "   6426/500000: episode: 6328, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1915.000 [1915.000, 1915.000],  loss: 11115717.000000, mae: 3058.434570, mean_q: 4954.443359\n",
      "wrong_move\n",
      "   6427/500000: episode: 6329, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1915.000 [1915.000, 1915.000],  loss: 11139165.000000, mae: 3059.397705, mean_q: 3156.935059\n",
      "wrong_move\n",
      "   6428/500000: episode: 6330, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2238.000 [2238.000, 2238.000],  loss: 983450.875000, mae: 3080.248047, mean_q: 3528.764893\n",
      "wrong_move\n",
      "   6429/500000: episode: 6331, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1143.000 [1143.000, 1143.000],  loss: 11886414.000000, mae: 3057.628906, mean_q: 4001.152100\n",
      "wrong_move\n",
      "   6430/500000: episode: 6332, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1915.000 [1915.000, 1915.000],  loss: 11178704.000000, mae: 3059.204590, mean_q: 4941.178223\n",
      "wrong_move\n",
      "   6431/500000: episode: 6333, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1297.000 [1297.000, 1297.000],  loss: 910995.500000, mae: 3059.778564, mean_q: 3891.301270\n",
      "wrong_move\n",
      "   6432/500000: episode: 6334, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1980.000 [1980.000, 1980.000],  loss: 5053013.000000, mae: 3060.335693, mean_q: 3493.801758\n",
      "wrong_move\n",
      "   6433/500000: episode: 6335, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2511.000 [2511.000, 2511.000],  loss: 6835703.000000, mae: 3077.691650, mean_q: 5160.792480\n",
      "wrong_move\n",
      "   6435/500000: episode: 6336, duration: 0.171s, episode steps:   2, steps per second:  12, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4231679.500000, mae: 3063.218994, mean_q: 4062.101562\n",
      "wrong_move\n",
      "   6436/500000: episode: 6337, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2511.000 [2511.000, 2511.000],  loss: 9379732.000000, mae: 3063.003906, mean_q: 2285.715332\n",
      "wrong_move\n",
      "   6437/500000: episode: 6338, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 985.000 [985.000, 985.000],  loss: 3359998.750000, mae: 3065.054443, mean_q: 3941.280762\n",
      "wrong_move\n",
      "   6438/500000: episode: 6339, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3620.000 [3620.000, 3620.000],  loss: 37756996.000000, mae: 3066.790527, mean_q: 5235.957031\n",
      "wrong_move\n",
      "   6439/500000: episode: 6340, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3663.000 [3663.000, 3663.000],  loss: 1709887.000000, mae: 3067.774658, mean_q: 4530.537109\n",
      "wrong_move\n",
      "   6440/500000: episode: 6341, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3620.000 [3620.000, 3620.000],  loss: 5156328.500000, mae: 3067.600830, mean_q: 3688.686035\n",
      "wrong_move\n",
      "   6441/500000: episode: 6342, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2511.000 [2511.000, 2511.000],  loss: 8816209.000000, mae: 3068.157959, mean_q: 3968.672607\n",
      "wrong_move\n",
      "   6442/500000: episode: 6343, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3114.000 [3114.000, 3114.000],  loss: 11895957.000000, mae: 3069.766113, mean_q: 2656.848633\n",
      "wrong_move\n",
      "   6443/500000: episode: 6344, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 807.000 [807.000, 807.000],  loss: 1987366.500000, mae: 3072.767578, mean_q: 3288.486328\n",
      "wrong_move\n",
      "   6444/500000: episode: 6345, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1143.000 [1143.000, 1143.000],  loss: 5180418.000000, mae: 3073.367676, mean_q: 4869.250977\n",
      "wrong_move\n",
      "   6445/500000: episode: 6346, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 807.000 [807.000, 807.000],  loss: 5844366.500000, mae: 3095.447021, mean_q: 5584.666992\n",
      "wrong_move\n",
      "   6446/500000: episode: 6347, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3205.000 [3205.000, 3205.000],  loss: 10959571.000000, mae: 3071.905273, mean_q: 2584.418213\n",
      "wrong_move\n",
      "   6447/500000: episode: 6348, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2928.000 [2928.000, 2928.000],  loss: 4733921.500000, mae: 3072.465820, mean_q: 2965.559570\n",
      "wrong_move\n",
      "   6448/500000: episode: 6349, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 807.000 [807.000, 807.000],  loss: 6939992.500000, mae: 3072.589844, mean_q: 2794.276123\n",
      "wrong_move\n",
      "   6449/500000: episode: 6350, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3887.000 [3887.000, 3887.000],  loss: 965519.250000, mae: 3074.196289, mean_q: 3648.816162\n",
      "wrong_move\n",
      "   6450/500000: episode: 6351, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 807.000 [807.000, 807.000],  loss: 6954571.000000, mae: 3075.228027, mean_q: 4116.581055\n",
      "wrong_move\n",
      "   6451/500000: episode: 6352, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 807.000 [807.000, 807.000],  loss: 3001670.500000, mae: 3074.600342, mean_q: 3742.986328\n",
      "wrong_move\n",
      "   6452/500000: episode: 6353, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 807.000 [807.000, 807.000],  loss: 7713642.000000, mae: 3087.587891, mean_q: 3437.799805\n",
      "wrong_move\n",
      "   6453/500000: episode: 6354, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3437.000 [3437.000, 3437.000],  loss: 6945121.000000, mae: 3075.352539, mean_q: 3908.604492\n",
      "wrong_move\n",
      "   6454/500000: episode: 6355, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 807.000 [807.000, 807.000],  loss: 12136820.000000, mae: 3089.790039, mean_q: 4718.199219\n",
      "wrong_move\n",
      "   6455/500000: episode: 6356, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2401.000 [2401.000, 2401.000],  loss: 10051850.000000, mae: 3080.448242, mean_q: 3433.175781\n",
      "wrong_move\n",
      "   6456/500000: episode: 6357, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 807.000 [807.000, 807.000],  loss: 678939.875000, mae: 3082.312500, mean_q: 3523.163574\n",
      "wrong_move\n",
      "   6457/500000: episode: 6358, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 807.000 [807.000, 807.000],  loss: 2951459.000000, mae: 3078.771484, mean_q: 4357.264648\n",
      "wrong_move\n",
      "   6458/500000: episode: 6359, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3620.000 [3620.000, 3620.000],  loss: 6410920.500000, mae: 3077.708984, mean_q: 3052.587402\n",
      "wrong_move\n",
      "   6459/500000: episode: 6360, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2770.000 [2770.000, 2770.000],  loss: 1548395.000000, mae: 3077.266113, mean_q: 2588.215576\n",
      "wrong_move\n",
      "   6460/500000: episode: 6361, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2843.000 [2843.000, 2843.000],  loss: 27081692.000000, mae: 3084.961426, mean_q: 3580.891113\n",
      "wrong_move\n",
      "   6461/500000: episode: 6362, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 807.000 [807.000, 807.000],  loss: 1430122.000000, mae: 3079.014648, mean_q: 2264.992432\n",
      "wrong_move\n",
      "   6462/500000: episode: 6363, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1112.000 [1112.000, 1112.000],  loss: 3136657.000000, mae: 3081.396484, mean_q: 3957.496582\n",
      "wrong_move\n",
      "   6463/500000: episode: 6364, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1800.000 [1800.000, 1800.000],  loss: 6037731.000000, mae: 3079.173096, mean_q: 2101.641602\n",
      "wrong_move\n",
      "   6464/500000: episode: 6365, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 807.000 [807.000, 807.000],  loss: 14646793.000000, mae: 3081.213867, mean_q: 3161.887695\n",
      "wrong_move\n",
      "   6465/500000: episode: 6366, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1800.000 [1800.000, 1800.000],  loss: 6654698.000000, mae: 3088.084473, mean_q: 5786.100586\n",
      "wrong_move\n",
      "   6466/500000: episode: 6367, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1198.000 [1198.000, 1198.000],  loss: 6308345.000000, mae: 3082.268066, mean_q: 3635.152832\n",
      "wrong_move\n",
      "   6467/500000: episode: 6368, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3734.000 [3734.000, 3734.000],  loss: 7988101.000000, mae: 3081.398926, mean_q: 2354.442627\n",
      "wrong_move\n",
      "   6468/500000: episode: 6369, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1863.000 [1863.000, 1863.000],  loss: 2643350.500000, mae: 3083.391357, mean_q: 3189.857422\n",
      "wrong_move\n",
      "   6469/500000: episode: 6370, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1538.000 [1538.000, 1538.000],  loss: 701365.500000, mae: 3085.846436, mean_q: 3442.877686\n",
      "wrong_move\n",
      "   6470/500000: episode: 6371, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1863.000 [1863.000, 1863.000],  loss: 12351920.000000, mae: 3086.908203, mean_q: 4192.420410\n",
      "wrong_move\n",
      "   6471/500000: episode: 6372, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3620.000 [3620.000, 3620.000],  loss: 1371333.750000, mae: 3087.683350, mean_q: 4117.297852\n",
      "wrong_move\n",
      "   6472/500000: episode: 6373, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 257.000 [257.000, 257.000],  loss: 3434056.750000, mae: 3086.140625, mean_q: 2862.418457\n",
      "wrong_move\n",
      "   6473/500000: episode: 6374, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3620.000 [3620.000, 3620.000],  loss: 9000938.000000, mae: 3086.565430, mean_q: 4074.711914\n",
      "wrong_move\n",
      "   6474/500000: episode: 6375, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1863.000 [1863.000, 1863.000],  loss: 6196337.500000, mae: 3085.548340, mean_q: 3603.331787\n",
      "wrong_move\n",
      "   6475/500000: episode: 6376, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1799.000 [1799.000, 1799.000],  loss: 8028678.500000, mae: 3084.764160, mean_q: 2927.179688\n",
      "wrong_move\n",
      "   6476/500000: episode: 6377, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1863.000 [1863.000, 1863.000],  loss: 148243936.000000, mae: 3085.459473, mean_q: 3570.925049\n",
      "wrong_move\n",
      "   6477/500000: episode: 6378, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1863.000 [1863.000, 1863.000],  loss: 8547340.000000, mae: 3083.243408, mean_q: 2986.241699\n",
      "wrong_move\n",
      "   6478/500000: episode: 6379, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1863.000 [1863.000, 1863.000],  loss: 1776690.000000, mae: 3084.959473, mean_q: 4280.951172\n",
      "wrong_move\n",
      "   6479/500000: episode: 6380, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1863.000 [1863.000, 1863.000],  loss: 7421999.500000, mae: 3079.691650, mean_q: 2696.903809\n",
      "wrong_move\n",
      "   6480/500000: episode: 6381, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1681876.500000, mae: 3079.202393, mean_q: 2666.236084\n",
      "wrong_move\n",
      "   6481/500000: episode: 6382, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1863.000 [1863.000, 1863.000],  loss: 20324504.000000, mae: 3079.680664, mean_q: 3433.341309\n",
      "wrong_move\n",
      "   6482/500000: episode: 6383, duration: 0.163s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3620.000 [3620.000, 3620.000],  loss: 5661159.000000, mae: 3080.166504, mean_q: 3672.961914\n",
      "wrong_move\n",
      "   6483/500000: episode: 6384, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1863.000 [1863.000, 1863.000],  loss: 4226197.500000, mae: 3081.106445, mean_q: 3774.218018\n",
      "wrong_move\n",
      "   6484/500000: episode: 6385, duration: 0.158s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 4896589.000000, mae: 3080.694824, mean_q: 3432.349854\n",
      "wrong_move\n",
      "   6485/500000: episode: 6386, duration: 0.153s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2603.000 [2603.000, 2603.000],  loss: 4190274.500000, mae: 3080.641602, mean_q: 3158.380859\n",
      "wrong_move\n",
      "   6486/500000: episode: 6387, duration: 0.142s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 649.000 [649.000, 649.000],  loss: 14051313.000000, mae: 3083.461426, mean_q: 4733.354980\n",
      "wrong_move\n",
      "   6487/500000: episode: 6388, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 234.000 [234.000, 234.000],  loss: 3207127.500000, mae: 3081.549316, mean_q: 3612.001953\n",
      "wrong_move\n",
      "   6488/500000: episode: 6389, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1863.000 [1863.000, 1863.000],  loss: 2966279.000000, mae: 3079.457031, mean_q: 2874.191895\n",
      "wrong_move\n",
      "   6489/500000: episode: 6390, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3776.000 [3776.000, 3776.000],  loss: 1661365.125000, mae: 3079.217773, mean_q: 3112.715332\n",
      "wrong_move\n",
      "   6490/500000: episode: 6391, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 256.000 [256.000, 256.000],  loss: 24711036.000000, mae: 3085.774658, mean_q: 3332.748535\n",
      "wrong_move\n",
      "   6491/500000: episode: 6392, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3917.000 [3917.000, 3917.000],  loss: 2034844.250000, mae: 3078.572510, mean_q: 3057.913574\n",
      "wrong_move\n",
      "   6492/500000: episode: 6393, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3057.000 [3057.000, 3057.000],  loss: 6838630.000000, mae: 3079.191895, mean_q: 2795.997803\n",
      "wrong_move\n",
      "   6493/500000: episode: 6394, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3776.000 [3776.000, 3776.000],  loss: 17282796.000000, mae: 3080.476318, mean_q: 2444.958984\n",
      "wrong_move\n",
      "   6494/500000: episode: 6395, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3776.000 [3776.000, 3776.000],  loss: 7132055.500000, mae: 3149.854980, mean_q: 3403.827637\n",
      "wrong_move\n",
      "   6495/500000: episode: 6396, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2151.000 [2151.000, 2151.000],  loss: 2068371.000000, mae: 3081.247070, mean_q: 3300.232422\n",
      "wrong_move\n",
      "   6496/500000: episode: 6397, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3776.000 [3776.000, 3776.000],  loss: 1757549.875000, mae: 3083.020508, mean_q: 3066.258789\n",
      "wrong_move\n",
      "   6497/500000: episode: 6398, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 943.000 [943.000, 943.000],  loss: 12918512.000000, mae: 3083.774414, mean_q: 3151.936523\n",
      "wrong_move\n",
      "   6498/500000: episode: 6399, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 272.000 [272.000, 272.000],  loss: 2567841.500000, mae: 3091.492188, mean_q: 2575.446289\n",
      "wrong_move\n",
      "   6499/500000: episode: 6400, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3695.000 [3695.000, 3695.000],  loss: 4995104.000000, mae: 3087.655762, mean_q: 3941.854248\n",
      "wrong_move\n",
      "   6500/500000: episode: 6401, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1915.000 [1915.000, 1915.000],  loss: 2739345.500000, mae: 3092.928223, mean_q: 4993.351562\n",
      "wrong_move\n",
      "   6501/500000: episode: 6402, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3214.000 [3214.000, 3214.000],  loss: 7770573.000000, mae: 3092.041504, mean_q: 3215.774414\n",
      "wrong_move\n",
      "   6502/500000: episode: 6403, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1915.000 [1915.000, 1915.000],  loss: 17358656.000000, mae: 3214.438721, mean_q: 4673.765625\n",
      "wrong_move\n",
      "   6503/500000: episode: 6404, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1770.000 [1770.000, 1770.000],  loss: 2918179.250000, mae: 3094.294922, mean_q: 3033.139160\n",
      "wrong_move\n",
      "   6504/500000: episode: 6405, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3776.000 [3776.000, 3776.000],  loss: 12292326.000000, mae: 3094.286621, mean_q: 1826.407471\n",
      "wrong_move\n",
      "   6505/500000: episode: 6406, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3776.000 [3776.000, 3776.000],  loss: 3099910.500000, mae: 3095.635254, mean_q: 2489.985107\n",
      "wrong_move\n",
      "   6506/500000: episode: 6407, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1372.000 [1372.000, 1372.000],  loss: 1810104.000000, mae: 3102.136719, mean_q: 3569.353760\n",
      "wrong_move\n",
      "   6507/500000: episode: 6408, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 266.000 [266.000, 266.000],  loss: 5063291.500000, mae: 3099.865723, mean_q: 3745.252686\n",
      "wrong_move\n",
      "   6508/500000: episode: 6409, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3776.000 [3776.000, 3776.000],  loss: 15387179.000000, mae: 3139.831543, mean_q: 3919.580566\n",
      "wrong_move\n",
      "   6509/500000: episode: 6410, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3776.000 [3776.000, 3776.000],  loss: 2650711.750000, mae: 3121.729004, mean_q: 4638.713867\n",
      "wrong_move\n",
      "   6510/500000: episode: 6411, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3776.000 [3776.000, 3776.000],  loss: 13318214.000000, mae: 3104.362793, mean_q: 3547.613037\n",
      "wrong_move\n",
      "   6511/500000: episode: 6412, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1518.000 [1518.000, 1518.000],  loss: 788770.375000, mae: 3103.253906, mean_q: 2393.090576\n",
      "wrong_move\n",
      "   6512/500000: episode: 6413, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3244.000 [3244.000, 3244.000],  loss: 93148592.000000, mae: 3117.036377, mean_q: 3547.118652\n",
      "wrong_move\n",
      "   6513/500000: episode: 6414, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3884.000 [3884.000, 3884.000],  loss: 10550609.000000, mae: 3105.445312, mean_q: 3262.654297\n",
      "wrong_move\n",
      "   6514/500000: episode: 6415, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2052.000 [2052.000, 2052.000],  loss: 3437430.500000, mae: 3107.489746, mean_q: 2222.758057\n",
      "wrong_move\n",
      "   6515/500000: episode: 6416, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3244.000 [3244.000, 3244.000],  loss: 1913448.750000, mae: 3106.802002, mean_q: 3190.865723\n",
      "wrong_move\n",
      "   6516/500000: episode: 6417, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 3908542.500000, mae: 3138.729980, mean_q: 3331.470947\n",
      "wrong_move\n",
      "   6517/500000: episode: 6418, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 320.000 [320.000, 320.000],  loss: 3831544.500000, mae: 3108.934570, mean_q: 3516.949707\n",
      "wrong_move\n",
      "   6518/500000: episode: 6419, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3214.000 [3214.000, 3214.000],  loss: 691211.000000, mae: 3107.319824, mean_q: 2360.749512\n",
      "wrong_move\n",
      "   6519/500000: episode: 6420, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3894.000 [3894.000, 3894.000],  loss: 6116650.500000, mae: 3108.450684, mean_q: 3474.197998\n",
      "wrong_move\n",
      "   6520/500000: episode: 6421, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3214.000 [3214.000, 3214.000],  loss: 4399261.000000, mae: 3108.005371, mean_q: 2212.401367\n",
      "wrong_move\n",
      "   6521/500000: episode: 6422, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 6065521.500000, mae: 3110.029053, mean_q: 3403.965576\n",
      "wrong_move\n",
      "   6522/500000: episode: 6423, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3554.000 [3554.000, 3554.000],  loss: 5519782.000000, mae: 3109.507812, mean_q: 1805.325317\n",
      "wrong_move\n",
      "   6523/500000: episode: 6424, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3318.000 [3318.000, 3318.000],  loss: 4299847.500000, mae: 3143.150635, mean_q: 4044.807617\n",
      "wrong_move\n",
      "   6524/500000: episode: 6425, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3529.000 [3529.000, 3529.000],  loss: 26862804.000000, mae: 3111.729736, mean_q: 3136.407471\n",
      "wrong_move\n",
      "   6525/500000: episode: 6426, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3214.000 [3214.000, 3214.000],  loss: 2283938.500000, mae: 3112.308594, mean_q: 2758.908203\n",
      "wrong_move\n",
      "   6526/500000: episode: 6427, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3786.000 [3786.000, 3786.000],  loss: 10272296.000000, mae: 3114.228271, mean_q: 3277.378906\n",
      "wrong_move\n",
      "   6527/500000: episode: 6428, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 3636280.000000, mae: 3113.762207, mean_q: 3268.477051\n",
      "wrong_move\n",
      "   6528/500000: episode: 6429, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 923.000 [923.000, 923.000],  loss: 819262.875000, mae: 3114.272949, mean_q: 3129.071777\n",
      "wrong_move\n",
      "   6529/500000: episode: 6430, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1372.000 [1372.000, 1372.000],  loss: 1487780.750000, mae: 3113.111572, mean_q: 2131.162598\n",
      "wrong_move\n",
      "   6530/500000: episode: 6431, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 1947137.625000, mae: 3120.553955, mean_q: 3597.022217\n",
      "wrong_move\n",
      "   6531/500000: episode: 6432, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 5476336.500000, mae: 3115.617920, mean_q: 3718.938965\n",
      "wrong_move\n",
      "   6532/500000: episode: 6433, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2689.000 [2689.000, 2689.000],  loss: 4018554.250000, mae: 3113.527588, mean_q: 2048.591309\n",
      "wrong_move\n",
      "   6533/500000: episode: 6434, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3511.000 [3511.000, 3511.000],  loss: 2069651.000000, mae: 3114.698730, mean_q: 3137.561768\n",
      "wrong_move\n",
      "   6534/500000: episode: 6435, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2484.000 [2484.000, 2484.000],  loss: 1036051.625000, mae: 3114.756348, mean_q: 2033.515503\n",
      "wrong_move\n",
      "   6535/500000: episode: 6436, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2417.000 [2417.000, 2417.000],  loss: 4021828.250000, mae: 3116.812012, mean_q: 2754.902832\n",
      "wrong_move\n",
      "   6536/500000: episode: 6437, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 256.000 [256.000, 256.000],  loss: 14144612.000000, mae: 3116.953369, mean_q: 2959.486328\n",
      "wrong_move\n",
      "   6537/500000: episode: 6438, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 5743272.000000, mae: 3116.701172, mean_q: 3101.430176\n",
      "wrong_move\n",
      "   6538/500000: episode: 6439, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 303.000 [303.000, 303.000],  loss: 1247817.125000, mae: 3116.786377, mean_q: 2662.194824\n",
      "wrong_move\n",
      "   6539/500000: episode: 6440, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 885.000 [885.000, 885.000],  loss: 33975820.000000, mae: 3118.810547, mean_q: 2975.296631\n",
      "wrong_move\n",
      "   6540/500000: episode: 6441, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1759.000 [1759.000, 1759.000],  loss: 3158924.000000, mae: 3128.065918, mean_q: 4429.163086\n",
      "wrong_move\n",
      "   6541/500000: episode: 6442, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 11404552.000000, mae: 3124.466309, mean_q: 3001.353760\n",
      "wrong_move\n",
      "   6542/500000: episode: 6443, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1488.000 [1488.000, 1488.000],  loss: 3142363.000000, mae: 3123.292969, mean_q: 2769.770264\n",
      "wrong_move\n",
      "   6543/500000: episode: 6444, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3837.000 [3837.000, 3837.000],  loss: 3650960.250000, mae: 3123.199707, mean_q: 2778.066406\n",
      "wrong_move\n",
      "   6544/500000: episode: 6445, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2484.000 [2484.000, 2484.000],  loss: 7338818.500000, mae: 3135.227051, mean_q: 3210.414307\n",
      "wrong_move\n",
      "   6545/500000: episode: 6446, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 3328608.000000, mae: 3125.485352, mean_q: 2033.453857\n",
      "wrong_move\n",
      "   6546/500000: episode: 6447, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 10616990.000000, mae: 3154.037109, mean_q: 5324.069336\n",
      "wrong_move\n",
      "   6547/500000: episode: 6448, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 7762264.000000, mae: 3129.290039, mean_q: 2930.995605\n",
      "wrong_move\n",
      "   6548/500000: episode: 6449, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 6637088.500000, mae: 3130.667969, mean_q: 2328.588379\n",
      "wrong_move\n",
      "   6549/500000: episode: 6450, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2478.000 [2478.000, 2478.000],  loss: 1716273.000000, mae: 3131.376465, mean_q: 3036.130127\n",
      "wrong_move\n",
      "   6550/500000: episode: 6451, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 6158781.000000, mae: 3130.875000, mean_q: 2096.748779\n",
      "wrong_move\n",
      "   6551/500000: episode: 6452, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2228.000 [2228.000, 2228.000],  loss: 3132710.500000, mae: 3131.645752, mean_q: 2620.058594\n",
      "wrong_move\n",
      "   6552/500000: episode: 6453, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 831.000 [831.000, 831.000],  loss: 7019927.000000, mae: 3131.196289, mean_q: 2636.383789\n",
      "wrong_move\n",
      "   6553/500000: episode: 6454, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 402.000 [402.000, 402.000],  loss: 2643666.500000, mae: 3175.877441, mean_q: 3274.826172\n",
      "wrong_move\n",
      "   6554/500000: episode: 6455, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3554.000 [3554.000, 3554.000],  loss: 808997.375000, mae: 3132.300781, mean_q: 3551.190674\n",
      "wrong_move\n",
      "   6555/500000: episode: 6456, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 768.000 [768.000, 768.000],  loss: 8097115.000000, mae: 3130.959717, mean_q: 2176.815430\n",
      "wrong_move\n",
      "   6556/500000: episode: 6457, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3321.000 [3321.000, 3321.000],  loss: 4697408.000000, mae: 3130.218506, mean_q: 2979.684570\n",
      "wrong_move\n",
      "   6557/500000: episode: 6458, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 4222321.000000, mae: 3129.427734, mean_q: 2040.848511\n",
      "wrong_move\n",
      "   6558/500000: episode: 6459, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 5916906.500000, mae: 3133.355713, mean_q: 4973.286133\n",
      "wrong_move\n",
      "   6559/500000: episode: 6460, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1613.000 [1613.000, 1613.000],  loss: 14237438.000000, mae: 3128.752686, mean_q: 2198.392578\n",
      "wrong_move\n",
      "   6560/500000: episode: 6461, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 303.000 [303.000, 303.000],  loss: 11253721.000000, mae: 3129.441162, mean_q: 2657.785645\n",
      "wrong_move\n",
      "   6561/500000: episode: 6462, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 341111680.000000, mae: 3223.081543, mean_q: 5575.125000\n",
      "wrong_move\n",
      "   6562/500000: episode: 6463, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 3523075.750000, mae: 3127.591309, mean_q: 1665.120605\n",
      "wrong_move\n",
      "   6563/500000: episode: 6464, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 3677448.000000, mae: 3129.747314, mean_q: 2910.285889\n",
      "wrong_move\n",
      "   6564/500000: episode: 6465, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2573.000 [2573.000, 2573.000],  loss: 1135768.250000, mae: 3131.500488, mean_q: 3111.021973\n",
      "wrong_move\n",
      "   6565/500000: episode: 6466, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 831.000 [831.000, 831.000],  loss: 2791235.250000, mae: 3132.233887, mean_q: 2200.976562\n",
      "wrong_move\n",
      "   6566/500000: episode: 6467, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 14091094.000000, mae: 3134.327393, mean_q: 2339.509766\n",
      "wrong_move\n",
      "   6567/500000: episode: 6468, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 370.000 [370.000, 370.000],  loss: 25892848.000000, mae: 3141.239990, mean_q: 2868.356445\n",
      "wrong_move\n",
      "   6568/500000: episode: 6469, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1410.000 [1410.000, 1410.000],  loss: 1516326.750000, mae: 3138.286621, mean_q: 2171.537109\n",
      "wrong_move\n",
      "   6569/500000: episode: 6470, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 370.000 [370.000, 370.000],  loss: 744090.125000, mae: 3139.753906, mean_q: 2809.308594\n",
      "wrong_move\n",
      "   6570/500000: episode: 6471, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3505.000 [3505.000, 3505.000],  loss: 5666012.000000, mae: 3150.145020, mean_q: 3518.936523\n",
      "wrong_move\n",
      "   6571/500000: episode: 6472, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1307.000 [1307.000, 1307.000],  loss: 1351222.750000, mae: 3143.179199, mean_q: 2246.458496\n",
      "wrong_move\n",
      "   6572/500000: episode: 6473, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1963.000 [1963.000, 1963.000],  loss: 6481126.000000, mae: 3155.914062, mean_q: 4539.297852\n",
      "wrong_move\n",
      "   6573/500000: episode: 6474, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2573.000 [2573.000, 2573.000],  loss: 4524325.000000, mae: 3146.513184, mean_q: 2133.829834\n",
      "wrong_move\n",
      "   6574/500000: episode: 6475, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 370.000 [370.000, 370.000],  loss: 1353911.375000, mae: 3147.945068, mean_q: 2552.211670\n",
      "wrong_move\n",
      "   6575/500000: episode: 6476, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 370.000 [370.000, 370.000],  loss: 8747773.000000, mae: 3150.854248, mean_q: 3089.159912\n",
      "wrong_move\n",
      "   6576/500000: episode: 6477, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 299.000 [299.000, 299.000],  loss: 4735084.500000, mae: 3150.924072, mean_q: 2110.862305\n",
      "wrong_move\n",
      "   6577/500000: episode: 6478, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3232.000 [3232.000, 3232.000],  loss: 1156393.250000, mae: 3149.368164, mean_q: 2303.604492\n",
      "wrong_move\n",
      "   6578/500000: episode: 6479, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2144.000 [2144.000, 2144.000],  loss: 4734136.000000, mae: 3149.830322, mean_q: 2079.682373\n",
      "wrong_move\n",
      "   6579/500000: episode: 6480, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 306.000 [306.000, 306.000],  loss: 1829982.500000, mae: 3150.757568, mean_q: 2453.771484\n",
      "wrong_move\n",
      "   6580/500000: episode: 6481, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3244.000 [3244.000, 3244.000],  loss: 7900767.500000, mae: 3152.878174, mean_q: 3218.974609\n",
      "wrong_move\n",
      "   6581/500000: episode: 6482, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3870.000 [3870.000, 3870.000],  loss: 1042744.812500, mae: 3151.827881, mean_q: 2709.181152\n",
      "wrong_move\n",
      "   6582/500000: episode: 6483, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3323.000 [3323.000, 3323.000],  loss: 6527730.000000, mae: 3151.247559, mean_q: 2614.262207\n",
      "wrong_move\n",
      "   6583/500000: episode: 6484, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3356.000 [3356.000, 3356.000],  loss: 5892041.000000, mae: 3151.423096, mean_q: 2859.520752\n",
      "wrong_move\n",
      "   6584/500000: episode: 6485, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3244.000 [3244.000, 3244.000],  loss: 713576.125000, mae: 3150.944092, mean_q: 3284.958984\n",
      "wrong_move\n",
      "   6585/500000: episode: 6486, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1538.000 [1538.000, 1538.000],  loss: 1752670.250000, mae: 3151.695801, mean_q: 2354.837402\n",
      "wrong_move\n",
      "   6586/500000: episode: 6487, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1017.000 [1017.000, 1017.000],  loss: 6369376.500000, mae: 3151.307373, mean_q: 2567.104980\n",
      "wrong_move\n",
      "   6587/500000: episode: 6488, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3217.000 [3217.000, 3217.000],  loss: 3166123.500000, mae: 3152.122070, mean_q: 3000.493652\n",
      "wrong_move\n",
      "   6588/500000: episode: 6489, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3244.000 [3244.000, 3244.000],  loss: 1825082.125000, mae: 3151.797852, mean_q: 1907.576416\n",
      "wrong_move\n",
      "   6589/500000: episode: 6490, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 6.000 [6.000, 6.000],  loss: 5346342.500000, mae: 3153.203613, mean_q: 2924.605469\n",
      "wrong_move\n",
      "   6590/500000: episode: 6491, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1655242.750000, mae: 3155.459961, mean_q: 2136.241699\n",
      "wrong_move\n",
      "   6591/500000: episode: 6492, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2682.000 [2682.000, 2682.000],  loss: 24709100.000000, mae: 3156.603027, mean_q: 3013.952148\n",
      "wrong_move\n",
      "   6592/500000: episode: 6493, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 478.000 [478.000, 478.000],  loss: 10046437.000000, mae: 3158.447021, mean_q: 3028.700439\n",
      "wrong_move\n",
      "   6593/500000: episode: 6494, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 908.000 [908.000, 908.000],  loss: 19804150.000000, mae: 3168.076416, mean_q: 3185.567383\n",
      "wrong_move\n",
      "   6594/500000: episode: 6495, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 14454455.000000, mae: 3159.946777, mean_q: 2639.722412\n",
      "wrong_move\n",
      "   6595/500000: episode: 6496, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1367.000 [1367.000, 1367.000],  loss: 1104085.250000, mae: 3163.869385, mean_q: 3926.927490\n",
      "wrong_move\n",
      "   6596/500000: episode: 6497, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2748.000 [2748.000, 2748.000],  loss: 14391570.000000, mae: 3162.586426, mean_q: 2139.606445\n",
      "wrong_move\n",
      "   6597/500000: episode: 6498, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1538.000 [1538.000, 1538.000],  loss: 13438093.000000, mae: 3162.748535, mean_q: 2690.253906\n",
      "wrong_move\n",
      "   6598/500000: episode: 6499, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 935.000 [935.000, 935.000],  loss: 19099542.000000, mae: 3162.631592, mean_q: 2234.232910\n",
      "wrong_move\n",
      "   6599/500000: episode: 6500, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1367.000 [1367.000, 1367.000],  loss: 2641251.000000, mae: 3162.370850, mean_q: 2504.181152\n",
      "wrong_move\n",
      "   6600/500000: episode: 6501, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2748.000 [2748.000, 2748.000],  loss: 1068033.750000, mae: 3162.446777, mean_q: 2580.207520\n",
      "wrong_move\n",
      "   6601/500000: episode: 6502, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 240.000 [240.000, 240.000],  loss: 7374272.000000, mae: 3161.592041, mean_q: 2530.311523\n",
      "wrong_move\n",
      "   6602/500000: episode: 6503, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1448.000 [1448.000, 1448.000],  loss: 568382.250000, mae: 3159.737793, mean_q: 2169.184082\n",
      "wrong_move\n",
      "   6603/500000: episode: 6504, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3170.000 [3170.000, 3170.000],  loss: 53010592.000000, mae: 3161.697266, mean_q: 4221.025391\n",
      "wrong_move\n",
      "   6604/500000: episode: 6505, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4010.000 [4010.000, 4010.000],  loss: 2635929.500000, mae: 3159.700195, mean_q: 2962.431641\n",
      "wrong_move\n",
      "   6605/500000: episode: 6506, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3533.000 [3533.000, 3533.000],  loss: 14149498.000000, mae: 3159.484375, mean_q: 2750.136719\n",
      "wrong_move\n",
      "   6606/500000: episode: 6507, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 12600736.000000, mae: 3158.625000, mean_q: 2493.979980\n",
      "wrong_move\n",
      "   6607/500000: episode: 6508, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3369.000 [3369.000, 3369.000],  loss: 4216033.000000, mae: 3158.523438, mean_q: 2984.574219\n",
      "wrong_move\n",
      "   6608/500000: episode: 6509, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9392495.000000, mae: 3157.101318, mean_q: 1972.491821\n",
      "wrong_move\n",
      "   6609/500000: episode: 6510, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1697.000 [1697.000, 1697.000],  loss: 18751604.000000, mae: 3157.057617, mean_q: 3148.310059\n",
      "wrong_move\n",
      "   6610/500000: episode: 6511, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3449.000 [3449.000, 3449.000],  loss: 3000937.000000, mae: 3162.678711, mean_q: 2645.071533\n",
      "wrong_move\n",
      "   6611/500000: episode: 6512, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2573.000 [2573.000, 2573.000],  loss: 3509247.000000, mae: 3154.851074, mean_q: 2693.743652\n",
      "wrong_move\n",
      "   6612/500000: episode: 6513, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1716396.625000, mae: 3155.148438, mean_q: 3793.387207\n",
      "wrong_move\n",
      "   6613/500000: episode: 6514, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2000.000 [2000.000, 2000.000],  loss: 10390857.000000, mae: 3154.314941, mean_q: 2812.014648\n",
      "wrong_move\n",
      "   6614/500000: episode: 6515, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3449.000 [3449.000, 3449.000],  loss: 7474938.000000, mae: 3152.335205, mean_q: 2886.013672\n",
      "wrong_move\n",
      "   6615/500000: episode: 6516, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2061.000 [2061.000, 2061.000],  loss: 14030996.000000, mae: 3149.899170, mean_q: 2701.479980\n",
      "wrong_move\n",
      "   6616/500000: episode: 6517, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 315.000 [315.000, 315.000],  loss: 4756271.500000, mae: 3149.487549, mean_q: 3156.487305\n",
      "wrong_move\n",
      "   6617/500000: episode: 6518, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1648.000 [1648.000, 1648.000],  loss: 8396826.000000, mae: 3147.761230, mean_q: 2546.288574\n",
      "wrong_move\n",
      "   6618/500000: episode: 6519, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 246.000 [246.000, 246.000],  loss: 3264920.250000, mae: 3147.076172, mean_q: 2611.981934\n",
      "wrong_move\n",
      "   6619/500000: episode: 6520, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3010.000 [3010.000, 3010.000],  loss: 2995044.750000, mae: 3156.650635, mean_q: 2454.892090\n",
      "wrong_move\n",
      "   6620/500000: episode: 6521, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 388.000 [388.000, 388.000],  loss: 5051670.000000, mae: 3145.724609, mean_q: 2100.500977\n",
      "wrong_move\n",
      "   6621/500000: episode: 6522, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1890.000 [1890.000, 1890.000],  loss: 1723211.000000, mae: 3150.594238, mean_q: 3427.582275\n",
      "wrong_move\n",
      "   6622/500000: episode: 6523, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 942.000 [942.000, 942.000],  loss: 1365518.250000, mae: 3147.362305, mean_q: 2595.825195\n",
      "wrong_move\n",
      "   6623/500000: episode: 6524, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 20191796.000000, mae: 3149.125488, mean_q: 2859.235596\n",
      "wrong_move\n",
      "   6624/500000: episode: 6525, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 246.000 [246.000, 246.000],  loss: 2500636.000000, mae: 3152.159668, mean_q: 3905.206543\n",
      "wrong_move\n",
      "   6625/500000: episode: 6526, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3344.000 [3344.000, 3344.000],  loss: 964212.125000, mae: 3150.543701, mean_q: 2259.312012\n",
      "wrong_move\n",
      "   6626/500000: episode: 6527, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3992.000 [3992.000, 3992.000],  loss: 4353214.000000, mae: 3157.569336, mean_q: 3243.489746\n",
      "wrong_move\n",
      "   6627/500000: episode: 6528, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1718.000 [1718.000, 1718.000],  loss: 1130383.500000, mae: 3152.809082, mean_q: 1817.459839\n",
      "wrong_move\n",
      "   6628/500000: episode: 6529, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3932.000 [3932.000, 3932.000],  loss: 987222.937500, mae: 3153.826172, mean_q: 2568.037109\n",
      "wrong_move\n",
      "   6629/500000: episode: 6530, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1910.000 [1910.000, 1910.000],  loss: 7307273.000000, mae: 3400.167725, mean_q: 5497.763184\n",
      "wrong_move\n",
      "   6630/500000: episode: 6531, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 942.000 [942.000, 942.000],  loss: 1049728.250000, mae: 3164.573730, mean_q: 4941.119141\n",
      "wrong_move\n",
      "   6631/500000: episode: 6532, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2573.000 [2573.000, 2573.000],  loss: 1669482.125000, mae: 3158.561035, mean_q: 3175.285645\n",
      "wrong_move\n",
      "   6632/500000: episode: 6533, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3952.000 [3952.000, 3952.000],  loss: 4045259.500000, mae: 3158.739502, mean_q: 2213.504883\n",
      "wrong_move\n",
      "   6633/500000: episode: 6534, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 942.000 [942.000, 942.000],  loss: 1645492.250000, mae: 3160.939941, mean_q: 2283.509277\n",
      "wrong_move\n",
      "   6634/500000: episode: 6535, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 942.000 [942.000, 942.000],  loss: 3114616.750000, mae: 3196.947510, mean_q: 2686.793457\n",
      "wrong_move\n",
      "   6635/500000: episode: 6536, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3626.000 [3626.000, 3626.000],  loss: 11303704.000000, mae: 3164.930908, mean_q: 2574.001465\n",
      "wrong_move\n",
      "   6636/500000: episode: 6537, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3449.000 [3449.000, 3449.000],  loss: 1869397.000000, mae: 3165.578125, mean_q: 2440.913818\n",
      "wrong_move\n",
      "   6637/500000: episode: 6538, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1095.000 [1095.000, 1095.000],  loss: 139765344.000000, mae: 3192.965088, mean_q: 4242.596680\n",
      "wrong_move\n",
      "   6638/500000: episode: 6539, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3344.000 [3344.000, 3344.000],  loss: 2154502.500000, mae: 3170.214844, mean_q: 2842.822266\n",
      "wrong_move\n",
      "   6639/500000: episode: 6540, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 14239881.000000, mae: 3172.430664, mean_q: 2417.563477\n",
      "wrong_move\n",
      "   6640/500000: episode: 6541, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2362.000 [2362.000, 2362.000],  loss: 7344277.500000, mae: 3173.655762, mean_q: 2952.905273\n",
      "wrong_move\n",
      "   6641/500000: episode: 6542, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1145.000 [1145.000, 1145.000],  loss: 7158115.500000, mae: 3174.857910, mean_q: 2516.489258\n",
      "wrong_move\n",
      "   6642/500000: episode: 6543, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3344.000 [3344.000, 3344.000],  loss: 5821133.500000, mae: 3176.370361, mean_q: 2516.918701\n",
      "wrong_move\n",
      "   6643/500000: episode: 6544, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3344.000 [3344.000, 3344.000],  loss: 1672271.000000, mae: 3177.672363, mean_q: 1780.360718\n",
      "wrong_move\n",
      "   6644/500000: episode: 6545, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3344.000 [3344.000, 3344.000],  loss: 16751651.000000, mae: 3180.583008, mean_q: 3297.749268\n",
      "wrong_move\n",
      "   6645/500000: episode: 6546, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2587.000 [2587.000, 2587.000],  loss: 916684.875000, mae: 3181.627930, mean_q: 3271.465820\n",
      "wrong_move\n",
      "   6646/500000: episode: 6547, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2848.000 [2848.000, 2848.000],  loss: 3540829.750000, mae: 3181.147705, mean_q: 2725.538574\n",
      "wrong_move\n",
      "   6647/500000: episode: 6548, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3858.000 [3858.000, 3858.000],  loss: 7866069.000000, mae: 3181.500488, mean_q: 2208.484619\n",
      "wrong_move\n",
      "   6648/500000: episode: 6549, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2682.000 [2682.000, 2682.000],  loss: 1191201.500000, mae: 3181.452637, mean_q: 2346.052246\n",
      "wrong_move\n",
      "   6649/500000: episode: 6550, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2321.000 [2321.000, 2321.000],  loss: 764270.125000, mae: 3189.206543, mean_q: 2822.606445\n",
      "wrong_move\n",
      "   6650/500000: episode: 6551, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 8997425.000000, mae: 3180.541504, mean_q: 1854.627686\n",
      "wrong_move\n",
      "   6651/500000: episode: 6552, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1843.000 [1843.000, 1843.000],  loss: 3813042.000000, mae: 3180.812500, mean_q: 2324.296875\n",
      "wrong_move\n",
      "   6652/500000: episode: 6553, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 82.000 [82.000, 82.000],  loss: 2615378.500000, mae: 3179.251465, mean_q: 1682.489258\n",
      "wrong_move\n",
      "   6653/500000: episode: 6554, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3344.000 [3344.000, 3344.000],  loss: 15739073.000000, mae: 3180.179443, mean_q: 2894.099854\n",
      "wrong_move\n",
      "   6654/500000: episode: 6555, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 648.000 [648.000, 648.000],  loss: 928523.875000, mae: 3180.378418, mean_q: 2823.810303\n",
      "wrong_move\n",
      "   6655/500000: episode: 6556, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1648.000 [1648.000, 1648.000],  loss: 8162426.000000, mae: 3179.654053, mean_q: 1493.684082\n",
      "wrong_move\n",
      "   6656/500000: episode: 6557, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2023.000 [2023.000, 2023.000],  loss: 16213302.000000, mae: 3181.749268, mean_q: 2583.686768\n",
      "wrong_move\n",
      "   6657/500000: episode: 6558, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2116.000 [2116.000, 2116.000],  loss: 4914449.500000, mae: 3179.784912, mean_q: 1874.733154\n",
      "wrong_move\n",
      "   6658/500000: episode: 6559, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3344.000 [3344.000, 3344.000],  loss: 1657710.125000, mae: 3179.537598, mean_q: 2269.118652\n",
      "wrong_move\n",
      "   6659/500000: episode: 6560, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2362.000 [2362.000, 2362.000],  loss: 828787.937500, mae: 3177.271973, mean_q: 1477.454346\n",
      "wrong_move\n",
      "   6660/500000: episode: 6561, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2321.000 [2321.000, 2321.000],  loss: 11071012.000000, mae: 3176.312500, mean_q: 2036.760742\n",
      "wrong_move\n",
      "   6661/500000: episode: 6562, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4008.000 [4008.000, 4008.000],  loss: 5668197.000000, mae: 3173.949951, mean_q: 2714.132080\n",
      "wrong_move\n",
      "   6662/500000: episode: 6563, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1145.000 [1145.000, 1145.000],  loss: 8368406.000000, mae: 3171.932129, mean_q: 2804.262207\n",
      "wrong_move\n",
      "   6663/500000: episode: 6564, duration: 0.155s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 23.000 [23.000, 23.000],  loss: 1480575.250000, mae: 3168.846436, mean_q: 3350.043213\n",
      "wrong_move\n",
      "   6664/500000: episode: 6565, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2362.000 [2362.000, 2362.000],  loss: 3111853.000000, mae: 3167.244873, mean_q: 3096.239258\n",
      "wrong_move\n",
      "   6665/500000: episode: 6566, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2321.000 [2321.000, 2321.000],  loss: 2443322.500000, mae: 3164.655762, mean_q: 2685.008301\n",
      "wrong_move\n",
      "   6666/500000: episode: 6567, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1145.000 [1145.000, 1145.000],  loss: 6638609.000000, mae: 3163.142090, mean_q: 2888.585205\n",
      "wrong_move\n",
      "   6667/500000: episode: 6568, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2829.000 [2829.000, 2829.000],  loss: 3062836.250000, mae: 3162.068115, mean_q: 3152.918457\n",
      "wrong_move\n",
      "   6668/500000: episode: 6569, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2321.000 [2321.000, 2321.000],  loss: 10792910.000000, mae: 3159.920166, mean_q: 3160.008789\n",
      "wrong_move\n",
      "   6669/500000: episode: 6570, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1145.000 [1145.000, 1145.000],  loss: 4401925.500000, mae: 3157.997314, mean_q: 2445.738281\n",
      "wrong_move\n",
      "   6670/500000: episode: 6571, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2321.000 [2321.000, 2321.000],  loss: 3499048.000000, mae: 3155.958008, mean_q: 2305.753906\n",
      "wrong_move\n",
      "   6671/500000: episode: 6572, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 2036841.000000, mae: 3155.574463, mean_q: 3009.197266\n",
      "wrong_move\n",
      "   6672/500000: episode: 6573, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1608.000 [1608.000, 1608.000],  loss: 3450863.000000, mae: 3156.201660, mean_q: 2870.828125\n",
      "wrong_move\n",
      "   6673/500000: episode: 6574, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2321.000 [2321.000, 2321.000],  loss: 1388563.000000, mae: 3155.953613, mean_q: 2650.603516\n",
      "wrong_move\n",
      "   6674/500000: episode: 6575, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1145.000 [1145.000, 1145.000],  loss: 4498527.500000, mae: 3155.482178, mean_q: 1939.221436\n",
      "wrong_move\n",
      "   6675/500000: episode: 6576, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1718.000 [1718.000, 1718.000],  loss: 13502755.000000, mae: 3156.782715, mean_q: 3127.933350\n",
      "wrong_move\n",
      "   6676/500000: episode: 6577, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1145.000 [1145.000, 1145.000],  loss: 3030982.000000, mae: 3154.942871, mean_q: 1849.452148\n",
      "wrong_move\n",
      "   6677/500000: episode: 6578, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2167.000 [2167.000, 2167.000],  loss: 13028557.000000, mae: 3199.760498, mean_q: 3898.847168\n",
      "wrong_move\n",
      "   6678/500000: episode: 6579, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1648.000 [1648.000, 1648.000],  loss: 7661536.500000, mae: 3156.298340, mean_q: 2703.161133\n",
      "wrong_move\n",
      "   6679/500000: episode: 6580, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1976.000 [1976.000, 1976.000],  loss: 11691132.000000, mae: 3157.664795, mean_q: 3421.898438\n",
      "wrong_move\n",
      "   6680/500000: episode: 6581, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1145.000 [1145.000, 1145.000],  loss: 1758874.625000, mae: 3156.179199, mean_q: 2579.221191\n",
      "wrong_move\n",
      "   6681/500000: episode: 6582, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 2205091.000000, mae: 3156.562012, mean_q: 2981.222656\n",
      "wrong_move\n",
      "   6682/500000: episode: 6583, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1145.000 [1145.000, 1145.000],  loss: 8001318.500000, mae: 3156.570312, mean_q: 1617.860596\n",
      "wrong_move\n",
      "   6683/500000: episode: 6584, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2301.000 [2301.000, 2301.000],  loss: 8249112.000000, mae: 3157.744629, mean_q: 3077.923584\n",
      "wrong_move\n",
      "   6684/500000: episode: 6585, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3303.000 [3303.000, 3303.000],  loss: 4954011.000000, mae: 3158.098633, mean_q: 2709.536133\n",
      "wrong_move\n",
      "   6685/500000: episode: 6586, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 979.000 [979.000, 979.000],  loss: 3215427.500000, mae: 3240.251953, mean_q: 2632.715088\n",
      "wrong_move\n",
      "   6686/500000: episode: 6587, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1145.000 [1145.000, 1145.000],  loss: 2648605.250000, mae: 3158.022461, mean_q: 2522.149658\n",
      "wrong_move\n",
      "   6687/500000: episode: 6588, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 950.000 [950.000, 950.000],  loss: 1640434.750000, mae: 3157.938721, mean_q: 2955.030029\n",
      "wrong_move\n",
      "   6688/500000: episode: 6589, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 808.000 [808.000, 808.000],  loss: 9206465.000000, mae: 3157.962402, mean_q: 3258.563477\n",
      "wrong_move\n",
      "   6689/500000: episode: 6590, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1145.000 [1145.000, 1145.000],  loss: 3303471.250000, mae: 3158.755615, mean_q: 3484.880371\n",
      "wrong_move\n",
      "   6690/500000: episode: 6591, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 808.000 [808.000, 808.000],  loss: 6040164.000000, mae: 3160.745117, mean_q: 3704.485107\n",
      "wrong_move\n",
      "   6691/500000: episode: 6592, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2576.000 [2576.000, 2576.000],  loss: 4062228.250000, mae: 3156.957275, mean_q: 2509.300781\n",
      "wrong_move\n",
      "   6692/500000: episode: 6593, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 808.000 [808.000, 808.000],  loss: 2365027.500000, mae: 3160.262695, mean_q: 2285.549561\n",
      "wrong_move\n",
      "   6693/500000: episode: 6594, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3533.000 [3533.000, 3533.000],  loss: 1010155.750000, mae: 3158.879639, mean_q: 3189.395508\n",
      "wrong_move\n",
      "   6694/500000: episode: 6595, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 808.000 [808.000, 808.000],  loss: 851126.875000, mae: 3159.504395, mean_q: 3500.749023\n",
      "wrong_move\n",
      "   6695/500000: episode: 6596, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 14003280.000000, mae: 3158.600098, mean_q: 2121.775879\n",
      "wrong_move\n",
      "   6696/500000: episode: 6597, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 808.000 [808.000, 808.000],  loss: 7082933.000000, mae: 3159.927490, mean_q: 3654.103027\n",
      "wrong_move\n",
      "   6697/500000: episode: 6598, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 808.000 [808.000, 808.000],  loss: 8704056.000000, mae: 3163.272461, mean_q: 3658.351074\n",
      "wrong_move\n",
      "   6698/500000: episode: 6599, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3430.000 [3430.000, 3430.000],  loss: 13082700.000000, mae: 3162.828613, mean_q: 3867.876953\n",
      "wrong_move\n",
      "   6699/500000: episode: 6600, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 246.000 [246.000, 246.000],  loss: 4520073.000000, mae: 3164.741699, mean_q: 3487.958984\n",
      "wrong_move\n",
      "   6700/500000: episode: 6601, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 68.000 [68.000, 68.000],  loss: 8086154.000000, mae: 3163.548096, mean_q: 3214.823242\n",
      "wrong_move\n",
      "   6701/500000: episode: 6602, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3936.000 [3936.000, 3936.000],  loss: 3447080.250000, mae: 3164.493652, mean_q: 3310.804199\n",
      "wrong_move\n",
      "   6702/500000: episode: 6603, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3430.000 [3430.000, 3430.000],  loss: 12196004.000000, mae: 3163.133789, mean_q: 2561.009277\n",
      "wrong_move\n",
      "   6703/500000: episode: 6604, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3430.000 [3430.000, 3430.000],  loss: 4677941.500000, mae: 3163.397949, mean_q: 3359.930176\n",
      "wrong_move\n",
      "   6704/500000: episode: 6605, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3430.000 [3430.000, 3430.000],  loss: 3247917.500000, mae: 3163.859863, mean_q: 2914.849121\n",
      "wrong_move\n",
      "   6705/500000: episode: 6606, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3165.000 [3165.000, 3165.000],  loss: 4782537.000000, mae: 3161.889648, mean_q: 2438.818848\n",
      "wrong_move\n",
      "   6706/500000: episode: 6607, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2023.000 [2023.000, 2023.000],  loss: 2564468.500000, mae: 3161.375244, mean_q: 2849.461426\n",
      "wrong_move\n",
      "   6708/500000: episode: 6608, duration: 0.136s, episode steps:   2, steps per second:  15, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 3494.000 [3494.000, 3494.000],  loss: 8754974.000000, mae: 3160.166992, mean_q: 2994.891602\n",
      "wrong_move\n",
      "   6709/500000: episode: 6609, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2371.000 [2371.000, 2371.000],  loss: 6722560.000000, mae: 3157.010986, mean_q: 2570.942627\n",
      "wrong_move\n",
      "   6710/500000: episode: 6610, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2714.000 [2714.000, 2714.000],  loss: 8751760.000000, mae: 3155.100098, mean_q: 3382.855957\n",
      "wrong_move\n",
      "   6711/500000: episode: 6611, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2023.000 [2023.000, 2023.000],  loss: 7153816.500000, mae: 3161.273193, mean_q: 2393.811523\n",
      "wrong_move\n",
      "   6712/500000: episode: 6612, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3430.000 [3430.000, 3430.000],  loss: 7982007.500000, mae: 3160.502441, mean_q: 3354.128662\n",
      "wrong_move\n",
      "   6713/500000: episode: 6613, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3430.000 [3430.000, 3430.000],  loss: 14197987.000000, mae: 3151.425781, mean_q: 2433.445312\n",
      "wrong_move\n",
      "   6714/500000: episode: 6614, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3430.000 [3430.000, 3430.000],  loss: 4131369.000000, mae: 3148.281494, mean_q: 2501.434082\n",
      "wrong_move\n",
      "   6715/500000: episode: 6615, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2714.000 [2714.000, 2714.000],  loss: 13596442.000000, mae: 3147.889648, mean_q: 2899.871338\n",
      "wrong_move\n",
      "   6716/500000: episode: 6616, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4041.000 [4041.000, 4041.000],  loss: 22158672.000000, mae: 3181.171387, mean_q: 3489.337646\n",
      "wrong_move\n",
      "   6717/500000: episode: 6617, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2656.000 [2656.000, 2656.000],  loss: 3152483.500000, mae: 3148.362549, mean_q: 2412.317383\n",
      "wrong_move\n",
      "   6718/500000: episode: 6618, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1231.000 [1231.000, 1231.000],  loss: 2804376.000000, mae: 3144.203125, mean_q: 2502.781738\n",
      "wrong_move\n",
      "   6719/500000: episode: 6619, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1231.000 [1231.000, 1231.000],  loss: 1236069.000000, mae: 3143.363770, mean_q: 2105.850098\n",
      "wrong_move\n",
      "   6720/500000: episode: 6620, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1328.000 [1328.000, 1328.000],  loss: 12785962.000000, mae: 3143.421631, mean_q: 2626.106201\n",
      "wrong_move\n",
      "   6721/500000: episode: 6621, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3706.000 [3706.000, 3706.000],  loss: 3966918.500000, mae: 3142.161133, mean_q: 2280.816895\n",
      "wrong_move\n",
      "   6722/500000: episode: 6622, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1649638.500000, mae: 3142.787598, mean_q: 2502.724609\n",
      "wrong_move\n",
      "   6723/500000: episode: 6623, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 958.000 [958.000, 958.000],  loss: 14777400.000000, mae: 3144.086670, mean_q: 2785.701172\n",
      "wrong_move\n",
      "   6724/500000: episode: 6624, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3706.000 [3706.000, 3706.000],  loss: 14031581.000000, mae: 3148.639648, mean_q: 2723.499268\n",
      "wrong_move\n",
      "   6725/500000: episode: 6625, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2398.000 [2398.000, 2398.000],  loss: 6809306.500000, mae: 3153.299072, mean_q: 3441.239746\n",
      "wrong_move\n",
      "   6726/500000: episode: 6626, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2467.000 [2467.000, 2467.000],  loss: 7953272.000000, mae: 3148.464844, mean_q: 3577.941895\n",
      "wrong_move\n",
      "   6727/500000: episode: 6627, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3494.000 [3494.000, 3494.000],  loss: 1587377.500000, mae: 3149.313232, mean_q: 2000.833984\n",
      "wrong_move\n",
      "   6728/500000: episode: 6628, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 68.000 [68.000, 68.000],  loss: 3387723.500000, mae: 3151.103760, mean_q: 1810.091919\n",
      "wrong_move\n",
      "   6729/500000: episode: 6629, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 229.000 [229.000, 229.000],  loss: 6153953.500000, mae: 3153.793701, mean_q: 2796.791748\n",
      "wrong_move\n",
      "   6730/500000: episode: 6630, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 246.000 [246.000, 246.000],  loss: 2789940.000000, mae: 3157.801758, mean_q: 2594.062744\n",
      "wrong_move\n",
      "   6731/500000: episode: 6631, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 68.000 [68.000, 68.000],  loss: 6664591.500000, mae: 3157.219727, mean_q: 2201.016113\n",
      "wrong_move\n",
      "   6732/500000: episode: 6632, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 68.000 [68.000, 68.000],  loss: 594282.937500, mae: 3159.617432, mean_q: 3431.756348\n",
      "wrong_move\n",
      "   6733/500000: episode: 6633, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 966.000 [966.000, 966.000],  loss: 2528383.000000, mae: 3160.055420, mean_q: 2491.918213\n",
      "wrong_move\n",
      "   6734/500000: episode: 6634, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4045.000 [4045.000, 4045.000],  loss: 16653850.000000, mae: 3163.897217, mean_q: 3712.489258\n",
      "wrong_move\n",
      "   6735/500000: episode: 6635, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 68.000 [68.000, 68.000],  loss: 5717041.500000, mae: 3162.013916, mean_q: 3415.259521\n",
      "wrong_move\n",
      "   6736/500000: episode: 6636, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1608.000 [1608.000, 1608.000],  loss: 10523942.000000, mae: 3161.535645, mean_q: 2462.073730\n",
      "wrong_move\n",
      "   6737/500000: episode: 6637, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1377.000 [1377.000, 1377.000],  loss: 1913352.500000, mae: 3160.749512, mean_q: 1681.675171\n",
      "wrong_move\n",
      "   6738/500000: episode: 6638, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2883.000 [2883.000, 2883.000],  loss: 13411963.000000, mae: 3163.081787, mean_q: 3729.048828\n",
      "wrong_move\n",
      "   6739/500000: episode: 6639, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2862.000 [2862.000, 2862.000],  loss: 1129925.500000, mae: 3162.813477, mean_q: 1929.821777\n",
      "wrong_move\n",
      "   6740/500000: episode: 6640, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 396.000 [396.000, 396.000],  loss: 1324362.750000, mae: 3164.698975, mean_q: 2563.495605\n",
      "wrong_move\n",
      "   6741/500000: episode: 6641, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 13676709.000000, mae: 3199.283936, mean_q: 4168.880371\n",
      "wrong_move\n",
      "   6742/500000: episode: 6642, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 246.000 [246.000, 246.000],  loss: 2775436.500000, mae: 3167.927979, mean_q: 2774.660645\n",
      "wrong_move\n",
      "   6743/500000: episode: 6643, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2411.000 [2411.000, 2411.000],  loss: 1997320.750000, mae: 3168.905273, mean_q: 1687.253540\n",
      "wrong_move\n",
      "   6744/500000: episode: 6644, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 813.000 [813.000, 813.000],  loss: 6335659.000000, mae: 3171.616943, mean_q: 2738.823730\n",
      "wrong_move\n",
      "   6745/500000: episode: 6645, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2914.000 [2914.000, 2914.000],  loss: 1885827.250000, mae: 3172.960938, mean_q: 2508.119141\n",
      "wrong_move\n",
      "   6746/500000: episode: 6646, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1514.000 [1514.000, 1514.000],  loss: 1013615.125000, mae: 3175.244141, mean_q: 2663.424316\n",
      "wrong_move\n",
      "   6747/500000: episode: 6647, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2914.000 [2914.000, 2914.000],  loss: 9526639.000000, mae: 3177.590332, mean_q: 2435.745117\n",
      "wrong_move\n",
      "   6748/500000: episode: 6648, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3201.000 [3201.000, 3201.000],  loss: 6345539.000000, mae: 3178.969727, mean_q: 2741.191406\n",
      "wrong_move\n",
      "   6749/500000: episode: 6649, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3462.000 [3462.000, 3462.000],  loss: 765567.375000, mae: 3180.236816, mean_q: 2853.875000\n",
      "wrong_move\n",
      "   6750/500000: episode: 6650, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3462.000 [3462.000, 3462.000],  loss: 1097838.125000, mae: 3181.090332, mean_q: 2212.901611\n",
      "wrong_move\n",
      "   6751/500000: episode: 6651, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2914.000 [2914.000, 2914.000],  loss: 3351144.500000, mae: 3186.077393, mean_q: 2369.623047\n",
      "wrong_move\n",
      "   6752/500000: episode: 6652, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3462.000 [3462.000, 3462.000],  loss: 1121717.125000, mae: 3181.886230, mean_q: 2020.265137\n",
      "wrong_move\n",
      "   6753/500000: episode: 6653, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2167.000 [2167.000, 2167.000],  loss: 1324078.625000, mae: 3210.021484, mean_q: 3374.882568\n",
      "wrong_move\n",
      "   6754/500000: episode: 6654, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2914.000 [2914.000, 2914.000],  loss: 4145142.000000, mae: 3184.062988, mean_q: 2709.075195\n",
      "wrong_move\n",
      "   6755/500000: episode: 6655, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 357.000 [357.000, 357.000],  loss: 1362096.500000, mae: 3184.187256, mean_q: 1908.809082\n",
      "wrong_move\n",
      "   6756/500000: episode: 6656, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 526.000 [526.000, 526.000],  loss: 3126851.000000, mae: 3185.660645, mean_q: 2340.814209\n",
      "wrong_move\n",
      "   6757/500000: episode: 6657, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1514.000 [1514.000, 1514.000],  loss: 14621474.000000, mae: 3187.158447, mean_q: 2955.015625\n",
      "wrong_move\n",
      "   6758/500000: episode: 6658, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 936.000 [936.000, 936.000],  loss: 2747516.500000, mae: 3186.713379, mean_q: 2578.152832\n",
      "wrong_move\n",
      "   6759/500000: episode: 6659, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3201.000 [3201.000, 3201.000],  loss: 3648818.000000, mae: 3186.399414, mean_q: 1537.474854\n",
      "wrong_move\n",
      "   6760/500000: episode: 6660, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: 907087.687500, mae: 3187.722168, mean_q: 2242.630859\n",
      "wrong_move\n",
      "   6761/500000: episode: 6661, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2211.000 [2211.000, 2211.000],  loss: 6279187.000000, mae: 3187.460938, mean_q: 2671.076904\n",
      "wrong_move\n",
      "   6762/500000: episode: 6662, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2889.000 [2889.000, 2889.000],  loss: 1049515.750000, mae: 3188.541016, mean_q: 2622.476074\n",
      "wrong_move\n",
      "   6763/500000: episode: 6663, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2914.000 [2914.000, 2914.000],  loss: 2081210.000000, mae: 3186.378418, mean_q: 1706.419067\n",
      "wrong_move\n",
      "   6764/500000: episode: 6664, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2914.000 [2914.000, 2914.000],  loss: 9001897.000000, mae: 3187.535156, mean_q: 2991.185059\n",
      "wrong_move\n",
      "   6766/500000: episode: 6665, duration: 0.188s, episode steps:   2, steps per second:  11, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2444.000 [1426.000, 3462.000],  loss: 5135556.000000, mae: 3185.847656, mean_q: 2077.845215\n",
      "wrong_move\n",
      "   6767/500000: episode: 6666, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2485.000 [2485.000, 2485.000],  loss: 9948471.000000, mae: 3185.811523, mean_q: 1781.166016\n",
      "wrong_move\n",
      "   6768/500000: episode: 6667, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3462.000 [3462.000, 3462.000],  loss: 2846792.500000, mae: 3187.790283, mean_q: 2918.854004\n",
      "wrong_move\n",
      "   6769/500000: episode: 6668, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1562.000 [1562.000, 1562.000],  loss: 3646076.500000, mae: 3178.534668, mean_q: 2428.947266\n",
      "wrong_move\n",
      "   6770/500000: episode: 6669, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 517.000 [517.000, 517.000],  loss: 4070465.500000, mae: 3188.235840, mean_q: 2661.613770\n",
      "wrong_move\n",
      "   6771/500000: episode: 6670, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 982104.375000, mae: 3149.966309, mean_q: 4356.602051\n",
      "wrong_move\n",
      "   6772/500000: episode: 6671, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2873.000 [2873.000, 2873.000],  loss: 12355055.000000, mae: 3140.462891, mean_q: 3455.172119\n",
      "wrong_move\n",
      "   6773/500000: episode: 6672, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 526.000 [526.000, 526.000],  loss: 20247568.000000, mae: 3137.405273, mean_q: 5264.428711\n",
      "wrong_move\n",
      "   6775/500000: episode: 6673, duration: 0.146s, episode steps:   2, steps per second:  14, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1864.500 [1007.000, 2722.000],  loss: 5015370.000000, mae: 3147.718262, mean_q: 5347.954590\n",
      "wrong_move\n",
      "   6776/500000: episode: 6674, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 12896773.000000, mae: 3114.788330, mean_q: 4874.734375\n",
      "wrong_move\n",
      "   6777/500000: episode: 6675, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6219977.000000, mae: 3200.432617, mean_q: 9692.228516\n",
      "wrong_move\n",
      "   6778/500000: episode: 6676, duration: 0.147s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1051.000 [1051.000, 1051.000],  loss: 4637034.000000, mae: 3177.936768, mean_q: 6121.086914\n",
      "wrong_move\n",
      "   6779/500000: episode: 6677, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3467223.000000, mae: 3135.406738, mean_q: 7974.647949\n",
      "wrong_move\n",
      "   6780/500000: episode: 6678, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2448017.000000, mae: 3296.642822, mean_q: 10447.358398\n",
      "wrong_move\n",
      "   6781/500000: episode: 6679, duration: 0.156s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2843.000 [2843.000, 2843.000],  loss: 5365542.000000, mae: 3106.527832, mean_q: 4897.609863\n",
      "wrong_move\n",
      "   6782/500000: episode: 6680, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 26063598.000000, mae: 3185.490723, mean_q: 7496.689941\n",
      "wrong_move\n",
      "   6783/500000: episode: 6681, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1645945.500000, mae: 3196.760986, mean_q: 7195.883789\n",
      "wrong_move\n",
      "   6784/500000: episode: 6682, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2511.000 [2511.000, 2511.000],  loss: 5575432.500000, mae: 3113.727783, mean_q: 6130.488281\n",
      "wrong_move\n",
      "   6785/500000: episode: 6683, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 770.000 [770.000, 770.000],  loss: 2140951.250000, mae: 3111.798828, mean_q: 5885.189453\n",
      "wrong_move\n",
      "   6786/500000: episode: 6684, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4962274.000000, mae: 3115.433105, mean_q: 6454.690918\n",
      "wrong_move\n",
      "   6787/500000: episode: 6685, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 24537680.000000, mae: 3115.269531, mean_q: 5775.506348\n",
      "wrong_move\n",
      "   6789/500000: episode: 6686, duration: 0.133s, episode steps:   2, steps per second:  15, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 60542656.000000, mae: 3162.820801, mean_q: 7734.025391\n",
      "wrong_move\n",
      "   6790/500000: episode: 6687, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2162.000 [2162.000, 2162.000],  loss: 2829776.500000, mae: 3122.364746, mean_q: 6275.539062\n",
      "wrong_move\n",
      "   6791/500000: episode: 6688, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5111618.500000, mae: 3124.247070, mean_q: 5378.687012\n",
      "wrong_move\n",
      "   6792/500000: episode: 6689, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 10453079.000000, mae: 3127.535645, mean_q: 6051.436035\n",
      "wrong_move\n",
      "   6793/500000: episode: 6690, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3133.000 [3133.000, 3133.000],  loss: 3003338.500000, mae: 3129.669434, mean_q: 5932.004883\n",
      "wrong_move\n",
      "   6794/500000: episode: 6691, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1574.000 [1574.000, 1574.000],  loss: 538628.937500, mae: 3145.339844, mean_q: 6454.584961\n",
      "wrong_move\n",
      "   6795/500000: episode: 6692, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1011565.937500, mae: 3232.332031, mean_q: 8323.052734\n",
      "wrong_move\n",
      "   6796/500000: episode: 6693, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 531.000 [531.000, 531.000],  loss: 7699560.000000, mae: 3133.095215, mean_q: 4780.492676\n",
      "wrong_move\n",
      "   6797/500000: episode: 6694, duration: 0.155s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 618.000 [618.000, 618.000],  loss: 3887787.500000, mae: 3134.666016, mean_q: 5310.026367\n",
      "wrong_move\n",
      "   6798/500000: episode: 6695, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2341.000 [2341.000, 2341.000],  loss: 2890785.000000, mae: 3135.843262, mean_q: 5244.158203\n",
      "wrong_move\n",
      "   6799/500000: episode: 6696, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6688936.500000, mae: 3150.559082, mean_q: 6723.989746\n",
      "wrong_move\n",
      "   6800/500000: episode: 6697, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2107.000 [2107.000, 2107.000],  loss: 856942528.000000, mae: 3363.134277, mean_q: 9942.914062\n",
      "wrong_move\n",
      "   6801/500000: episode: 6698, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1651096.750000, mae: 3139.605469, mean_q: 4319.405762\n",
      "wrong_move\n",
      "   6802/500000: episode: 6699, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3065.000 [3065.000, 3065.000],  loss: 1867143.375000, mae: 3148.938721, mean_q: 5886.788574\n",
      "wrong_move\n",
      "   6803/500000: episode: 6700, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 15512853.000000, mae: 3144.968994, mean_q: 5343.224609\n",
      "wrong_move\n",
      "   6804/500000: episode: 6701, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3912.000 [3912.000, 3912.000],  loss: 9970208.000000, mae: 3147.949463, mean_q: 6432.020508\n",
      "wrong_move\n",
      "   6805/500000: episode: 6702, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 636.000 [636.000, 636.000],  loss: 49076332.000000, mae: 3150.212646, mean_q: 4892.046875\n",
      "wrong_move\n",
      "   6806/500000: episode: 6703, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2547505.000000, mae: 3153.982910, mean_q: 4795.899902\n",
      "wrong_move\n",
      "   6807/500000: episode: 6704, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2602.000 [2602.000, 2602.000],  loss: 11811270.000000, mae: 3153.512695, mean_q: 6134.470703\n",
      "wrong_move\n",
      "   6808/500000: episode: 6705, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 16221286.000000, mae: 3154.917480, mean_q: 4585.933594\n",
      "wrong_move\n",
      "   6809/500000: episode: 6706, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 732.000 [732.000, 732.000],  loss: 9467768.000000, mae: 3158.080811, mean_q: 4849.441895\n",
      "wrong_move\n",
      "   6810/500000: episode: 6707, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 15971292.000000, mae: 3174.514648, mean_q: 7597.849609\n",
      "wrong_move\n",
      "   6811/500000: episode: 6708, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4134604.500000, mae: 3167.206543, mean_q: 5143.252441\n",
      "wrong_move\n",
      "   6812/500000: episode: 6709, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 16169556.000000, mae: 3173.924316, mean_q: 4790.522949\n",
      "wrong_move\n",
      "   6813/500000: episode: 6710, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4692166.000000, mae: 3172.068359, mean_q: 5309.268066\n",
      "wrong_move\n",
      "   6814/500000: episode: 6711, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3912.000 [3912.000, 3912.000],  loss: 1617710.500000, mae: 3173.372070, mean_q: 4147.732422\n",
      "wrong_move\n",
      "   6815/500000: episode: 6712, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9751653.000000, mae: 3175.406494, mean_q: 3410.881348\n",
      "wrong_move\n",
      "   6816/500000: episode: 6713, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1965.000 [1965.000, 1965.000],  loss: 6257251.000000, mae: 3179.603516, mean_q: 4401.116211\n",
      "wrong_move\n",
      "   6817/500000: episode: 6714, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2664.000 [2664.000, 2664.000],  loss: 1007505.625000, mae: 3180.215820, mean_q: 3466.837891\n",
      "wrong_move\n",
      "   6818/500000: episode: 6715, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3917188.250000, mae: 3181.975586, mean_q: 3909.162109\n",
      "wrong_move\n",
      "   6819/500000: episode: 6716, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2434.000 [2434.000, 2434.000],  loss: 3867336.500000, mae: 3183.895996, mean_q: 3971.997803\n",
      "wrong_move\n",
      "   6820/500000: episode: 6717, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 434.000 [434.000, 434.000],  loss: 3046714.000000, mae: 3186.330566, mean_q: 4514.059570\n",
      "wrong_move\n",
      "   6821/500000: episode: 6718, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3559147.000000, mae: 3186.153076, mean_q: 4577.437500\n",
      "wrong_move\n",
      "   6822/500000: episode: 6719, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2557.000 [2557.000, 2557.000],  loss: 3871083.250000, mae: 3187.818359, mean_q: 4832.193848\n",
      "wrong_move\n",
      "   6823/500000: episode: 6720, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2300425.000000, mae: 3188.029785, mean_q: 4776.980957\n",
      "wrong_move\n",
      "   6824/500000: episode: 6721, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 10840526.000000, mae: 3186.935547, mean_q: 4476.524902\n",
      "wrong_move\n",
      "   6825/500000: episode: 6722, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 18643006.000000, mae: 3187.850586, mean_q: 4492.791016\n",
      "wrong_move\n",
      "   6826/500000: episode: 6723, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 16971732.000000, mae: 3188.099365, mean_q: 4330.115234\n",
      "wrong_move\n",
      "   6827/500000: episode: 6724, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1033736.750000, mae: 3189.749756, mean_q: 4863.765137\n",
      "wrong_move\n",
      "   6828/500000: episode: 6725, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1915.000 [1915.000, 1915.000],  loss: 1878644.500000, mae: 3189.139648, mean_q: 4734.055664\n",
      "wrong_move\n",
      "   6829/500000: episode: 6726, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5520181.000000, mae: 3188.642578, mean_q: 4119.322266\n",
      "wrong_move\n",
      "   6830/500000: episode: 6727, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2432.000 [2432.000, 2432.000],  loss: 34568556.000000, mae: 3289.147461, mean_q: 5037.019531\n",
      "wrong_move\n",
      "   6831/500000: episode: 6728, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1761.000 [1761.000, 1761.000],  loss: 2564061.000000, mae: 3195.498535, mean_q: 4778.203125\n",
      "wrong_move\n",
      "   6832/500000: episode: 6729, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3353.000 [3353.000, 3353.000],  loss: 55623720.000000, mae: 3216.884766, mean_q: 4899.055664\n",
      "wrong_move\n",
      "   6833/500000: episode: 6730, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1165.000 [1165.000, 1165.000],  loss: 17055722.000000, mae: 3319.522705, mean_q: 7276.057617\n",
      "wrong_move\n",
      "   6834/500000: episode: 6731, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 420.000 [420.000, 420.000],  loss: 12553241.000000, mae: 3193.084961, mean_q: 4011.075195\n",
      "wrong_move\n",
      "   6835/500000: episode: 6732, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 422.000 [422.000, 422.000],  loss: 116345408.000000, mae: 3261.517090, mean_q: 3989.099121\n",
      "wrong_move\n",
      "   6836/500000: episode: 6733, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1028.000 [1028.000, 1028.000],  loss: 879547.000000, mae: 3196.177002, mean_q: 4276.276367\n",
      "wrong_move\n",
      "   6837/500000: episode: 6734, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1322.000 [1322.000, 1322.000],  loss: 1848279.375000, mae: 3197.422607, mean_q: 4256.854492\n",
      "wrong_move\n",
      "   6838/500000: episode: 6735, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 434.000 [434.000, 434.000],  loss: 3504916.000000, mae: 3199.794678, mean_q: 4564.281250\n",
      "wrong_move\n",
      "   6839/500000: episode: 6736, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3997.000 [3997.000, 3997.000],  loss: 2640596.250000, mae: 3199.722656, mean_q: 2733.084717\n",
      "wrong_move\n",
      "   6840/500000: episode: 6737, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 434.000 [434.000, 434.000],  loss: 4384031.000000, mae: 3203.677979, mean_q: 4492.304688\n",
      "wrong_move\n",
      "   6841/500000: episode: 6738, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1165.000 [1165.000, 1165.000],  loss: 4647517.000000, mae: 3211.246094, mean_q: 2850.555664\n",
      "wrong_move\n",
      "   6842/500000: episode: 6739, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2434.000 [2434.000, 2434.000],  loss: 13476152.000000, mae: 3208.208496, mean_q: 5342.731445\n",
      "wrong_move\n",
      "   6843/500000: episode: 6740, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 434.000 [434.000, 434.000],  loss: 14158554.000000, mae: 3238.383789, mean_q: 6443.833008\n",
      "wrong_move\n",
      "   6844/500000: episode: 6741, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2812.000 [2812.000, 2812.000],  loss: 4923816.500000, mae: 3206.985107, mean_q: 4343.459961\n",
      "wrong_move\n",
      "   6845/500000: episode: 6742, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 434.000 [434.000, 434.000],  loss: 7967951.000000, mae: 3208.070312, mean_q: 4500.936523\n",
      "wrong_move\n",
      "   6846/500000: episode: 6743, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4009.000 [4009.000, 4009.000],  loss: 3701316.750000, mae: 3208.998047, mean_q: 3944.534912\n",
      "wrong_move\n",
      "   6847/500000: episode: 6744, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2788.000 [2788.000, 2788.000],  loss: 5866417.000000, mae: 3213.015625, mean_q: 3980.772949\n",
      "wrong_move\n",
      "   6848/500000: episode: 6745, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1165.000 [1165.000, 1165.000],  loss: 4485157.000000, mae: 3211.157959, mean_q: 4361.442871\n",
      "wrong_move\n",
      "   6849/500000: episode: 6746, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3878.000 [3878.000, 3878.000],  loss: 6103458.500000, mae: 3241.467773, mean_q: 3530.886230\n",
      "wrong_move\n",
      "   6850/500000: episode: 6747, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1165.000 [1165.000, 1165.000],  loss: 6961162.500000, mae: 3212.898926, mean_q: 4426.046875\n",
      "wrong_move\n",
      "   6851/500000: episode: 6748, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1187.000 [1187.000, 1187.000],  loss: 9435460.000000, mae: 3212.987061, mean_q: 4024.628418\n",
      "wrong_move\n",
      "   6852/500000: episode: 6749, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 1158254.000000, mae: 3247.987061, mean_q: 4467.113770\n",
      "wrong_move\n",
      "   6853/500000: episode: 6750, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1746.000 [1746.000, 1746.000],  loss: 10394148.000000, mae: 3213.982910, mean_q: 4033.788086\n",
      "wrong_move\n",
      "   6854/500000: episode: 6751, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 145.000 [145.000, 145.000],  loss: 2782625.500000, mae: 3213.160645, mean_q: 3325.260254\n",
      "wrong_move\n",
      "   6855/500000: episode: 6752, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 526.000 [526.000, 526.000],  loss: 1239891.250000, mae: 3214.034668, mean_q: 3454.299561\n",
      "wrong_move\n",
      "   6856/500000: episode: 6753, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1746.000 [1746.000, 1746.000],  loss: 2960415.250000, mae: 3214.385742, mean_q: 3018.155273\n",
      "wrong_move\n",
      "   6857/500000: episode: 6754, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1746.000 [1746.000, 1746.000],  loss: 3908663.750000, mae: 3216.220215, mean_q: 4248.298340\n",
      "wrong_move\n",
      "   6858/500000: episode: 6755, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 474.000 [474.000, 474.000],  loss: 16089248.000000, mae: 3216.569580, mean_q: 3923.461426\n",
      "wrong_move\n",
      "   6859/500000: episode: 6756, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3963.000 [3963.000, 3963.000],  loss: 2862970.500000, mae: 3218.453613, mean_q: 3066.315186\n",
      "wrong_move\n",
      "   6860/500000: episode: 6757, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 145.000 [145.000, 145.000],  loss: 2956350.750000, mae: 4033.940918, mean_q: 11918.741211\n",
      "wrong_move\n",
      "   6861/500000: episode: 6758, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 110.000 [110.000, 110.000],  loss: 10041446.000000, mae: 3220.248535, mean_q: 3704.196045\n",
      "wrong_move\n",
      "   6862/500000: episode: 6759, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 579.000 [579.000, 579.000],  loss: 923460.625000, mae: 3220.886719, mean_q: 3788.380371\n",
      "wrong_move\n",
      "   6863/500000: episode: 6760, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 145.000 [145.000, 145.000],  loss: 16677824.000000, mae: 3327.732910, mean_q: 3978.935303\n",
      "wrong_move\n",
      "   6864/500000: episode: 6761, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1176.000 [1176.000, 1176.000],  loss: 3620457.250000, mae: 3221.959961, mean_q: 2457.605713\n",
      "wrong_move\n",
      "   6865/500000: episode: 6762, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1488.000 [1488.000, 1488.000],  loss: 4710387.500000, mae: 3224.140625, mean_q: 4053.494141\n",
      "wrong_move\n",
      "   6866/500000: episode: 6763, duration: 0.141s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1427.000 [1427.000, 1427.000],  loss: 5511341.000000, mae: 3224.336426, mean_q: 3956.499512\n",
      "wrong_move\n",
      "   6867/500000: episode: 6764, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 145.000 [145.000, 145.000],  loss: 4256932.000000, mae: 3223.585449, mean_q: 2611.476074\n",
      "wrong_move\n",
      "   6868/500000: episode: 6765, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3153.000 [3153.000, 3153.000],  loss: 3094547.500000, mae: 3228.358643, mean_q: 3536.481934\n",
      "wrong_move\n",
      "   6869/500000: episode: 6766, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3927.000 [3927.000, 3927.000],  loss: 691051.375000, mae: 3247.945312, mean_q: 5655.180664\n",
      "wrong_move\n",
      "   6870/500000: episode: 6767, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 145.000 [145.000, 145.000],  loss: 1968645.750000, mae: 3228.392822, mean_q: 3163.946289\n",
      "wrong_move\n",
      "   6871/500000: episode: 6768, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2309.000 [2309.000, 2309.000],  loss: 30624620.000000, mae: 3228.401123, mean_q: 4164.572266\n",
      "wrong_move\n",
      "   6872/500000: episode: 6769, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 5539099.500000, mae: 3259.325684, mean_q: 4502.750977\n",
      "wrong_move\n",
      "   6873/500000: episode: 6770, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3397.000 [3397.000, 3397.000],  loss: 7186084.000000, mae: 3227.549805, mean_q: 2981.237061\n",
      "wrong_move\n",
      "   6874/500000: episode: 6771, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1176.000 [1176.000, 1176.000],  loss: 5623818.500000, mae: 3226.902832, mean_q: 3110.920898\n",
      "wrong_move\n",
      "   6875/500000: episode: 6772, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 145.000 [145.000, 145.000],  loss: 800575.625000, mae: 3228.364258, mean_q: 2964.079346\n",
      "wrong_move\n",
      "   6876/500000: episode: 6773, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 145.000 [145.000, 145.000],  loss: 8801037.000000, mae: 3228.432617, mean_q: 2988.169189\n",
      "wrong_move\n",
      "   6877/500000: episode: 6774, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 422.000 [422.000, 422.000],  loss: 11467739.000000, mae: 3228.455078, mean_q: 2508.919434\n",
      "wrong_move\n",
      "   6878/500000: episode: 6775, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 145.000 [145.000, 145.000],  loss: 2157788.500000, mae: 3228.987305, mean_q: 2355.880371\n",
      "wrong_move\n",
      "   6879/500000: episode: 6776, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1169.000 [1169.000, 1169.000],  loss: 1114221.750000, mae: 3230.124512, mean_q: 3525.899902\n",
      "wrong_move\n",
      "   6880/500000: episode: 6777, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1746.000 [1746.000, 1746.000],  loss: 2342491.250000, mae: 3230.132080, mean_q: 3322.317627\n",
      "wrong_move\n",
      "   6881/500000: episode: 6778, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2324.000 [2324.000, 2324.000],  loss: 4373332.000000, mae: 3231.674316, mean_q: 2354.605957\n",
      "wrong_move\n",
      "   6882/500000: episode: 6779, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 145.000 [145.000, 145.000],  loss: 1366340.000000, mae: 3315.838623, mean_q: 2931.481445\n",
      "wrong_move\n",
      "   6883/500000: episode: 6780, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3731.000 [3731.000, 3731.000],  loss: 715915.375000, mae: 3238.338623, mean_q: 3273.548096\n",
      "wrong_move\n",
      "   6884/500000: episode: 6781, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 145.000 [145.000, 145.000],  loss: 2290280.500000, mae: 3230.542969, mean_q: 4044.228516\n",
      "wrong_move\n",
      "   6885/500000: episode: 6782, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 145.000 [145.000, 145.000],  loss: 11597213.000000, mae: 3232.561523, mean_q: 4780.549805\n",
      "wrong_move\n",
      "   6886/500000: episode: 6783, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 597.000 [597.000, 597.000],  loss: 4067158.000000, mae: 3230.072510, mean_q: 2408.458496\n",
      "wrong_move\n",
      "   6887/500000: episode: 6784, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1781.000 [1781.000, 1781.000],  loss: 2413280.500000, mae: 3230.756836, mean_q: 2292.958496\n",
      "wrong_move\n",
      "   6888/500000: episode: 6785, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3796.000 [3796.000, 3796.000],  loss: 3500390.500000, mae: 3235.909912, mean_q: 3535.013184\n",
      "wrong_move\n",
      "   6889/500000: episode: 6786, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8016549.500000, mae: 3232.519531, mean_q: 2719.009521\n",
      "wrong_move\n",
      "   6890/500000: episode: 6787, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6935780.500000, mae: 3232.581055, mean_q: 2615.656006\n",
      "wrong_move\n",
      "   6891/500000: episode: 6788, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 145.000 [145.000, 145.000],  loss: 809979.875000, mae: 3233.366699, mean_q: 3235.830322\n",
      "wrong_move\n",
      "   6892/500000: episode: 6789, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3144.000 [3144.000, 3144.000],  loss: 7014337.000000, mae: 3234.442871, mean_q: 2891.157715\n",
      "wrong_move\n",
      "   6893/500000: episode: 6790, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 145.000 [145.000, 145.000],  loss: 2586726.500000, mae: 3234.687012, mean_q: 2058.376953\n",
      "wrong_move\n",
      "   6894/500000: episode: 6791, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 417.000 [417.000, 417.000],  loss: 6295152.500000, mae: 3240.387939, mean_q: 2340.680176\n",
      "wrong_move\n",
      "   6895/500000: episode: 6792, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 422.000 [422.000, 422.000],  loss: 7554776.000000, mae: 3237.068115, mean_q: 2262.457520\n",
      "wrong_move\n",
      "   6896/500000: episode: 6793, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 286.000 [286.000, 286.000],  loss: 1301848.625000, mae: 3237.710449, mean_q: 2763.612793\n",
      "wrong_move\n",
      "   6897/500000: episode: 6794, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 145.000 [145.000, 145.000],  loss: 7529647.000000, mae: 3242.087891, mean_q: 2682.351807\n",
      "wrong_move\n",
      "   6898/500000: episode: 6795, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 422.000 [422.000, 422.000],  loss: 22014218.000000, mae: 3241.601318, mean_q: 3458.007812\n",
      "wrong_move\n",
      "   6899/500000: episode: 6796, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1746.000 [1746.000, 1746.000],  loss: 1158496.375000, mae: 3239.171631, mean_q: 2906.702148\n",
      "wrong_move\n",
      "   6900/500000: episode: 6797, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 145.000 [145.000, 145.000],  loss: 843528.312500, mae: 3238.030273, mean_q: 3015.996582\n",
      "wrong_move\n",
      "   6901/500000: episode: 6798, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 396.000 [396.000, 396.000],  loss: 11362514.000000, mae: 3243.130859, mean_q: 3475.300537\n",
      "wrong_move\n",
      "   6902/500000: episode: 6799, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 145.000 [145.000, 145.000],  loss: 1857867.875000, mae: 3238.290527, mean_q: 3192.327148\n",
      "wrong_move\n",
      "   6903/500000: episode: 6800, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2428.000 [2428.000, 2428.000],  loss: 3283282.500000, mae: 3269.031494, mean_q: 6550.740723\n",
      "wrong_move\n",
      "   6904/500000: episode: 6801, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3721.000 [3721.000, 3721.000],  loss: 4920666.000000, mae: 3237.478271, mean_q: 3693.122314\n",
      "wrong_move\n",
      "   6905/500000: episode: 6802, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1746.000 [1746.000, 1746.000],  loss: 5859231.000000, mae: 3236.826172, mean_q: 2732.380615\n",
      "wrong_move\n",
      "   6906/500000: episode: 6803, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2066.000 [2066.000, 2066.000],  loss: 2819764.500000, mae: 3236.228027, mean_q: 2641.825684\n",
      "wrong_move\n",
      "   6907/500000: episode: 6804, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1746.000 [1746.000, 1746.000],  loss: 777745.000000, mae: 3235.404785, mean_q: 1877.094238\n",
      "wrong_move\n",
      "   6908/500000: episode: 6805, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1746.000 [1746.000, 1746.000],  loss: 10191149.000000, mae: 3239.306885, mean_q: 2324.393066\n",
      "wrong_move\n",
      "   6909/500000: episode: 6806, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3886.000 [3886.000, 3886.000],  loss: 4053830.500000, mae: 3242.506348, mean_q: 3659.581055\n",
      "wrong_move\n",
      "   6910/500000: episode: 6807, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1746.000 [1746.000, 1746.000],  loss: 4316722.000000, mae: 3237.031738, mean_q: 3017.292969\n",
      "wrong_move\n",
      "   6911/500000: episode: 6808, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2605.000 [2605.000, 2605.000],  loss: 4310999.000000, mae: 3236.821777, mean_q: 2567.193359\n",
      "wrong_move\n",
      "   6912/500000: episode: 6809, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1554.000 [1554.000, 1554.000],  loss: 4893969.000000, mae: 3235.573975, mean_q: 2199.189453\n",
      "wrong_move\n",
      "   6913/500000: episode: 6810, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2093.000 [2093.000, 2093.000],  loss: 2987188.750000, mae: 3236.183594, mean_q: 3043.270264\n",
      "wrong_move\n",
      "   6914/500000: episode: 6811, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 15074651.000000, mae: 3238.364014, mean_q: 2681.440918\n",
      "wrong_move\n",
      "   6915/500000: episode: 6812, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1355.000 [1355.000, 1355.000],  loss: 1504658.375000, mae: 3234.666992, mean_q: 2865.515625\n",
      "wrong_move\n",
      "   6916/500000: episode: 6813, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1355.000 [1355.000, 1355.000],  loss: 1077716.375000, mae: 3242.126465, mean_q: 2850.060059\n",
      "wrong_move\n",
      "   6917/500000: episode: 6814, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 286.000 [286.000, 286.000],  loss: 15480704.000000, mae: 3252.246094, mean_q: 2875.660156\n",
      "wrong_move\n",
      "   6918/500000: episode: 6815, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1554.000 [1554.000, 1554.000],  loss: 5007488.000000, mae: 3236.308350, mean_q: 3327.830322\n",
      "wrong_move\n",
      "   6919/500000: episode: 6816, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1349.000 [1349.000, 1349.000],  loss: 1447604.375000, mae: 3236.426758, mean_q: 2719.024902\n",
      "wrong_move\n",
      "   6920/500000: episode: 6817, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1355.000 [1355.000, 1355.000],  loss: 22176420.000000, mae: 3241.220215, mean_q: 3807.905273\n",
      "wrong_move\n",
      "   6921/500000: episode: 6818, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1355.000 [1355.000, 1355.000],  loss: 3012626.500000, mae: 3238.086914, mean_q: 2965.706543\n",
      "wrong_move\n",
      "   6922/500000: episode: 6819, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1677.000 [1677.000, 1677.000],  loss: 2594335.500000, mae: 3238.530273, mean_q: 2116.233398\n",
      "wrong_move\n",
      "   6923/500000: episode: 6820, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1355.000 [1355.000, 1355.000],  loss: 5557518.000000, mae: 3239.407471, mean_q: 2544.025879\n",
      "wrong_move\n",
      "   6924/500000: episode: 6821, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1355.000 [1355.000, 1355.000],  loss: 9548470.000000, mae: 3262.280273, mean_q: 3992.564697\n",
      "wrong_move\n",
      "   6925/500000: episode: 6822, duration: 0.061s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1355.000 [1355.000, 1355.000],  loss: 2597796.000000, mae: 3244.785156, mean_q: 2511.210449\n",
      "wrong_move\n",
      "   6926/500000: episode: 6823, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1355.000 [1355.000, 1355.000],  loss: 5674439.000000, mae: 3243.434570, mean_q: 3442.784180\n",
      "wrong_move\n",
      "   6927/500000: episode: 6824, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1355.000 [1355.000, 1355.000],  loss: 3133835.500000, mae: 3241.573242, mean_q: 2490.275391\n",
      "wrong_move\n",
      "   6928/500000: episode: 6825, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1355.000 [1355.000, 1355.000],  loss: 2471254.000000, mae: 3240.336914, mean_q: 3076.782227\n",
      "wrong_move\n",
      "   6929/500000: episode: 6826, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 923.000 [923.000, 923.000],  loss: 9852957.000000, mae: 3238.888672, mean_q: 2554.375977\n",
      "wrong_move\n",
      "   6930/500000: episode: 6827, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1355.000 [1355.000, 1355.000],  loss: 865846.250000, mae: 3237.935303, mean_q: 2149.498535\n",
      "wrong_move\n",
      "   6931/500000: episode: 6828, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3361698.500000, mae: 3242.143066, mean_q: 4094.627686\n",
      "wrong_move\n",
      "   6932/500000: episode: 6829, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 788.000 [788.000, 788.000],  loss: 13546990.000000, mae: 3248.114502, mean_q: 2680.490234\n",
      "wrong_move\n",
      "   6933/500000: episode: 6830, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1355.000 [1355.000, 1355.000],  loss: 792612.125000, mae: 3238.056396, mean_q: 2252.780273\n",
      "wrong_move\n",
      "   6934/500000: episode: 6831, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3245.000 [3245.000, 3245.000],  loss: 6166023.000000, mae: 3240.336914, mean_q: 2537.302734\n",
      "wrong_move\n",
      "   6935/500000: episode: 6832, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1184.000 [1184.000, 1184.000],  loss: 8800972.000000, mae: 3241.617188, mean_q: 2784.923096\n",
      "wrong_move\n",
      "   6936/500000: episode: 6833, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3116.000 [3116.000, 3116.000],  loss: 5529025.000000, mae: 3241.986328, mean_q: 1934.642578\n",
      "wrong_move\n",
      "   6937/500000: episode: 6834, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1310.000 [1310.000, 1310.000],  loss: 7146595.000000, mae: 3269.618408, mean_q: 2629.588867\n",
      "wrong_move\n",
      "   6938/500000: episode: 6835, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2418.000 [2418.000, 2418.000],  loss: 2507177.750000, mae: 3251.964600, mean_q: 1694.420898\n",
      "wrong_move\n",
      "   6939/500000: episode: 6836, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3325.000 [3325.000, 3325.000],  loss: 6104100.000000, mae: 3273.360352, mean_q: 3153.485107\n",
      "wrong_move\n",
      "   6940/500000: episode: 6837, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1184.000 [1184.000, 1184.000],  loss: 3062950.500000, mae: 3245.534668, mean_q: 2625.618164\n",
      "wrong_move\n",
      "   6941/500000: episode: 6838, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1184.000 [1184.000, 1184.000],  loss: 10363439.000000, mae: 3246.394775, mean_q: 2813.217773\n",
      "wrong_move\n",
      "   6942/500000: episode: 6839, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 286.000 [286.000, 286.000],  loss: 5198065.000000, mae: 3246.501953, mean_q: 2434.988281\n",
      "wrong_move\n",
      "   6943/500000: episode: 6840, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3579.000 [3579.000, 3579.000],  loss: 18227712.000000, mae: 3251.389160, mean_q: 2531.127441\n",
      "wrong_move\n",
      "   6944/500000: episode: 6841, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1330.000 [1330.000, 1330.000],  loss: 3116619.500000, mae: 3243.712646, mean_q: 2629.414307\n",
      "wrong_move\n",
      "   6945/500000: episode: 6842, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1763.000 [1763.000, 1763.000],  loss: 15589057.000000, mae: 3245.125244, mean_q: 2666.804199\n",
      "wrong_move\n",
      "   6946/500000: episode: 6843, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1184.000 [1184.000, 1184.000],  loss: 7674809.000000, mae: 3244.789551, mean_q: 3075.772949\n",
      "wrong_move\n",
      "   6947/500000: episode: 6844, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1184.000 [1184.000, 1184.000],  loss: 2989585.750000, mae: 3246.271484, mean_q: 2440.160400\n",
      "wrong_move\n",
      "   6948/500000: episode: 6845, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1599.000 [1599.000, 1599.000],  loss: 3108648.500000, mae: 3244.135742, mean_q: 1847.715332\n",
      "wrong_move\n",
      "   6949/500000: episode: 6846, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2491.000 [2491.000, 2491.000],  loss: 692395008.000000, mae: 3323.721191, mean_q: 2989.106689\n",
      "wrong_move\n",
      "   6950/500000: episode: 6847, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1976.000 [1976.000, 1976.000],  loss: 28180948.000000, mae: 3245.723633, mean_q: 3334.502197\n",
      "wrong_move\n",
      "   6951/500000: episode: 6848, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2491.000 [2491.000, 2491.000],  loss: 1161678.250000, mae: 3244.464844, mean_q: 3557.877197\n",
      "wrong_move\n",
      "   6952/500000: episode: 6849, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2491.000 [2491.000, 2491.000],  loss: 2557193.750000, mae: 3243.688721, mean_q: 3067.859863\n",
      "wrong_move\n",
      "   6953/500000: episode: 6850, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 313.000 [313.000, 313.000],  loss: 5059137.000000, mae: 3241.706299, mean_q: 2920.071289\n",
      "wrong_move\n",
      "   6954/500000: episode: 6851, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1922.000 [1922.000, 1922.000],  loss: 266092707840.000000, mae: 3331.354004, mean_q: 6156.093262\n",
      "wrong_move\n",
      "   6955/500000: episode: 6852, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 459.000 [459.000, 459.000],  loss: 2422819.250000, mae: 3223.429199, mean_q: 3040.314453\n",
      "wrong_move\n",
      "   6956/500000: episode: 6853, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 34164340.000000, mae: 3210.545654, mean_q: 5553.763672\n",
      "wrong_move\n",
      "   6957/500000: episode: 6854, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17036946.000000, mae: 3197.765381, mean_q: 3773.241699\n",
      "wrong_move\n",
      "   6958/500000: episode: 6855, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1856.000 [1856.000, 1856.000],  loss: 734338.125000, mae: 3191.896484, mean_q: 5887.943359\n",
      "wrong_move\n",
      "   6959/500000: episode: 6856, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2870.000 [2870.000, 2870.000],  loss: 1992805.250000, mae: 3181.289551, mean_q: 5362.128418\n",
      "wrong_move\n",
      "   6960/500000: episode: 6857, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 38479408.000000, mae: 3179.782227, mean_q: 7222.829102\n",
      "wrong_move\n",
      "   6961/500000: episode: 6858, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 489.000 [489.000, 489.000],  loss: 41644768.000000, mae: 3173.347412, mean_q: 7221.117188\n",
      "wrong_move\n",
      "   6962/500000: episode: 6859, duration: 0.157s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17117136.000000, mae: 3168.090332, mean_q: 6143.010254\n",
      "wrong_move\n",
      "   6963/500000: episode: 6860, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6653960.500000, mae: 3168.305176, mean_q: 6480.863770\n",
      "wrong_move\n",
      "   6964/500000: episode: 6861, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2660.000 [2660.000, 2660.000],  loss: 8270630.500000, mae: 3166.309082, mean_q: 6970.836914\n",
      "wrong_move\n",
      "   6965/500000: episode: 6862, duration: 0.163s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4948625.000000, mae: 3164.233887, mean_q: 6345.904297\n",
      "wrong_move\n",
      "   6966/500000: episode: 6863, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4069.000 [4069.000, 4069.000],  loss: 66262176.000000, mae: 3171.187744, mean_q: 6709.585938\n",
      "wrong_move\n",
      "   6967/500000: episode: 6864, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8837626.000000, mae: 3166.840820, mean_q: 6169.693848\n",
      "wrong_move\n",
      "   6968/500000: episode: 6865, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1356721.750000, mae: 3167.072754, mean_q: 6493.016602\n",
      "wrong_move\n",
      "   6969/500000: episode: 6866, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2755.000 [2755.000, 2755.000],  loss: 9438861.000000, mae: 3171.105469, mean_q: 6654.076172\n",
      "wrong_move\n",
      "   6970/500000: episode: 6867, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1024.000 [1024.000, 1024.000],  loss: 943421.250000, mae: 3211.966309, mean_q: 7991.653809\n",
      "wrong_move\n",
      "   6971/500000: episode: 6868, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 12949704.000000, mae: 3168.708496, mean_q: 6394.646973\n",
      "wrong_move\n",
      "   6972/500000: episode: 6869, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1437571.500000, mae: 3168.934570, mean_q: 5833.563477\n",
      "wrong_move\n",
      "   6973/500000: episode: 6870, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9625797.000000, mae: 3280.411621, mean_q: 9703.931641\n",
      "wrong_move\n",
      "   6975/500000: episode: 6871, duration: 0.217s, episode steps:   2, steps per second:   9, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5655042.500000, mae: 3172.244141, mean_q: 7150.573242\n",
      "wrong_move\n",
      "   6976/500000: episode: 6872, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 779.000 [779.000, 779.000],  loss: 54271804.000000, mae: 3193.854004, mean_q: 8289.298828\n",
      "wrong_move\n",
      "   6977/500000: episode: 6873, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1691966.000000, mae: 3171.875000, mean_q: 6450.571289\n",
      "wrong_move\n",
      "   6978/500000: episode: 6874, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2552.000 [2552.000, 2552.000],  loss: 20206098.000000, mae: 3285.225586, mean_q: 6971.065430\n",
      "wrong_move\n",
      "   6979/500000: episode: 6875, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 19102902.000000, mae: 3170.108643, mean_q: 5642.386719\n",
      "wrong_move\n",
      "   6980/500000: episode: 6876, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3814390.750000, mae: 3172.227539, mean_q: 5470.786133\n",
      "wrong_move\n",
      "   6981/500000: episode: 6877, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11638592.000000, mae: 3178.533447, mean_q: 6543.514648\n",
      "wrong_move\n",
      "   6982/500000: episode: 6878, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7436897.000000, mae: 3170.579590, mean_q: 6030.575195\n",
      "wrong_move\n",
      "   6983/500000: episode: 6879, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 28144810.000000, mae: 3171.641846, mean_q: 5827.248047\n",
      "wrong_move\n",
      "   6984/500000: episode: 6880, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1768.000 [1768.000, 1768.000],  loss: 2608759.250000, mae: 3172.519531, mean_q: 5670.484375\n",
      "wrong_move\n",
      "   6985/500000: episode: 6881, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2632202.500000, mae: 3173.459473, mean_q: 6100.905273\n",
      "wrong_move\n",
      "   6986/500000: episode: 6882, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9039077.000000, mae: 3174.161133, mean_q: 5270.159180\n",
      "wrong_move\n",
      "   6987/500000: episode: 6883, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 867.000 [867.000, 867.000],  loss: 27533584.000000, mae: 3179.727539, mean_q: 6220.291016\n",
      "wrong_move\n",
      "   6988/500000: episode: 6884, duration: 0.142s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 24344924.000000, mae: 3180.265869, mean_q: 7332.269531\n",
      "wrong_move\n",
      "   6989/500000: episode: 6885, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1996.000 [1996.000, 1996.000],  loss: 20641892.000000, mae: 3184.862793, mean_q: 6616.959961\n",
      "wrong_move\n",
      "   6990/500000: episode: 6886, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3982.000 [3982.000, 3982.000],  loss: 2634389.500000, mae: 3179.115723, mean_q: 4365.378906\n",
      "wrong_move\n",
      "   6991/500000: episode: 6887, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 33.000 [33.000, 33.000],  loss: 6809777.500000, mae: 3182.338867, mean_q: 6118.544434\n",
      "wrong_move\n",
      "   6992/500000: episode: 6888, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4579586.000000, mae: 3240.235596, mean_q: 7131.236816\n",
      "wrong_move\n",
      "   6993/500000: episode: 6889, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2728.000 [2728.000, 2728.000],  loss: 953474.812500, mae: 3282.972168, mean_q: 5322.173340\n",
      "wrong_move\n",
      "   6994/500000: episode: 6890, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2606.000 [2606.000, 2606.000],  loss: 8718152.000000, mae: 3191.177734, mean_q: 5132.616211\n",
      "wrong_move\n",
      "   6995/500000: episode: 6891, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3630762.000000, mae: 3183.968994, mean_q: 4362.673340\n",
      "wrong_move\n",
      "   6996/500000: episode: 6892, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 32784968.000000, mae: 3188.531982, mean_q: 6862.716797\n",
      "wrong_move\n",
      "   6997/500000: episode: 6893, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2887.000 [2887.000, 2887.000],  loss: 31240640.000000, mae: 3246.565918, mean_q: 5951.836426\n",
      "wrong_move\n",
      "   6998/500000: episode: 6894, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 12190105.000000, mae: 3187.687012, mean_q: 4963.031250\n",
      "wrong_move\n",
      "   6999/500000: episode: 6895, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11500267.000000, mae: 3187.032715, mean_q: 4820.045898\n",
      "wrong_move\n",
      "   7000/500000: episode: 6896, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4976083.000000, mae: 3188.746094, mean_q: 4855.830078\n",
      "wrong_move\n",
      "   7001/500000: episode: 6897, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2244.000 [2244.000, 2244.000],  loss: 6552033.000000, mae: 3205.880371, mean_q: 5078.741211\n",
      "wrong_move\n",
      "   7002/500000: episode: 6898, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1239181.000000, mae: 3190.386719, mean_q: 4946.632324\n",
      "wrong_move\n",
      "   7003/500000: episode: 6899, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 782.000 [782.000, 782.000],  loss: 23282356.000000, mae: 3193.135742, mean_q: 5036.384766\n",
      "wrong_move\n",
      "   7004/500000: episode: 6900, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8954915.000000, mae: 3194.198242, mean_q: 5078.244629\n",
      "wrong_move\n",
      "   7005/500000: episode: 6901, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3560.000 [3560.000, 3560.000],  loss: 3125796.250000, mae: 3200.295410, mean_q: 4581.619141\n",
      "wrong_move\n",
      "   7006/500000: episode: 6902, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3494.000 [3494.000, 3494.000],  loss: 19209306.000000, mae: 3252.321777, mean_q: 5793.401855\n",
      "wrong_move\n",
      "   7007/500000: episode: 6903, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8938152.000000, mae: 3198.805176, mean_q: 4320.293945\n",
      "wrong_move\n",
      "   7008/500000: episode: 6904, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2465.000 [2465.000, 2465.000],  loss: 1960755.125000, mae: 3200.930176, mean_q: 4142.707520\n",
      "wrong_move\n",
      "   7009/500000: episode: 6905, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3514.000 [3514.000, 3514.000],  loss: 2079215.375000, mae: 3201.621094, mean_q: 4910.160645\n",
      "wrong_move\n",
      "   7010/500000: episode: 6906, duration: 0.140s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4502004.000000, mae: 3202.113281, mean_q: 3889.718750\n",
      "wrong_move\n",
      "   7011/500000: episode: 6907, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2465.000 [2465.000, 2465.000],  loss: 12496403.000000, mae: 3210.589111, mean_q: 5544.178711\n",
      "wrong_move\n",
      "   7012/500000: episode: 6908, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5081221.500000, mae: 3206.903809, mean_q: 4421.256836\n",
      "wrong_move\n",
      "   7013/500000: episode: 6909, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 967.000 [967.000, 967.000],  loss: 7769678.000000, mae: 3206.970215, mean_q: 3833.431396\n",
      "wrong_move\n",
      "   7014/500000: episode: 6910, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 29704920.000000, mae: 3212.815186, mean_q: 5163.012207\n",
      "wrong_move\n",
      "   7015/500000: episode: 6911, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2003.000 [2003.000, 2003.000],  loss: 175195376.000000, mae: 3232.313965, mean_q: 5247.664062\n",
      "wrong_move\n",
      "   7016/500000: episode: 6912, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 18055732.000000, mae: 3211.511230, mean_q: 4482.893555\n",
      "wrong_move\n",
      "   7017/500000: episode: 6913, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2486.000 [2486.000, 2486.000],  loss: 359573472.000000, mae: 3222.677246, mean_q: 4805.244141\n",
      "wrong_move\n",
      "   7018/500000: episode: 6914, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2003.000 [2003.000, 2003.000],  loss: 2393755.750000, mae: 3214.760986, mean_q: 3973.467773\n",
      "wrong_move\n",
      "   7019/500000: episode: 6915, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 424.000 [424.000, 424.000],  loss: 21782996.000000, mae: 3239.243164, mean_q: 4264.025391\n",
      "wrong_move\n",
      "   7020/500000: episode: 6916, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2003.000 [2003.000, 2003.000],  loss: 5021512.500000, mae: 3217.379395, mean_q: 3603.625977\n",
      "wrong_move\n",
      "   7021/500000: episode: 6917, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2003.000 [2003.000, 2003.000],  loss: 8914318.000000, mae: 3218.577148, mean_q: 3659.023193\n",
      "wrong_move\n",
      "   7022/500000: episode: 6918, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3415.000 [3415.000, 3415.000],  loss: 10690838.000000, mae: 3303.699707, mean_q: 4548.440918\n",
      "wrong_move\n",
      "   7023/500000: episode: 6919, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6507377.000000, mae: 3222.164795, mean_q: 3729.405762\n",
      "wrong_move\n",
      "   7024/500000: episode: 6920, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2003.000 [2003.000, 2003.000],  loss: 22707544.000000, mae: 3226.370361, mean_q: 5389.557129\n",
      "wrong_move\n",
      "   7025/500000: episode: 6921, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2003.000 [2003.000, 2003.000],  loss: 9445790.000000, mae: 3252.294922, mean_q: 4209.873047\n",
      "wrong_move\n",
      "   7026/500000: episode: 6922, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1488.000 [1488.000, 1488.000],  loss: 6154128.000000, mae: 3237.700928, mean_q: 3314.836426\n",
      "wrong_move\n",
      "   7027/500000: episode: 6923, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2003.000 [2003.000, 2003.000],  loss: 5575886.000000, mae: 3226.055664, mean_q: 3121.275879\n",
      "wrong_move\n",
      "   7028/500000: episode: 6924, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1067.000 [1067.000, 1067.000],  loss: 10085170.000000, mae: 3228.075684, mean_q: 4326.094238\n",
      "wrong_move\n",
      "   7029/500000: episode: 6925, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2003.000 [2003.000, 2003.000],  loss: 9895491.000000, mae: 3227.296875, mean_q: 3287.466064\n",
      "wrong_move\n",
      "   7030/500000: episode: 6926, duration: 0.145s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2003.000 [2003.000, 2003.000],  loss: 674610.125000, mae: 3235.086182, mean_q: 3804.409180\n",
      "wrong_move\n",
      "   7032/500000: episode: 6927, duration: 0.220s, episode steps:   2, steps per second:   9, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 1264.500 [1256.000, 1273.000],  loss: 4334240.000000, mae: 3229.356445, mean_q: 3463.187256\n",
      "wrong_move\n",
      "   7033/500000: episode: 6928, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 759.000 [759.000, 759.000],  loss: 8544730.000000, mae: 3233.845703, mean_q: 5495.773438\n",
      "wrong_move\n",
      "   7034/500000: episode: 6929, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2003.000 [2003.000, 2003.000],  loss: 4238878.000000, mae: 3231.154297, mean_q: 3268.865234\n",
      "wrong_move\n",
      "   7035/500000: episode: 6930, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 7158629.000000, mae: 3250.763672, mean_q: 4625.360352\n",
      "wrong_move\n",
      "   7036/500000: episode: 6931, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2003.000 [2003.000, 2003.000],  loss: 1647389.625000, mae: 3248.624512, mean_q: 4211.583008\n",
      "wrong_move\n",
      "   7037/500000: episode: 6932, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1375.000 [1375.000, 1375.000],  loss: 15652413.000000, mae: 3245.405518, mean_q: 5444.020020\n",
      "wrong_move\n",
      "   7038/500000: episode: 6933, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2003.000 [2003.000, 2003.000],  loss: 6864213.500000, mae: 3236.055176, mean_q: 3606.126221\n",
      "wrong_move\n",
      "   7039/500000: episode: 6934, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2003.000 [2003.000, 2003.000],  loss: 4044961.750000, mae: 3235.323242, mean_q: 3974.962402\n",
      "wrong_move\n",
      "   7040/500000: episode: 6935, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 47.000 [47.000, 47.000],  loss: 1454944.125000, mae: 3235.644531, mean_q: 3744.436035\n",
      "wrong_move\n",
      "   7041/500000: episode: 6936, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2887.000 [2887.000, 2887.000],  loss: 5697444.000000, mae: 3233.999023, mean_q: 3024.026855\n",
      "wrong_move\n",
      "   7042/500000: episode: 6937, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1201.000 [1201.000, 1201.000],  loss: 19432490.000000, mae: 3234.485352, mean_q: 3866.939941\n",
      "wrong_move\n",
      "   7043/500000: episode: 6938, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2003.000 [2003.000, 2003.000],  loss: 7795652.500000, mae: 3233.588867, mean_q: 3057.115723\n",
      "wrong_move\n",
      "   7044/500000: episode: 6939, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2003.000 [2003.000, 2003.000],  loss: 2711556.500000, mae: 3235.622314, mean_q: 3854.464355\n",
      "wrong_move\n",
      "   7045/500000: episode: 6940, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 682.000 [682.000, 682.000],  loss: 4910549.000000, mae: 3243.413574, mean_q: 3319.867432\n",
      "wrong_move\n",
      "   7046/500000: episode: 6941, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 682.000 [682.000, 682.000],  loss: 31405438.000000, mae: 3237.307373, mean_q: 3807.885498\n",
      "wrong_move\n",
      "   7047/500000: episode: 6942, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: 1226997.000000, mae: 3233.385254, mean_q: 3268.003662\n",
      "wrong_move\n",
      "   7048/500000: episode: 6943, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 682.000 [682.000, 682.000],  loss: 6695553.000000, mae: 3234.372559, mean_q: 2590.937988\n",
      "wrong_move\n",
      "   7049/500000: episode: 6944, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 682.000 [682.000, 682.000],  loss: 3271116.750000, mae: 3236.343750, mean_q: 3090.902588\n",
      "wrong_move\n",
      "   7050/500000: episode: 6945, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 682.000 [682.000, 682.000],  loss: 3925597.000000, mae: 3236.243652, mean_q: 3082.963867\n",
      "wrong_move\n",
      "   7051/500000: episode: 6946, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3149.000 [3149.000, 3149.000],  loss: 15539381.000000, mae: 3237.832031, mean_q: 3482.923828\n",
      "wrong_move\n",
      "   7052/500000: episode: 6947, duration: 0.146s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3933.000 [3933.000, 3933.000],  loss: 3430145.250000, mae: 3237.875488, mean_q: 2646.502930\n",
      "wrong_move\n",
      "   7053/500000: episode: 6948, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 99.000 [99.000, 99.000],  loss: 13533148.000000, mae: 3238.825195, mean_q: 3436.551514\n",
      "wrong_move\n",
      "   7054/500000: episode: 6949, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1147.000 [1147.000, 1147.000],  loss: 3181609.500000, mae: 3259.438965, mean_q: 5414.638672\n",
      "wrong_move\n",
      "   7055/500000: episode: 6950, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 1295964672.000000, mae: 3242.952148, mean_q: 3810.009277\n",
      "wrong_move\n",
      "   7056/500000: episode: 6951, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3831.000 [3831.000, 3831.000],  loss: 1803074.500000, mae: 3240.533447, mean_q: 2536.114746\n",
      "wrong_move\n",
      "   7057/500000: episode: 6952, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 612.000 [612.000, 612.000],  loss: 18544796.000000, mae: 3245.522461, mean_q: 5302.956055\n",
      "wrong_move\n",
      "   7058/500000: episode: 6953, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1928.000 [1928.000, 1928.000],  loss: 3977664.500000, mae: 3243.959961, mean_q: 3614.264160\n",
      "wrong_move\n",
      "   7059/500000: episode: 6954, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 682.000 [682.000, 682.000],  loss: 3799295.500000, mae: 3244.357910, mean_q: 2481.236816\n",
      "wrong_move\n",
      "   7060/500000: episode: 6955, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 682.000 [682.000, 682.000],  loss: 261314128.000000, mae: 3248.044434, mean_q: 3491.880371\n",
      "wrong_move\n",
      "   7061/500000: episode: 6956, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 35.000 [35.000, 35.000],  loss: 1672662.000000, mae: 3248.221680, mean_q: 3742.278809\n",
      "wrong_move\n",
      "   7062/500000: episode: 6957, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 682.000 [682.000, 682.000],  loss: 4133421.000000, mae: 3246.231445, mean_q: 3315.580078\n",
      "wrong_move\n",
      "   7063/500000: episode: 6958, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1067.000 [1067.000, 1067.000],  loss: 1427977.125000, mae: 3247.076660, mean_q: 3692.477539\n",
      "wrong_move\n",
      "   7064/500000: episode: 6959, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1570.000 [1570.000, 1570.000],  loss: 12500345.000000, mae: 3586.195801, mean_q: 10316.795898\n",
      "wrong_move\n",
      "   7065/500000: episode: 6960, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 682.000 [682.000, 682.000],  loss: 1374212.500000, mae: 3247.694824, mean_q: 3321.768799\n",
      "wrong_move\n",
      "   7066/500000: episode: 6961, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 438.000 [438.000, 438.000],  loss: 18838708.000000, mae: 3342.872314, mean_q: 9319.945312\n",
      "wrong_move\n",
      "   7067/500000: episode: 6962, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1570.000 [1570.000, 1570.000],  loss: 30194974.000000, mae: 3253.847656, mean_q: 6059.191406\n",
      "wrong_move\n",
      "   7068/500000: episode: 6963, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 242.000 [242.000, 242.000],  loss: 15303750.000000, mae: 3249.085693, mean_q: 4099.787598\n",
      "wrong_move\n",
      "   7069/500000: episode: 6964, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3364.000 [3364.000, 3364.000],  loss: 14931957.000000, mae: 3247.970703, mean_q: 3481.212158\n",
      "wrong_move\n",
      "   7070/500000: episode: 6965, duration: 0.133s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3339.000 [3339.000, 3339.000],  loss: 79628704.000000, mae: 3256.113525, mean_q: 5328.935059\n",
      "wrong_move\n",
      "   7071/500000: episode: 6966, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 751.000 [751.000, 751.000],  loss: 1714507.250000, mae: 3250.691895, mean_q: 2816.703613\n",
      "wrong_move\n",
      "   7072/500000: episode: 6967, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2034.000 [2034.000, 2034.000],  loss: 3925417.250000, mae: 3250.407715, mean_q: 2861.419434\n",
      "wrong_move\n",
      "   7073/500000: episode: 6968, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1642.000 [1642.000, 1642.000],  loss: 1449656.500000, mae: 3253.951660, mean_q: 3644.535645\n",
      "wrong_move\n",
      "   7074/500000: episode: 6969, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2571.000 [2571.000, 2571.000],  loss: 797766.000000, mae: 3254.195068, mean_q: 4107.985840\n",
      "wrong_move\n",
      "   7075/500000: episode: 6970, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1058.000 [1058.000, 1058.000],  loss: 1564755.875000, mae: 3254.317871, mean_q: 2488.319336\n",
      "wrong_move\n",
      "   7076/500000: episode: 6971, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 898.000 [898.000, 898.000],  loss: 995418.125000, mae: 3256.678467, mean_q: 3350.255615\n",
      "wrong_move\n",
      "   7077/500000: episode: 6972, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3599.000 [3599.000, 3599.000],  loss: 1910859.250000, mae: 3258.588623, mean_q: 3342.462891\n",
      "wrong_move\n",
      "   7078/500000: episode: 6973, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3034.000 [3034.000, 3034.000],  loss: 783268.125000, mae: 3259.545166, mean_q: 3300.610107\n",
      "wrong_move\n",
      "   7079/500000: episode: 6974, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2563.000 [2563.000, 2563.000],  loss: 3823907.750000, mae: 3263.270996, mean_q: 3234.851562\n",
      "wrong_move\n",
      "   7080/500000: episode: 6975, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 751.000 [751.000, 751.000],  loss: 1479559.750000, mae: 3265.542969, mean_q: 3793.973145\n",
      "wrong_move\n",
      "   7081/500000: episode: 6976, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 610.000 [610.000, 610.000],  loss: 8390134.000000, mae: 3265.578613, mean_q: 4283.395508\n",
      "wrong_move\n",
      "   7082/500000: episode: 6977, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3957.000 [3957.000, 3957.000],  loss: 10215960.000000, mae: 3264.957031, mean_q: 3901.990967\n",
      "wrong_move\n",
      "   7083/500000: episode: 6978, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 20341634.000000, mae: 3267.293213, mean_q: 4250.221680\n",
      "wrong_move\n",
      "   7084/500000: episode: 6979, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 751.000 [751.000, 751.000],  loss: 9588804.000000, mae: 3267.734375, mean_q: 2590.243652\n",
      "wrong_move\n",
      "   7085/500000: episode: 6980, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4018.000 [4018.000, 4018.000],  loss: 3923110.500000, mae: 3268.152344, mean_q: 2452.546875\n",
      "wrong_move\n",
      "   7086/500000: episode: 6981, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1787.000 [1787.000, 1787.000],  loss: 27646350.000000, mae: 3274.520996, mean_q: 3842.877686\n",
      "wrong_move\n",
      "   7087/500000: episode: 6982, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 751.000 [751.000, 751.000],  loss: 2692985.000000, mae: 3272.989014, mean_q: 3448.903564\n",
      "wrong_move\n",
      "   7088/500000: episode: 6983, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 751.000 [751.000, 751.000],  loss: 3122577.500000, mae: 3268.973633, mean_q: 2891.486328\n",
      "wrong_move\n",
      "   7089/500000: episode: 6984, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3351.000 [3351.000, 3351.000],  loss: 8323329.000000, mae: 3275.749512, mean_q: 4512.895508\n",
      "wrong_move\n",
      "   7090/500000: episode: 6985, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3709.000 [3709.000, 3709.000],  loss: 798698.000000, mae: 3268.495117, mean_q: 3049.995361\n",
      "wrong_move\n",
      "   7091/500000: episode: 6986, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 751.000 [751.000, 751.000],  loss: 4130872.000000, mae: 3272.277100, mean_q: 3995.048828\n",
      "wrong_move\n",
      "   7092/500000: episode: 6987, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1746.000 [1746.000, 1746.000],  loss: 6631261.000000, mae: 3270.215088, mean_q: 3001.591797\n",
      "wrong_move\n",
      "   7093/500000: episode: 6988, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1283.000 [1283.000, 1283.000],  loss: 16054103.000000, mae: 3271.325195, mean_q: 3757.018555\n",
      "wrong_move\n",
      "   7094/500000: episode: 6989, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2045.000 [2045.000, 2045.000],  loss: 3847248.000000, mae: 3272.821533, mean_q: 3783.661621\n",
      "wrong_move\n",
      "   7095/500000: episode: 6990, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2057.000 [2057.000, 2057.000],  loss: 8074748.000000, mae: 3274.127686, mean_q: 4349.905273\n",
      "wrong_move\n",
      "   7096/500000: episode: 6991, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2045.000 [2045.000, 2045.000],  loss: 11787352.000000, mae: 3273.031494, mean_q: 2873.474121\n",
      "wrong_move\n",
      "   7097/500000: episode: 6992, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2045.000 [2045.000, 2045.000],  loss: 16217907.000000, mae: 3272.444092, mean_q: 3413.396484\n",
      "wrong_move\n",
      "   7098/500000: episode: 6993, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2057.000 [2057.000, 2057.000],  loss: 2812412.500000, mae: 3271.220703, mean_q: 2702.546387\n",
      "wrong_move\n",
      "   7099/500000: episode: 6994, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2045.000 [2045.000, 2045.000],  loss: 2164557.000000, mae: 3271.238525, mean_q: 2543.720215\n",
      "wrong_move\n",
      "   7100/500000: episode: 6995, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2037.000 [2037.000, 2037.000],  loss: 25362188.000000, mae: 3271.331543, mean_q: 1771.145508\n",
      "wrong_move\n",
      "   7101/500000: episode: 6996, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2045.000 [2045.000, 2045.000],  loss: 8968611.000000, mae: 3271.804199, mean_q: 2798.259277\n",
      "wrong_move\n",
      "   7102/500000: episode: 6997, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 380.000 [380.000, 380.000],  loss: 7383219.000000, mae: 3272.859375, mean_q: 3711.733887\n",
      "wrong_move\n",
      "   7103/500000: episode: 6998, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1895.000 [1895.000, 1895.000],  loss: 10483150.000000, mae: 3271.520996, mean_q: 2630.672119\n",
      "wrong_move\n",
      "   7104/500000: episode: 6999, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2045.000 [2045.000, 2045.000],  loss: 10200213.000000, mae: 3320.733887, mean_q: 6005.395020\n",
      "wrong_move\n",
      "   7105/500000: episode: 7000, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1559.000 [1559.000, 1559.000],  loss: 3319470.000000, mae: 3271.554688, mean_q: 3100.315186\n",
      "wrong_move\n",
      "   7107/500000: episode: 7001, duration: 0.162s, episode steps:   2, steps per second:  12, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 3364.000 [3364.000, 3364.000],  loss: 23121568.000000, mae: 3273.334717, mean_q: 4085.861084\n",
      "wrong_move\n",
      "   7108/500000: episode: 7002, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 792.000 [792.000, 792.000],  loss: 4988917.000000, mae: 3271.103027, mean_q: 3170.294434\n",
      "wrong_move\n",
      "   7109/500000: episode: 7003, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2045.000 [2045.000, 2045.000],  loss: 617059.812500, mae: 3271.859375, mean_q: 4101.428711\n",
      "wrong_move\n",
      "   7110/500000: episode: 7004, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 7810070.000000, mae: 3272.989502, mean_q: 3464.298340\n",
      "wrong_move\n",
      "   7111/500000: episode: 7005, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1019.000 [1019.000, 1019.000],  loss: 8282468.500000, mae: 3272.426025, mean_q: 3789.424561\n",
      "wrong_move\n",
      "   7112/500000: episode: 7006, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3668.000 [3668.000, 3668.000],  loss: 1225359.250000, mae: 3272.123291, mean_q: 3897.104492\n",
      "wrong_move\n",
      "   7113/500000: episode: 7007, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1672.000 [1672.000, 1672.000],  loss: 6221303.500000, mae: 3272.050293, mean_q: 3790.155273\n",
      "wrong_move\n",
      "   7114/500000: episode: 7008, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2045.000 [2045.000, 2045.000],  loss: 2993930.000000, mae: 3271.313965, mean_q: 2635.927734\n",
      "wrong_move\n",
      "   7115/500000: episode: 7009, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1919.000 [1919.000, 1919.000],  loss: 1700237.250000, mae: 3271.592285, mean_q: 4172.986328\n",
      "wrong_move\n",
      "   7116/500000: episode: 7010, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4062.000 [4062.000, 4062.000],  loss: 3219348.000000, mae: 3271.042236, mean_q: 2628.750000\n",
      "wrong_move\n",
      "   7117/500000: episode: 7011, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: 12461679.000000, mae: 3270.962158, mean_q: 2850.482422\n",
      "wrong_move\n",
      "   7118/500000: episode: 7012, duration: 0.146s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2732.000 [2732.000, 2732.000],  loss: 12391028.000000, mae: 3272.487061, mean_q: 3299.562988\n",
      "wrong_move\n",
      "   7119/500000: episode: 7013, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2431.000 [2431.000, 2431.000],  loss: 650420.187500, mae: 3272.836670, mean_q: 2557.176270\n",
      "wrong_move\n",
      "   7120/500000: episode: 7014, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3364.000 [3364.000, 3364.000],  loss: 3319022.000000, mae: 3275.523193, mean_q: 2730.574219\n",
      "wrong_move\n",
      "   7121/500000: episode: 7015, duration: 0.119s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2652.000 [2652.000, 2652.000],  loss: 4563280.000000, mae: 3277.493652, mean_q: 3171.379883\n",
      "wrong_move\n",
      "   7122/500000: episode: 7016, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3327.000 [3327.000, 3327.000],  loss: 4165103.750000, mae: 3283.351807, mean_q: 3095.004150\n",
      "wrong_move\n",
      "   7123/500000: episode: 7017, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1995.000 [1995.000, 1995.000],  loss: 11301470.000000, mae: 3279.323730, mean_q: 2933.214111\n",
      "wrong_move\n",
      "   7124/500000: episode: 7018, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 4475515.000000, mae: 3329.620117, mean_q: 2437.657715\n",
      "wrong_move\n",
      "   7126/500000: episode: 7019, duration: 0.210s, episode steps:   2, steps per second:  10, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 3364.000 [3364.000, 3364.000],  loss: 4113751.250000, mae: 3281.557617, mean_q: 2554.398438\n",
      "wrong_move\n",
      "   7127/500000: episode: 7020, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1501.000 [1501.000, 1501.000],  loss: 31526170.000000, mae: 3283.776367, mean_q: 3460.443359\n",
      "wrong_move\n",
      "   7128/500000: episode: 7021, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2045.000 [2045.000, 2045.000],  loss: 7000429.500000, mae: 3282.735107, mean_q: 2348.659180\n",
      "wrong_move\n",
      "   7129/500000: episode: 7022, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2613.000 [2613.000, 2613.000],  loss: 1327058.625000, mae: 3284.026367, mean_q: 3277.249023\n",
      "wrong_move\n",
      "   7130/500000: episode: 7023, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3364.000 [3364.000, 3364.000],  loss: 607646.875000, mae: 3285.682129, mean_q: 3388.908691\n",
      "wrong_move\n",
      "   7131/500000: episode: 7024, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3364.000 [3364.000, 3364.000],  loss: 40538796.000000, mae: 3286.354980, mean_q: 4301.978516\n",
      "wrong_move\n",
      "   7132/500000: episode: 7025, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 281.000 [281.000, 281.000],  loss: 7342274.000000, mae: 3285.522217, mean_q: 2703.410156\n",
      "wrong_move\n",
      "   7133/500000: episode: 7026, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1818.000 [1818.000, 1818.000],  loss: 3899878.500000, mae: 3287.542480, mean_q: 2906.970459\n",
      "wrong_move\n",
      "   7134/500000: episode: 7027, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2045.000 [2045.000, 2045.000],  loss: 5805384.000000, mae: 3285.589844, mean_q: 2562.355225\n",
      "wrong_move\n",
      "   7135/500000: episode: 7028, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2045.000 [2045.000, 2045.000],  loss: 21707900.000000, mae: 3285.523926, mean_q: 2866.904053\n",
      "wrong_move\n",
      "   7136/500000: episode: 7029, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2045.000 [2045.000, 2045.000],  loss: 8711830.000000, mae: 3285.207520, mean_q: 2379.442871\n",
      "wrong_move\n",
      "   7137/500000: episode: 7030, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1501.000 [1501.000, 1501.000],  loss: 3459735.500000, mae: 3285.289062, mean_q: 2683.232910\n",
      "wrong_move\n",
      "   7138/500000: episode: 7031, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1570.000 [1570.000, 1570.000],  loss: 14005633.000000, mae: 3286.060059, mean_q: 3559.398682\n",
      "wrong_move\n",
      "   7139/500000: episode: 7032, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2045.000 [2045.000, 2045.000],  loss: 8681013.000000, mae: 3287.209961, mean_q: 3441.873047\n",
      "wrong_move\n",
      "   7140/500000: episode: 7033, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1501.000 [1501.000, 1501.000],  loss: 5846005.000000, mae: 3284.145264, mean_q: 2644.624512\n",
      "wrong_move\n",
      "   7141/500000: episode: 7034, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1501.000 [1501.000, 1501.000],  loss: 3336636.500000, mae: 3283.607178, mean_q: 2391.361328\n",
      "wrong_move\n",
      "   7142/500000: episode: 7035, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1729.000 [1729.000, 1729.000],  loss: 10499595.000000, mae: 3273.496338, mean_q: 3765.681641\n",
      "wrong_move\n",
      "   7143/500000: episode: 7036, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 32228250.000000, mae: 3254.745117, mean_q: 4551.393066\n",
      "wrong_move\n",
      "   7144/500000: episode: 7037, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 945171.125000, mae: 3238.350342, mean_q: 5297.612305\n",
      "wrong_move\n",
      "   7145/500000: episode: 7038, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: 1257381.250000, mae: 3230.523926, mean_q: 6288.568359\n",
      "wrong_move\n",
      "   7146/500000: episode: 7039, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 3272520.000000, mae: 3243.584229, mean_q: 7917.470703\n",
      "wrong_move\n",
      "   7147/500000: episode: 7040, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3868.000 [3868.000, 3868.000],  loss: 7030113.000000, mae: 3209.689941, mean_q: 7128.737305\n",
      "wrong_move\n",
      "   7148/500000: episode: 7041, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3169.000 [3169.000, 3169.000],  loss: 1374851.250000, mae: 3202.550781, mean_q: 7684.590820\n",
      "wrong_move\n",
      "   7149/500000: episode: 7042, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5364129.000000, mae: 3196.252930, mean_q: 8344.539062\n",
      "wrong_move\n",
      "   7151/500000: episode: 7043, duration: 0.171s, episode steps:   2, steps per second:  12, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 2495.500 [2269.000, 2722.000],  loss: 14901068.000000, mae: 3214.688965, mean_q: 10103.103516\n",
      "wrong_move\n",
      "   7152/500000: episode: 7044, duration: 0.144s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 45317112.000000, mae: 3191.232666, mean_q: 9448.490234\n",
      "wrong_move\n",
      "   7153/500000: episode: 7045, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3595793.250000, mae: 3191.998535, mean_q: 9787.106445\n",
      "wrong_move\n",
      "   7154/500000: episode: 7046, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8320789.500000, mae: 3188.031250, mean_q: 10021.291016\n",
      "wrong_move\n",
      "   7155/500000: episode: 7047, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3788.000 [3788.000, 3788.000],  loss: 2122815.250000, mae: 3198.562500, mean_q: 10522.767578\n",
      "wrong_move\n",
      "   7156/500000: episode: 7048, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 32119278.000000, mae: 3191.400879, mean_q: 9110.268555\n",
      "wrong_move\n",
      "   7157/500000: episode: 7049, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1642137.500000, mae: 3190.636230, mean_q: 11079.199219\n",
      "wrong_move\n",
      "   7158/500000: episode: 7050, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 58919568.000000, mae: 3194.597168, mean_q: 11548.152344\n",
      "wrong_move\n",
      "   7159/500000: episode: 7051, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 48861012.000000, mae: 3205.548340, mean_q: 12166.623047\n",
      "wrong_move\n",
      "   7160/500000: episode: 7052, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3479.000 [3479.000, 3479.000],  loss: 200606336.000000, mae: 3211.169678, mean_q: 10600.498047\n",
      "wrong_move\n",
      "   7161/500000: episode: 7053, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 19047840.000000, mae: 3192.226074, mean_q: 9424.820312\n",
      "wrong_move\n",
      "   7162/500000: episode: 7054, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2129.000 [2129.000, 2129.000],  loss: 1059604.750000, mae: 3196.487305, mean_q: 11329.201172\n",
      "wrong_move\n",
      "   7163/500000: episode: 7055, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 64397216.000000, mae: 3194.253418, mean_q: 10594.574219\n",
      "wrong_move\n",
      "   7164/500000: episode: 7056, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 806.000 [806.000, 806.000],  loss: 3152925.000000, mae: 3190.713623, mean_q: 10344.869141\n",
      "wrong_move\n",
      "   7165/500000: episode: 7057, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1043333.375000, mae: 3186.071289, mean_q: 9121.507812\n",
      "wrong_move\n",
      "   7166/500000: episode: 7058, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 97364368.000000, mae: 3207.553223, mean_q: 12513.718750\n",
      "wrong_move\n",
      "   7167/500000: episode: 7059, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 27663372.000000, mae: 3184.997559, mean_q: 9053.863281\n",
      "wrong_move\n",
      "   7168/500000: episode: 7060, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4033272.750000, mae: 3199.419434, mean_q: 9662.442383\n",
      "wrong_move\n",
      "   7169/500000: episode: 7061, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3959.000 [3959.000, 3959.000],  loss: 67026124.000000, mae: 3190.867432, mean_q: 10647.665039\n",
      "wrong_move\n",
      "   7170/500000: episode: 7062, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3442.000 [3442.000, 3442.000],  loss: 3546600.500000, mae: 3183.094727, mean_q: 8827.719727\n",
      "wrong_move\n",
      "   7171/500000: episode: 7063, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 14885321.000000, mae: 3183.036621, mean_q: 9472.554688\n",
      "wrong_move\n",
      "   7172/500000: episode: 7064, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 77962952.000000, mae: 3184.558105, mean_q: 9590.010742\n",
      "wrong_move\n",
      "   7173/500000: episode: 7065, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 23356084.000000, mae: 3180.341309, mean_q: 9282.562500\n",
      "wrong_move\n",
      "   7174/500000: episode: 7066, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2270216.250000, mae: 3180.918457, mean_q: 8857.830078\n",
      "wrong_move\n",
      "   7175/500000: episode: 7067, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7033033.000000, mae: 3185.950195, mean_q: 9647.004883\n",
      "wrong_move\n",
      "   7176/500000: episode: 7068, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2081.000 [2081.000, 2081.000],  loss: 1288712.000000, mae: 3182.881348, mean_q: 9350.335938\n",
      "wrong_move\n",
      "   7177/500000: episode: 7069, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4042610.500000, mae: 3183.118652, mean_q: 8476.185547\n",
      "wrong_move\n",
      "   7178/500000: episode: 7070, duration: 0.129s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2621964.500000, mae: 3186.190918, mean_q: 10877.640625\n",
      "wrong_move\n",
      "   7179/500000: episode: 7071, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8160087.000000, mae: 3192.529785, mean_q: 9470.287109\n",
      "wrong_move\n",
      "   7181/500000: episode: 7072, duration: 0.189s, episode steps:   2, steps per second:  11, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 13814900.000000, mae: 3184.672363, mean_q: 8681.867188\n",
      "wrong_move\n",
      "   7182/500000: episode: 7073, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9056172.000000, mae: 3218.401123, mean_q: 10256.313477\n",
      "wrong_move\n",
      "   7183/500000: episode: 7074, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 34173048.000000, mae: 3192.487549, mean_q: 11445.994141\n",
      "wrong_move\n",
      "   7184/500000: episode: 7075, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2244333.250000, mae: 3183.368652, mean_q: 9048.227539\n",
      "wrong_move\n",
      "   7185/500000: episode: 7076, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8733717.000000, mae: 3190.782715, mean_q: 10684.193359\n",
      "wrong_move\n",
      "   7187/500000: episode: 7077, duration: 0.161s, episode steps:   2, steps per second:  12, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4773707.000000, mae: 3598.146729, mean_q: 13300.812500\n",
      "wrong_move\n",
      "   7188/500000: episode: 7078, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11205216.000000, mae: 3195.488281, mean_q: 8641.228516\n",
      "wrong_move\n",
      "   7190/500000: episode: 7079, duration: 0.179s, episode steps:   2, steps per second:  11, episode reward: -4961.000, mean reward: -2480.500 [-5000.000, 39.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 20864568.000000, mae: 3180.732666, mean_q: 8336.640625\n",
      "wrong_move\n",
      "   7191/500000: episode: 7080, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2859334.000000, mae: 3190.429199, mean_q: 10059.431641\n",
      "wrong_move\n",
      "   7192/500000: episode: 7081, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4704663.000000, mae: 3180.419922, mean_q: 9238.187500\n",
      "wrong_move\n",
      "   7193/500000: episode: 7082, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8004201.000000, mae: 3183.920410, mean_q: 9540.851562\n",
      "wrong_move\n",
      "   7194/500000: episode: 7083, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1649124.375000, mae: 3192.592285, mean_q: 8092.906250\n",
      "wrong_move\n",
      "   7195/500000: episode: 7084, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 13695141.000000, mae: 3177.191406, mean_q: 8241.794922\n",
      "wrong_move\n",
      "   7196/500000: episode: 7085, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 19929470.000000, mae: 3177.952637, mean_q: 8346.909180\n",
      "wrong_move\n",
      "   7197/500000: episode: 7086, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6780914.000000, mae: 3178.543457, mean_q: 8687.308594\n",
      "wrong_move\n",
      "   7198/500000: episode: 7087, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 66263788.000000, mae: 3180.930420, mean_q: 8622.495117\n",
      "wrong_move\n",
      "   7199/500000: episode: 7088, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 12587887.000000, mae: 3180.417725, mean_q: 8265.449219\n",
      "wrong_move\n",
      "   7200/500000: episode: 7089, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 16039617.000000, mae: 3180.298340, mean_q: 7863.221680\n",
      "wrong_move\n",
      "   7201/500000: episode: 7090, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1253381.375000, mae: 3179.530273, mean_q: 7987.918945\n",
      "wrong_move\n",
      "   7202/500000: episode: 7091, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 440.000 [440.000, 440.000],  loss: 2694256.000000, mae: 3181.199707, mean_q: 7370.555664\n",
      "wrong_move\n",
      "   7203/500000: episode: 7092, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 336741466112.000000, mae: 3632.121582, mean_q: 12823.722656\n",
      "wrong_move\n",
      "   7204/500000: episode: 7093, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3413.000 [3413.000, 3413.000],  loss: 27071584.000000, mae: 3258.139648, mean_q: 11068.484375\n",
      "wrong_move\n",
      "   7205/500000: episode: 7094, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3528.000 [3528.000, 3528.000],  loss: 16375766.000000, mae: 3192.123291, mean_q: 8531.468750\n",
      "wrong_move\n",
      "   7206/500000: episode: 7095, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 20944396.000000, mae: 3197.306641, mean_q: 8807.936523\n",
      "wrong_move\n",
      "   7207/500000: episode: 7096, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 601290.937500, mae: 3213.162598, mean_q: 10526.594727\n",
      "wrong_move\n",
      "   7208/500000: episode: 7097, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5510139.500000, mae: 3207.451660, mean_q: 10832.710938\n",
      "wrong_move\n",
      "   7209/500000: episode: 7098, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4891254.500000, mae: 3205.790283, mean_q: 9396.597656\n",
      "wrong_move\n",
      "   7210/500000: episode: 7099, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5517690.000000, mae: 3212.206055, mean_q: 10445.509766\n",
      "wrong_move\n",
      "   7211/500000: episode: 7100, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5655194.500000, mae: 3212.490723, mean_q: 10083.071289\n",
      "wrong_move\n",
      "   7212/500000: episode: 7101, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8306536.000000, mae: 3212.273193, mean_q: 9883.643555\n",
      "wrong_move\n",
      "   7213/500000: episode: 7102, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2081.000 [2081.000, 2081.000],  loss: 12083706.000000, mae: 3214.223633, mean_q: 9837.119141\n",
      "wrong_move\n",
      "   7214/500000: episode: 7103, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 16124820.000000, mae: 3222.468750, mean_q: 10884.083984\n",
      "wrong_move\n",
      "   7215/500000: episode: 7104, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1317.000 [1317.000, 1317.000],  loss: 39727872.000000, mae: 3215.502686, mean_q: 10283.097656\n",
      "wrong_move\n",
      "   7216/500000: episode: 7105, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2623.000 [2623.000, 2623.000],  loss: 16589070.000000, mae: 3217.202148, mean_q: 10020.304688\n",
      "wrong_move\n",
      "   7217/500000: episode: 7106, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17714324.000000, mae: 3222.986816, mean_q: 12620.390625\n",
      "wrong_move\n",
      "   7218/500000: episode: 7107, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 513.000 [513.000, 513.000],  loss: 13038193.000000, mae: 3217.044434, mean_q: 10191.266602\n",
      "wrong_move\n",
      "   7219/500000: episode: 7108, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 46503304.000000, mae: 3227.350586, mean_q: 10895.046875\n",
      "wrong_move\n",
      "   7220/500000: episode: 7109, duration: 0.136s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 66339860.000000, mae: 3215.201172, mean_q: 9527.304688\n",
      "wrong_move\n",
      "   7221/500000: episode: 7110, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6776578.000000, mae: 3214.865479, mean_q: 10280.703125\n",
      "wrong_move\n",
      "   7222/500000: episode: 7111, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2131807.500000, mae: 3214.816406, mean_q: 9819.338867\n",
      "wrong_move\n",
      "   7223/500000: episode: 7112, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2771783.000000, mae: 3213.468750, mean_q: 10697.570312\n",
      "wrong_move\n",
      "   7224/500000: episode: 7113, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 38618044.000000, mae: 3212.566650, mean_q: 10567.343750\n",
      "wrong_move\n",
      "   7225/500000: episode: 7114, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11098212.000000, mae: 3208.857422, mean_q: 8922.498047\n",
      "wrong_move\n",
      "   7226/500000: episode: 7115, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2081.000 [2081.000, 2081.000],  loss: 14693361.000000, mae: 3210.166016, mean_q: 10223.109375\n",
      "wrong_move\n",
      "   7227/500000: episode: 7116, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 24567734.000000, mae: 3208.918457, mean_q: 10290.860352\n",
      "wrong_move\n",
      "   7228/500000: episode: 7117, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 20049330.000000, mae: 3302.669922, mean_q: 11066.214844\n",
      "wrong_move\n",
      "   7229/500000: episode: 7118, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 410.000 [410.000, 410.000],  loss: 36665472.000000, mae: 3206.965820, mean_q: 9886.147461\n",
      "wrong_move\n",
      "   7230/500000: episode: 7119, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 26517970.000000, mae: 3207.883301, mean_q: 9874.917969\n",
      "wrong_move\n",
      "   7231/500000: episode: 7120, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 35010032.000000, mae: 3236.224121, mean_q: 10541.405273\n",
      "wrong_move\n",
      "   7232/500000: episode: 7121, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 769.000 [769.000, 769.000],  loss: 2500542.250000, mae: 3204.067139, mean_q: 10013.296875\n",
      "wrong_move\n",
      "   7233/500000: episode: 7122, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8799007.000000, mae: 3211.864258, mean_q: 10192.699219\n",
      "wrong_move\n",
      "   7234/500000: episode: 7123, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 20109184.000000, mae: 3208.161133, mean_q: 10510.062500\n",
      "wrong_move\n",
      "   7235/500000: episode: 7124, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8474228.000000, mae: 3207.739258, mean_q: 10738.834961\n",
      "wrong_move\n",
      "   7237/500000: episode: 7125, duration: 0.228s, episode steps:   2, steps per second:   9, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2234.500 [1747.000, 2722.000],  loss: 41987812.000000, mae: 3221.197998, mean_q: 11044.306641\n",
      "wrong_move\n",
      "   7238/500000: episode: 7126, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6357205.000000, mae: 3217.626221, mean_q: 10476.164062\n",
      "wrong_move\n",
      "   7239/500000: episode: 7127, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2523.000 [2523.000, 2523.000],  loss: 26266578.000000, mae: 3207.060547, mean_q: 9890.447266\n",
      "wrong_move\n",
      "   7240/500000: episode: 7128, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3692.000 [3692.000, 3692.000],  loss: 3630823.000000, mae: 3230.383301, mean_q: 10547.433594\n",
      "wrong_move\n",
      "   7241/500000: episode: 7129, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 10724363.000000, mae: 3209.645264, mean_q: 9776.459961\n",
      "wrong_move\n",
      "   7242/500000: episode: 7130, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1808.000 [1808.000, 1808.000],  loss: 14326024.000000, mae: 3208.337891, mean_q: 9254.429688\n",
      "wrong_move\n",
      "   7243/500000: episode: 7131, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 22934908.000000, mae: 3220.007324, mean_q: 7798.499512\n",
      "wrong_move\n",
      "   7244/500000: episode: 7132, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2133.000 [2133.000, 2133.000],  loss: 7460286.000000, mae: 3243.653320, mean_q: 9423.132812\n",
      "wrong_move\n",
      "   7245/500000: episode: 7133, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 19410572.000000, mae: 3214.074707, mean_q: 9784.238281\n",
      "wrong_move\n",
      "   7246/500000: episode: 7134, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 15085802.000000, mae: 3210.107422, mean_q: 8888.978516\n",
      "wrong_move\n",
      "   7247/500000: episode: 7135, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 12887248.000000, mae: 3209.072266, mean_q: 8238.066406\n",
      "wrong_move\n",
      "   7248/500000: episode: 7136, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4498355.000000, mae: 3215.588867, mean_q: 9824.646484\n",
      "wrong_move\n",
      "   7249/500000: episode: 7137, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 999.000 [999.000, 999.000],  loss: 32213866.000000, mae: 3213.371094, mean_q: 9491.321289\n",
      "wrong_move\n",
      "   7250/500000: episode: 7138, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2950.000 [2950.000, 2950.000],  loss: 17964394.000000, mae: 3209.409180, mean_q: 7068.107422\n",
      "wrong_move\n",
      "   7251/500000: episode: 7139, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2167.000 [2167.000, 2167.000],  loss: 31018276.000000, mae: 3212.822266, mean_q: 8907.982422\n",
      "wrong_move\n",
      "   7252/500000: episode: 7140, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1618.000 [1618.000, 1618.000],  loss: 33190826.000000, mae: 3214.559326, mean_q: 9191.456055\n",
      "wrong_move\n",
      "   7253/500000: episode: 7141, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3494.000 [3494.000, 3494.000],  loss: 26158656.000000, mae: 3213.042480, mean_q: 9642.943359\n",
      "wrong_move\n",
      "   7254/500000: episode: 7142, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 543.000 [543.000, 543.000],  loss: 5289741.500000, mae: 3274.653320, mean_q: 8970.021484\n",
      "wrong_move\n",
      "   7255/500000: episode: 7143, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8896928.000000, mae: 3214.455078, mean_q: 8605.535156\n",
      "wrong_move\n",
      "   7256/500000: episode: 7144, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 35.000 [35.000, 35.000],  loss: 2062639.000000, mae: 3214.105957, mean_q: 10001.348633\n",
      "wrong_move\n",
      "   7257/500000: episode: 7145, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2393.000 [2393.000, 2393.000],  loss: 28769908.000000, mae: 3211.744629, mean_q: 8374.717773\n",
      "wrong_move\n",
      "   7258/500000: episode: 7146, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3364.000 [3364.000, 3364.000],  loss: 12184302.000000, mae: 3216.351074, mean_q: 8408.781250\n",
      "wrong_move\n",
      "   7259/500000: episode: 7147, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5488447.500000, mae: 3219.812256, mean_q: 9266.363281\n",
      "wrong_move\n",
      "   7260/500000: episode: 7148, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 966817.500000, mae: 3211.485596, mean_q: 8513.846680\n",
      "wrong_move\n",
      "   7261/500000: episode: 7149, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 196.000 [196.000, 196.000],  loss: 5558568.000000, mae: 3210.522461, mean_q: 6177.566406\n",
      "wrong_move\n",
      "   7262/500000: episode: 7150, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 57546984.000000, mae: 3214.004395, mean_q: 8509.164062\n",
      "wrong_move\n",
      "   7263/500000: episode: 7151, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1447.000 [1447.000, 1447.000],  loss: 3480945.750000, mae: 3211.602051, mean_q: 8488.712891\n",
      "wrong_move\n",
      "   7264/500000: episode: 7152, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2643.000 [2643.000, 2643.000],  loss: 7808239.000000, mae: 3214.326416, mean_q: 6999.850098\n",
      "wrong_move\n",
      "   7265/500000: episode: 7153, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: 26881426.000000, mae: 3216.332520, mean_q: 7488.235840\n",
      "wrong_move\n",
      "   7266/500000: episode: 7154, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3004.000 [3004.000, 3004.000],  loss: 13360318.000000, mae: 3213.968262, mean_q: 8180.164551\n",
      "wrong_move\n",
      "   7267/500000: episode: 7155, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 14253857.000000, mae: 3309.934082, mean_q: 9105.904297\n",
      "wrong_move\n",
      "   7268/500000: episode: 7156, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 15211673.000000, mae: 3245.966064, mean_q: 8024.540527\n",
      "wrong_move\n",
      "   7269/500000: episode: 7157, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1895.000 [1895.000, 1895.000],  loss: 1517729480704.000000, mae: 3744.325928, mean_q: 13224.171875\n",
      "wrong_move\n",
      "   7270/500000: episode: 7158, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2523.000 [2523.000, 2523.000],  loss: 4561105.000000, mae: 3218.163818, mean_q: 7918.527344\n",
      "wrong_move\n",
      "   7271/500000: episode: 7159, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2227190.500000, mae: 3222.874512, mean_q: 10893.970703\n",
      "wrong_move\n",
      "   7272/500000: episode: 7160, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1316557.125000, mae: 3259.130127, mean_q: 12747.498047\n",
      "wrong_move\n",
      "   7273/500000: episode: 7161, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3987.000 [3987.000, 3987.000],  loss: 11748152.000000, mae: 3566.059570, mean_q: 14467.253906\n",
      "wrong_move\n",
      "   7274/500000: episode: 7162, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8296770.000000, mae: 3230.662109, mean_q: 12015.606445\n",
      "wrong_move\n",
      "   7275/500000: episode: 7163, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 29068188.000000, mae: 3228.061035, mean_q: 12403.459961\n",
      "wrong_move\n",
      "   7276/500000: episode: 7164, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 19178070.000000, mae: 3228.304932, mean_q: 12001.790039\n",
      "wrong_move\n",
      "   7277/500000: episode: 7165, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1222.000 [1222.000, 1222.000],  loss: 29263068.000000, mae: 3233.379395, mean_q: 13521.351562\n",
      "wrong_move\n",
      "   7278/500000: episode: 7166, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4600147.500000, mae: 3524.513184, mean_q: 16515.312500\n",
      "wrong_move\n",
      "   7279/500000: episode: 7167, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2515.000 [2515.000, 2515.000],  loss: 133529680.000000, mae: 3239.613770, mean_q: 13916.511719\n",
      "wrong_move\n",
      "   7280/500000: episode: 7168, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 67706168.000000, mae: 3235.244629, mean_q: 13672.101562\n",
      "wrong_move\n",
      "   7281/500000: episode: 7169, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3788626.500000, mae: 3233.817871, mean_q: 12652.101562\n",
      "wrong_move\n",
      "   7282/500000: episode: 7170, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8267688.500000, mae: 3250.063477, mean_q: 12851.664062\n",
      "wrong_move\n",
      "   7283/500000: episode: 7171, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1297.000 [1297.000, 1297.000],  loss: 28800848.000000, mae: 3239.304932, mean_q: 13268.011719\n",
      "wrong_move\n",
      "   7284/500000: episode: 7172, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1857.000 [1857.000, 1857.000],  loss: 19588048.000000, mae: 3240.951660, mean_q: 14132.541016\n",
      "wrong_move\n",
      "   7285/500000: episode: 7173, duration: 0.130s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3055.000 [3055.000, 3055.000],  loss: 16014049.000000, mae: 3237.393555, mean_q: 12531.070312\n",
      "wrong_move\n",
      "   7286/500000: episode: 7174, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 80969784.000000, mae: 3284.592773, mean_q: 15223.603516\n",
      "wrong_move\n",
      "   7287/500000: episode: 7175, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2523.000 [2523.000, 2523.000],  loss: 11845894.000000, mae: 3313.789795, mean_q: 13832.064453\n",
      "wrong_move\n",
      "   7288/500000: episode: 7176, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 207256896.000000, mae: 3262.101807, mean_q: 12838.928711\n",
      "wrong_move\n",
      "   7289/500000: episode: 7177, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 93.000 [93.000, 93.000],  loss: 8457018.000000, mae: 3238.118652, mean_q: 11713.657227\n",
      "wrong_move\n",
      "   7290/500000: episode: 7178, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 59140840.000000, mae: 3241.981201, mean_q: 13923.702148\n",
      "wrong_move\n",
      "   7291/500000: episode: 7179, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2625230.500000, mae: 3238.122070, mean_q: 12482.271484\n",
      "wrong_move\n",
      "   7292/500000: episode: 7180, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 999006.812500, mae: 3320.843994, mean_q: 14623.297852\n",
      "wrong_move\n",
      "   7293/500000: episode: 7181, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 200888768.000000, mae: 3336.293213, mean_q: 13440.947266\n",
      "wrong_move\n",
      "   7294/500000: episode: 7182, duration: 0.118s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 35087372.000000, mae: 3238.874512, mean_q: 13156.048828\n",
      "wrong_move\n",
      "   7295/500000: episode: 7183, duration: 0.153s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17189680.000000, mae: 3239.890625, mean_q: 13361.613281\n",
      "wrong_move\n",
      "   7296/500000: episode: 7184, duration: 0.132s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 27093578.000000, mae: 3235.225098, mean_q: 13079.894531\n",
      "wrong_move\n",
      "   7297/500000: episode: 7185, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 15734259.000000, mae: 3235.240723, mean_q: 13351.451172\n",
      "wrong_move\n",
      "   7298/500000: episode: 7186, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8246798.000000, mae: 3237.909180, mean_q: 12613.316406\n",
      "wrong_move\n",
      "   7299/500000: episode: 7187, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 14317113.000000, mae: 3241.747070, mean_q: 13728.937500\n",
      "wrong_move\n",
      "   7300/500000: episode: 7188, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3777780.750000, mae: 3240.560059, mean_q: 14882.541016\n",
      "wrong_move\n",
      "   7301/500000: episode: 7189, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 27796956.000000, mae: 3241.057129, mean_q: 13154.407227\n",
      "wrong_move\n",
      "   7302/500000: episode: 7190, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 22491436.000000, mae: 3266.636719, mean_q: 12642.235352\n",
      "wrong_move\n",
      "   7304/500000: episode: 7191, duration: 0.164s, episode steps:   2, steps per second:  12, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 15803683.000000, mae: 3240.969727, mean_q: 12261.563477\n",
      "wrong_move\n",
      "   7305/500000: episode: 7192, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17153808.000000, mae: 3255.260742, mean_q: 12419.222656\n",
      "wrong_move\n",
      "   7306/500000: episode: 7193, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2894.000 [2894.000, 2894.000],  loss: 30148568.000000, mae: 3241.316895, mean_q: 13095.599609\n",
      "wrong_move\n",
      "   7307/500000: episode: 7194, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 1862131.500000, mae: 3258.109375, mean_q: 12890.521484\n",
      "wrong_move\n",
      "   7308/500000: episode: 7195, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3917.000 [3917.000, 3917.000],  loss: 5100193.000000, mae: 3246.799072, mean_q: 12960.882812\n",
      "wrong_move\n",
      "   7309/500000: episode: 7196, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3235.000 [3235.000, 3235.000],  loss: 14096460.000000, mae: 3239.694336, mean_q: 10585.955078\n",
      "wrong_move\n",
      "   7310/500000: episode: 7197, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2955.000 [2955.000, 2955.000],  loss: 48487964.000000, mae: 3253.785156, mean_q: 12876.453125\n",
      "wrong_move\n",
      "   7311/500000: episode: 7198, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 431.000 [431.000, 431.000],  loss: 9444778.000000, mae: 3244.197021, mean_q: 12452.209961\n",
      "wrong_move\n",
      "   7312/500000: episode: 7199, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 1670000000.000000, mae: 3742.049316, mean_q: 15496.864258\n",
      "wrong_move\n",
      "   7313/500000: episode: 7200, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 27190980.000000, mae: 3242.384277, mean_q: 11727.540039\n",
      "wrong_move\n",
      "   7314/500000: episode: 7201, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1924.000 [1924.000, 1924.000],  loss: 735971136.000000, mae: 3318.151367, mean_q: 13855.902344\n",
      "wrong_move\n",
      "   7315/500000: episode: 7202, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 354.000 [354.000, 354.000],  loss: 17815322.000000, mae: 3244.840820, mean_q: 12207.056641\n",
      "wrong_move\n",
      "   7316/500000: episode: 7203, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 15916934.000000, mae: 3239.180664, mean_q: 10859.171875\n",
      "wrong_move\n",
      "   7317/500000: episode: 7204, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17453788.000000, mae: 3239.115479, mean_q: 10969.022461\n",
      "wrong_move\n",
      "   7318/500000: episode: 7205, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2970.000 [2970.000, 2970.000],  loss: 17381388.000000, mae: 3294.256836, mean_q: 12557.365234\n",
      "wrong_move\n",
      "   7320/500000: episode: 7206, duration: 0.155s, episode steps:   2, steps per second:  13, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6141944.000000, mae: 3241.491699, mean_q: 11088.197266\n",
      "wrong_move\n",
      "   7321/500000: episode: 7207, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2740.000 [2740.000, 2740.000],  loss: 67441744.000000, mae: 3242.177002, mean_q: 10847.976562\n",
      "wrong_move\n",
      "   7322/500000: episode: 7208, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7416585.000000, mae: 3237.970215, mean_q: 11105.708984\n",
      "wrong_move\n",
      "   7323/500000: episode: 7209, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 608.000 [608.000, 608.000],  loss: 20253744.000000, mae: 3238.537598, mean_q: 10105.035156\n",
      "wrong_move\n",
      "   7324/500000: episode: 7210, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3360.000 [3360.000, 3360.000],  loss: 270108288.000000, mae: 3244.322266, mean_q: 10715.416016\n",
      "wrong_move\n",
      "   7325/500000: episode: 7211, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3847.000 [3847.000, 3847.000],  loss: 11318656.000000, mae: 3239.595215, mean_q: 10490.607422\n",
      "wrong_move\n",
      "   7326/500000: episode: 7212, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 21218678.000000, mae: 3236.879883, mean_q: 9689.261719\n",
      "wrong_move\n",
      "   7327/500000: episode: 7213, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1887707.000000, mae: 3243.403320, mean_q: 12052.833984\n",
      "wrong_move\n",
      "   7328/500000: episode: 7214, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 16241362.000000, mae: 3240.716309, mean_q: 10749.863281\n",
      "wrong_move\n",
      "   7329/500000: episode: 7215, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7269939.000000, mae: 3598.933594, mean_q: 15319.646484\n",
      "wrong_move\n",
      "   7330/500000: episode: 7216, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4335211.000000, mae: 3248.071289, mean_q: 12727.445312\n",
      "wrong_move\n",
      "   7331/500000: episode: 7217, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1316536192.000000, mae: 3514.681641, mean_q: 13457.445312\n",
      "wrong_move\n",
      "   7332/500000: episode: 7218, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 56450812.000000, mae: 3245.028809, mean_q: 12291.355469\n",
      "wrong_move\n",
      "   7333/500000: episode: 7219, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 54306312.000000, mae: 3248.784668, mean_q: 13240.306641\n",
      "wrong_move\n",
      "   7334/500000: episode: 7220, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 718.000 [718.000, 718.000],  loss: 7679512.000000, mae: 3248.550293, mean_q: 13661.838867\n",
      "wrong_move\n",
      "   7335/500000: episode: 7221, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 28755542.000000, mae: 3251.470215, mean_q: 14977.115234\n",
      "wrong_move\n",
      "   7336/500000: episode: 7222, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 41208380.000000, mae: 3247.483398, mean_q: 13130.025391\n",
      "wrong_move\n",
      "   7337/500000: episode: 7223, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17304204.000000, mae: 3246.242188, mean_q: 12875.078125\n",
      "wrong_move\n",
      "   7338/500000: episode: 7224, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 26678192.000000, mae: 3242.770996, mean_q: 12121.455078\n",
      "wrong_move\n",
      "   7339/500000: episode: 7225, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 724.000 [724.000, 724.000],  loss: 24396092.000000, mae: 3247.048340, mean_q: 14822.133789\n",
      "wrong_move\n",
      "   7340/500000: episode: 7226, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 56992060.000000, mae: 3247.536133, mean_q: 13639.985352\n",
      "wrong_move\n",
      "   7341/500000: episode: 7227, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3228.000 [3228.000, 3228.000],  loss: 5172549.000000, mae: 3244.510986, mean_q: 12826.500000\n",
      "wrong_move\n",
      "   7342/500000: episode: 7228, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 464962093056.000000, mae: 4306.380859, mean_q: 20992.607422\n",
      "wrong_move\n",
      "   7343/500000: episode: 7229, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 24056726.000000, mae: 3294.938477, mean_q: 15076.381836\n",
      "wrong_move\n",
      "   7344/500000: episode: 7230, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1845.000 [1845.000, 1845.000],  loss: 45122912.000000, mae: 3252.919189, mean_q: 16332.969727\n",
      "wrong_move\n",
      "   7345/500000: episode: 7231, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2698.000 [2698.000, 2698.000],  loss: 35869372.000000, mae: 3256.567627, mean_q: 14600.869141\n",
      "wrong_move\n",
      "   7346/500000: episode: 7232, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 41123660.000000, mae: 3249.037109, mean_q: 16137.167969\n",
      "wrong_move\n",
      "   7347/500000: episode: 7233, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1023817088.000000, mae: 3362.569824, mean_q: 19569.205078\n",
      "wrong_move\n",
      "   7348/500000: episode: 7234, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1125302.750000, mae: 3247.376465, mean_q: 15474.728516\n",
      "wrong_move\n",
      "   7349/500000: episode: 7235, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3056.000 [3056.000, 3056.000],  loss: 59877708.000000, mae: 3254.467773, mean_q: 17996.638672\n",
      "wrong_move\n",
      "   7350/500000: episode: 7236, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5656044.000000, mae: 3249.684082, mean_q: 16138.609375\n",
      "wrong_move\n",
      "   7351/500000: episode: 7237, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 4359916.000000, mae: 3255.949219, mean_q: 15875.742188\n",
      "wrong_move\n",
      "   7352/500000: episode: 7238, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 151916768.000000, mae: 3257.431641, mean_q: 15231.922852\n",
      "wrong_move\n",
      "   7353/500000: episode: 7239, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 70677384.000000, mae: 3242.686523, mean_q: 14845.894531\n",
      "wrong_move\n",
      "   7354/500000: episode: 7240, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1505.000 [1505.000, 1505.000],  loss: 13299187.000000, mae: 3249.712402, mean_q: 16286.008789\n",
      "wrong_move\n",
      "   7355/500000: episode: 7241, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1395.000 [1395.000, 1395.000],  loss: 32364128.000000, mae: 3244.591309, mean_q: 16086.388672\n",
      "wrong_move\n",
      "   7356/500000: episode: 7242, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 37407460.000000, mae: 3245.851807, mean_q: 16034.234375\n",
      "wrong_move\n",
      "   7357/500000: episode: 7243, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 99552848.000000, mae: 3246.159424, mean_q: 16018.138672\n",
      "wrong_move\n",
      "   7358/500000: episode: 7244, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 29125840.000000, mae: 3242.353760, mean_q: 14967.191406\n",
      "wrong_move\n",
      "   7359/500000: episode: 7245, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 211.000 [211.000, 211.000],  loss: 32771288.000000, mae: 3976.402344, mean_q: 22079.613281\n",
      "wrong_move\n",
      "   7360/500000: episode: 7246, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9207342.000000, mae: 3247.766357, mean_q: 15380.070312\n",
      "wrong_move\n",
      "   7361/500000: episode: 7247, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1513.000 [1513.000, 1513.000],  loss: 63363680.000000, mae: 3241.973145, mean_q: 14474.729492\n",
      "wrong_move\n",
      "   7362/500000: episode: 7248, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 51418152.000000, mae: 3245.272949, mean_q: 14650.972656\n",
      "wrong_move\n",
      "   7363/500000: episode: 7249, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 22365030.000000, mae: 3244.106934, mean_q: 16419.996094\n",
      "wrong_move\n",
      "   7364/500000: episode: 7250, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11769386.000000, mae: 3242.250977, mean_q: 14979.410156\n",
      "wrong_move\n",
      "   7365/500000: episode: 7251, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11978252.000000, mae: 3243.489502, mean_q: 14579.582031\n",
      "wrong_move\n",
      "   7366/500000: episode: 7252, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 27271486.000000, mae: 3242.450195, mean_q: 14595.892578\n",
      "wrong_move\n",
      "   7367/500000: episode: 7253, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4022.000 [4022.000, 4022.000],  loss: 16790244.000000, mae: 3244.083984, mean_q: 14359.546875\n",
      "wrong_move\n",
      "   7368/500000: episode: 7254, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 79091952.000000, mae: 3243.025879, mean_q: 13931.002930\n",
      "wrong_move\n",
      "   7369/500000: episode: 7255, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1747.000 [1747.000, 1747.000],  loss: 34766596.000000, mae: 3251.669434, mean_q: 16351.795898\n",
      "wrong_move\n",
      "   7370/500000: episode: 7256, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 81261768.000000, mae: 3249.751953, mean_q: 15459.714844\n",
      "wrong_move\n",
      "   7371/500000: episode: 7257, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 36847456.000000, mae: 3241.602051, mean_q: 12938.832031\n",
      "wrong_move\n",
      "   7372/500000: episode: 7258, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 78664392.000000, mae: 3245.930664, mean_q: 14213.947266\n",
      "wrong_move\n",
      "   7373/500000: episode: 7259, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17783472.000000, mae: 3250.927734, mean_q: 13171.254883\n",
      "wrong_move\n",
      "   7375/500000: episode: 7260, duration: 0.124s, episode steps:   2, steps per second:  16, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 3157.500 [2722.000, 3593.000],  loss: 32624080.000000, mae: 3248.759033, mean_q: 13998.798828\n",
      "wrong_move\n",
      "   7376/500000: episode: 7261, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 26889310.000000, mae: 3247.831055, mean_q: 13228.297852\n",
      "wrong_move\n",
      "   7377/500000: episode: 7262, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 30973460.000000, mae: 3250.490234, mean_q: 16016.149414\n",
      "wrong_move\n",
      "   7378/500000: episode: 7263, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 36805536.000000, mae: 3248.185547, mean_q: 14149.336914\n",
      "wrong_move\n",
      "   7380/500000: episode: 7264, duration: 0.094s, episode steps:   2, steps per second:  21, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 3311.500 [2722.000, 3901.000],  loss: 47679452.000000, mae: 3248.258789, mean_q: 14926.434570\n",
      "wrong_move\n",
      "   7381/500000: episode: 7265, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9679278.000000, mae: 3247.212158, mean_q: 13256.699219\n",
      "wrong_move\n",
      "   7382/500000: episode: 7266, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 29877552.000000, mae: 3247.456787, mean_q: 13985.162109\n",
      "wrong_move\n",
      "   7383/500000: episode: 7267, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 47329176.000000, mae: 3254.437988, mean_q: 15131.515625\n",
      "wrong_move\n",
      "   7384/500000: episode: 7268, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 28286640.000000, mae: 3246.718262, mean_q: 13657.312500\n",
      "wrong_move\n",
      "   7385/500000: episode: 7269, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3765.000 [3765.000, 3765.000],  loss: 2047399.375000, mae: 3248.781982, mean_q: 14063.375000\n",
      "wrong_move\n",
      "   7386/500000: episode: 7270, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8722155.000000, mae: 3247.333740, mean_q: 13504.654297\n",
      "wrong_move\n",
      "   7387/500000: episode: 7271, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 21183640.000000, mae: 3250.495117, mean_q: 14595.623047\n",
      "wrong_move\n",
      "   7389/500000: episode: 7272, duration: 0.147s, episode steps:   2, steps per second:  14, episode reward: -4981.000, mean reward: -2490.500 [-5000.000, 19.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 65038020.000000, mae: 3248.267822, mean_q: 13609.070312\n",
      "wrong_move\n",
      "   7390/500000: episode: 7273, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 1675929.000000, mae: 3257.512207, mean_q: 15246.847656\n",
      "wrong_move\n",
      "   7391/500000: episode: 7274, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 28140316.000000, mae: 3246.902344, mean_q: 11846.118164\n",
      "wrong_move\n",
      "   7392/500000: episode: 7275, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11822870.000000, mae: 3254.782715, mean_q: 14349.757812\n",
      "wrong_move\n",
      "   7393/500000: episode: 7276, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2365.000 [2365.000, 2365.000],  loss: 13721971.000000, mae: 3250.583984, mean_q: 14609.609375\n",
      "wrong_move\n",
      "   7395/500000: episode: 7277, duration: 0.119s, episode steps:   2, steps per second:  17, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 705436864.000000, mae: 3257.549561, mean_q: 16093.341797\n",
      "wrong_move\n",
      "   7397/500000: episode: 7278, duration: 0.120s, episode steps:   2, steps per second:  17, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 1480.000 [238.000, 2722.000],  loss: 61947032.000000, mae: 3255.481445, mean_q: 16691.968750\n",
      "wrong_move\n",
      "   7398/500000: episode: 7279, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 25715012.000000, mae: 3268.433838, mean_q: 17941.623047\n",
      "wrong_move\n",
      "   7399/500000: episode: 7280, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5058390.000000, mae: 3665.830322, mean_q: 20256.496094\n",
      "wrong_move\n",
      "   7400/500000: episode: 7281, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 14172018.000000, mae: 3467.560059, mean_q: 19533.558594\n",
      "wrong_move\n",
      "   7401/500000: episode: 7282, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1314.000 [1314.000, 1314.000],  loss: 60400784.000000, mae: 3450.828613, mean_q: 18486.757812\n",
      "wrong_move\n",
      "   7402/500000: episode: 7283, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 76411584.000000, mae: 3268.504639, mean_q: 17748.316406\n",
      "wrong_move\n",
      "   7403/500000: episode: 7284, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 23277512.000000, mae: 3266.573975, mean_q: 18022.001953\n",
      "wrong_move\n",
      "   7404/500000: episode: 7285, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 84805912.000000, mae: 3279.533203, mean_q: 17631.917969\n",
      "wrong_move\n",
      "   7405/500000: episode: 7286, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3031.000 [3031.000, 3031.000],  loss: 34911504.000000, mae: 4276.803711, mean_q: 24949.578125\n",
      "wrong_move\n",
      "   7406/500000: episode: 7287, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3209.000 [3209.000, 3209.000],  loss: 20709172.000000, mae: 3257.921143, mean_q: 18118.171875\n",
      "wrong_move\n",
      "   7407/500000: episode: 7288, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1860.000 [1860.000, 1860.000],  loss: 50875388.000000, mae: 3258.489746, mean_q: 19025.457031\n",
      "wrong_move\n",
      "   7408/500000: episode: 7289, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 22461384.000000, mae: 3344.368164, mean_q: 19224.429688\n",
      "wrong_move\n",
      "   7409/500000: episode: 7290, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 808.000 [808.000, 808.000],  loss: 27167454.000000, mae: 3288.043213, mean_q: 17043.214844\n",
      "wrong_move\n",
      "   7410/500000: episode: 7291, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 30821574.000000, mae: 3273.219971, mean_q: 19289.542969\n",
      "wrong_move\n",
      "   7411/500000: episode: 7292, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 31257416.000000, mae: 3257.301270, mean_q: 17964.769531\n",
      "wrong_move\n",
      "   7412/500000: episode: 7293, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 42500804.000000, mae: 3259.589844, mean_q: 18369.900391\n",
      "wrong_move\n",
      "   7413/500000: episode: 7294, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 225553904.000000, mae: 3259.051514, mean_q: 18433.833984\n",
      "wrong_move\n",
      "   7414/500000: episode: 7295, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 326.000 [326.000, 326.000],  loss: 68828952.000000, mae: 3310.574219, mean_q: 18926.238281\n",
      "wrong_move\n",
      "   7415/500000: episode: 7296, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 36674484.000000, mae: 3405.311035, mean_q: 20898.646484\n",
      "wrong_move\n",
      "   7416/500000: episode: 7297, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 51013596.000000, mae: 3600.236816, mean_q: 19864.919922\n",
      "wrong_move\n",
      "   7417/500000: episode: 7298, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3370.000 [3370.000, 3370.000],  loss: 8560578.000000, mae: 3252.331299, mean_q: 17686.080078\n",
      "wrong_move\n",
      "   7418/500000: episode: 7299, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1558.000 [1558.000, 1558.000],  loss: 46704900.000000, mae: 3993.187256, mean_q: 24295.078125\n",
      "wrong_move\n",
      "   7419/500000: episode: 7300, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2464.000 [2464.000, 2464.000],  loss: 17299748.000000, mae: 3270.112793, mean_q: 17188.777344\n",
      "wrong_move\n",
      "   7420/500000: episode: 7301, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7390252.000000, mae: 3248.255859, mean_q: 16505.396484\n",
      "wrong_move\n",
      "   7421/500000: episode: 7302, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 16614326.000000, mae: 3243.576660, mean_q: 17144.886719\n",
      "wrong_move\n",
      "   7422/500000: episode: 7303, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3973.000 [3973.000, 3973.000],  loss: 65003920.000000, mae: 3256.083496, mean_q: 18764.994141\n",
      "wrong_move\n",
      "   7423/500000: episode: 7304, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 43214636.000000, mae: 3255.582520, mean_q: 18473.636719\n",
      "wrong_move\n",
      "   7424/500000: episode: 7305, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3815.000 [3815.000, 3815.000],  loss: 57929788.000000, mae: 3253.763916, mean_q: 17841.175781\n",
      "wrong_move\n",
      "   7425/500000: episode: 7306, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 51358672.000000, mae: 3653.110352, mean_q: 22676.019531\n",
      "wrong_move\n",
      "   7426/500000: episode: 7307, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 86647424.000000, mae: 3253.181152, mean_q: 17614.083984\n",
      "wrong_move\n",
      "   7427/500000: episode: 7308, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 893303.062500, mae: 3261.428711, mean_q: 16626.234375\n",
      "wrong_move\n",
      "   7428/500000: episode: 7309, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 18389736.000000, mae: 3240.706543, mean_q: 15776.820312\n",
      "wrong_move\n",
      "   7429/500000: episode: 7310, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 61682152.000000, mae: 3294.520508, mean_q: 17442.824219\n",
      "wrong_move\n",
      "   7430/500000: episode: 7311, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 91128720.000000, mae: 3253.977783, mean_q: 18533.746094\n",
      "wrong_move\n",
      "   7431/500000: episode: 7312, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8934102.000000, mae: 3251.157715, mean_q: 17033.835938\n",
      "wrong_move\n",
      "   7432/500000: episode: 7313, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 261.000 [261.000, 261.000],  loss: 59188176.000000, mae: 3319.302246, mean_q: 19146.007812\n",
      "wrong_move\n",
      "   7433/500000: episode: 7314, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1370.000 [1370.000, 1370.000],  loss: 78219328.000000, mae: 3247.072998, mean_q: 15720.690430\n",
      "wrong_move\n",
      "   7434/500000: episode: 7315, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1640.000 [1640.000, 1640.000],  loss: 100349288.000000, mae: 3246.167969, mean_q: 14785.949219\n",
      "wrong_move\n",
      "   7435/500000: episode: 7316, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 86185408.000000, mae: 3247.121582, mean_q: 14357.095703\n",
      "wrong_move\n",
      "   7436/500000: episode: 7317, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 188140544.000000, mae: 3291.833496, mean_q: 16876.906250\n",
      "wrong_move\n",
      "   7437/500000: episode: 7318, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1045.000 [1045.000, 1045.000],  loss: 193155744.000000, mae: 3284.296143, mean_q: 15635.681641\n",
      "wrong_move\n",
      "   7438/500000: episode: 7319, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 29436218.000000, mae: 3239.281250, mean_q: 14493.500977\n",
      "wrong_move\n",
      "   7439/500000: episode: 7320, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1890.000 [1890.000, 1890.000],  loss: 60682796.000000, mae: 3261.866699, mean_q: 17858.259766\n",
      "wrong_move\n",
      "   7440/500000: episode: 7321, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3921.000 [3921.000, 3921.000],  loss: 15413344.000000, mae: 3249.182373, mean_q: 15877.192383\n",
      "wrong_move\n",
      "   7441/500000: episode: 7322, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9263476.000000, mae: 3467.978027, mean_q: 17360.724609\n",
      "wrong_move\n",
      "   7442/500000: episode: 7323, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 24828072.000000, mae: 3242.516602, mean_q: 13860.823242\n",
      "wrong_move\n",
      "   7443/500000: episode: 7324, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2758.000 [2758.000, 2758.000],  loss: 29035678.000000, mae: 3263.063721, mean_q: 16138.082031\n",
      "wrong_move\n",
      "   7444/500000: episode: 7325, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 35400020.000000, mae: 3250.706787, mean_q: 17285.634766\n",
      "wrong_move\n",
      "   7445/500000: episode: 7326, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: 46277568.000000, mae: 3247.645996, mean_q: 13659.029297\n",
      "wrong_move\n",
      "   7446/500000: episode: 7327, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 70257800.000000, mae: 3331.928223, mean_q: 16190.232422\n",
      "wrong_move\n",
      "   7447/500000: episode: 7328, duration: 0.126s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1569.000 [1569.000, 1569.000],  loss: 42758820.000000, mae: 3248.917969, mean_q: 15819.143555\n",
      "wrong_move\n",
      "   7448/500000: episode: 7329, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2097092.250000, mae: 3251.759277, mean_q: 17083.373047\n",
      "wrong_move\n",
      "   7449/500000: episode: 7330, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 66164688.000000, mae: 3246.333252, mean_q: 14654.337891\n",
      "wrong_move\n",
      "   7450/500000: episode: 7331, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 21367366.000000, mae: 3538.258789, mean_q: 18300.136719\n",
      "wrong_move\n",
      "   7451/500000: episode: 7332, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3299.000 [3299.000, 3299.000],  loss: 70320464.000000, mae: 3297.606689, mean_q: 14750.994141\n",
      "wrong_move\n",
      "   7452/500000: episode: 7333, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 12995480.000000, mae: 3251.157227, mean_q: 16582.857422\n",
      "wrong_move\n",
      "   7453/500000: episode: 7334, duration: 0.074s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 76425680.000000, mae: 3315.520020, mean_q: 13075.300781\n",
      "wrong_move\n",
      "   7454/500000: episode: 7335, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 51196400.000000, mae: 3299.057129, mean_q: 15910.993164\n",
      "wrong_move\n",
      "   7455/500000: episode: 7336, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3131.000 [3131.000, 3131.000],  loss: 3307714.000000, mae: 3243.163574, mean_q: 15262.159180\n",
      "wrong_move\n",
      "   7456/500000: episode: 7337, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 20385426.000000, mae: 3246.363770, mean_q: 13712.830078\n",
      "wrong_move\n",
      "   7457/500000: episode: 7338, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 49654140.000000, mae: 3245.391846, mean_q: 13362.129883\n",
      "wrong_move\n",
      "   7458/500000: episode: 7339, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 174597120.000000, mae: 3258.433350, mean_q: 15835.241211\n",
      "wrong_move\n",
      "   7459/500000: episode: 7340, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 64176156.000000, mae: 3261.165771, mean_q: 14617.017578\n",
      "wrong_move\n",
      "   7460/500000: episode: 7341, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 493.000 [493.000, 493.000],  loss: 23572428.000000, mae: 3345.063721, mean_q: 17069.357422\n",
      "wrong_move\n",
      "   7461/500000: episode: 7342, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4037.000 [4037.000, 4037.000],  loss: 841204416.000000, mae: 3314.589844, mean_q: 15274.111328\n",
      "wrong_move\n",
      "   7462/500000: episode: 7343, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 3227429.250000, mae: 3253.069336, mean_q: 14672.287109\n",
      "wrong_move\n",
      "   7463/500000: episode: 7344, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 12732966.000000, mae: 3296.767578, mean_q: 14113.741211\n",
      "wrong_move\n",
      "   7464/500000: episode: 7345, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1447.000 [1447.000, 1447.000],  loss: 73169776.000000, mae: 3252.379395, mean_q: 14156.751953\n",
      "wrong_move\n",
      "   7466/500000: episode: 7346, duration: 0.112s, episode steps:   2, steps per second:  18, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 37406072.000000, mae: 3248.712891, mean_q: 13352.961914\n",
      "wrong_move\n",
      "   7467/500000: episode: 7347, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3677.000 [3677.000, 3677.000],  loss: 5891014.000000, mae: 3249.390625, mean_q: 12860.353516\n",
      "wrong_move\n",
      "   7468/500000: episode: 7348, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2404.000 [2404.000, 2404.000],  loss: 51389680.000000, mae: 3248.863281, mean_q: 11925.708008\n",
      "wrong_move\n",
      "   7469/500000: episode: 7349, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 49819600.000000, mae: 3253.308838, mean_q: 14461.870117\n",
      "wrong_move\n",
      "   7470/500000: episode: 7350, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 119343256.000000, mae: 3312.583740, mean_q: 14306.492188\n",
      "wrong_move\n",
      "   7471/500000: episode: 7351, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 89450320.000000, mae: 3259.226074, mean_q: 13850.285156\n",
      "wrong_move\n",
      "   7472/500000: episode: 7352, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2127092.250000, mae: 3262.846924, mean_q: 13064.893555\n",
      "wrong_move\n",
      "   7473/500000: episode: 7353, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 728.000 [728.000, 728.000],  loss: 6430605.000000, mae: 3254.364014, mean_q: 11519.449219\n",
      "wrong_move\n",
      "   7474/500000: episode: 7354, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 125682208.000000, mae: 3265.872559, mean_q: 14486.917969\n",
      "wrong_move\n",
      "   7475/500000: episode: 7355, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 929057.687500, mae: 3255.544434, mean_q: 10456.677734\n",
      "wrong_move\n",
      "   7476/500000: episode: 7356, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 23916340.000000, mae: 3268.315186, mean_q: 12323.373047\n",
      "wrong_move\n",
      "   7477/500000: episode: 7357, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2935.000 [2935.000, 2935.000],  loss: 15126940.000000, mae: 3256.979980, mean_q: 11535.578125\n",
      "wrong_move\n",
      "   7478/500000: episode: 7358, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 25018808.000000, mae: 3266.733887, mean_q: 12528.027344\n",
      "wrong_move\n",
      "   7479/500000: episode: 7359, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3735143.000000, mae: 3277.276367, mean_q: 13602.871094\n",
      "wrong_move\n",
      "   7480/500000: episode: 7360, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 10460659.000000, mae: 3261.849121, mean_q: 12876.226562\n",
      "wrong_move\n",
      "   7481/500000: episode: 7361, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 42476104.000000, mae: 3264.106689, mean_q: 11970.841797\n",
      "wrong_move\n",
      "   7482/500000: episode: 7362, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 72111560.000000, mae: 3275.447754, mean_q: 12096.761719\n",
      "wrong_move\n",
      "   7483/500000: episode: 7363, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5570328.500000, mae: 3262.627686, mean_q: 12083.105469\n",
      "wrong_move\n",
      "   7484/500000: episode: 7364, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1166.000 [1166.000, 1166.000],  loss: 13291806.000000, mae: 3269.932129, mean_q: 12796.238281\n",
      "wrong_move\n",
      "   7485/500000: episode: 7365, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 29835548.000000, mae: 3264.966553, mean_q: 10135.678711\n",
      "wrong_move\n",
      "   7486/500000: episode: 7366, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 3670087.000000, mae: 3263.824463, mean_q: 10889.447266\n",
      "wrong_move\n",
      "   7487/500000: episode: 7367, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 5617427.000000, mae: 3283.152100, mean_q: 11514.129883\n",
      "wrong_move\n",
      "   7488/500000: episode: 7368, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 38937060.000000, mae: 3281.156982, mean_q: 13620.026367\n",
      "wrong_move\n",
      "   7489/500000: episode: 7369, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17567234.000000, mae: 3371.323242, mean_q: 13283.177734\n",
      "wrong_move\n",
      "   7490/500000: episode: 7370, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 61791728.000000, mae: 3280.301758, mean_q: 14017.025391\n",
      "wrong_move\n",
      "   7491/500000: episode: 7371, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9418960.000000, mae: 3272.591553, mean_q: 12610.455078\n",
      "wrong_move\n",
      "   7492/500000: episode: 7372, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1408.000 [1408.000, 1408.000],  loss: 3157698.500000, mae: 3283.424805, mean_q: 14010.125000\n",
      "wrong_move\n",
      "   7493/500000: episode: 7373, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 5234019.000000, mae: 3285.663086, mean_q: 13717.407227\n",
      "wrong_move\n",
      "   7494/500000: episode: 7374, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 609862720.000000, mae: 3279.850830, mean_q: 12699.782227\n",
      "wrong_move\n",
      "   7495/500000: episode: 7375, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3503.000 [3503.000, 3503.000],  loss: 57699284.000000, mae: 3271.431641, mean_q: 11774.308594\n",
      "wrong_move\n",
      "   7496/500000: episode: 7376, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6388614.000000, mae: 3278.331299, mean_q: 12286.023438\n",
      "wrong_move\n",
      "   7498/500000: episode: 7377, duration: 0.108s, episode steps:   2, steps per second:  18, episode reward: -5051.000, mean reward: -2525.500 [-5000.000, -51.000], mean action: 1441.000 [160.000, 2722.000],  loss: 26950126.000000, mae: 3275.582520, mean_q: 11963.440430\n",
      "wrong_move\n",
      "   7499/500000: episode: 7378, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 743.000 [743.000, 743.000],  loss: 3611599.000000, mae: 3288.594727, mean_q: 11616.984375\n",
      "wrong_move\n",
      "   7500/500000: episode: 7379, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 99.000 [99.000, 99.000],  loss: 56294288.000000, mae: 3278.718262, mean_q: 12959.373047\n",
      "wrong_move\n",
      "   7501/500000: episode: 7380, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2413.000 [2413.000, 2413.000],  loss: 26184204.000000, mae: 3275.155762, mean_q: 11819.443359\n",
      "wrong_move\n",
      "   7502/500000: episode: 7381, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1049.000 [1049.000, 1049.000],  loss: 26301916.000000, mae: 3283.729980, mean_q: 13966.102539\n",
      "wrong_move\n",
      "   7503/500000: episode: 7382, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3458.000 [3458.000, 3458.000],  loss: 9001032.000000, mae: 3314.784668, mean_q: 13969.173828\n",
      "wrong_move\n",
      "   7504/500000: episode: 7383, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1750362.500000, mae: 3277.691895, mean_q: 10662.244141\n",
      "wrong_move\n",
      "   7505/500000: episode: 7384, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 96469872.000000, mae: 3353.759766, mean_q: 15369.257812\n",
      "wrong_move\n",
      "   7506/500000: episode: 7385, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2342795.500000, mae: 3287.483398, mean_q: 10298.861328\n",
      "wrong_move\n",
      "   7507/500000: episode: 7386, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2751.000 [2751.000, 2751.000],  loss: 5254821.500000, mae: 3296.813721, mean_q: 13354.689453\n",
      "wrong_move\n",
      "   7508/500000: episode: 7387, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2106.000 [2106.000, 2106.000],  loss: 11704005.000000, mae: 3279.514648, mean_q: 9910.449219\n",
      "wrong_move\n",
      "   7509/500000: episode: 7388, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1580.000 [1580.000, 1580.000],  loss: 1433252480.000000, mae: 3417.528809, mean_q: 12837.055664\n",
      "wrong_move\n",
      "   7510/500000: episode: 7389, duration: 0.105s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1175.000 [1175.000, 1175.000],  loss: 19284272.000000, mae: 3289.533447, mean_q: 11288.379883\n",
      "wrong_move\n",
      "   7511/500000: episode: 7390, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 18261592.000000, mae: 3296.793457, mean_q: 12631.175781\n",
      "wrong_move\n",
      "   7512/500000: episode: 7391, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1359.000 [1359.000, 1359.000],  loss: 9799680.000000, mae: 3394.508057, mean_q: 13193.988281\n",
      "wrong_move\n",
      "   7513/500000: episode: 7392, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2762.000 [2762.000, 2762.000],  loss: 15441761.000000, mae: 3327.791504, mean_q: 12449.953125\n",
      "wrong_move\n",
      "   7514/500000: episode: 7393, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1175.000 [1175.000, 1175.000],  loss: 37703328.000000, mae: 3305.232666, mean_q: 10914.181641\n",
      "wrong_move\n",
      "   7515/500000: episode: 7394, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 859.000 [859.000, 859.000],  loss: 40901712.000000, mae: 3284.785156, mean_q: 9499.812500\n",
      "wrong_move\n",
      "   7516/500000: episode: 7395, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1679.000 [1679.000, 1679.000],  loss: 51936296.000000, mae: 3288.942383, mean_q: 10680.407227\n",
      "wrong_move\n",
      "   7517/500000: episode: 7396, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9382204.000000, mae: 3283.538086, mean_q: 10150.323242\n",
      "wrong_move\n",
      "   7518/500000: episode: 7397, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 11425869.000000, mae: 3288.233398, mean_q: 12262.380859\n",
      "wrong_move\n",
      "   7519/500000: episode: 7398, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8430520.000000, mae: 3290.643311, mean_q: 10155.098633\n",
      "wrong_move\n",
      "   7520/500000: episode: 7399, duration: 0.151s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 33065596.000000, mae: 3293.114502, mean_q: 13120.417969\n",
      "wrong_move\n",
      "   7521/500000: episode: 7400, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 38182448.000000, mae: 3284.603516, mean_q: 9859.304688\n",
      "wrong_move\n",
      "   7522/500000: episode: 7401, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 639.000 [639.000, 639.000],  loss: 42205248.000000, mae: 3349.086914, mean_q: 12944.705078\n",
      "wrong_move\n",
      "   7523/500000: episode: 7402, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 24652160.000000, mae: 3311.419922, mean_q: 10932.801758\n",
      "wrong_move\n",
      "   7524/500000: episode: 7403, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 8207511.000000, mae: 3381.719971, mean_q: 12545.003906\n",
      "wrong_move\n",
      "   7525/500000: episode: 7404, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1418.000 [1418.000, 1418.000],  loss: 2553761.250000, mae: 3627.366211, mean_q: 11416.105469\n",
      "wrong_move\n",
      "   7526/500000: episode: 7405, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 343085472.000000, mae: 3327.514648, mean_q: 11172.294922\n",
      "wrong_move\n",
      "   7527/500000: episode: 7406, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 11105412.000000, mae: 3294.433105, mean_q: 10119.677734\n",
      "wrong_move\n",
      "   7528/500000: episode: 7407, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2784.000 [2784.000, 2784.000],  loss: 164529840.000000, mae: 3301.616211, mean_q: 12270.492188\n",
      "wrong_move\n",
      "   7529/500000: episode: 7408, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 639.000 [639.000, 639.000],  loss: 20293456.000000, mae: 3290.788574, mean_q: 10960.996094\n",
      "wrong_move\n",
      "   7530/500000: episode: 7409, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6857091.000000, mae: 3285.660645, mean_q: 9225.958008\n",
      "wrong_move\n",
      "   7531/500000: episode: 7410, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1467746.500000, mae: 3292.346191, mean_q: 9937.527344\n",
      "wrong_move\n",
      "   7532/500000: episode: 7411, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 23373058.000000, mae: 3293.649414, mean_q: 10296.462891\n",
      "wrong_move\n",
      "   7533/500000: episode: 7412, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1690.000 [1690.000, 1690.000],  loss: 4891851.000000, mae: 3289.051270, mean_q: 8981.988281\n",
      "wrong_move\n",
      "   7534/500000: episode: 7413, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3699.000 [3699.000, 3699.000],  loss: 29536810.000000, mae: 3290.912354, mean_q: 8610.158203\n",
      "wrong_move\n",
      "   7535/500000: episode: 7414, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3669.000 [3669.000, 3669.000],  loss: 8575322.000000, mae: 3318.381104, mean_q: 9583.530273\n",
      "wrong_move\n",
      "   7536/500000: episode: 7415, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 56151948.000000, mae: 3288.354004, mean_q: 10763.267578\n",
      "wrong_move\n",
      "   7537/500000: episode: 7416, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3669.000 [3669.000, 3669.000],  loss: 55162524.000000, mae: 3342.961670, mean_q: 12028.166992\n",
      "wrong_move\n",
      "   7538/500000: episode: 7417, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 33576180.000000, mae: 3540.263672, mean_q: 11698.183594\n",
      "wrong_move\n",
      "   7539/500000: episode: 7418, duration: 0.120s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 61633024.000000, mae: 3995.394043, mean_q: 19565.529297\n",
      "wrong_move\n",
      "   7540/500000: episode: 7419, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1892.000 [1892.000, 1892.000],  loss: 3605996.000000, mae: 3284.717285, mean_q: 8669.176758\n",
      "wrong_move\n",
      "   7541/500000: episode: 7420, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 34632632.000000, mae: 3287.344727, mean_q: 9340.066406\n",
      "wrong_move\n",
      "   7542/500000: episode: 7421, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 34014496.000000, mae: 3291.990479, mean_q: 10575.749023\n",
      "wrong_move\n",
      "   7543/500000: episode: 7422, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 5218617.000000, mae: 3290.164062, mean_q: 10280.392578\n",
      "wrong_move\n",
      "   7544/500000: episode: 7423, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 37884828.000000, mae: 3299.075928, mean_q: 11600.412109\n",
      "wrong_move\n",
      "   7545/500000: episode: 7424, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 77441504.000000, mae: 3304.613281, mean_q: 11734.095703\n",
      "wrong_move\n",
      "   7546/500000: episode: 7425, duration: 0.170s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 11458953.000000, mae: 3293.889893, mean_q: 8026.928711\n",
      "wrong_move\n",
      "   7547/500000: episode: 7426, duration: 0.159s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1714.000 [1714.000, 1714.000],  loss: 6379832.000000, mae: 3294.961914, mean_q: 9354.547852\n",
      "wrong_move\n",
      "   7548/500000: episode: 7427, duration: 0.128s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 12668824.000000, mae: 3294.358643, mean_q: 10031.376953\n",
      "wrong_move\n",
      "   7549/500000: episode: 7428, duration: 0.161s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3820.000 [3820.000, 3820.000],  loss: 9387998.000000, mae: 4419.898438, mean_q: 20007.072266\n",
      "wrong_move\n",
      "   7550/500000: episode: 7429, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 529.000 [529.000, 529.000],  loss: 12429086.000000, mae: 3304.192871, mean_q: 9728.881836\n",
      "wrong_move\n",
      "   7551/500000: episode: 7430, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3669.000 [3669.000, 3669.000],  loss: 59575308.000000, mae: 3369.619385, mean_q: 9692.717773\n",
      "wrong_move\n",
      "   7552/500000: episode: 7431, duration: 0.116s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1747.000 [1747.000, 1747.000],  loss: 39784972.000000, mae: 3297.406250, mean_q: 10241.906250\n",
      "wrong_move\n",
      "   7553/500000: episode: 7432, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3669.000 [3669.000, 3669.000],  loss: 78591464.000000, mae: 3479.439453, mean_q: 7361.954590\n",
      "wrong_move\n",
      "   7554/500000: episode: 7433, duration: 0.139s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 16094547.000000, mae: 3298.367188, mean_q: 9868.201172\n",
      "wrong_move\n",
      "   7555/500000: episode: 7434, duration: 0.135s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3004.000 [3004.000, 3004.000],  loss: 14767232.000000, mae: 3534.340088, mean_q: 11438.060547\n",
      "wrong_move\n",
      "   7556/500000: episode: 7435, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3669.000 [3669.000, 3669.000],  loss: 46853128.000000, mae: 3344.192871, mean_q: 10525.756836\n",
      "wrong_move\n",
      "   7557/500000: episode: 7436, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 753.000 [753.000, 753.000],  loss: 4096238.750000, mae: 3304.043457, mean_q: 8294.712891\n",
      "wrong_move\n",
      "   7558/500000: episode: 7437, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 932.000 [932.000, 932.000],  loss: 4893331.500000, mae: 3309.623779, mean_q: 9079.294922\n",
      "wrong_move\n",
      "   7559/500000: episode: 7438, duration: 0.141s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3669.000 [3669.000, 3669.000],  loss: 10696128.000000, mae: 3315.284668, mean_q: 10842.732422\n",
      "wrong_move\n",
      "   7560/500000: episode: 7439, duration: 0.131s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3669.000 [3669.000, 3669.000],  loss: 11235880.000000, mae: 3302.557617, mean_q: 9649.582031\n",
      "wrong_move\n",
      "   7561/500000: episode: 7440, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 61308624.000000, mae: 3302.652588, mean_q: 8760.468750\n",
      "wrong_move\n",
      "   7562/500000: episode: 7441, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3669.000 [3669.000, 3669.000],  loss: 13413428.000000, mae: 3308.925537, mean_q: 7135.809570\n",
      "wrong_move\n",
      "   7563/500000: episode: 7442, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3669.000 [3669.000, 3669.000],  loss: 3928338.000000, mae: 3305.650879, mean_q: 8903.764648\n",
      "wrong_move\n",
      "   7564/500000: episode: 7443, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3565.000 [3565.000, 3565.000],  loss: 16993626.000000, mae: 3308.145508, mean_q: 9487.638672\n",
      "wrong_move\n",
      "   7565/500000: episode: 7444, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3568.000 [3568.000, 3568.000],  loss: 40343972.000000, mae: 3307.957275, mean_q: 9025.173828\n",
      "wrong_move\n",
      "   7566/500000: episode: 7445, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3419.000 [3419.000, 3419.000],  loss: 14038350.000000, mae: 3410.364258, mean_q: 8766.270508\n",
      "wrong_move\n",
      "   7567/500000: episode: 7446, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 48506480.000000, mae: 3313.602051, mean_q: 9117.278320\n",
      "wrong_move\n",
      "   7568/500000: episode: 7447, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2471705.000000, mae: 3321.661133, mean_q: 9612.783203\n",
      "wrong_move\n",
      "   7569/500000: episode: 7448, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3330.000 [3330.000, 3330.000],  loss: 13907547.000000, mae: 3310.366943, mean_q: 7415.187500\n",
      "wrong_move\n",
      "   7570/500000: episode: 7449, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1048.000 [1048.000, 1048.000],  loss: 24350468.000000, mae: 3311.725098, mean_q: 7846.426758\n",
      "wrong_move\n",
      "   7571/500000: episode: 7450, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3568.000 [3568.000, 3568.000],  loss: 59266132.000000, mae: 3313.417236, mean_q: 7776.968750\n",
      "wrong_move\n",
      "   7572/500000: episode: 7451, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3568.000 [3568.000, 3568.000],  loss: 22992386.000000, mae: 3321.242676, mean_q: 8529.070312\n",
      "wrong_move\n",
      "   7573/500000: episode: 7452, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3568.000 [3568.000, 3568.000],  loss: 79404624.000000, mae: 3320.709961, mean_q: 9799.998047\n",
      "wrong_move\n",
      "   7574/500000: episode: 7453, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3568.000 [3568.000, 3568.000],  loss: 74299368.000000, mae: 3314.044434, mean_q: 9042.671875\n",
      "wrong_move\n",
      "   7575/500000: episode: 7454, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2508.000 [2508.000, 2508.000],  loss: 47651308.000000, mae: 3324.741211, mean_q: 8776.939453\n",
      "wrong_move\n",
      "   7576/500000: episode: 7455, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2146.000 [2146.000, 2146.000],  loss: 29519470.000000, mae: 3314.844727, mean_q: 8819.349609\n",
      "wrong_move\n",
      "   7577/500000: episode: 7456, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 2369440256.000000, mae: 3841.990967, mean_q: 14571.336914\n",
      "wrong_move\n",
      "   7578/500000: episode: 7457, duration: 0.125s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3004.000 [3004.000, 3004.000],  loss: 21084552.000000, mae: 3317.743652, mean_q: 9363.364258\n",
      "wrong_move\n",
      "   7579/500000: episode: 7458, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 14656550.000000, mae: 3337.650391, mean_q: 8239.285156\n",
      "wrong_move\n",
      "   7580/500000: episode: 7459, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2737.000 [2737.000, 2737.000],  loss: 136555504.000000, mae: 3326.074219, mean_q: 9369.812500\n",
      "wrong_move\n",
      "   7581/500000: episode: 7460, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 932.000 [932.000, 932.000],  loss: 27879920.000000, mae: 3523.390625, mean_q: 11449.558594\n",
      "wrong_move\n",
      "   7582/500000: episode: 7461, duration: 0.137s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1560.000 [1560.000, 1560.000],  loss: 23358868.000000, mae: 3318.573730, mean_q: 7463.779297\n",
      "wrong_move\n",
      "   7583/500000: episode: 7462, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 119.000 [119.000, 119.000],  loss: 19701764.000000, mae: 3322.330078, mean_q: 7601.598633\n",
      "wrong_move\n",
      "   7584/500000: episode: 7463, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 20491200.000000, mae: 3319.209717, mean_q: 6953.785156\n",
      "wrong_move\n",
      "   7585/500000: episode: 7464, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 837.000 [837.000, 837.000],  loss: 14728220.000000, mae: 3327.720703, mean_q: 8083.261719\n",
      "wrong_move\n",
      "   7586/500000: episode: 7465, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1560.000 [1560.000, 1560.000],  loss: 14978000.000000, mae: 3330.904297, mean_q: 8818.558594\n",
      "wrong_move\n",
      "   7587/500000: episode: 7466, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 496.000 [496.000, 496.000],  loss: 47937000.000000, mae: 3322.089111, mean_q: 7370.344238\n",
      "wrong_move\n",
      "   7588/500000: episode: 7467, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 4175117.250000, mae: 3332.216797, mean_q: 8498.484375\n",
      "wrong_move\n",
      "   7589/500000: episode: 7468, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 932.000 [932.000, 932.000],  loss: 14703706.000000, mae: 3321.858887, mean_q: 7413.115234\n",
      "wrong_move\n",
      "   7590/500000: episode: 7469, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2313.000 [2313.000, 2313.000],  loss: 6477514.000000, mae: 3337.480225, mean_q: 8652.066406\n",
      "wrong_move\n",
      "   7591/500000: episode: 7470, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 7889135.500000, mae: 3327.651855, mean_q: 5822.540039\n",
      "wrong_move\n",
      "   7592/500000: episode: 7471, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 932.000 [932.000, 932.000],  loss: 6105584.000000, mae: 3468.946045, mean_q: 8709.214844\n",
      "wrong_move\n",
      "   7593/500000: episode: 7472, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 13675494.000000, mae: 3333.035156, mean_q: 8400.515625\n",
      "wrong_move\n",
      "   7594/500000: episode: 7473, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 932.000 [932.000, 932.000],  loss: 7836711.500000, mae: 3332.358398, mean_q: 6265.735352\n",
      "wrong_move\n",
      "   7595/500000: episode: 7474, duration: 0.124s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1560.000 [1560.000, 1560.000],  loss: 13875618.000000, mae: 3329.095215, mean_q: 7617.431641\n",
      "wrong_move\n",
      "   7596/500000: episode: 7475, duration: 0.123s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3305.000 [3305.000, 3305.000],  loss: 39713500.000000, mae: 3539.812012, mean_q: 13834.329102\n",
      "wrong_move\n",
      "   7597/500000: episode: 7476, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3719.000 [3719.000, 3719.000],  loss: 12624383.000000, mae: 3482.333496, mean_q: 7127.965820\n",
      "wrong_move\n",
      "   7598/500000: episode: 7477, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2712.000 [2712.000, 2712.000],  loss: 7162563.000000, mae: 3334.196533, mean_q: 6790.166992\n",
      "wrong_move\n",
      "   7599/500000: episode: 7478, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 932.000 [932.000, 932.000],  loss: 16080785.000000, mae: 3329.842773, mean_q: 6932.128418\n",
      "wrong_move\n",
      "   7600/500000: episode: 7479, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2266.000 [2266.000, 2266.000],  loss: 16692496.000000, mae: 3330.606445, mean_q: 7407.548828\n",
      "wrong_move\n",
      "   7601/500000: episode: 7480, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2963.000 [2963.000, 2963.000],  loss: 11441286.000000, mae: 3326.785889, mean_q: 5509.573242\n",
      "wrong_move\n",
      "   7602/500000: episode: 7481, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2625.000 [2625.000, 2625.000],  loss: 15639732.000000, mae: 3328.518555, mean_q: 7251.655273\n",
      "wrong_move\n",
      "   7603/500000: episode: 7482, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 950.000 [950.000, 950.000],  loss: 8156028.000000, mae: 3330.691895, mean_q: 8146.354492\n",
      "wrong_move\n",
      "   7604/500000: episode: 7483, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2625.000 [2625.000, 2625.000],  loss: 24744712.000000, mae: 3333.475098, mean_q: 6762.308594\n",
      "wrong_move\n",
      "   7605/500000: episode: 7484, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3089.000 [3089.000, 3089.000],  loss: 11642248.000000, mae: 3334.987061, mean_q: 7640.776855\n",
      "wrong_move\n",
      "   7606/500000: episode: 7485, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3584.000 [3584.000, 3584.000],  loss: 10926135.000000, mae: 3334.114258, mean_q: 7186.045898\n",
      "wrong_move\n",
      "   7607/500000: episode: 7486, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2674.000 [2674.000, 2674.000],  loss: 3945468.500000, mae: 3335.526367, mean_q: 6454.080078\n",
      "wrong_move\n",
      "   7608/500000: episode: 7487, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2625.000 [2625.000, 2625.000],  loss: 29631940.000000, mae: 3334.316406, mean_q: 8786.380859\n",
      "wrong_move\n",
      "   7609/500000: episode: 7488, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2625.000 [2625.000, 2625.000],  loss: 3986414.750000, mae: 3330.369629, mean_q: 7371.353027\n",
      "wrong_move\n",
      "   7610/500000: episode: 7489, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2110.000 [2110.000, 2110.000],  loss: 1986987.000000, mae: 3341.779541, mean_q: 7891.805664\n",
      "wrong_move\n",
      "   7611/500000: episode: 7490, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 17961744.000000, mae: 3335.240479, mean_q: 7664.404297\n",
      "wrong_move\n",
      "   7612/500000: episode: 7491, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1900.000 [1900.000, 1900.000],  loss: 6279585.000000, mae: 3342.132080, mean_q: 8554.614258\n",
      "wrong_move\n",
      "   7613/500000: episode: 7492, duration: 0.087s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1032.000 [1032.000, 1032.000],  loss: 11659796.000000, mae: 3334.383789, mean_q: 7170.064453\n",
      "wrong_move\n",
      "   7614/500000: episode: 7493, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2361.000 [2361.000, 2361.000],  loss: 21441698.000000, mae: 3357.473633, mean_q: 6672.560059\n",
      "wrong_move\n",
      "   7615/500000: episode: 7494, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2608.000 [2608.000, 2608.000],  loss: 96478208.000000, mae: 3343.286865, mean_q: 9091.168945\n",
      "wrong_move\n",
      "   7616/500000: episode: 7495, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 711.000 [711.000, 711.000],  loss: 65917272.000000, mae: 3336.867920, mean_q: 7845.339844\n",
      "wrong_move\n",
      "   7617/500000: episode: 7496, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2212.000 [2212.000, 2212.000],  loss: 37607184.000000, mae: 3335.653076, mean_q: 7423.503418\n",
      "wrong_move\n",
      "   7618/500000: episode: 7497, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2625.000 [2625.000, 2625.000],  loss: 16707329.000000, mae: 3338.636230, mean_q: 8231.287109\n",
      "wrong_move\n",
      "   7619/500000: episode: 7498, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 189.000 [189.000, 189.000],  loss: 4308510.000000, mae: 3444.514404, mean_q: 9225.769531\n",
      "wrong_move\n",
      "   7620/500000: episode: 7499, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2212.000 [2212.000, 2212.000],  loss: 106595520.000000, mae: 3353.110596, mean_q: 8128.130859\n",
      "wrong_move\n",
      "   7621/500000: episode: 7500, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 711.000 [711.000, 711.000],  loss: 8093578.000000, mae: 3332.371582, mean_q: 6406.393555\n",
      "wrong_move\n",
      "   7622/500000: episode: 7501, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9599928.000000, mae: 3336.844238, mean_q: 8064.223633\n",
      "wrong_move\n",
      "   7623/500000: episode: 7502, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1073.000 [1073.000, 1073.000],  loss: 14869108.000000, mae: 3419.309814, mean_q: 7547.753906\n",
      "wrong_move\n",
      "   7624/500000: episode: 7503, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1741.000 [1741.000, 1741.000],  loss: 30370618.000000, mae: 3333.940674, mean_q: 6289.615234\n",
      "wrong_move\n",
      "   7625/500000: episode: 7504, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3015.000 [3015.000, 3015.000],  loss: 41971752.000000, mae: 3334.188721, mean_q: 6833.277344\n",
      "wrong_move\n",
      "   7626/500000: episode: 7505, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1185857.500000, mae: 3334.620117, mean_q: 6283.391602\n",
      "wrong_move\n",
      "   7627/500000: episode: 7506, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 711.000 [711.000, 711.000],  loss: 6164584.000000, mae: 3332.402832, mean_q: 7513.266602\n",
      "wrong_move\n",
      "   7628/500000: episode: 7507, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2013.000 [2013.000, 2013.000],  loss: 13022728.000000, mae: 3328.906738, mean_q: 5788.707031\n",
      "wrong_move\n",
      "   7629/500000: episode: 7508, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2361.000 [2361.000, 2361.000],  loss: 35398548.000000, mae: 3335.677979, mean_q: 7248.555664\n",
      "wrong_move\n",
      "   7630/500000: episode: 7509, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 189.000 [189.000, 189.000],  loss: 11213421.000000, mae: 3335.984375, mean_q: 7019.310547\n",
      "wrong_move\n",
      "   7631/500000: episode: 7510, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2212.000 [2212.000, 2212.000],  loss: 49667680.000000, mae: 4175.390137, mean_q: 17337.414062\n",
      "wrong_move\n",
      "   7632/500000: episode: 7511, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2212.000 [2212.000, 2212.000],  loss: 12946610.000000, mae: 3331.985840, mean_q: 5641.004883\n",
      "wrong_move\n",
      "   7633/500000: episode: 7512, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2212.000 [2212.000, 2212.000],  loss: 7072122.000000, mae: 3327.378906, mean_q: 6969.100586\n",
      "wrong_move\n",
      "   7634/500000: episode: 7513, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2212.000 [2212.000, 2212.000],  loss: 16344129.000000, mae: 3329.452393, mean_q: 7654.592773\n",
      "wrong_move\n",
      "   7635/500000: episode: 7514, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3466.000 [3466.000, 3466.000],  loss: 12610638.000000, mae: 3343.295898, mean_q: 6666.503418\n",
      "wrong_move\n",
      "   7636/500000: episode: 7515, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2506.000 [2506.000, 2506.000],  loss: 18567042.000000, mae: 3327.809570, mean_q: 6330.272461\n",
      "wrong_move\n",
      "   7637/500000: episode: 7516, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3501.000 [3501.000, 3501.000],  loss: 49715980.000000, mae: 4028.464600, mean_q: 13871.084961\n",
      "wrong_move\n",
      "   7638/500000: episode: 7517, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2212.000 [2212.000, 2212.000],  loss: 3303526.750000, mae: 3330.769775, mean_q: 7542.438477\n",
      "wrong_move\n",
      "   7639/500000: episode: 7518, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2212.000 [2212.000, 2212.000],  loss: 29137948.000000, mae: 3332.924805, mean_q: 8614.750977\n",
      "wrong_move\n",
      "   7640/500000: episode: 7519, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1580.000 [1580.000, 1580.000],  loss: 1966131.000000, mae: 3328.947510, mean_q: 6712.749512\n",
      "wrong_move\n",
      "   7641/500000: episode: 7520, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 98027128.000000, mae: 3332.458984, mean_q: 7652.685547\n",
      "wrong_move\n",
      "   7642/500000: episode: 7521, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 711.000 [711.000, 711.000],  loss: 31709088.000000, mae: 3332.806641, mean_q: 7263.843750\n",
      "wrong_move\n",
      "   7643/500000: episode: 7522, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 270.000 [270.000, 270.000],  loss: 80064880.000000, mae: 3331.874023, mean_q: 7299.723145\n",
      "wrong_move\n",
      "   7644/500000: episode: 7523, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 48765936.000000, mae: 3506.144043, mean_q: 7613.196777\n",
      "wrong_move\n",
      "   7645/500000: episode: 7524, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3466.000 [3466.000, 3466.000],  loss: 6963239.000000, mae: 3329.837402, mean_q: 7134.701172\n",
      "wrong_move\n",
      "   7646/500000: episode: 7525, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3466.000 [3466.000, 3466.000],  loss: 42057860.000000, mae: 3329.780762, mean_q: 6583.832031\n",
      "wrong_move\n",
      "   7647/500000: episode: 7526, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3466.000 [3466.000, 3466.000],  loss: 5457577.000000, mae: 3326.853271, mean_q: 5876.415039\n",
      "wrong_move\n",
      "   7648/500000: episode: 7527, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 965.000 [965.000, 965.000],  loss: 10930895.000000, mae: 3326.849365, mean_q: 6375.374023\n",
      "wrong_move\n",
      "   7649/500000: episode: 7528, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3877.000 [3877.000, 3877.000],  loss: 18202888.000000, mae: 3332.496826, mean_q: 6908.357422\n",
      "wrong_move\n",
      "   7650/500000: episode: 7529, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 236.000 [236.000, 236.000],  loss: 1648031616.000000, mae: 3340.760742, mean_q: 6074.487305\n",
      "wrong_move\n",
      "   7651/500000: episode: 7530, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 833.000 [833.000, 833.000],  loss: 17675166.000000, mae: 3349.437500, mean_q: 9455.053711\n",
      "wrong_move\n",
      "   7652/500000: episode: 7531, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3466.000 [3466.000, 3466.000],  loss: 2997717.000000, mae: 3364.337158, mean_q: 6749.186523\n",
      "wrong_move\n",
      "   7653/500000: episode: 7532, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2633.000 [2633.000, 2633.000],  loss: 17837970.000000, mae: 3333.446289, mean_q: 4971.964844\n",
      "wrong_move\n",
      "   7654/500000: episode: 7533, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3466.000 [3466.000, 3466.000],  loss: 3055638.500000, mae: 3340.531494, mean_q: 7668.526367\n",
      "wrong_move\n",
      "   7655/500000: episode: 7534, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1741.000 [1741.000, 1741.000],  loss: 2571043584.000000, mae: 4220.608887, mean_q: 15357.499023\n",
      "wrong_move\n",
      "   7656/500000: episode: 7535, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2633.000 [2633.000, 2633.000],  loss: 13860136.000000, mae: 3336.265625, mean_q: 5139.894531\n",
      "wrong_move\n",
      "   7657/500000: episode: 7536, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 614.000 [614.000, 614.000],  loss: 5441286.000000, mae: 3340.837402, mean_q: 6582.227051\n",
      "wrong_move\n",
      "   7658/500000: episode: 7537, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 270.000 [270.000, 270.000],  loss: 65136184.000000, mae: 3354.731445, mean_q: 7227.093262\n",
      "wrong_move\n",
      "   7659/500000: episode: 7538, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3877.000 [3877.000, 3877.000],  loss: 8140888.500000, mae: 3343.793213, mean_q: 7220.092285\n",
      "wrong_move\n",
      "   7660/500000: episode: 7539, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3873.000 [3873.000, 3873.000],  loss: 20033150.000000, mae: 3341.410889, mean_q: 6394.287598\n",
      "wrong_move\n",
      "   7661/500000: episode: 7540, duration: 0.095s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 900.000 [900.000, 900.000],  loss: 7208032.000000, mae: 3350.713135, mean_q: 8069.101562\n",
      "wrong_move\n",
      "   7662/500000: episode: 7541, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1404.000 [1404.000, 1404.000],  loss: 2211542.000000, mae: 3364.110352, mean_q: 8033.039062\n",
      "wrong_move\n",
      "   7663/500000: episode: 7542, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 900.000 [900.000, 900.000],  loss: 30868250.000000, mae: 3376.253906, mean_q: 7172.133301\n",
      "wrong_move\n",
      "   7664/500000: episode: 7543, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4046.000 [4046.000, 4046.000],  loss: 8300462.500000, mae: 3351.397461, mean_q: 6340.254395\n",
      "wrong_move\n",
      "   7665/500000: episode: 7544, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 666.000 [666.000, 666.000],  loss: 4457710.000000, mae: 3356.477783, mean_q: 6171.571289\n",
      "wrong_move\n",
      "   7666/500000: episode: 7545, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 900.000 [900.000, 900.000],  loss: 17655746.000000, mae: 3343.910400, mean_q: 4782.850586\n",
      "wrong_move\n",
      "   7667/500000: episode: 7546, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1004.000 [1004.000, 1004.000],  loss: 34200428.000000, mae: 3349.404297, mean_q: 6459.725586\n",
      "wrong_move\n",
      "   7668/500000: episode: 7547, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2242.000 [2242.000, 2242.000],  loss: 8650448.000000, mae: 3344.783691, mean_q: 5784.331055\n",
      "wrong_move\n",
      "   7669/500000: episode: 7548, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 204.000 [204.000, 204.000],  loss: 3443728.250000, mae: 3347.382568, mean_q: 6750.437012\n",
      "wrong_move\n",
      "   7670/500000: episode: 7549, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3877.000 [3877.000, 3877.000],  loss: 6255082.000000, mae: 3351.379883, mean_q: 6838.171875\n",
      "wrong_move\n",
      "   7671/500000: episode: 7550, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2242.000 [2242.000, 2242.000],  loss: 34490952.000000, mae: 3349.046875, mean_q: 5952.214844\n",
      "wrong_move\n",
      "   7672/500000: episode: 7551, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2048.000 [2048.000, 2048.000],  loss: 5258500.500000, mae: 3353.850586, mean_q: 8172.317383\n",
      "wrong_move\n",
      "   7673/500000: episode: 7552, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1383.000 [1383.000, 1383.000],  loss: 8075185.500000, mae: 3345.699707, mean_q: 5088.513184\n",
      "wrong_move\n",
      "   7674/500000: episode: 7553, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1529.000 [1529.000, 1529.000],  loss: 17752320.000000, mae: 3346.358887, mean_q: 5716.455078\n",
      "wrong_move\n",
      "   7675/500000: episode: 7554, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 204.000 [204.000, 204.000],  loss: 12007864.000000, mae: 3347.540771, mean_q: 6286.064453\n",
      "wrong_move\n",
      "   7676/500000: episode: 7555, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 204.000 [204.000, 204.000],  loss: 21896310.000000, mae: 3347.701660, mean_q: 6408.911133\n",
      "wrong_move\n",
      "   7677/500000: episode: 7556, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4016.000 [4016.000, 4016.000],  loss: 13313927.000000, mae: 3382.055908, mean_q: 7128.914062\n",
      "wrong_move\n",
      "   7678/500000: episode: 7557, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1383.000 [1383.000, 1383.000],  loss: 12809355.000000, mae: 3345.539062, mean_q: 6478.011230\n",
      "wrong_move\n",
      "   7679/500000: episode: 7558, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 400.000 [400.000, 400.000],  loss: 23794028.000000, mae: 3346.526611, mean_q: 7590.691895\n",
      "wrong_move\n",
      "   7680/500000: episode: 7559, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2242.000 [2242.000, 2242.000],  loss: 5788061.500000, mae: 3342.824463, mean_q: 6158.271484\n",
      "wrong_move\n",
      "   7681/500000: episode: 7560, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 124.000 [124.000, 124.000],  loss: 21714388.000000, mae: 3343.420898, mean_q: 6001.565430\n",
      "wrong_move\n",
      "   7682/500000: episode: 7561, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1060.000 [1060.000, 1060.000],  loss: 11719532.000000, mae: 3349.402100, mean_q: 6370.625000\n",
      "wrong_move\n",
      "   7683/500000: episode: 7562, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1970.000 [1970.000, 1970.000],  loss: 18937960.000000, mae: 3378.802246, mean_q: 7285.303711\n",
      "wrong_move\n",
      "   7684/500000: episode: 7563, duration: 0.108s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 204.000 [204.000, 204.000],  loss: 12274946.000000, mae: 3346.435059, mean_q: 6927.953125\n",
      "wrong_move\n",
      "   7685/500000: episode: 7564, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 204.000 [204.000, 204.000],  loss: 18974460.000000, mae: 3349.063477, mean_q: 6882.997070\n",
      "wrong_move\n",
      "   7686/500000: episode: 7565, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3869.000 [3869.000, 3869.000],  loss: 5147830.000000, mae: 3379.163330, mean_q: 6592.337402\n",
      "wrong_move\n",
      "   7687/500000: episode: 7566, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 428.000 [428.000, 428.000],  loss: 5362733.500000, mae: 3353.104004, mean_q: 7859.812012\n",
      "wrong_move\n",
      "   7688/500000: episode: 7567, duration: 0.103s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1383.000 [1383.000, 1383.000],  loss: 15471725.000000, mae: 3363.317871, mean_q: 6215.519043\n",
      "wrong_move\n",
      "   7689/500000: episode: 7568, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3877.000 [3877.000, 3877.000],  loss: 16535186.000000, mae: 3349.305176, mean_q: 5900.875488\n",
      "wrong_move\n",
      "   7690/500000: episode: 7569, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1268.000 [1268.000, 1268.000],  loss: 2068397.750000, mae: 3356.285156, mean_q: 7366.903320\n",
      "wrong_move\n",
      "   7691/500000: episode: 7570, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3385.000 [3385.000, 3385.000],  loss: 26004714.000000, mae: 3345.957520, mean_q: 6151.675781\n",
      "wrong_move\n",
      "   7692/500000: episode: 7571, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2633.000 [2633.000, 2633.000],  loss: 10158114.000000, mae: 3358.687012, mean_q: 6546.358887\n",
      "wrong_move\n",
      "   7693/500000: episode: 7572, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 536.000 [536.000, 536.000],  loss: 16401380.000000, mae: 3361.255859, mean_q: 8205.289062\n",
      "wrong_move\n",
      "   7694/500000: episode: 7573, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3869.000 [3869.000, 3869.000],  loss: 6812390.000000, mae: 3351.038086, mean_q: 5853.083008\n",
      "wrong_move\n",
      "   7695/500000: episode: 7574, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3461.000 [3461.000, 3461.000],  loss: 13117657.000000, mae: 3357.241699, mean_q: 5391.018555\n",
      "wrong_move\n",
      "   7696/500000: episode: 7575, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 900.000 [900.000, 900.000],  loss: 1884092.125000, mae: 3358.851074, mean_q: 6591.298828\n",
      "wrong_move\n",
      "   7697/500000: episode: 7576, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 900.000 [900.000, 900.000],  loss: 381493968896.000000, mae: 3679.743896, mean_q: 11018.382812\n",
      "wrong_move\n",
      "   7698/500000: episode: 7577, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 43.000 [43.000, 43.000],  loss: 35697592.000000, mae: 3344.328613, mean_q: 6746.270508\n",
      "wrong_move\n",
      "   7699/500000: episode: 7578, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 5680711.000000, mae: 3334.176270, mean_q: 6592.067871\n",
      "wrong_move\n",
      "   7700/500000: episode: 7579, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: 33200664.000000, mae: 3326.485596, mean_q: 7462.350586\n",
      "wrong_move\n",
      "   7701/500000: episode: 7580, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 1843236.500000, mae: 3335.365723, mean_q: 9874.655273\n",
      "wrong_move\n",
      "   7702/500000: episode: 7581, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 678.000 [678.000, 678.000],  loss: 14973362.000000, mae: 3314.137695, mean_q: 7698.481934\n",
      "wrong_move\n",
      "   7703/500000: episode: 7582, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 412.000 [412.000, 412.000],  loss: 22018816.000000, mae: 3347.244141, mean_q: 8384.340820\n",
      "wrong_move\n",
      "   7704/500000: episode: 7583, duration: 0.106s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 52103292.000000, mae: 3312.767578, mean_q: 9626.902344\n",
      "wrong_move\n",
      "   7705/500000: episode: 7584, duration: 0.071s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 32462824.000000, mae: 3338.918945, mean_q: 10222.380859\n",
      "wrong_move\n",
      "   7706/500000: episode: 7585, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1015.000 [1015.000, 1015.000],  loss: 15175638.000000, mae: 3306.263184, mean_q: 7268.933105\n",
      "wrong_move\n",
      "   7707/500000: episode: 7586, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 13412305.000000, mae: 3314.543457, mean_q: 9031.395508\n",
      "wrong_move\n",
      "   7708/500000: episode: 7587, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 715594688.000000, mae: 3624.547852, mean_q: 11155.029297\n",
      "wrong_move\n",
      "   7709/500000: episode: 7588, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3850.000 [3850.000, 3850.000],  loss: 5191608.000000, mae: 3304.443848, mean_q: 7249.465332\n",
      "wrong_move\n",
      "   7710/500000: episode: 7589, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 14765467.000000, mae: 3316.781006, mean_q: 10253.236328\n",
      "wrong_move\n",
      "   7711/500000: episode: 7590, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 34521804.000000, mae: 3323.791748, mean_q: 11669.903320\n",
      "wrong_move\n",
      "   7712/500000: episode: 7591, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 6392555.000000, mae: 4289.355957, mean_q: 16523.515625\n",
      "wrong_move\n",
      "   7713/500000: episode: 7592, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1386.000 [1386.000, 1386.000],  loss: 24789606.000000, mae: 3316.470947, mean_q: 10363.619141\n",
      "wrong_move\n",
      "   7714/500000: episode: 7593, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9043554.000000, mae: 3314.304688, mean_q: 10180.888672\n",
      "wrong_move\n",
      "   7715/500000: episode: 7594, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9163338.000000, mae: 3308.144531, mean_q: 6432.994629\n",
      "wrong_move\n",
      "   7716/500000: episode: 7595, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3998.000 [3998.000, 3998.000],  loss: 29341240.000000, mae: 3320.434570, mean_q: 9316.828125\n",
      "wrong_move\n",
      "   7717/500000: episode: 7596, duration: 0.122s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 9239864.000000, mae: 3324.671387, mean_q: 9059.195312\n",
      "wrong_move\n",
      "   7718/500000: episode: 7597, duration: 0.104s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 219.000 [219.000, 219.000],  loss: 26639830.000000, mae: 3346.276367, mean_q: 10569.056641\n",
      "wrong_move\n",
      "   7719/500000: episode: 7598, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 31232468.000000, mae: 3313.159912, mean_q: 8382.410156\n",
      "wrong_move\n",
      "   7720/500000: episode: 7599, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2633.000 [2633.000, 2633.000],  loss: 11071826.000000, mae: 3316.447510, mean_q: 7917.777344\n",
      "wrong_move\n",
      "   7721/500000: episode: 7600, duration: 0.095s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2586.000 [2586.000, 2586.000],  loss: 64954468.000000, mae: 3321.717041, mean_q: 8414.398438\n",
      "wrong_move\n",
      "   7722/500000: episode: 7601, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 7451066.500000, mae: 3485.100098, mean_q: 12213.429688\n",
      "wrong_move\n",
      "   7723/500000: episode: 7602, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1840.000 [1840.000, 1840.000],  loss: 45514320.000000, mae: 3336.180664, mean_q: 11261.182617\n",
      "wrong_move\n",
      "   7724/500000: episode: 7603, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 656.000 [656.000, 656.000],  loss: 1595879296.000000, mae: 3889.741211, mean_q: 12134.805664\n",
      "wrong_move\n",
      "   7725/500000: episode: 7604, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 31653998.000000, mae: 3326.970215, mean_q: 7767.238281\n",
      "wrong_move\n",
      "   7726/500000: episode: 7605, duration: 0.141s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 22789622.000000, mae: 3322.670410, mean_q: 9578.104492\n",
      "wrong_move\n",
      "   7727/500000: episode: 7606, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 21581260.000000, mae: 3376.531738, mean_q: 8323.646484\n",
      "wrong_move\n",
      "   7728/500000: episode: 7607, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2541.000 [2541.000, 2541.000],  loss: 6590433.000000, mae: 3423.433594, mean_q: 9882.819336\n",
      "wrong_move\n",
      "   7729/500000: episode: 7608, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2065.000 [2065.000, 2065.000],  loss: 4319465.000000, mae: 3324.185547, mean_q: 7649.746582\n",
      "wrong_move\n",
      "   7730/500000: episode: 7609, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: 53280248.000000, mae: 3323.365723, mean_q: 8178.062012\n",
      "wrong_move\n",
      "done, took 545.298 seconds\n"
     ]
    }
   ],
   "source": [
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "for i in range (10):\n",
    "  policy = EpsGreedyQPolicy(0.3)\n",
    "  dqn = DQNAgent(model=model, nb_actions=NB_ACTIONS, memory=memory,\n",
    "                target_model_update=1e-2, policy=policy)\n",
    "  dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "  # Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "  # slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "  # Ctrl + C.\n",
    "  his = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
    "  \n",
    "  model.save('chess_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save dqn\n",
    "# dqn.save_weights('dqn_{}_weights.h5f'.format('chess'), overwrite=True)\n",
    "\n",
    "# # save model\n",
    "model.save('chess_model.h5')\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "files.download('chess_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# model = keras.models.load_model('chess_model.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
