{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aEsaesKwwFnf",
    "outputId": "c020eac5-a8d3-42da-a1c8-a1ef30ecce3b"
   },
   "outputs": [],
   "source": [
    "! pip install keras-rl2\n",
    "! pip install chess\n",
    "! pip install python-chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ocqViokxFxM",
    "outputId": "a32fdb04-daf7-474b-ab41-be5bee6850bd"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "INImGNcRyKhX",
    "outputId": "afe25e4d-72d3-4bc4-88df-4bb6be1b84d1"
   },
   "outputs": [],
   "source": [
    "# ls drive/MyDrive/Data/Chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 10:57:54.769508: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-15 10:57:54.769534: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten,\\\n",
    "     Input,BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# import gym_chess\n",
    "\n",
    "import chess\n",
    "from sys import platform\n",
    "import os\n",
    "import chess.engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12608206137687737734\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 10:57:56.324652: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-15 10:57:56.325300: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-15 10:57:56.325313: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-15 10:57:56.325333: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LoG): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "os.system('chmod +x stockfish_14.1_linux_x64')\n",
    "engine = chess.engine.SimpleEngine.popen_uci(r\"./stockfish_14.1_linux_x64\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%\n",
    "# if platform == \"linux\" or platform == \"linux2\":\n",
    "#     os.system('chmod +x ../stockfish/stockfish_14.1_linux_x64')\n",
    "#     engine = chess.engine.SimpleEngine.popen_uci(r\"../stockfish/stockfish_14.1_linux_x64\")\n",
    "# elif platform == \"win32\":\n",
    "#     engine = chess.engine.SimpleEngine.popen_uci(r\"../stockfish/stockfish_14.1_win_32bit.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_move(env):\n",
    "    result = engine.play(env.env, chess.engine.Limit(time=0.05))\n",
    "    return result.move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SHAPE = (65, )\n",
    "NB_ACTIONS = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessEnv:\n",
    "    '''\n",
    "    state - obser: ndarray - (65,): [:65] is flatten from int_board; [65] is color of bot; 1 is white and -1 is black\n",
    "    step: int. step_range = (0, 4096) , is encoded from square A to square B (64 x 64 val)\n",
    "    reward: int\n",
    "    '''\n",
    "\n",
    "    mapped = {\n",
    "            'P': 10,     # White Pawn\n",
    "            'p': -10,    # Black Pawn\n",
    "            'N': 20,     # White Knight\n",
    "            'n': -20,    # Black Knight\n",
    "            'B': 30,     # White Bishop\n",
    "            'b': -30,    # Black Bishop\n",
    "            'R': 40,     # White Rook\n",
    "            'r': -40,    # Black Rook\n",
    "            'Q': 50,     # White Queen\n",
    "            'q': -50,    # Black Queen\n",
    "            'K': 900,     # White King\n",
    "            'k': -900     # Black King\n",
    "    }\n",
    "    # state_shape = (8, 8)\n",
    "    # nb_actions = 4096\n",
    "    model = None\n",
    "    \n",
    "    def __init__(self, model: Sequential, neg_r_each_step = -1) -> None:\n",
    "        self.env = chess.Board()\n",
    "        self.model = model\n",
    "        self.state = self.reset()\n",
    "        # [-1] = 1 -> white, -1 -> black\n",
    "        self.bot_color = self.env.turn * 2 - 1\n",
    "        self.neg_r_each_step = neg_r_each_step\n",
    "\n",
    "    def is_draw(self):\n",
    "        if self.env.is_stalemate():\n",
    "            print(\"statlemate\")\n",
    "            return True\n",
    "        if self.env.is_fivefold_repetition():\n",
    "            print(\"fivefold repetition\")\n",
    "            return True\n",
    "        if self.env.is_seventyfive_moves():\n",
    "            print(\"75 moves\")\n",
    "            return True\n",
    "        if self.env.is_insufficient_material():\n",
    "            print(\"Insufficient Material\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_checkmate(self):\n",
    "        # If There is checkmate then it will be TRUE else FALSE.It will be a boolean value.\n",
    "        return self.env.is_checkmate()\n",
    "\n",
    "    def convert_board_to_int(self):\n",
    "        epd_string = self.env.epd()\n",
    "        list_int = np.empty((0, ))\n",
    "        for i in epd_string:\n",
    "            if i == \" \":\n",
    "                list_int = list_int.reshape((8, 8))\n",
    "                return list_int\n",
    "            elif i != \"/\":\n",
    "                if i in self.mapped:\n",
    "                    list_int = np.append(list_int, self.mapped[i])\n",
    "                else:\n",
    "                    for counter in range(0, int(i)):\n",
    "                        list_int = np.append(list_int, 0)\n",
    "        list_int = list_int.reshape((8, 8))\n",
    "        return list_int\n",
    "\n",
    "    def get_state(self) -> np.ndarray:\n",
    "        return np.append(self.convert_board_to_int().reshape(64,), self.env.turn * 2 - 1)\n",
    "\n",
    "    def legal_moves(self):\n",
    "        return list(self.env.legal_moves)\n",
    "\n",
    "    def encodeMove(self, move_uci:str):\n",
    "        if len(move_uci) != 4:\n",
    "            raise ValueError()\n",
    "        a, b = chess.parse_square(move_uci[:2]), chess.parse_square(move_uci[2:])\n",
    "        return a * 64 + b\n",
    "\n",
    "    def decodeMove(self, move_int:int):\n",
    "        a, b = move_int//64, move_int%64\n",
    "        # a, b = chess.square_name(a), chess.square_name(b)\n",
    "\n",
    "        move = self.env.find_move(from_square= a,to_square= b)\n",
    "        return move\n",
    "\n",
    "    def render(self):\n",
    "        print(self.env.unicode())\n",
    "\n",
    "    def reset(self):\n",
    "        # random state\n",
    "        redo = True\n",
    "        num_sample_steps = 0\n",
    "        while redo:\n",
    "            redo = False\n",
    "            self.env = chess.Board()\n",
    "            num_sample_steps = np.random.randint(0, 50)\n",
    "            for i in range (num_sample_steps):\n",
    "                lg_move = self.legal_moves()\n",
    "                if len(lg_move) != 0:\n",
    "                    move = np.random.choice(self.legal_moves())\n",
    "                    self.env.push(move)\n",
    "                else:\n",
    "                    redo = True\n",
    "                    break\n",
    "\n",
    "        Q_val = self.model.predict(self.get_state().reshape((1, 1) + STATE_SHAPE)).reshape(-1, )\n",
    "        print('Val:', min(Q_val), max(Q_val))\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action: int):\n",
    "        reward = 0\n",
    "        done = True\n",
    "        \n",
    "        try:\n",
    "            # move in legal move\n",
    "            move = self.decodeMove(action)\n",
    "\n",
    "            # neg reward each step\n",
    "            reward = self.neg_r_each_step\n",
    "\n",
    "            # location to_square\n",
    "            to_r, to_c = move.to_square//8, move.to_square%8\n",
    "            reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
    "\n",
    "            # action\n",
    "            self.env.push(move)\n",
    "            self.state = self.get_state()\n",
    "\n",
    "            # check end game\n",
    "            if self.is_checkmate():\n",
    "                reward += self.mapped['K']\n",
    "                done = True\n",
    "                print('Win')\n",
    "            elif self.is_draw():\n",
    "                reward += 300\n",
    "                done = True\n",
    "\n",
    "            # opponent's turn   \n",
    "            else:\n",
    "                done = False\n",
    "\n",
    "                move = find_move(self)\n",
    "\n",
    "                # location to_square\n",
    "                to_r, to_c = move.to_square//8, move.to_square%8\n",
    "                reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
    "\n",
    "                # action\n",
    "                self.env.push(move)\n",
    "                self.state = self.get_state()\n",
    "\n",
    "                # check end game\n",
    "                if self.is_checkmate():\n",
    "                    reward -= self.mapped['K']\n",
    "                    done = True\n",
    "                    print(\"Lose\")\n",
    "                elif self.is_draw():\n",
    "                    reward += 300\n",
    "                    done = True\n",
    "\n",
    "        except:\n",
    "            # wrong move\n",
    "            reward = -5000\n",
    "            done = True\n",
    "            print('wrong_move')\n",
    "\n",
    "        return self.state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "WARNING:tensorflow:From /home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/layers/normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 65)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8448      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              528384    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4096)              0         \n",
      "=================================================================\n",
      "Total params: 554,368\n",
      "Trainable params: 553,856\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = Sequential()\n",
    "model.add(Input((1, ) + STATE_SHAPE))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(NB_ACTIONS))\n",
    "model.add(Activation('linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: -51.18316 55.089497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "2021-12-15 10:57:56.915777: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 1999965000 Hz\n"
     ]
    }
   ],
   "source": [
    "env = ChessEnv(model, neg_r_each_step=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('chess_model_opt_init.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Val: -42.17766 12371.252\n",
      "wrong_move\n",
      "     1/50000: episode: 1, duration: 0.196s, episode steps:   1, steps per second:   5, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.908497 12078.172\n",
      "wrong_move\n",
      "     2/50000: episode: 2, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3164.000 [3164.000, 3164.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.2683 12000.288\n",
      "wrong_move\n",
      "     3/50000: episode: 3, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.717342 11960.793\n",
      "wrong_move\n",
      "     4/50000: episode: 4, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.104637 12004.15\n",
      "wrong_move\n",
      "     5/50000: episode: 5, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.069977 12338.572\n",
      "wrong_move\n",
      "     6/50000: episode: 6, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.336517 12054.241\n",
      "wrong_move\n",
      "     7/50000: episode: 7, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.13391 12018.017\n",
      "wrong_move\n",
      "     8/50000: episode: 8, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.85553 12113.259\n",
      "wrong_move\n",
      "     9/50000: episode: 9, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -47.46628 12365.113\n",
      "wrong_move\n",
      "    10/50000: episode: 10, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1360.000 [1360.000, 1360.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.647854 12001.729\n",
      "wrong_move\n",
      "    11/50000: episode: 11, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.29517 12023.329\n",
      "wrong_move\n",
      "    12/50000: episode: 12, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.244747 12200.677\n",
      "wrong_move\n",
      "    13/50000: episode: 13, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3466.000 [3466.000, 3466.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.187176 12256.824\n",
      "wrong_move\n",
      "    14/50000: episode: 14, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2949.000 [2949.000, 2949.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.691654 11956.128\n",
      "wrong_move\n",
      "    15/50000: episode: 15, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.782776 12051.928\n",
      "wrong_move\n",
      "    16/50000: episode: 16, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.689095 12065.356\n",
      "wrong_move\n",
      "    17/50000: episode: 17, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.548267 11972.592\n",
      "wrong_move\n",
      "    18/50000: episode: 18, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 631.000 [631.000, 631.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.03018 11984.056\n",
      "wrong_move\n",
      "    19/50000: episode: 19, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.179825 12001.95\n",
      "wrong_move\n",
      "    20/50000: episode: 20, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.48474 12034.071\n",
      "wrong_move\n",
      "    21/50000: episode: 21, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.16539 12017.953\n",
      "wrong_move\n",
      "    22/50000: episode: 22, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.000874 12218.0205\n",
      "wrong_move\n",
      "    23/50000: episode: 23, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1098.000 [1098.000, 1098.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.066566 11955.937\n",
      "wrong_move\n",
      "    24/50000: episode: 24, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.3317 12004.395\n",
      "wrong_move\n",
      "    25/50000: episode: 25, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.922447 12001.269\n",
      "wrong_move\n",
      "    26/50000: episode: 26, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.77086 12050.887\n",
      "wrong_move\n",
      "    27/50000: episode: 27, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.13678 12028.789\n",
      "wrong_move\n",
      "    28/50000: episode: 28, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.003666 12023.02\n",
      "wrong_move\n",
      "    29/50000: episode: 29, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.744343 12454.436\n",
      "wrong_move\n",
      "    30/50000: episode: 30, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.903183 12028.684\n",
      "wrong_move\n",
      "    31/50000: episode: 31, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.549896 12001.9\n",
      "wrong_move\n",
      "    32/50000: episode: 32, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 885.000 [885.000, 885.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.405556 11990.829\n",
      "wrong_move\n",
      "    33/50000: episode: 33, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.555096 12065.612\n",
      "wrong_move\n",
      "    34/50000: episode: 34, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1892.000 [1892.000, 1892.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.509815 11974.331\n",
      "wrong_move\n",
      "    35/50000: episode: 35, duration: 0.006s, episode steps:   1, steps per second: 164, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.6177 12118.6045\n",
      "wrong_move\n",
      "    36/50000: episode: 36, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2730.000 [2730.000, 2730.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.127995 11758.116\n",
      "wrong_move\n",
      "    37/50000: episode: 37, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.1066 12003.599\n",
      "wrong_move\n",
      "    38/50000: episode: 38, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.53721 12007.038\n",
      "wrong_move\n",
      "    39/50000: episode: 39, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.565426 12325.542\n",
      "wrong_move\n",
      "    40/50000: episode: 40, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.39583 11995.628\n",
      "wrong_move\n",
      "    41/50000: episode: 41, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.56428 11966.814\n",
      "wrong_move\n",
      "    42/50000: episode: 42, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.976677 12449.87\n",
      "wrong_move\n",
      "    43/50000: episode: 43, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 219.000 [219.000, 219.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.472126 11969.434\n",
      "wrong_move\n",
      "    44/50000: episode: 44, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1378.000 [1378.000, 1378.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.16578 12006.313\n",
      "wrong_move\n",
      "    45/50000: episode: 45, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.2584 12024.303\n",
      "wrong_move\n",
      "    46/50000: episode: 46, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 865.000 [865.000, 865.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -49.122925 12309.891\n",
      "wrong_move\n",
      "    47/50000: episode: 47, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2234.000 [2234.000, 2234.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.57383 12027.069\n",
      "wrong_move\n",
      "    48/50000: episode: 48, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.320496 12033.38\n",
      "wrong_move\n",
      "    49/50000: episode: 49, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.980125 12049.596\n",
      "wrong_move\n",
      "    50/50000: episode: 50, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.169292 12008.738\n",
      "wrong_move\n",
      "    51/50000: episode: 51, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.594666 12023.087\n",
      "wrong_move\n",
      "    52/50000: episode: 52, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.335354 11981.285\n",
      "wrong_move\n",
      "    53/50000: episode: 53, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1745.000 [1745.000, 1745.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.987633 12346.351\n",
      "wrong_move\n",
      "    54/50000: episode: 54, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.029972 12005.269\n",
      "wrong_move\n",
      "    55/50000: episode: 55, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.07346 12058.973\n",
      "wrong_move\n",
      "    56/50000: episode: 56, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.781204 12038.063\n",
      "wrong_move\n",
      "    57/50000: episode: 57, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2994.000 [2994.000, 2994.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.044178 12051.243\n",
      "wrong_move\n",
      "    58/50000: episode: 58, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.53818 11976.29\n",
      "wrong_move\n",
      "    59/50000: episode: 59, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.853184 12043.971\n",
      "wrong_move\n",
      "    60/50000: episode: 60, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -37.796795 12293.633\n",
      "wrong_move\n",
      "    61/50000: episode: 61, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.20127 12071.321\n",
      "wrong_move\n",
      "    62/50000: episode: 62, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.610172 11956.862\n",
      "wrong_move\n",
      "    63/50000: episode: 63, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.356663 12059.0625\n",
      "wrong_move\n",
      "    64/50000: episode: 64, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 639.000 [639.000, 639.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.5863 12080.489\n",
      "wrong_move\n",
      "    65/50000: episode: 65, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.656425 11966.237\n",
      "wrong_move\n",
      "    66/50000: episode: 66, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "    67/50000: episode: 67, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3191.000 [3191.000, 3191.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.248222 12088.114\n",
      "wrong_move\n",
      "    68/50000: episode: 68, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.004467 12034.323\n",
      "wrong_move\n",
      "    69/50000: episode: 69, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.428802 11984.053\n",
      "wrong_move\n",
      "    70/50000: episode: 70, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.40422 12007.257\n",
      "wrong_move\n",
      "    71/50000: episode: 71, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.658432 12051.243\n",
      "wrong_move\n",
      "    72/50000: episode: 72, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.043976 12256.512\n",
      "wrong_move\n",
      "    73/50000: episode: 73, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.871742 12021.357\n",
      "wrong_move\n",
      "    74/50000: episode: 74, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.929832 12034.683\n",
      "wrong_move\n",
      "    75/50000: episode: 75, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.092064 12004.588\n",
      "wrong_move\n",
      "    76/50000: episode: 76, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.75832 12065.566\n",
      "wrong_move\n",
      "    77/50000: episode: 77, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1655.000 [1655.000, 1655.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.50094 12050.177\n",
      "wrong_move\n",
      "    78/50000: episode: 78, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -38.73105 12224.523\n",
      "wrong_move\n",
      "    79/50000: episode: 79, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 703.000 [703.000, 703.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -47.7925 12271.89\n",
      "wrong_move\n",
      "    80/50000: episode: 80, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.817215 11994.322\n",
      "wrong_move\n",
      "    81/50000: episode: 81, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.7207 12040.649\n",
      "wrong_move\n",
      "    82/50000: episode: 82, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2257.000 [2257.000, 2257.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.313564 12005.272\n",
      "wrong_move\n",
      "    83/50000: episode: 83, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2371.000 [2371.000, 2371.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -38.61962 12222.368\n",
      "wrong_move\n",
      "    84/50000: episode: 84, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2629.000 [2629.000, 2629.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.54793 11980.976\n",
      "wrong_move\n",
      "    85/50000: episode: 85, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.56054 12002.876\n",
      "wrong_move\n",
      "    86/50000: episode: 86, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1655.000 [1655.000, 1655.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.967945 11862.772\n",
      "wrong_move\n",
      "    87/50000: episode: 87, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.689697 12060.13\n",
      "wrong_move\n",
      "    88/50000: episode: 88, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.318108 12012.728\n",
      "wrong_move\n",
      "    89/50000: episode: 89, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.795437 12169.476\n",
      "wrong_move\n",
      "    90/50000: episode: 90, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.314877 12027.66\n",
      "wrong_move\n",
      "    91/50000: episode: 91, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.013115 12276.049\n",
      "wrong_move\n",
      "    92/50000: episode: 92, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2989.000 [2989.000, 2989.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.754143 11955.918\n",
      "wrong_move\n",
      "    93/50000: episode: 93, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.23993 11998.567\n",
      "wrong_move\n",
      "    94/50000: episode: 94, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.61202 12046.814\n",
      "wrong_move\n",
      "    95/50000: episode: 95, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.541615 11979.138\n",
      "wrong_move\n",
      "    96/50000: episode: 96, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.70601 12058.0625\n",
      "wrong_move\n",
      "    97/50000: episode: 97, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.116848 12289.914\n",
      "wrong_move\n",
      "    98/50000: episode: 98, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2394.000 [2394.000, 2394.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.24105 12024.571\n",
      "wrong_move\n",
      "    99/50000: episode: 99, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2568.000 [2568.000, 2568.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -46.25062 12292.431\n",
      "wrong_move\n",
      "   100/50000: episode: 100, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3428.000 [3428.000, 3428.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.76936 11982.21\n",
      "wrong_move\n",
      "   101/50000: episode: 101, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3649.000 [3649.000, 3649.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.70121 12129.857\n",
      "wrong_move\n",
      "   102/50000: episode: 102, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1769.000 [1769.000, 1769.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.584087 11970.851\n",
      "wrong_move\n",
      "   103/50000: episode: 103, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2285.000 [2285.000, 2285.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.65821 12052.688\n",
      "wrong_move\n",
      "   104/50000: episode: 104, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.965813 11978.827\n",
      "wrong_move\n",
      "   105/50000: episode: 105, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.340767 12073.497\n",
      "wrong_move\n",
      "   106/50000: episode: 106, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.044907 12105.393\n",
      "wrong_move\n",
      "   107/50000: episode: 107, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.83046 12063.425\n",
      "wrong_move\n",
      "   108/50000: episode: 108, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3104.000 [3104.000, 3104.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -47.4559 12407.229\n",
      "wrong_move\n",
      "   109/50000: episode: 109, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.029945 12443.871\n",
      "wrong_move\n",
      "   110/50000: episode: 110, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -37.623947 12309.967\n",
      "wrong_move\n",
      "   111/50000: episode: 111, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.55859 11956.119\n",
      "wrong_move\n",
      "   112/50000: episode: 112, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.220955 12261.859\n",
      "wrong_move\n",
      "   113/50000: episode: 113, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.007904 12198.836\n",
      "wrong_move\n",
      "   114/50000: episode: 114, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3829.000 [3829.000, 3829.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.86092 12066.808\n",
      "wrong_move\n",
      "   115/50000: episode: 115, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.601 12170.253\n",
      "wrong_move\n",
      "   116/50000: episode: 116, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 887.000 [887.000, 887.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.99752 12315.218\n",
      "wrong_move\n",
      "   117/50000: episode: 117, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.07673 12165.093\n",
      "wrong_move\n",
      "   118/50000: episode: 118, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.98282 12028.112\n",
      "wrong_move\n",
      "   119/50000: episode: 119, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.453396 11944.658\n",
      "wrong_move\n",
      "   120/50000: episode: 120, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1536.000 [1536.000, 1536.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.97502 12044.807\n",
      "wrong_move\n",
      "   121/50000: episode: 121, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -48.38819 12287.052\n",
      "wrong_move\n",
      "   122/50000: episode: 122, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3911.000 [3911.000, 3911.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.017845 11898.691\n",
      "wrong_move\n",
      "   123/50000: episode: 123, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.743416 12069.798\n",
      "wrong_move\n",
      "   124/50000: episode: 124, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.20443 12011.978\n",
      "wrong_move\n",
      "   125/50000: episode: 125, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.756325 12292.932\n",
      "wrong_move\n",
      "   126/50000: episode: 126, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.400127 12020.445\n",
      "wrong_move\n",
      "   127/50000: episode: 127, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.842094 11881.036\n",
      "wrong_move\n",
      "   128/50000: episode: 128, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.22451 12205.928\n",
      "wrong_move\n",
      "   129/50000: episode: 129, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1098.000 [1098.000, 1098.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.27182 12071.529\n",
      "wrong_move\n",
      "   130/50000: episode: 130, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.189045 12038.929\n",
      "wrong_move\n",
      "   131/50000: episode: 131, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.45984 11919.8125\n",
      "wrong_move\n",
      "   132/50000: episode: 132, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.08881 11986.467\n",
      "wrong_move\n",
      "   133/50000: episode: 133, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.31858 12026.722\n",
      "wrong_move\n",
      "   134/50000: episode: 134, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2056.000 [2056.000, 2056.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.02397 12231.502\n",
      "wrong_move\n",
      "   135/50000: episode: 135, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1098.000 [1098.000, 1098.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.60711 11951.071\n",
      "wrong_move\n",
      "   136/50000: episode: 136, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.502758 11983.3\n",
      "wrong_move\n",
      "   137/50000: episode: 137, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.83435 12092.782\n",
      "wrong_move\n",
      "   138/50000: episode: 138, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2780.000 [2780.000, 2780.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.672367 11971.214\n",
      "wrong_move\n",
      "   139/50000: episode: 139, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.121166 12059.108\n",
      "wrong_move\n",
      "   140/50000: episode: 140, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.92069 12156.145\n",
      "wrong_move\n",
      "   141/50000: episode: 141, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 619.000 [619.000, 619.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.550552 12028.577\n",
      "wrong_move\n",
      "   142/50000: episode: 142, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3309.000 [3309.000, 3309.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.69025 11783.33\n",
      "wrong_move\n",
      "   143/50000: episode: 143, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.349564 11991.663\n",
      "wrong_move\n",
      "   144/50000: episode: 144, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.353554 11991.017\n",
      "wrong_move\n",
      "   145/50000: episode: 145, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.286987 11990.0\n",
      "wrong_move\n",
      "   146/50000: episode: 146, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.812496 12044.987\n",
      "wrong_move\n",
      "   147/50000: episode: 147, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.06251 12025.922\n",
      "wrong_move\n",
      "   148/50000: episode: 148, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.118053 12022.051\n",
      "wrong_move\n",
      "   149/50000: episode: 149, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.4607 12162.375\n",
      "wrong_move\n",
      "   150/50000: episode: 150, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.44261 11998.948\n",
      "wrong_move\n",
      "   151/50000: episode: 151, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.648304 11971.544\n",
      "wrong_move\n",
      "   152/50000: episode: 152, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.17315 11866.477\n",
      "wrong_move\n",
      "   153/50000: episode: 153, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.196507 12000.581\n",
      "wrong_move\n",
      "   154/50000: episode: 154, duration: 0.022s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.491066 12016.062\n",
      "wrong_move\n",
      "   155/50000: episode: 155, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.228836 12438.7\n",
      "wrong_move\n",
      "   156/50000: episode: 156, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2150.000 [2150.000, 2150.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.93229 11993.393\n",
      "wrong_move\n",
      "   157/50000: episode: 157, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.40541 11929.227\n",
      "wrong_move\n",
      "   158/50000: episode: 158, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1787.000 [1787.000, 1787.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.952045 12196.609\n",
      "wrong_move\n",
      "   159/50000: episode: 159, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4076.000 [4076.000, 4076.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.474678 11983.6045\n",
      "wrong_move\n",
      "   160/50000: episode: 160, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.310974 11876.915\n",
      "wrong_move\n",
      "   161/50000: episode: 161, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 43.000 [43.000, 43.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.445473 11990.982\n",
      "wrong_move\n",
      "   162/50000: episode: 162, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.267838 12029.8545\n",
      "wrong_move\n",
      "   163/50000: episode: 163, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.533634 12135.998\n",
      "wrong_move\n",
      "   164/50000: episode: 164, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.697796 12224.926\n",
      "wrong_move\n",
      "   165/50000: episode: 165, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2260.000 [2260.000, 2260.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.647232 11708.34\n",
      "wrong_move\n",
      "   166/50000: episode: 166, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.3964 12033.152\n",
      "wrong_move\n",
      "   167/50000: episode: 167, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3705.000 [3705.000, 3705.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.278366 12185.48\n",
      "wrong_move\n",
      "   168/50000: episode: 168, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.05716 12068.682\n",
      "wrong_move\n",
      "   169/50000: episode: 169, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.856785 12156.205\n",
      "wrong_move\n",
      "   170/50000: episode: 170, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.35728 12036.557\n",
      "wrong_move\n",
      "   171/50000: episode: 171, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.925747 12046.073\n",
      "wrong_move\n",
      "   172/50000: episode: 172, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.90832 12012.8545\n",
      "wrong_move\n",
      "   173/50000: episode: 173, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.606506 12026.579\n",
      "wrong_move\n",
      "   174/50000: episode: 174, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.655163 12049.431\n",
      "wrong_move\n",
      "   175/50000: episode: 175, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.202286 12038.261\n",
      "wrong_move\n",
      "   176/50000: episode: 176, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.492638 11828.799\n",
      "wrong_move\n",
      "   177/50000: episode: 177, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2048.000 [2048.000, 2048.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.19516 12026.256\n",
      "wrong_move\n",
      "   178/50000: episode: 178, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.0852 12023.515\n",
      "wrong_move\n",
      "   179/50000: episode: 179, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.734337 12485.888\n",
      "wrong_move\n",
      "   180/50000: episode: 180, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.695557 11987.293\n",
      "wrong_move\n",
      "   181/50000: episode: 181, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.007854 12100.02\n",
      "wrong_move\n",
      "   182/50000: episode: 182, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3791.000 [3791.000, 3791.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.868465 12053.412\n",
      "wrong_move\n",
      "   183/50000: episode: 183, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -46.322704 12498.009\n",
      "wrong_move\n",
      "   184/50000: episode: 184, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3290.000 [3290.000, 3290.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.88807 12042.477\n",
      "wrong_move\n",
      "   185/50000: episode: 185, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.80556 12001.664\n",
      "wrong_move\n",
      "   186/50000: episode: 186, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.57172 12117.331\n",
      "wrong_move\n",
      "   187/50000: episode: 187, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.071762 12248.8125\n",
      "wrong_move\n",
      "   188/50000: episode: 188, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.908634 12009.072\n",
      "wrong_move\n",
      "   189/50000: episode: 189, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.26952 11990.635\n",
      "wrong_move\n",
      "   190/50000: episode: 190, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.865772 12107.139\n",
      "wrong_move\n",
      "   191/50000: episode: 191, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1210.000 [1210.000, 1210.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.792988 12386.567\n",
      "wrong_move\n",
      "   192/50000: episode: 192, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.613323 12256.584\n",
      "wrong_move\n",
      "   193/50000: episode: 193, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.381645 11971.868\n",
      "wrong_move\n",
      "   194/50000: episode: 194, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3650.000 [3650.000, 3650.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.380783 12107.216\n",
      "wrong_move\n",
      "   195/50000: episode: 195, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1937.000 [1937.000, 1937.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.43509 11702.974\n",
      "wrong_move\n",
      "   196/50000: episode: 196, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.639313 12110.09\n",
      "wrong_move\n",
      "   197/50000: episode: 197, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3721.000 [3721.000, 3721.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.153343 12007.674\n",
      "wrong_move\n",
      "   198/50000: episode: 198, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.553223 12049.348\n",
      "wrong_move\n",
      "   199/50000: episode: 199, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.09634 12008.359\n",
      "wrong_move\n",
      "   200/50000: episode: 200, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2718.000 [2718.000, 2718.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.192825 12001.046\n",
      "wrong_move\n",
      "   201/50000: episode: 201, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.703896 11868.233\n",
      "wrong_move\n",
      "   202/50000: episode: 202, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.079765 12033.624\n",
      "wrong_move\n",
      "   203/50000: episode: 203, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 866.000 [866.000, 866.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.551884 12024.567\n",
      "wrong_move\n",
      "   204/50000: episode: 204, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3309.000 [3309.000, 3309.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.586254 11958.747\n",
      "wrong_move\n",
      "   205/50000: episode: 205, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.09346 12021.338\n",
      "wrong_move\n",
      "   206/50000: episode: 206, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.1759 12030.957\n",
      "wrong_move\n",
      "   207/50000: episode: 207, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 519.000 [519.000, 519.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.332767 11978.326\n",
      "wrong_move\n",
      "   208/50000: episode: 208, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4050.000 [4050.000, 4050.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.141197 12031.395\n",
      "wrong_move\n",
      "   209/50000: episode: 209, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1008.000 [1008.000, 1008.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -48.66934 12275.39\n",
      "wrong_move\n",
      "   210/50000: episode: 210, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.358086 11989.677\n",
      "wrong_move\n",
      "   211/50000: episode: 211, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1874.000 [1874.000, 1874.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.877068 12068.217\n",
      "wrong_move\n",
      "   212/50000: episode: 212, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.570084 11981.254\n",
      "wrong_move\n",
      "   213/50000: episode: 213, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.362846 12043.555\n",
      "wrong_move\n",
      "   214/50000: episode: 214, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.60373 12404.582\n",
      "wrong_move\n",
      "   215/50000: episode: 215, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.535496 11981.284\n",
      "wrong_move\n",
      "   216/50000: episode: 216, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.260757 12448.436\n",
      "wrong_move\n",
      "   217/50000: episode: 217, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.44734 12361.535\n",
      "wrong_move\n",
      "   218/50000: episode: 218, duration: 0.022s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.79976 12055.178\n",
      "wrong_move\n",
      "   219/50000: episode: 219, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1963.000 [1963.000, 1963.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.97057 12100.198\n",
      "wrong_move\n",
      "   220/50000: episode: 220, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.28871 12310.17\n",
      "wrong_move\n",
      "   221/50000: episode: 221, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.16368 12036.76\n",
      "wrong_move\n",
      "   222/50000: episode: 222, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.014553 12045.933\n",
      "wrong_move\n",
      "   223/50000: episode: 223, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.818016 12115.349\n",
      "wrong_move\n",
      "   224/50000: episode: 224, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2730.000 [2730.000, 2730.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -48.270805 12249.699\n",
      "wrong_move\n",
      "   225/50000: episode: 225, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.695545 11983.564\n",
      "wrong_move\n",
      "   226/50000: episode: 226, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3843.000 [3843.000, 3843.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.25636 12051.625\n",
      "wrong_move\n",
      "   227/50000: episode: 227, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.96469 12330.466\n",
      "wrong_move\n",
      "   228/50000: episode: 228, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1186.000 [1186.000, 1186.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.031197 12036.713\n",
      "wrong_move\n",
      "   229/50000: episode: 229, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 687.000 [687.000, 687.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.63601 11970.771\n",
      "wrong_move\n",
      "   230/50000: episode: 230, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.454254 12062.581\n",
      "wrong_move\n",
      "   231/50000: episode: 231, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -49.021317 12302.591\n",
      "wrong_move\n",
      "   232/50000: episode: 232, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.446938 12076.8955\n",
      "wrong_move\n",
      "   233/50000: episode: 233, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.209484 12008.875\n",
      "wrong_move\n",
      "   234/50000: episode: 234, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.257294 12438.006\n",
      "wrong_move\n",
      "   235/50000: episode: 235, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.934673 12065.76\n",
      "wrong_move\n",
      "   236/50000: episode: 236, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -38.737225 12207.064\n",
      "wrong_move\n",
      "   237/50000: episode: 237, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3044.000 [3044.000, 3044.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.15444 12354.553\n",
      "wrong_move\n",
      "   238/50000: episode: 238, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1536.000 [1536.000, 1536.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -47.10337 12267.982\n",
      "wrong_move\n",
      "   239/50000: episode: 239, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3737.000 [3737.000, 3737.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.24308 12125.517\n",
      "wrong_move\n",
      "   240/50000: episode: 240, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.682564 12290.27\n",
      "wrong_move\n",
      "   241/50000: episode: 241, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1169.000 [1169.000, 1169.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.812054 12048.649\n",
      "wrong_move\n",
      "   242/50000: episode: 242, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -35.185925 12314.914\n",
      "wrong_move\n",
      "   243/50000: episode: 243, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3272.000 [3272.000, 3272.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.75643 12425.781\n",
      "wrong_move\n",
      "   244/50000: episode: 244, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.95043 12019.149\n",
      "wrong_move\n",
      "   245/50000: episode: 245, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1725.000 [1725.000, 1725.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.949486 12315.867\n",
      "wrong_move\n",
      "   246/50000: episode: 246, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.75015 11989.723\n",
      "wrong_move\n",
      "   247/50000: episode: 247, duration: 0.022s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "   248/50000: episode: 248, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 847.000 [847.000, 847.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.937695 12031.173\n",
      "wrong_move\n",
      "   249/50000: episode: 249, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.28601 11888.471\n",
      "wrong_move\n",
      "   250/50000: episode: 250, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.395695 11991.502\n",
      "wrong_move\n",
      "   251/50000: episode: 251, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.94294 12412.373\n",
      "wrong_move\n",
      "   252/50000: episode: 252, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.455334 12042.627\n",
      "wrong_move\n",
      "   253/50000: episode: 253, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.386547 12234.225\n",
      "wrong_move\n",
      "   254/50000: episode: 254, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4076.000 [4076.000, 4076.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.6317 12015.528\n",
      "wrong_move\n",
      "   255/50000: episode: 255, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.838024 12124.166\n",
      "wrong_move\n",
      "   256/50000: episode: 256, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.64776 11993.782\n",
      "wrong_move\n",
      "   258/50000: episode: 257, duration: 0.085s, episode steps:   2, steps per second:  23, episode reward: -5041.000, mean reward: -2520.500 [-5000.000, -41.000], mean action: 748.000 [105.000, 1391.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.949425 12118.058\n",
      "wrong_move\n",
      "   259/50000: episode: 258, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.320694 11978.263\n",
      "wrong_move\n",
      "   260/50000: episode: 259, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 546.000 [546.000, 546.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.760334 11978.743\n",
      "wrong_move\n",
      "   261/50000: episode: 260, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.468746 11965.671\n",
      "wrong_move\n",
      "   262/50000: episode: 261, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.066597 12016.088\n",
      "wrong_move\n",
      "   263/50000: episode: 262, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.15899 12072.645\n",
      "wrong_move\n",
      "   264/50000: episode: 263, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -47.820957 12252.109\n",
      "wrong_move\n",
      "   265/50000: episode: 264, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.97836 11910.355\n",
      "wrong_move\n",
      "   266/50000: episode: 265, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.041615 11987.544\n",
      "wrong_move\n",
      "   267/50000: episode: 266, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.721497 12005.249\n",
      "wrong_move\n",
      "   268/50000: episode: 267, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1610.000 [1610.000, 1610.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.811607 11968.754\n",
      "wrong_move\n",
      "   269/50000: episode: 268, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.505 11971.192\n",
      "wrong_move\n",
      "   270/50000: episode: 269, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.397785 11999.054\n",
      "wrong_move\n",
      "   271/50000: episode: 270, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 518.000 [518.000, 518.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.061752 12076.299\n",
      "wrong_move\n",
      "   272/50000: episode: 271, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4091.000 [4091.000, 4091.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.631992 11974.698\n",
      "wrong_move\n",
      "   273/50000: episode: 272, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.6186 12043.93\n",
      "wrong_move\n",
      "   274/50000: episode: 273, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.32981 12071.672\n",
      "wrong_move\n",
      "   275/50000: episode: 274, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.99498 12013.333\n",
      "wrong_move\n",
      "   276/50000: episode: 275, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.30025 12010.599\n",
      "wrong_move\n",
      "   277/50000: episode: 276, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.519264 12384.088\n",
      "wrong_move\n",
      "   278/50000: episode: 277, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2552.000 [2552.000, 2552.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -38.81084 12180.855\n",
      "wrong_move\n",
      "   279/50000: episode: 278, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3466.000 [3466.000, 3466.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.733994 12137.795\n",
      "wrong_move\n",
      "   280/50000: episode: 279, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "   281/50000: episode: 280, duration: 0.006s, episode steps:   1, steps per second: 154, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 403.000 [403.000, 403.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.641914 12446.042\n",
      "wrong_move\n",
      "   282/50000: episode: 281, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.466537 12066.524\n",
      "wrong_move\n",
      "   283/50000: episode: 282, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2859.000 [2859.000, 2859.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.61287 12278.504\n",
      "wrong_move\n",
      "   284/50000: episode: 283, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.09051 12016.643\n",
      "wrong_move\n",
      "   285/50000: episode: 284, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.12848 12005.804\n",
      "wrong_move\n",
      "   286/50000: episode: 285, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.16774 11892.367\n",
      "wrong_move\n",
      "   287/50000: episode: 286, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.65617 12062.009\n",
      "wrong_move\n",
      "   288/50000: episode: 287, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.767975 12045.865\n",
      "wrong_move\n",
      "   289/50000: episode: 288, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 39.000 [39.000, 39.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.876797 12419.398\n",
      "wrong_move\n",
      "   290/50000: episode: 289, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.494907 12102.297\n",
      "wrong_move\n",
      "   291/50000: episode: 290, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.74276 12042.681\n",
      "wrong_move\n",
      "   292/50000: episode: 291, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.018173 12051.372\n",
      "wrong_move\n",
      "   293/50000: episode: 292, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1185.000 [1185.000, 1185.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.70588 12004.802\n",
      "wrong_move\n",
      "   294/50000: episode: 293, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1481.000 [1481.000, 1481.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.058456 12192.329\n",
      "wrong_move\n",
      "   295/50000: episode: 294, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2077.000 [2077.000, 2077.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.304504 12017.567\n",
      "wrong_move\n",
      "   296/50000: episode: 295, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.32115 12090.056\n",
      "wrong_move\n",
      "   297/50000: episode: 296, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3719.000 [3719.000, 3719.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "   298/50000: episode: 297, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3362.000 [3362.000, 3362.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.412884 12002.832\n",
      "wrong_move\n",
      "   299/50000: episode: 298, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -48.378838 12261.51\n",
      "wrong_move\n",
      "   300/50000: episode: 299, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3521.000 [3521.000, 3521.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.9149 12125.515\n",
      "wrong_move\n",
      "   301/50000: episode: 300, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.13525 12040.94\n",
      "wrong_move\n",
      "   302/50000: episode: 301, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3379.000 [3379.000, 3379.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.36635 12071.998\n",
      "wrong_move\n",
      "   303/50000: episode: 302, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3882.000 [3882.000, 3882.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.586254 11958.747\n",
      "wrong_move\n",
      "   304/50000: episode: 303, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.09414 12139.571\n",
      "wrong_move\n",
      "   305/50000: episode: 304, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2800.000 [2800.000, 2800.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.4276 12006.332\n",
      "wrong_move\n",
      "   306/50000: episode: 305, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.841125 12350.24\n",
      "wrong_move\n",
      "   307/50000: episode: 306, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.333893 12167.239\n",
      "wrong_move\n",
      "   308/50000: episode: 307, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.23666 12314.622\n",
      "wrong_move\n",
      "   309/50000: episode: 308, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.36159 11997.884\n",
      "wrong_move\n",
      "   310/50000: episode: 309, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.627117 12221.468\n",
      "wrong_move\n",
      "   311/50000: episode: 310, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2326.000 [2326.000, 2326.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.018723 11920.723\n",
      "wrong_move\n",
      "   312/50000: episode: 311, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.20725 12008.644\n",
      "wrong_move\n",
      "   313/50000: episode: 312, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2510.000 [2510.000, 2510.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.713284 12058.999\n",
      "wrong_move\n",
      "   314/50000: episode: 313, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2027.000 [2027.000, 2027.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.911396 12415.838\n",
      "wrong_move\n",
      "   315/50000: episode: 314, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "   316/50000: episode: 315, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -46.126064 12212.113\n",
      "wrong_move\n",
      "   317/50000: episode: 316, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2520.000 [2520.000, 2520.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.207664 12086.087\n",
      "wrong_move\n",
      "   318/50000: episode: 317, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1360.000 [1360.000, 1360.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.907677 12395.854\n",
      "wrong_move\n",
      "   319/50000: episode: 318, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.7291 11722.823\n",
      "wrong_move\n",
      "   320/50000: episode: 319, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3973.000 [3973.000, 3973.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.471653 12053.067\n",
      "wrong_move\n",
      "   321/50000: episode: 320, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.082645 12285.968\n",
      "wrong_move\n",
      "   322/50000: episode: 321, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.4217 11989.244\n",
      "wrong_move\n",
      "   323/50000: episode: 322, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.243065 11992.288\n",
      "wrong_move\n",
      "   324/50000: episode: 323, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -46.02964 12327.349\n",
      "wrong_move\n",
      "   325/50000: episode: 324, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3732.000 [3732.000, 3732.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -48.185272 12274.6\n",
      "wrong_move\n",
      "   326/50000: episode: 325, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.485695 12006.91\n",
      "wrong_move\n",
      "   327/50000: episode: 326, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.725803 12422.74\n",
      "wrong_move\n",
      "   328/50000: episode: 327, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.933754 12047.694\n",
      "wrong_move\n",
      "   329/50000: episode: 328, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.54284 11968.344\n",
      "wrong_move\n",
      "   330/50000: episode: 329, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.054764 12044.874\n",
      "wrong_move\n",
      "   331/50000: episode: 330, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.714306 12049.747\n",
      "wrong_move\n",
      "   332/50000: episode: 331, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.087147 12017.412\n",
      "wrong_move\n",
      "   333/50000: episode: 332, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.60219 11980.192\n",
      "wrong_move\n",
      "   334/50000: episode: 333, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3189.000 [3189.000, 3189.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.188183 12056.624\n",
      "wrong_move\n",
      "   335/50000: episode: 334, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2211.000 [2211.000, 2211.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.250584 12070.303\n",
      "wrong_move\n",
      "   336/50000: episode: 335, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.760216 12049.2295\n",
      "wrong_move\n",
      "   337/50000: episode: 336, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -37.466434 11974.309\n",
      "wrong_move\n",
      "   338/50000: episode: 337, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.241974 12119.96\n",
      "wrong_move\n",
      "   339/50000: episode: 338, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1877.000 [1877.000, 1877.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.984085 12030.974\n",
      "wrong_move\n",
      "   340/50000: episode: 339, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 341.000 [341.000, 341.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.300617 11989.131\n",
      "wrong_move\n",
      "   341/50000: episode: 340, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.83349 11877.864\n",
      "wrong_move\n",
      "   342/50000: episode: 341, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3501.000 [3501.000, 3501.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.131897 11880.628\n",
      "wrong_move\n",
      "   343/50000: episode: 342, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2253.000 [2253.000, 2253.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.028866 12185.025\n",
      "wrong_move\n",
      "   344/50000: episode: 343, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2119.000 [2119.000, 2119.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "   345/50000: episode: 344, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.793476 12040.738\n",
      "wrong_move\n",
      "   346/50000: episode: 345, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.56839 11923.603\n",
      "wrong_move\n",
      "   347/50000: episode: 346, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.474735 12054.847\n",
      "wrong_move\n",
      "   348/50000: episode: 347, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.648026 11959.076\n",
      "wrong_move\n",
      "   349/50000: episode: 348, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.237865 12019.283\n",
      "wrong_move\n",
      "   350/50000: episode: 349, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2761.000 [2761.000, 2761.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.272877 12019.9\n",
      "wrong_move\n",
      "   351/50000: episode: 350, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.36078 12023.091\n",
      "wrong_move\n",
      "   352/50000: episode: 351, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3274.000 [3274.000, 3274.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.19359 12200.839\n",
      "wrong_move\n",
      "   353/50000: episode: 352, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.41059 11998.194\n",
      "wrong_move\n",
      "   354/50000: episode: 353, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2688.000 [2688.000, 2688.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.995346 12044.958\n",
      "wrong_move\n",
      "   355/50000: episode: 354, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 34.000 [34.000, 34.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.595493 11977.3545\n",
      "wrong_move\n",
      "   356/50000: episode: 355, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -47.7239 12406.205\n",
      "wrong_move\n",
      "   357/50000: episode: 356, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.983734 11973.281\n",
      "wrong_move\n",
      "   358/50000: episode: 357, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.58517 11967.123\n",
      "wrong_move\n",
      "   359/50000: episode: 358, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1102.000 [1102.000, 1102.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.630157 11964.011\n",
      "wrong_move\n",
      "   360/50000: episode: 359, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.784786 12392.593\n",
      "wrong_move\n",
      "   361/50000: episode: 360, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.591908 12068.319\n",
      "wrong_move\n",
      "   362/50000: episode: 361, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.265842 12049.503\n",
      "wrong_move\n",
      "   363/50000: episode: 362, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.347908 11999.426\n",
      "wrong_move\n",
      "   364/50000: episode: 363, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.781677 12345.365\n",
      "wrong_move\n",
      "   365/50000: episode: 364, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3097.000 [3097.000, 3097.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.035408 12033.068\n",
      "wrong_move\n",
      "   366/50000: episode: 365, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1019.000 [1019.000, 1019.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.50701 11839.517\n",
      "wrong_move\n",
      "   367/50000: episode: 366, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1051.000 [1051.000, 1051.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.645454 12050.817\n",
      "wrong_move\n",
      "   368/50000: episode: 367, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3766.000 [3766.000, 3766.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.00377 12257.073\n",
      "wrong_move\n",
      "   369/50000: episode: 368, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -48.024826 12409.186\n",
      "wrong_move\n",
      "   370/50000: episode: 369, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1237.000 [1237.000, 1237.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.408405 11997.915\n",
      "wrong_move\n",
      "   371/50000: episode: 370, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.244843 12016.853\n",
      "wrong_move\n",
      "   372/50000: episode: 371, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.061554 11894.066\n",
      "wrong_move\n",
      "   373/50000: episode: 372, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1242.000 [1242.000, 1242.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.428932 12053.611\n",
      "wrong_move\n",
      "   374/50000: episode: 373, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.618618 12039.428\n",
      "wrong_move\n",
      "   375/50000: episode: 374, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.261757 11998.379\n",
      "wrong_move\n",
      "   376/50000: episode: 375, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.994293 12048.353\n",
      "wrong_move\n",
      "   377/50000: episode: 376, duration: 0.022s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -38.52884 12223.853\n",
      "wrong_move\n",
      "   378/50000: episode: 377, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1098.000 [1098.000, 1098.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.86655 11989.521\n",
      "wrong_move\n",
      "   379/50000: episode: 378, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "   380/50000: episode: 379, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1686.000 [1686.000, 1686.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.458546 12005.156\n",
      "wrong_move\n",
      "   381/50000: episode: 380, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.095943 12044.877\n",
      "wrong_move\n",
      "   382/50000: episode: 381, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.273605 11896.331\n",
      "wrong_move\n",
      "   383/50000: episode: 382, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.48945 11968.262\n",
      "wrong_move\n",
      "   384/50000: episode: 383, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.933704 11983.778\n",
      "wrong_move\n",
      "   385/50000: episode: 384, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.423077 12000.605\n",
      "wrong_move\n",
      "   386/50000: episode: 385, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -46.015648 12093.759\n",
      "wrong_move\n",
      "   387/50000: episode: 386, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.752525 11928.424\n",
      "wrong_move\n",
      "   388/50000: episode: 387, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.198093 12013.588\n",
      "wrong_move\n",
      "   389/50000: episode: 388, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.54137 12371.234\n",
      "wrong_move\n",
      "   390/50000: episode: 389, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 790.000 [790.000, 790.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "   391/50000: episode: 390, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -49.619747 12194.351\n",
      "wrong_move\n",
      "   392/50000: episode: 391, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -48.39102 12279.051\n",
      "wrong_move\n",
      "   393/50000: episode: 392, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3325.000 [3325.000, 3325.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.174347 12001.877\n",
      "wrong_move\n",
      "   394/50000: episode: 393, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600098 11953.857\n",
      "wrong_move\n",
      "   395/50000: episode: 394, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -38.721977 12241.36\n",
      "wrong_move\n",
      "   396/50000: episode: 395, duration: 0.027s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3655.000 [3655.000, 3655.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.54352 11988.834\n",
      "wrong_move\n",
      "   397/50000: episode: 396, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -48.39582 12175.974\n",
      "wrong_move\n",
      "   398/50000: episode: 397, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3262.000 [3262.000, 3262.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.268314 12132.898\n",
      "wrong_move\n",
      "   399/50000: episode: 398, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.58295 11936.719\n",
      "wrong_move\n",
      "   400/50000: episode: 399, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.023354 12013.439\n",
      "wrong_move\n",
      "   401/50000: episode: 400, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -46.18949 12328.34\n",
      "wrong_move\n",
      "   402/50000: episode: 401, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.48821 11992.023\n",
      "wrong_move\n",
      "   403/50000: episode: 402, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.072372 12008.391\n",
      "wrong_move\n",
      "   404/50000: episode: 403, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.905327 12236.027\n",
      "wrong_move\n",
      "   405/50000: episode: 404, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3813.000 [3813.000, 3813.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.66721 12046.465\n",
      "wrong_move\n",
      "   406/50000: episode: 405, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3610.000 [3610.000, 3610.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.68787 12254.241\n",
      "wrong_move\n",
      "   407/50000: episode: 406, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.802814 12011.87\n",
      "wrong_move\n",
      "   408/50000: episode: 407, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.22221 12062.676\n",
      "wrong_move\n",
      "   409/50000: episode: 408, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.01762 12046.46\n",
      "wrong_move\n",
      "   410/50000: episode: 409, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3943.000 [3943.000, 3943.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.760338 12050.869\n",
      "wrong_move\n",
      "   411/50000: episode: 410, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.53051 12059.455\n",
      "wrong_move\n",
      "   412/50000: episode: 411, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.149986 12238.075\n",
      "wrong_move\n",
      "   413/50000: episode: 412, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.39185 12008.943\n",
      "wrong_move\n",
      "   414/50000: episode: 413, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.13555 12023.849\n",
      "wrong_move\n",
      "   415/50000: episode: 414, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.004047 11841.702\n",
      "wrong_move\n",
      "   416/50000: episode: 415, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.128853 12004.95\n",
      "wrong_move\n",
      "   417/50000: episode: 416, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 402.000 [402.000, 402.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -46.263855 12741.175\n",
      "wrong_move\n",
      "   418/50000: episode: 417, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 999.000 [999.000, 999.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.069706 12050.485\n",
      "wrong_move\n",
      "   419/50000: episode: 418, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2960.000 [2960.000, 2960.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.411976 11992.514\n",
      "wrong_move\n",
      "   420/50000: episode: 419, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 133.000 [133.000, 133.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.593754 11979.748\n",
      "wrong_move\n",
      "   421/50000: episode: 420, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.500496 11971.729\n",
      "wrong_move\n",
      "   422/50000: episode: 421, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -48.06547 12286.701\n",
      "wrong_move\n",
      "   423/50000: episode: 422, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -47.017097 12153.576\n",
      "wrong_move\n",
      "   424/50000: episode: 423, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1979.000 [1979.000, 1979.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.708466 12037.692\n",
      "wrong_move\n",
      "   425/50000: episode: 424, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.176605 12110.657\n",
      "wrong_move\n",
      "   426/50000: episode: 425, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1326.000 [1326.000, 1326.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.168526 11991.349\n",
      "wrong_move\n",
      "   427/50000: episode: 426, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 65.000 [65.000, 65.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.485157 11926.271\n",
      "wrong_move\n",
      "   428/50000: episode: 427, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 178.000 [178.000, 178.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.413742 12042.538\n",
      "wrong_move\n",
      "   429/50000: episode: 428, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -47.44206 12158.22\n",
      "wrong_move\n",
      "   430/50000: episode: 429, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.606594 11959.455\n",
      "wrong_move\n",
      "   431/50000: episode: 430, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.199253 12191.925\n",
      "wrong_move\n",
      "   432/50000: episode: 431, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3799.000 [3799.000, 3799.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.59602 12428.367\n",
      "wrong_move\n",
      "   433/50000: episode: 432, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.567818 11964.422\n",
      "wrong_move\n",
      "   434/50000: episode: 433, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.577007 11973.85\n",
      "wrong_move\n",
      "   435/50000: episode: 434, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1666.000 [1666.000, 1666.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.327923 12004.366\n",
      "wrong_move\n",
      "   436/50000: episode: 435, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.706806 12006.144\n",
      "wrong_move\n",
      "   437/50000: episode: 436, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 592.000 [592.000, 592.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.082287 12451.208\n",
      "wrong_move\n",
      "   438/50000: episode: 437, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.856403 12043.554\n",
      "wrong_move\n",
      "   439/50000: episode: 438, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2551.000 [2551.000, 2551.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.06758 12096.215\n",
      "wrong_move\n",
      "   440/50000: episode: 439, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.976933 12065.14\n",
      "wrong_move\n",
      "   441/50000: episode: 440, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.386417 12000.277\n",
      "wrong_move\n",
      "   442/50000: episode: 441, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.98331 12407.233\n",
      "wrong_move\n",
      "   443/50000: episode: 442, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.33637 12075.293\n",
      "wrong_move\n",
      "   444/50000: episode: 443, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.613266 11974.758\n",
      "wrong_move\n",
      "   445/50000: episode: 444, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3576.000 [3576.000, 3576.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.65538 11996.305\n",
      "wrong_move\n",
      "   446/50000: episode: 445, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.467663 12024.096\n",
      "wrong_move\n",
      "   447/50000: episode: 446, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 11.000 [11.000, 11.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.759586 12112.175\n",
      "wrong_move\n",
      "   448/50000: episode: 447, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.771942 11987.154\n",
      "wrong_move\n",
      "   449/50000: episode: 448, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.9279 11888.288\n",
      "wrong_move\n",
      "   450/50000: episode: 449, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.71063 12048.407\n",
      "wrong_move\n",
      "   451/50000: episode: 450, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.29505 12068.117\n",
      "wrong_move\n",
      "   452/50000: episode: 451, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.875546 12462.078\n",
      "wrong_move\n",
      "   453/50000: episode: 452, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3894.000 [3894.000, 3894.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.36653 11996.067\n",
      "wrong_move\n",
      "   454/50000: episode: 453, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 857.000 [857.000, 857.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.41498 12254.454\n",
      "wrong_move\n",
      "   455/50000: episode: 454, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.202335 11889.543\n",
      "wrong_move\n",
      "   456/50000: episode: 455, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.19242 12425.34\n",
      "wrong_move\n",
      "   457/50000: episode: 456, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 831.000 [831.000, 831.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.24619 12228.398\n",
      "wrong_move\n",
      "   458/50000: episode: 457, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.818737 11992.271\n",
      "wrong_move\n",
      "   459/50000: episode: 458, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.655827 11679.156\n",
      "wrong_move\n",
      "   460/50000: episode: 459, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.719494 12325.456\n",
      "wrong_move\n",
      "   461/50000: episode: 460, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 364.000 [364.000, 364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.134205 12168.89\n",
      "wrong_move\n",
      "   462/50000: episode: 461, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3309.000 [3309.000, 3309.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.69509 11968.792\n",
      "wrong_move\n",
      "   463/50000: episode: 462, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -38.729256 12245.343\n",
      "wrong_move\n",
      "   464/50000: episode: 463, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1974.000 [1974.000, 1974.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "   465/50000: episode: 464, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.976437 11902.409\n",
      "wrong_move\n",
      "   466/50000: episode: 465, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3309.000 [3309.000, 3309.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "   467/50000: episode: 466, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1313.000 [1313.000, 1313.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.92119 12051.835\n",
      "wrong_move\n",
      "   468/50000: episode: 467, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3169.000 [3169.000, 3169.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.16266 11993.806\n",
      "wrong_move\n",
      "   469/50000: episode: 468, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2694.000 [2694.000, 2694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.74017 12325.195\n",
      "wrong_move\n",
      "   470/50000: episode: 469, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3987.000 [3987.000, 3987.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.861805 12055.675\n",
      "wrong_move\n",
      "   471/50000: episode: 470, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3851.000 [3851.000, 3851.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.38208 11992.956\n",
      "wrong_move\n",
      "   472/50000: episode: 471, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.997982 12034.012\n",
      "wrong_move\n",
      "   473/50000: episode: 472, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2486.000 [2486.000, 2486.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -48.147594 12256.765\n",
      "wrong_move\n",
      "   474/50000: episode: 473, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.30493 12000.321\n",
      "wrong_move\n",
      "   475/50000: episode: 474, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.58674 11865.924\n",
      "wrong_move\n",
      "   476/50000: episode: 475, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1051.000 [1051.000, 1051.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.111366 12104.295\n",
      "wrong_move\n",
      "   477/50000: episode: 476, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.828667 12475.23\n",
      "wrong_move\n",
      "   478/50000: episode: 477, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.19777 12236.972\n",
      "wrong_move\n",
      "   479/50000: episode: 478, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.575584 12052.817\n",
      "wrong_move\n",
      "   480/50000: episode: 479, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -48.427963 12284.253\n",
      "wrong_move\n",
      "   481/50000: episode: 480, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 170.000 [170.000, 170.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -38.116817 12045.737\n",
      "wrong_move\n",
      "   482/50000: episode: 481, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.262115 11871.662\n",
      "wrong_move\n",
      "   483/50000: episode: 482, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.02071 12007.358\n",
      "wrong_move\n",
      "   484/50000: episode: 483, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.14323 12011.961\n",
      "wrong_move\n",
      "   485/50000: episode: 484, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -29.514637 11951.772\n",
      "wrong_move\n",
      "   486/50000: episode: 485, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2852.000 [2852.000, 2852.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.11869 12022.413\n",
      "wrong_move\n",
      "   487/50000: episode: 486, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.82885 12028.124\n",
      "wrong_move\n",
      "   488/50000: episode: 487, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.519466 12080.221\n",
      "wrong_move\n",
      "   489/50000: episode: 488, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.315903 12066.037\n",
      "wrong_move\n",
      "   490/50000: episode: 489, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.958164 12033.296\n",
      "wrong_move\n",
      "   491/50000: episode: 490, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.5467 12479.289\n",
      "wrong_move\n",
      "   492/50000: episode: 491, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.892017 11991.891\n",
      "wrong_move\n",
      "   493/50000: episode: 492, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.577744 11947.047\n",
      "wrong_move\n",
      "   494/50000: episode: 493, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.718056 12073.237\n",
      "wrong_move\n",
      "   495/50000: episode: 494, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.029903 12171.278\n",
      "wrong_move\n",
      "   496/50000: episode: 495, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.113453 12111.552\n",
      "wrong_move\n",
      "   497/50000: episode: 496, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.610107 11964.471\n",
      "wrong_move\n",
      "   498/50000: episode: 497, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 670.000 [670.000, 670.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.131542 12011.326\n",
      "wrong_move\n",
      "   499/50000: episode: 498, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.18926 12002.465\n",
      "wrong_move\n",
      "   500/50000: episode: 499, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.046204 11962.867\n",
      "wrong_move\n",
      "   501/50000: episode: 500, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2127.000 [2127.000, 2127.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "   502/50000: episode: 501, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.188698 12037.607\n",
      "wrong_move\n",
      "   503/50000: episode: 502, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.04825 12058.567\n",
      "wrong_move\n",
      "   504/50000: episode: 503, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.66226 12476.471\n",
      "wrong_move\n",
      "   505/50000: episode: 504, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1459.000 [1459.000, 1459.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.571934 11994.561\n",
      "wrong_move\n",
      "   506/50000: episode: 505, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.8888 12008.213\n",
      "wrong_move\n",
      "   507/50000: episode: 506, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.086067 11755.952\n",
      "wrong_move\n",
      "   508/50000: episode: 507, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.683823 11887.299\n",
      "wrong_move\n",
      "   509/50000: episode: 508, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.224087 11996.038\n",
      "wrong_move\n",
      "   510/50000: episode: 509, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.032227 12009.646\n",
      "wrong_move\n",
      "   511/50000: episode: 510, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.17271 12041.815\n",
      "wrong_move\n",
      "   512/50000: episode: 511, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.237476 12086.911\n",
      "wrong_move\n",
      "   513/50000: episode: 512, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.101627 12441.044\n",
      "wrong_move\n",
      "   514/50000: episode: 513, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.62982 11964.993\n",
      "wrong_move\n",
      "   515/50000: episode: 514, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -48.194138 12248.311\n",
      "wrong_move\n",
      "   516/50000: episode: 515, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.42047 11977.532\n",
      "wrong_move\n",
      "   517/50000: episode: 516, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.978844 12044.827\n",
      "wrong_move\n",
      "   518/50000: episode: 517, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.068954 12020.098\n",
      "wrong_move\n",
      "   519/50000: episode: 518, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.939922 11986.842\n",
      "wrong_move\n",
      "   520/50000: episode: 519, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.63386 12021.946\n",
      "wrong_move\n",
      "   521/50000: episode: 520, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1164.000 [1164.000, 1164.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.239155 12011.567\n",
      "wrong_move\n",
      "   522/50000: episode: 521, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -48.45542 12215.805\n",
      "wrong_move\n",
      "   523/50000: episode: 522, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3805.000 [3805.000, 3805.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.501827 12382.191\n",
      "wrong_move\n",
      "   524/50000: episode: 523, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3756.000 [3756.000, 3756.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.616 11895.446\n",
      "wrong_move\n",
      "   525/50000: episode: 524, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.99724 12017.009\n",
      "wrong_move\n",
      "   526/50000: episode: 525, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3667.000 [3667.000, 3667.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.01087 12022.517\n",
      "wrong_move\n",
      "   527/50000: episode: 526, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.478954 12424.593\n",
      "wrong_move\n",
      "   528/50000: episode: 527, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3328.000 [3328.000, 3328.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.696064 12067.729\n",
      "wrong_move\n",
      "   529/50000: episode: 528, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.389935 12023.015\n",
      "wrong_move\n",
      "   530/50000: episode: 529, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2858.000 [2858.000, 2858.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.975506 12322.043\n",
      "wrong_move\n",
      "   531/50000: episode: 530, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.41028 11991.792\n",
      "wrong_move\n",
      "   532/50000: episode: 531, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 142.000 [142.000, 142.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.26676 12205.317\n",
      "wrong_move\n",
      "   533/50000: episode: 532, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.303436 11990.208\n",
      "wrong_move\n",
      "   534/50000: episode: 533, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.064083 12055.674\n",
      "wrong_move\n",
      "   535/50000: episode: 534, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.476948 12315.147\n",
      "wrong_move\n",
      "   536/50000: episode: 535, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2225.000 [2225.000, 2225.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.715534 11957.287\n",
      "wrong_move\n",
      "   537/50000: episode: 536, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.200375 12011.898\n",
      "wrong_move\n",
      "   538/50000: episode: 537, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.37679 11988.9795\n",
      "wrong_move\n",
      "   539/50000: episode: 538, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.971375 11708.512\n",
      "wrong_move\n",
      "   540/50000: episode: 539, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1403.000 [1403.000, 1403.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.95908 12037.189\n",
      "wrong_move\n",
      "   541/50000: episode: 540, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.899197 12229.327\n",
      "wrong_move\n",
      "   542/50000: episode: 541, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.770615 12057.221\n",
      "wrong_move\n",
      "   543/50000: episode: 542, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2223.000 [2223.000, 2223.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.79411 12049.812\n",
      "wrong_move\n",
      "   544/50000: episode: 543, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.715267 12071.42\n",
      "wrong_move\n",
      "   545/50000: episode: 544, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.14874 12027.808\n",
      "wrong_move\n",
      "   546/50000: episode: 545, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -48.17912 12249.094\n",
      "wrong_move\n",
      "   547/50000: episode: 546, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.72408 12041.767\n",
      "wrong_move\n",
      "   548/50000: episode: 547, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.185852 11999.025\n",
      "wrong_move\n",
      "   549/50000: episode: 548, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.36548 12174.793\n",
      "wrong_move\n",
      "   550/50000: episode: 549, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1098.000 [1098.000, 1098.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.22194 12037.199\n",
      "wrong_move\n",
      "   551/50000: episode: 550, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.079086 12013.293\n",
      "wrong_move\n",
      "   552/50000: episode: 551, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.257633 12372.914\n",
      "wrong_move\n",
      "   553/50000: episode: 552, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.628918 11971.1455\n",
      "wrong_move\n",
      "   554/50000: episode: 553, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2389.000 [2389.000, 2389.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.999683 11951.963\n",
      "wrong_move\n",
      "   555/50000: episode: 554, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.5834 12222.1045\n",
      "wrong_move\n",
      "   556/50000: episode: 555, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3131.000 [3131.000, 3131.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.936356 12051.603\n",
      "wrong_move\n",
      "   557/50000: episode: 556, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.03312 11992.143\n",
      "wrong_move\n",
      "   558/50000: episode: 557, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2493.000 [2493.000, 2493.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.072304 12024.812\n",
      "wrong_move\n",
      "   559/50000: episode: 558, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.74563 12011.157\n",
      "wrong_move\n",
      "   560/50000: episode: 559, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.143944 11986.722\n",
      "wrong_move\n",
      "   561/50000: episode: 560, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 448.000 [448.000, 448.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.855057 12100.134\n",
      "wrong_move\n",
      "   562/50000: episode: 561, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.412357 11998.663\n",
      "wrong_move\n",
      "   563/50000: episode: 562, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.59672 12031.957\n",
      "wrong_move\n",
      "   564/50000: episode: 563, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 769.000 [769.000, 769.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.550697 11999.15\n",
      "wrong_move\n",
      "   565/50000: episode: 564, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2766.000 [2766.000, 2766.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.466766 11995.247\n",
      "wrong_move\n",
      "   566/50000: episode: 565, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 431.000 [431.000, 431.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.092216 12460.896\n",
      "wrong_move\n",
      "   567/50000: episode: 566, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -38.8014 12213.874\n",
      "wrong_move\n",
      "   568/50000: episode: 567, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1098.000 [1098.000, 1098.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.973305 12031.276\n",
      "wrong_move\n",
      "   569/50000: episode: 568, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.48564 12045.95\n",
      "wrong_move\n",
      "   570/50000: episode: 569, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.994934 12019.875\n",
      "wrong_move\n",
      "   571/50000: episode: 570, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1516.000 [1516.000, 1516.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.740665 11959.63\n",
      "wrong_move\n",
      "   572/50000: episode: 571, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.331753 12021.928\n",
      "wrong_move\n",
      "   573/50000: episode: 572, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.168602 12061.737\n",
      "wrong_move\n",
      "   574/50000: episode: 573, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.93478 12005.03\n",
      "wrong_move\n",
      "   575/50000: episode: 574, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.098537 12041.759\n",
      "wrong_move\n",
      "   576/50000: episode: 575, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.145596 12008.574\n",
      "wrong_move\n",
      "   577/50000: episode: 576, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 239.000 [239.000, 239.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.57322 11976.586\n",
      "wrong_move\n",
      "   578/50000: episode: 577, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.439358 11985.141\n",
      "wrong_move\n",
      "   579/50000: episode: 578, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 671.000 [671.000, 671.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.59509 11978.543\n",
      "wrong_move\n",
      "   580/50000: episode: 579, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.414948 11986.212\n",
      "wrong_move\n",
      "   581/50000: episode: 580, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.36241 12009.398\n",
      "wrong_move\n",
      "   582/50000: episode: 581, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3113.000 [3113.000, 3113.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -48.171307 12279.366\n",
      "wrong_move\n",
      "   583/50000: episode: 582, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3320.000 [3320.000, 3320.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.47523 11964.31\n",
      "wrong_move\n",
      "   584/50000: episode: 583, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.596157 11973.772\n",
      "wrong_move\n",
      "   585/50000: episode: 584, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.42838 11995.036\n",
      "wrong_move\n",
      "   586/50000: episode: 585, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3398.000 [3398.000, 3398.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.6807 12089.884\n",
      "wrong_move\n",
      "   587/50000: episode: 586, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1998.000 [1998.000, 1998.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.29141 11929.738\n",
      "wrong_move\n",
      "   588/50000: episode: 587, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3377.000 [3377.000, 3377.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.17985 12000.239\n",
      "wrong_move\n",
      "   589/50000: episode: 588, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3401.000 [3401.000, 3401.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.228886 11990.797\n",
      "wrong_move\n",
      "   590/50000: episode: 589, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4076.000 [4076.000, 4076.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.789223 12340.603\n",
      "wrong_move\n",
      "   591/50000: episode: 590, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.050785 11993.481\n",
      "wrong_move\n",
      "   592/50000: episode: 591, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.747128 11807.7705\n",
      "wrong_move\n",
      "   593/50000: episode: 592, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 817.000 [817.000, 817.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.0288 12031.379\n",
      "wrong_move\n",
      "   594/50000: episode: 593, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 311.000 [311.000, 311.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.85166 12034.806\n",
      "wrong_move\n",
      "   595/50000: episode: 594, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.17863 12151.026\n",
      "wrong_move\n",
      "   596/50000: episode: 595, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1374.000 [1374.000, 1374.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.55588 12065.019\n",
      "wrong_move\n",
      "   597/50000: episode: 596, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.83792 12375.577\n",
      "wrong_move\n",
      "   598/50000: episode: 597, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1316.000 [1316.000, 1316.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.860264 11997.127\n",
      "wrong_move\n",
      "   599/50000: episode: 598, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3135.000 [3135.000, 3135.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.388405 12132.267\n",
      "wrong_move\n",
      "   600/50000: episode: 599, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.076214 12006.539\n",
      "wrong_move\n",
      "   601/50000: episode: 600, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.40959 12434.509\n",
      "wrong_move\n",
      "   602/50000: episode: 601, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2619.000 [2619.000, 2619.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.356827 11902.0625\n",
      "wrong_move\n",
      "   603/50000: episode: 602, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4088.000 [4088.000, 4088.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -49.07473 12303.929\n",
      "wrong_move\n",
      "   604/50000: episode: 603, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2054.000 [2054.000, 2054.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.21957 12220.963\n",
      "wrong_move\n",
      "   605/50000: episode: 604, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1817.000 [1817.000, 1817.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.98382 12044.266\n",
      "wrong_move\n",
      "   606/50000: episode: 605, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.82872 12446.411\n",
      "wrong_move\n",
      "   607/50000: episode: 606, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4038.000 [4038.000, 4038.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.441387 11650.406\n",
      "wrong_move\n",
      "   608/50000: episode: 607, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 374.000 [374.000, 374.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.727386 12015.555\n",
      "wrong_move\n",
      "   609/50000: episode: 608, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.086964 12091.593\n",
      "wrong_move\n",
      "   610/50000: episode: 609, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.1033 12054.169\n",
      "wrong_move\n",
      "   611/50000: episode: 610, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.586956 11971.053\n",
      "wrong_move\n",
      "   612/50000: episode: 611, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.387527 11903.837\n",
      "wrong_move\n",
      "   613/50000: episode: 612, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.09678 12059.482\n",
      "wrong_move\n",
      "   614/50000: episode: 613, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1430.000 [1430.000, 1430.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.901936 12100.362\n",
      "wrong_move\n",
      "   615/50000: episode: 614, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.517605 12046.669\n",
      "wrong_move\n",
      "   616/50000: episode: 615, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.84821 12341.539\n",
      "wrong_move\n",
      "   617/50000: episode: 616, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.258766 12005.927\n",
      "wrong_move\n",
      "   618/50000: episode: 617, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.428787 11987.017\n",
      "wrong_move\n",
      "   619/50000: episode: 618, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.967636 12406.644\n",
      "wrong_move\n",
      "   620/50000: episode: 619, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3227.000 [3227.000, 3227.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.201633 12022.214\n",
      "wrong_move\n",
      "   621/50000: episode: 620, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.502342 11990.996\n",
      "wrong_move\n",
      "   622/50000: episode: 621, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2892.000 [2892.000, 2892.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.090645 12158.972\n",
      "wrong_move\n",
      "   623/50000: episode: 622, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.42288 12009.516\n",
      "wrong_move\n",
      "   624/50000: episode: 623, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.67786 12410.074\n",
      "wrong_move\n",
      "   625/50000: episode: 624, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.70866 12334.714\n",
      "wrong_move\n",
      "   626/50000: episode: 625, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.578884 12366.949\n",
      "wrong_move\n",
      "   627/50000: episode: 626, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.297134 12316.847\n",
      "wrong_move\n",
      "   628/50000: episode: 627, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -47.63417 12230.62\n",
      "wrong_move\n",
      "   629/50000: episode: 628, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.159668 12022.339\n",
      "wrong_move\n",
      "   630/50000: episode: 629, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.322464 12010.751\n",
      "wrong_move\n",
      "   631/50000: episode: 630, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.374645 12189.873\n",
      "wrong_move\n",
      "   632/50000: episode: 631, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.312195 12055.536\n",
      "wrong_move\n",
      "   633/50000: episode: 632, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.68094 12009.655\n",
      "wrong_move\n",
      "   634/50000: episode: 633, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 864.000 [864.000, 864.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.475643 11887.315\n",
      "wrong_move\n",
      "   635/50000: episode: 634, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 142.000 [142.000, 142.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.031334 12028.811\n",
      "wrong_move\n",
      "   636/50000: episode: 635, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 698.000 [698.000, 698.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.5846 11973.249\n",
      "wrong_move\n",
      "   637/50000: episode: 636, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.80684 12306.278\n",
      "wrong_move\n",
      "   638/50000: episode: 637, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.097652 12326.642\n",
      "wrong_move\n",
      "   639/50000: episode: 638, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.567 12453.91\n",
      "wrong_move\n",
      "   640/50000: episode: 639, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.53818 11976.29\n",
      "wrong_move\n",
      "   641/50000: episode: 640, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.89637 11995.834\n",
      "wrong_move\n",
      "   642/50000: episode: 641, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2850.000 [2850.000, 2850.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.44041 12055.954\n",
      "wrong_move\n",
      "   643/50000: episode: 642, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.100273 12024.56\n",
      "wrong_move\n",
      "   644/50000: episode: 643, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.06775 12152.364\n",
      "wrong_move\n",
      "   645/50000: episode: 644, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3466.000 [3466.000, 3466.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.28907 11929.258\n",
      "wrong_move\n",
      "   646/50000: episode: 645, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3222.000 [3222.000, 3222.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -47.889465 12187.4795\n",
      "wrong_move\n",
      "   647/50000: episode: 646, duration: 0.022s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.040287 12039.042\n",
      "wrong_move\n",
      "   648/50000: episode: 647, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -46.245117 12121.703\n",
      "wrong_move\n",
      "   649/50000: episode: 648, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.55607 11971.162\n",
      "wrong_move\n",
      "   650/50000: episode: 649, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.032352 12146.609\n",
      "wrong_move\n",
      "   651/50000: episode: 650, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 401.000 [401.000, 401.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.703327 12160.959\n",
      "wrong_move\n",
      "   652/50000: episode: 651, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.94964 12422.998\n",
      "wrong_move\n",
      "   653/50000: episode: 652, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.22634 12045.319\n",
      "wrong_move\n",
      "   654/50000: episode: 653, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.82574 12034.409\n",
      "wrong_move\n",
      "   655/50000: episode: 654, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3686.000 [3686.000, 3686.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.264343 11938.706\n",
      "wrong_move\n",
      "   656/50000: episode: 655, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.418705 12053.013\n",
      "wrong_move\n",
      "   657/50000: episode: 656, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.071823 12025.509\n",
      "wrong_move\n",
      "   658/50000: episode: 657, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.393024 12173.476\n",
      "wrong_move\n",
      "   659/50000: episode: 658, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.0111 11993.386\n",
      "wrong_move\n",
      "   660/50000: episode: 659, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.457653 11994.75\n",
      "wrong_move\n",
      "   661/50000: episode: 660, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3962.000 [3962.000, 3962.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.36426 12067.619\n",
      "wrong_move\n",
      "   662/50000: episode: 661, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.566902 12072.909\n",
      "wrong_move\n",
      "   663/50000: episode: 662, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.198963 12013.34\n",
      "wrong_move\n",
      "   664/50000: episode: 663, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.074596 12013.771\n",
      "wrong_move\n",
      "   665/50000: episode: 664, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.766552 12049.879\n",
      "wrong_move\n",
      "   666/50000: episode: 665, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.195858 12563.998\n",
      "wrong_move\n",
      "   667/50000: episode: 666, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.285706 12002.587\n",
      "wrong_move\n",
      "   668/50000: episode: 667, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2790.000 [2790.000, 2790.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.48094 12031.472\n",
      "wrong_move\n",
      "   669/50000: episode: 668, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.064148 12009.542\n",
      "wrong_move\n",
      "   670/50000: episode: 669, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.983215 12023.946\n",
      "wrong_move\n",
      "   671/50000: episode: 670, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2618.000 [2618.000, 2618.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.498863 12159.416\n",
      "wrong_move\n",
      "   672/50000: episode: 671, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.458153 12005.538\n",
      "wrong_move\n",
      "   673/50000: episode: 672, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1518.000 [1518.000, 1518.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.178207 11951.89\n",
      "wrong_move\n",
      "   674/50000: episode: 673, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "   675/50000: episode: 674, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -46.325535 12220.993\n",
      "wrong_move\n",
      "   676/50000: episode: 675, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "   677/50000: episode: 676, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 78.000 [78.000, 78.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.456776 11973.48\n",
      "wrong_move\n",
      "   678/50000: episode: 677, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2368.000 [2368.000, 2368.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.41698 12050.208\n",
      "wrong_move\n",
      "   679/50000: episode: 678, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -38.742134 12220.157\n",
      "wrong_move\n",
      "   680/50000: episode: 679, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1105.000 [1105.000, 1105.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -46.291046 12751.448\n",
      "wrong_move\n",
      "   681/50000: episode: 680, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.643578 12400.868\n",
      "wrong_move\n",
      "   682/50000: episode: 681, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.438953 12005.634\n",
      "wrong_move\n",
      "   683/50000: episode: 682, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.357246 12031.889\n",
      "wrong_move\n",
      "   684/50000: episode: 683, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.387005 12038.479\n",
      "wrong_move\n",
      "   685/50000: episode: 684, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.479187 12045.693\n",
      "wrong_move\n",
      "   686/50000: episode: 685, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.140324 11884.521\n",
      "wrong_move\n",
      "   687/50000: episode: 686, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.272495 12062.893\n",
      "wrong_move\n",
      "   688/50000: episode: 687, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.469 11978.973\n",
      "wrong_move\n",
      "   689/50000: episode: 688, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.78559 12398.704\n",
      "wrong_move\n",
      "   690/50000: episode: 689, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.85184 12059.763\n",
      "wrong_move\n",
      "   691/50000: episode: 690, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2076.000 [2076.000, 2076.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.294495 12059.186\n",
      "wrong_move\n",
      "   692/50000: episode: 691, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 732.000 [732.000, 732.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.05606 12070.539\n",
      "wrong_move\n",
      "   693/50000: episode: 692, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.587948 12367.744\n",
      "wrong_move\n",
      "   694/50000: episode: 693, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.555984 11932.324\n",
      "wrong_move\n",
      "   695/50000: episode: 694, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.045403 12356.599\n",
      "wrong_move\n",
      "   696/50000: episode: 695, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.834644 12052.941\n",
      "wrong_move\n",
      "   697/50000: episode: 696, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 143.000 [143.000, 143.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "   698/50000: episode: 697, duration: 0.006s, episode steps:   1, steps per second: 158, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.04479 12038.831\n",
      "wrong_move\n",
      "   699/50000: episode: 698, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.12465 12040.712\n",
      "wrong_move\n",
      "   700/50000: episode: 699, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.518574 12054.037\n",
      "wrong_move\n",
      "   701/50000: episode: 700, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.635155 11964.932\n",
      "wrong_move\n",
      "   702/50000: episode: 701, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.814125 12056.685\n",
      "wrong_move\n",
      "   703/50000: episode: 702, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.312756 11999.012\n",
      "wrong_move\n",
      "   704/50000: episode: 703, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.035328 12065.962\n",
      "wrong_move\n",
      "   705/50000: episode: 704, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.20349 11979.087\n",
      "wrong_move\n",
      "   706/50000: episode: 705, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.223625 12035.338\n",
      "wrong_move\n",
      "   707/50000: episode: 706, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.82383 12044.161\n",
      "wrong_move\n",
      "   708/50000: episode: 707, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "   709/50000: episode: 708, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3351.000 [3351.000, 3351.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.65225 12529.236\n",
      "wrong_move\n",
      "   710/50000: episode: 709, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3844.000 [3844.000, 3844.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.20574 11850.228\n",
      "wrong_move\n",
      "   711/50000: episode: 710, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1051.000 [1051.000, 1051.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.810066 12161.47\n",
      "wrong_move\n",
      "   712/50000: episode: 711, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.918255 12060.856\n",
      "wrong_move\n",
      "   713/50000: episode: 712, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.744 12348.052\n",
      "wrong_move\n",
      "   714/50000: episode: 713, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1313.000 [1313.000, 1313.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.268394 12024.508\n",
      "wrong_move\n",
      "   715/50000: episode: 714, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -46.047806 12197.561\n",
      "wrong_move\n",
      "   716/50000: episode: 715, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.15388 12027.401\n",
      "wrong_move\n",
      "   717/50000: episode: 716, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.64673 12294.9\n",
      "wrong_move\n",
      "   718/50000: episode: 717, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.669815 12024.992\n",
      "wrong_move\n",
      "   719/50000: episode: 718, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.99186 11836.004\n",
      "wrong_move\n",
      "   720/50000: episode: 719, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.030827 12052.772\n",
      "wrong_move\n",
      "   721/50000: episode: 720, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2618.000 [2618.000, 2618.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.833702 12193.779\n",
      "wrong_move\n",
      "   722/50000: episode: 721, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.3767 12071.808\n",
      "wrong_move\n",
      "   723/50000: episode: 722, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.480377 11928.428\n",
      "wrong_move\n",
      "   724/50000: episode: 723, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2851.000 [2851.000, 2851.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.63703 12064.596\n",
      "wrong_move\n",
      "   725/50000: episode: 724, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.137844 11999.877\n",
      "wrong_move\n",
      "   726/50000: episode: 725, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1555.000 [1555.000, 1555.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.397717 11989.601\n",
      "wrong_move\n",
      "   727/50000: episode: 726, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.48685 11976.212\n",
      "wrong_move\n",
      "   728/50000: episode: 727, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.518818 11978.122\n",
      "wrong_move\n",
      "   729/50000: episode: 728, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.190525 11984.046\n",
      "wrong_move\n",
      "   730/50000: episode: 729, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.63815 12026.704\n",
      "wrong_move\n",
      "   731/50000: episode: 730, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.219303 12284.757\n",
      "wrong_move\n",
      "   732/50000: episode: 731, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.672157 11986.971\n",
      "wrong_move\n",
      "   733/50000: episode: 732, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.750305 12067.807\n",
      "wrong_move\n",
      "   734/50000: episode: 733, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.440636 11995.798\n",
      "wrong_move\n",
      "   735/50000: episode: 734, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.06432 12305.546\n",
      "wrong_move\n",
      "   736/50000: episode: 735, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.391045 12062.924\n",
      "wrong_move\n",
      "   737/50000: episode: 736, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.966526 12054.104\n",
      "wrong_move\n",
      "   738/50000: episode: 737, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.369793 11999.745\n",
      "wrong_move\n",
      "   739/50000: episode: 738, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.10253 11882.249\n",
      "wrong_move\n",
      "   740/50000: episode: 739, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.7394 12062.301\n",
      "wrong_move\n",
      "   741/50000: episode: 740, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1149.000 [1149.000, 1149.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.285133 12014.862\n",
      "wrong_move\n",
      "   742/50000: episode: 741, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 218.000 [218.000, 218.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.577007 11973.85\n",
      "wrong_move\n",
      "   743/50000: episode: 742, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.413303 12054.41\n",
      "wrong_move\n",
      "   744/50000: episode: 743, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.330917 12269.552\n",
      "wrong_move\n",
      "   745/50000: episode: 744, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1955.000 [1955.000, 1955.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.299393 11984.947\n",
      "wrong_move\n",
      "   746/50000: episode: 745, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.32164 12014.052\n",
      "wrong_move\n",
      "   747/50000: episode: 746, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.3202 12046.288\n",
      "wrong_move\n",
      "   748/50000: episode: 747, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.526432 11989.882\n",
      "wrong_move\n",
      "   749/50000: episode: 748, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3985.000 [3985.000, 3985.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.894966 11993.36\n",
      "wrong_move\n",
      "   750/50000: episode: 749, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2438.000 [2438.000, 2438.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.925056 12095.16\n",
      "wrong_move\n",
      "   751/50000: episode: 750, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1720.000 [1720.000, 1720.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.912075 12157.6\n",
      "wrong_move\n",
      "   752/50000: episode: 751, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1674.000 [1674.000, 1674.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.400322 12047.204\n",
      "wrong_move\n",
      "   753/50000: episode: 752, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.276234 11978.87\n",
      "wrong_move\n",
      "   754/50000: episode: 753, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.283432 11883.012\n",
      "wrong_move\n",
      "   755/50000: episode: 754, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.381687 11944.8545\n",
      "wrong_move\n",
      "   756/50000: episode: 755, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.88621 12019.008\n",
      "wrong_move\n",
      "   757/50000: episode: 756, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.966724 12040.255\n",
      "wrong_move\n",
      "   758/50000: episode: 757, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.961216 12009.5205\n",
      "wrong_move\n",
      "   759/50000: episode: 758, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.53147 12124.276\n",
      "wrong_move\n",
      "   760/50000: episode: 759, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.159588 11924.936\n",
      "wrong_move\n",
      "   761/50000: episode: 760, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.934925 12209.65\n",
      "wrong_move\n",
      "   762/50000: episode: 761, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.576183 12019.126\n",
      "wrong_move\n",
      "   763/50000: episode: 762, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.49402 11976.231\n",
      "wrong_move\n",
      "   764/50000: episode: 763, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -47.50518 12393.171\n",
      "wrong_move\n",
      "   765/50000: episode: 764, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.03016 12049.831\n",
      "wrong_move\n",
      "   766/50000: episode: 765, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3853.000 [3853.000, 3853.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.061996 12012.906\n",
      "wrong_move\n",
      "   767/50000: episode: 766, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3608.000 [3608.000, 3608.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -38.875652 12211.081\n",
      "wrong_move\n",
      "   768/50000: episode: 767, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1098.000 [1098.000, 1098.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.944595 12095.276\n",
      "wrong_move\n",
      "   769/50000: episode: 768, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 296.000 [296.000, 296.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.959435 12051.709\n",
      "wrong_move\n",
      "   770/50000: episode: 769, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.415703 11961.82\n",
      "wrong_move\n",
      "   771/50000: episode: 770, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -38.83638 12158.421\n",
      "wrong_move\n",
      "   772/50000: episode: 771, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3466.000 [3466.000, 3466.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.27254 11987.724\n",
      "wrong_move\n",
      "   773/50000: episode: 772, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2990.000 [2990.000, 2990.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.639057 12414.902\n",
      "wrong_move\n",
      "   774/50000: episode: 773, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.353268 12001.077\n",
      "wrong_move\n",
      "   775/50000: episode: 774, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.988586 12012.111\n",
      "wrong_move\n",
      "   776/50000: episode: 775, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.35642 12060.12\n",
      "wrong_move\n",
      "   777/50000: episode: 776, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.25211 11935.106\n",
      "wrong_move\n",
      "   778/50000: episode: 777, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.38586 11988.765\n",
      "wrong_move\n",
      "   779/50000: episode: 778, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.7881 12027.961\n",
      "wrong_move\n",
      "   780/50000: episode: 779, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2135.000 [2135.000, 2135.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.825134 11979.174\n",
      "wrong_move\n",
      "   781/50000: episode: 780, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.128563 12019.908\n",
      "wrong_move\n",
      "   782/50000: episode: 781, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.095448 11948.041\n",
      "wrong_move\n",
      "   783/50000: episode: 782, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.912018 12446.621\n",
      "wrong_move\n",
      "   784/50000: episode: 783, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2730.000 [2730.000, 2730.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.00788 12202.97\n",
      "wrong_move\n",
      "   785/50000: episode: 784, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 595.000 [595.000, 595.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.05256 12024.751\n",
      "wrong_move\n",
      "   786/50000: episode: 785, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.10193 12007.918\n",
      "wrong_move\n",
      "   787/50000: episode: 786, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.09914 12004.278\n",
      "wrong_move\n",
      "   788/50000: episode: 787, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.92561 11959.976\n",
      "wrong_move\n",
      "   789/50000: episode: 788, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1846.000 [1846.000, 1846.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.482807 11974.099\n",
      "wrong_move\n",
      "   790/50000: episode: 789, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.55555 11988.144\n",
      "wrong_move\n",
      "   791/50000: episode: 790, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2052.000 [2052.000, 2052.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.843746 12049.547\n",
      "wrong_move\n",
      "   792/50000: episode: 791, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2183.000 [2183.000, 2183.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.85441 11993.808\n",
      "wrong_move\n",
      "   793/50000: episode: 792, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.615467 12049.635\n",
      "wrong_move\n",
      "   794/50000: episode: 793, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.738594 12072.024\n",
      "wrong_move\n",
      "   795/50000: episode: 794, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.933205 12050.966\n",
      "wrong_move\n",
      "   796/50000: episode: 795, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.676044 12012.896\n",
      "wrong_move\n",
      "   797/50000: episode: 796, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.423786 12180.852\n",
      "wrong_move\n",
      "   798/50000: episode: 797, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1098.000 [1098.000, 1098.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.414192 11983.557\n",
      "wrong_move\n",
      "   799/50000: episode: 798, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.28208 12280.11\n",
      "wrong_move\n",
      "   800/50000: episode: 799, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1986.000 [1986.000, 1986.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.00156 12041.969\n",
      "wrong_move\n",
      "   801/50000: episode: 800, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.358776 12201.759\n",
      "wrong_move\n",
      "   802/50000: episode: 801, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.46251 11979.3955\n",
      "wrong_move\n",
      "   803/50000: episode: 802, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1888.000 [1888.000, 1888.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -46.353474 12288.189\n",
      "wrong_move\n",
      "   804/50000: episode: 803, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.744293 11992.418\n",
      "wrong_move\n",
      "   805/50000: episode: 804, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.951073 12291.354\n",
      "wrong_move\n",
      "   806/50000: episode: 805, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 551.000 [551.000, 551.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.196217 12016.872\n",
      "wrong_move\n",
      "   807/50000: episode: 806, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "   808/50000: episode: 807, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.16457 12430.922\n",
      "wrong_move\n",
      "   809/50000: episode: 808, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.0884 12003.441\n",
      "wrong_move\n",
      "   810/50000: episode: 809, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.55824 12055.258\n",
      "wrong_move\n",
      "   811/50000: episode: 810, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.824364 11963.733\n",
      "wrong_move\n",
      "   812/50000: episode: 811, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.023487 12033.718\n",
      "wrong_move\n",
      "   813/50000: episode: 812, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.965195 12088.222\n",
      "wrong_move\n",
      "   814/50000: episode: 813, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.627853 11963.813\n",
      "wrong_move\n",
      "   815/50000: episode: 814, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.837467 12076.722\n",
      "wrong_move\n",
      "   816/50000: episode: 815, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2559.000 [2559.000, 2559.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.037357 12127.805\n",
      "wrong_move\n",
      "   817/50000: episode: 816, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 290.000 [290.000, 290.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.66713 11962.309\n",
      "wrong_move\n",
      "   818/50000: episode: 817, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.788113 12090.481\n",
      "wrong_move\n",
      "   819/50000: episode: 818, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.875893 12046.421\n",
      "wrong_move\n",
      "   820/50000: episode: 819, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.27048 12021.259\n",
      "wrong_move\n",
      "   821/50000: episode: 820, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.653015 11998.271\n",
      "wrong_move\n",
      "   822/50000: episode: 821, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1070.000 [1070.000, 1070.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.613724 11971.209\n",
      "wrong_move\n",
      "   823/50000: episode: 822, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 543.000 [543.000, 543.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.038757 12097.794\n",
      "wrong_move\n",
      "   824/50000: episode: 823, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.15892 12016.915\n",
      "wrong_move\n",
      "   825/50000: episode: 824, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.72616 12464.595\n",
      "wrong_move\n",
      "   826/50000: episode: 825, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -48.30201 12319.19\n",
      "wrong_move\n",
      "   827/50000: episode: 826, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.407543 11978.3545\n",
      "wrong_move\n",
      "   828/50000: episode: 827, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3754.000 [3754.000, 3754.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.372917 12133.992\n",
      "wrong_move\n",
      "   829/50000: episode: 828, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1179.000 [1179.000, 1179.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.53768 12139.178\n",
      "wrong_move\n",
      "   830/50000: episode: 829, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.457012 11994.394\n",
      "wrong_move\n",
      "   831/50000: episode: 830, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.77674 12396.649\n",
      "wrong_move\n",
      "   832/50000: episode: 831, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.191326 12011.812\n",
      "wrong_move\n",
      "   833/50000: episode: 832, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.007328 12032.267\n",
      "wrong_move\n",
      "   834/50000: episode: 833, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.436676 11977.58\n",
      "wrong_move\n",
      "   835/50000: episode: 834, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.090065 12023.383\n",
      "wrong_move\n",
      "   836/50000: episode: 835, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.868237 12044.816\n",
      "wrong_move\n",
      "   837/50000: episode: 836, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.987633 12254.761\n",
      "wrong_move\n",
      "   838/50000: episode: 837, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.62899 11979.749\n",
      "wrong_move\n",
      "   839/50000: episode: 838, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.385704 12005.101\n",
      "wrong_move\n",
      "   840/50000: episode: 839, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1177.000 [1177.000, 1177.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.77574 12002.686\n",
      "wrong_move\n",
      "   841/50000: episode: 840, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.74087 12185.747\n",
      "wrong_move\n",
      "   842/50000: episode: 841, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.159546 12020.233\n",
      "wrong_move\n",
      "   843/50000: episode: 842, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.777607 11913.82\n",
      "wrong_move\n",
      "   844/50000: episode: 843, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1670.000 [1670.000, 1670.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.184498 12095.296\n",
      "wrong_move\n",
      "   845/50000: episode: 844, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3893.000 [3893.000, 3893.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.60143 11970.766\n",
      "wrong_move\n",
      "   846/50000: episode: 845, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.801292 12004.213\n",
      "wrong_move\n",
      "   847/50000: episode: 846, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.33296 11999.8955\n",
      "wrong_move\n",
      "   848/50000: episode: 847, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2290.000 [2290.000, 2290.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.363876 11981.199\n",
      "wrong_move\n",
      "   849/50000: episode: 848, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.708847 12131.691\n",
      "wrong_move\n",
      "   850/50000: episode: 849, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.290462 12066.488\n",
      "wrong_move\n",
      "   851/50000: episode: 850, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.8883 12062.012\n",
      "wrong_move\n",
      "   852/50000: episode: 851, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -47.15733 12149.279\n",
      "wrong_move\n",
      "   853/50000: episode: 852, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1554.000 [1554.000, 1554.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.273968 11991.208\n",
      "wrong_move\n",
      "   854/50000: episode: 853, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -46.36892 12109.566\n",
      "wrong_move\n",
      "   855/50000: episode: 854, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.415073 11976.649\n",
      "wrong_move\n",
      "   856/50000: episode: 855, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.808254 12107.039\n",
      "wrong_move\n",
      "   857/50000: episode: 856, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 151.000 [151.000, 151.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.954605 12180.466\n",
      "wrong_move\n",
      "   858/50000: episode: 857, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1324.000 [1324.000, 1324.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "   859/50000: episode: 858, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.73991 12074.092\n",
      "wrong_move\n",
      "   860/50000: episode: 859, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2527.000 [2527.000, 2527.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.069744 12016.819\n",
      "wrong_move\n",
      "   861/50000: episode: 860, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.46764 12061.468\n",
      "wrong_move\n",
      "   862/50000: episode: 861, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1524.000 [1524.000, 1524.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.343285 12426.051\n",
      "wrong_move\n",
      "   863/50000: episode: 862, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.567863 12002.854\n",
      "wrong_move\n",
      "   864/50000: episode: 863, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2555.000 [2555.000, 2555.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.838825 12053.841\n",
      "wrong_move\n",
      "   865/50000: episode: 864, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.459705 11981.206\n",
      "wrong_move\n",
      "   866/50000: episode: 865, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.693604 11763.947\n",
      "wrong_move\n",
      "   867/50000: episode: 866, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.009308 11890.82\n",
      "wrong_move\n",
      "   868/50000: episode: 867, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.501953 12069.078\n",
      "wrong_move\n",
      "   869/50000: episode: 868, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.455803 12034.575\n",
      "wrong_move\n",
      "   870/50000: episode: 869, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1897.000 [1897.000, 1897.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.187744 12021.382\n",
      "wrong_move\n",
      "   871/50000: episode: 870, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1773.000 [1773.000, 1773.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.312515 12034.249\n",
      "wrong_move\n",
      "   872/50000: episode: 871, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3908.000 [3908.000, 3908.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.808258 12140.953\n",
      "wrong_move\n",
      "   873/50000: episode: 872, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3216.000 [3216.000, 3216.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.51392 11967.774\n",
      "wrong_move\n",
      "   874/50000: episode: 873, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 108.000 [108.000, 108.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.493057 11991.392\n",
      "wrong_move\n",
      "   875/50000: episode: 874, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.113537 12042.569\n",
      "wrong_move\n",
      "   876/50000: episode: 875, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.395508 11975.908\n",
      "wrong_move\n",
      "   877/50000: episode: 876, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3381.000 [3381.000, 3381.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.71705 12445.884\n",
      "wrong_move\n",
      "   878/50000: episode: 877, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.18014 11990.425\n",
      "wrong_move\n",
      "   879/50000: episode: 878, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3907.000 [3907.000, 3907.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.450016 12314.007\n",
      "wrong_move\n",
      "   880/50000: episode: 879, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.33669 11995.345\n",
      "wrong_move\n",
      "   881/50000: episode: 880, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.72855 12419.179\n",
      "wrong_move\n",
      "   882/50000: episode: 881, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1720.000 [1720.000, 1720.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.926826 12027.97\n",
      "wrong_move\n",
      "   883/50000: episode: 882, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.297977 11997.858\n",
      "wrong_move\n",
      "   884/50000: episode: 883, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.257324 12007.187\n",
      "wrong_move\n",
      "   885/50000: episode: 884, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.68315 12375.5625\n",
      "wrong_move\n",
      "   886/50000: episode: 885, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.493298 12024.069\n",
      "wrong_move\n",
      "   887/50000: episode: 886, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.20312 12004.959\n",
      "wrong_move\n",
      "   888/50000: episode: 887, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.479267 12030.275\n",
      "wrong_move\n",
      "   889/50000: episode: 888, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4076.000 [4076.000, 4076.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.848274 11898.158\n",
      "wrong_move\n",
      "   890/50000: episode: 889, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 125.000 [125.000, 125.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.435448 11906.296\n",
      "wrong_move\n",
      "   891/50000: episode: 890, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.26411 12002.753\n",
      "wrong_move\n",
      "   892/50000: episode: 891, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3831.000 [3831.000, 3831.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.156143 12020.24\n",
      "wrong_move\n",
      "   893/50000: episode: 892, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.573505 11820.51\n",
      "wrong_move\n",
      "   894/50000: episode: 893, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.410744 12022.508\n",
      "wrong_move\n",
      "   895/50000: episode: 894, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.103092 12051.533\n",
      "wrong_move\n",
      "   896/50000: episode: 895, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "   897/50000: episode: 896, duration: 0.006s, episode steps:   1, steps per second: 164, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.99485 12068.427\n",
      "wrong_move\n",
      "   898/50000: episode: 897, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.265106 11996.604\n",
      "wrong_move\n",
      "   899/50000: episode: 898, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1208.000 [1208.000, 1208.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.600105 11964.223\n",
      "wrong_move\n",
      "   900/50000: episode: 899, duration: 0.007s, episode steps:   1, steps per second: 154, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.337265 12066.853\n",
      "wrong_move\n",
      "   901/50000: episode: 900, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3336.000 [3336.000, 3336.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.851364 12026.708\n",
      "wrong_move\n",
      "   902/50000: episode: 901, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 389.000 [389.000, 389.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.844746 12091.8\n",
      "wrong_move\n",
      "   903/50000: episode: 902, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.192673 12007.845\n",
      "wrong_move\n",
      "   904/50000: episode: 903, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.55567 12281.967\n",
      "wrong_move\n",
      "   905/50000: episode: 904, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.416733 12001.466\n",
      "wrong_move\n",
      "   906/50000: episode: 905, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1258.000 [1258.000, 1258.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.033928 12073.594\n",
      "wrong_move\n",
      "   907/50000: episode: 906, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.15551 12128.565\n",
      "wrong_move\n",
      "   908/50000: episode: 907, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4076.000 [4076.000, 4076.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.56428 11966.814\n",
      "wrong_move\n",
      "   909/50000: episode: 908, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.23237 11991.663\n",
      "wrong_move\n",
      "   910/50000: episode: 909, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.56783 11994.709\n",
      "wrong_move\n",
      "   911/50000: episode: 910, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.93713 12026.906\n",
      "wrong_move\n",
      "   912/50000: episode: 911, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.613823 12334.568\n",
      "wrong_move\n",
      "   913/50000: episode: 912, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2470.000 [2470.000, 2470.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.653645 12122.54\n",
      "wrong_move\n",
      "   914/50000: episode: 913, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2676.000 [2676.000, 2676.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.92411 12111.326\n",
      "wrong_move\n",
      "   915/50000: episode: 914, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.200512 11986.681\n",
      "wrong_move\n",
      "   916/50000: episode: 915, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.477837 11983.341\n",
      "wrong_move\n",
      "   917/50000: episode: 916, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.00161 12319.899\n",
      "wrong_move\n",
      "   918/50000: episode: 917, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.313316 12044.671\n",
      "wrong_move\n",
      "   919/50000: episode: 918, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.81632 12325.992\n",
      "wrong_move\n",
      "   920/50000: episode: 919, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.891964 12337.402\n",
      "wrong_move\n",
      "   921/50000: episode: 920, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.678356 11968.418\n",
      "wrong_move\n",
      "   922/50000: episode: 921, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.0225 11905.161\n",
      "wrong_move\n",
      "   923/50000: episode: 922, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1187.000 [1187.000, 1187.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.48931 11923.352\n",
      "wrong_move\n",
      "   924/50000: episode: 923, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.142765 11924.868\n",
      "wrong_move\n",
      "   925/50000: episode: 924, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.046875 12169.88\n",
      "wrong_move\n",
      "   926/50000: episode: 925, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3118.000 [3118.000, 3118.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.36769 12017.725\n",
      "wrong_move\n",
      "   927/50000: episode: 926, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.83147 12055.22\n",
      "wrong_move\n",
      "   928/50000: episode: 927, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.23281 12064.993\n",
      "wrong_move\n",
      "   929/50000: episode: 928, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.731846 12084.647\n",
      "wrong_move\n",
      "   930/50000: episode: 929, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.091152 11994.847\n",
      "wrong_move\n",
      "   931/50000: episode: 930, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 260.000 [260.000, 260.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.81432 12019.246\n",
      "wrong_move\n",
      "   932/50000: episode: 931, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.517937 12301.298\n",
      "wrong_move\n",
      "   933/50000: episode: 932, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.247013 11971.049\n",
      "wrong_move\n",
      "   934/50000: episode: 933, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2861.000 [2861.000, 2861.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.66549 11999.328\n",
      "wrong_move\n",
      "   935/50000: episode: 934, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.843174 12038.7\n",
      "wrong_move\n",
      "   936/50000: episode: 935, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1525.000 [1525.000, 1525.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.853165 12420.556\n",
      "wrong_move\n",
      "   937/50000: episode: 936, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2447.000 [2447.000, 2447.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.35762 12049.967\n",
      "wrong_move\n",
      "   938/50000: episode: 937, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.14032 12027.223\n",
      "wrong_move\n",
      "   939/50000: episode: 938, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4076.000 [4076.000, 4076.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.413883 12191.439\n",
      "wrong_move\n",
      "   940/50000: episode: 939, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2538.000 [2538.000, 2538.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.020294 12166.26\n",
      "wrong_move\n",
      "   941/50000: episode: 940, duration: 0.027s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.595905 12208.671\n",
      "wrong_move\n",
      "   942/50000: episode: 941, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.92511 12063.734\n",
      "wrong_move\n",
      "   943/50000: episode: 942, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -43.52128 12380.442\n",
      "wrong_move\n",
      "   944/50000: episode: 943, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1320.000 [1320.000, 1320.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.514076 11980.024\n",
      "wrong_move\n",
      "   945/50000: episode: 944, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.81609 12081.004\n",
      "wrong_move\n",
      "   946/50000: episode: 945, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.59881 12316.635\n",
      "wrong_move\n",
      "   947/50000: episode: 946, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3063.000 [3063.000, 3063.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.221462 12026.391\n",
      "wrong_move\n",
      "   948/50000: episode: 947, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.879932 11988.523\n",
      "wrong_move\n",
      "   949/50000: episode: 948, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.519352 12014.256\n",
      "wrong_move\n",
      "   950/50000: episode: 949, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.185734 12027.089\n",
      "wrong_move\n",
      "   951/50000: episode: 950, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -46.247265 12319.545\n",
      "wrong_move\n",
      "   952/50000: episode: 951, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1347.000 [1347.000, 1347.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.250267 12251.484\n",
      "wrong_move\n",
      "   953/50000: episode: 952, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.42965 12099.0205\n",
      "wrong_move\n",
      "   954/50000: episode: 953, duration: 0.039s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -46.17288 12064.733\n",
      "wrong_move\n",
      "   955/50000: episode: 954, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.92298 12047.049\n",
      "wrong_move\n",
      "   956/50000: episode: 955, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.306038 11994.999\n",
      "wrong_move\n",
      "   957/50000: episode: 956, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 255.000 [255.000, 255.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.34574 12513.863\n",
      "wrong_move\n",
      "   958/50000: episode: 957, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.079918 12504.363\n",
      "wrong_move\n",
      "   959/50000: episode: 958, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.09334 12219.556\n",
      "wrong_move\n",
      "   960/50000: episode: 959, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1098.000 [1098.000, 1098.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.363228 12332.934\n",
      "wrong_move\n",
      "   961/50000: episode: 960, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.878258 11998.53\n",
      "wrong_move\n",
      "   962/50000: episode: 961, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -37.199326 12308.371\n",
      "wrong_move\n",
      "   963/50000: episode: 962, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3973.000 [3973.000, 3973.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.847008 11965.383\n",
      "wrong_move\n",
      "   964/50000: episode: 963, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2066.000 [2066.000, 2066.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.543236 11973.182\n",
      "wrong_move\n",
      "   965/50000: episode: 964, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -34.77948 12358.051\n",
      "wrong_move\n",
      "   966/50000: episode: 965, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2169.000 [2169.000, 2169.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -47.026485 12099.472\n",
      "wrong_move\n",
      "   967/50000: episode: 966, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2965.000 [2965.000, 2965.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.016544 12220.261\n",
      "wrong_move\n",
      "   968/50000: episode: 967, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1098.000 [1098.000, 1098.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.324825 11981.452\n",
      "wrong_move\n",
      "   969/50000: episode: 968, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.25087 11992.294\n",
      "wrong_move\n",
      "   970/50000: episode: 969, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1355.000 [1355.000, 1355.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.556305 12362.57\n",
      "wrong_move\n",
      "   971/50000: episode: 970, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.999886 12065.868\n",
      "wrong_move\n",
      "   972/50000: episode: 971, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.984768 12163.987\n",
      "wrong_move\n",
      "   973/50000: episode: 972, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1956.000 [1956.000, 1956.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -37.670753 12292.107\n",
      "wrong_move\n",
      "   974/50000: episode: 973, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -46.238415 12735.962\n",
      "wrong_move\n",
      "   975/50000: episode: 974, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.839962 12126.797\n",
      "wrong_move\n",
      "   976/50000: episode: 975, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -37.780487 12302.529\n",
      "wrong_move\n",
      "   977/50000: episode: 976, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2938.000 [2938.000, 2938.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.13337 11974.802\n",
      "wrong_move\n",
      "   978/50000: episode: 977, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 385.000 [385.000, 385.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.44931 11986.162\n",
      "wrong_move\n",
      "   979/50000: episode: 978, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3182.000 [3182.000, 3182.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.92053 12002.522\n",
      "wrong_move\n",
      "   980/50000: episode: 979, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.855198 12403.714\n",
      "wrong_move\n",
      "   981/50000: episode: 980, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.253704 12018.853\n",
      "wrong_move\n",
      "   982/50000: episode: 981, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.015045 12465.026\n",
      "wrong_move\n",
      "   983/50000: episode: 982, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2526.000 [2526.000, 2526.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.48645 11994.704\n",
      "wrong_move\n",
      "   984/50000: episode: 983, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.25453 12056.557\n",
      "wrong_move\n",
      "   985/50000: episode: 984, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.676655 12056.811\n",
      "wrong_move\n",
      "   986/50000: episode: 985, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1549.000 [1549.000, 1549.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.957882 11960.015\n",
      "wrong_move\n",
      "   988/50000: episode: 986, duration: 0.090s, episode steps:   2, steps per second:  22, episode reward: -4981.000, mean reward: -2490.500 [-5000.000, 19.000], mean action: 2287.000 [480.000, 4094.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.41318 12014.259\n",
      "wrong_move\n",
      "   989/50000: episode: 987, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -48.60392 12318.417\n",
      "wrong_move\n",
      "   990/50000: episode: 988, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2109.000 [2109.000, 2109.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -44.88652 11759.812\n",
      "wrong_move\n",
      "   991/50000: episode: 989, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.773594 12031.6045\n",
      "wrong_move\n",
      "   992/50000: episode: 990, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3244.000 [3244.000, 3244.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.343605 12039.7\n",
      "wrong_move\n",
      "   993/50000: episode: 991, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2660.000 [2660.000, 2660.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.469364 12033.815\n",
      "wrong_move\n",
      "   994/50000: episode: 992, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.729652 12043.508\n",
      "wrong_move\n",
      "   995/50000: episode: 993, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2017.000 [2017.000, 2017.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -40.79907 12344.803\n",
      "wrong_move\n",
      "   996/50000: episode: 994, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.56522 12024.109\n",
      "wrong_move\n",
      "   997/50000: episode: 995, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.239117 11906.872\n",
      "wrong_move\n",
      "   998/50000: episode: 996, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2855.000 [2855.000, 2855.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -39.263638 12051.324\n",
      "wrong_move\n",
      "   999/50000: episode: 997, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -42.013824 12100.226\n",
      "wrong_move\n",
      "  1000/50000: episode: 998, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1319.000 [1319.000, 1319.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -45.83882 12602.032\n",
      "wrong_move\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1001/50000: episode: 999, duration: 2.146s, episode steps:   1, steps per second:   0, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: --, mae: --, mean_q: --\n",
      "Val: -41.61587 12002.155\n",
      "wrong_move\n",
      "  1002/50000: episode: 1000, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1786.000 [1786.000, 1786.000],  loss: 134609984.000000, mae: 9731.386719, mean_q: 12099.132812\n",
      "Val: -41.05755 12025.95\n",
      "wrong_move\n",
      "  1003/50000: episode: 1001, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 130477632.000000, mae: 9731.349609, mean_q: 12094.203125\n",
      "Val: -43.29867 12148.335\n",
      "wrong_move\n",
      "  1004/50000: episode: 1002, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2092.000 [2092.000, 2092.000],  loss: 131311672.000000, mae: 9731.308594, mean_q: 12094.104492\n",
      "Val: -40.797386 12046.145\n",
      "wrong_move\n",
      "  1005/50000: episode: 1003, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 137287904.000000, mae: 9731.265625, mean_q: 12099.726562\n",
      "Val: -41.101368 11997.562\n",
      "wrong_move\n",
      "  1006/50000: episode: 1004, duration: 0.175s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 135229712.000000, mae: 9731.223633, mean_q: 12088.334961\n",
      "Val: -40.328102 12043.951\n",
      "wrong_move\n",
      "  1007/50000: episode: 1005, duration: 0.157s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 136283952.000000, mae: 9731.179688, mean_q: 12102.904297\n",
      "Val: -41.363598 11994.641\n",
      "wrong_move\n",
      "  1008/50000: episode: 1006, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 410.000 [410.000, 410.000],  loss: 138301504.000000, mae: 9731.135742, mean_q: 12077.724609\n",
      "Val: -45.13449 11868.937\n",
      "wrong_move\n",
      "  1009/50000: episode: 1007, duration: 0.133s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: 130668520.000000, mae: 9731.089844, mean_q: 12082.832031\n",
      "Val: -41.103455 11983.442\n",
      "wrong_move\n",
      "  1010/50000: episode: 1008, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1242.000 [1242.000, 1242.000],  loss: 134184080.000000, mae: 9731.043945, mean_q: 12099.126953\n",
      "Val: -41.253788 11973.38\n",
      "wrong_move\n",
      "  1011/50000: episode: 1009, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3962.000 [3962.000, 3962.000],  loss: 129153408.000000, mae: 9730.996094, mean_q: 12056.634766\n",
      "Val: -47.184807 12315.551\n",
      "wrong_move\n",
      "  1012/50000: episode: 1010, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: 129364104.000000, mae: 9730.949219, mean_q: 12083.562500\n",
      "Val: -41.57858 11940.1455\n",
      "wrong_move\n",
      "  1013/50000: episode: 1011, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 139023680.000000, mae: 9730.901367, mean_q: 12065.649414\n",
      "Val: -41.09615 11971.07\n",
      "wrong_move\n",
      "  1014/50000: episode: 1012, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 134926512.000000, mae: 9730.853516, mean_q: 12080.856445\n",
      "Val: -40.582058 12015.651\n",
      "wrong_move\n",
      "  1015/50000: episode: 1013, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 138007056.000000, mae: 9730.806641, mean_q: 12088.273438\n",
      "Val: -38.674644 12215.891\n",
      "wrong_move\n",
      "  1016/50000: episode: 1014, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1098.000 [1098.000, 1098.000],  loss: 133172320.000000, mae: 9730.757812, mean_q: 12078.931641\n",
      "Val: -42.168957 12099.64\n",
      "wrong_move\n",
      "  1017/50000: episode: 1015, duration: 0.089s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: 135416992.000000, mae: 9730.709961, mean_q: 12084.165039\n",
      "Val: -40.92769 11996.645\n",
      "wrong_move\n",
      "  1018/50000: episode: 1016, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: 140427536.000000, mae: 9730.662109, mean_q: 12089.792969\n",
      "Val: -41.40728 11971.922\n",
      "wrong_move\n",
      "  1019/50000: episode: 1017, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 134087024.000000, mae: 9730.611328, mean_q: 12075.716797\n",
      "Val: -39.932636 12033.213\n",
      "wrong_move\n",
      "  1020/50000: episode: 1018, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 132791872.000000, mae: 9730.564453, mean_q: 12082.619141\n",
      "Val: -43.608955 12456.935\n",
      "wrong_move\n",
      "  1021/50000: episode: 1019, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: 133439256.000000, mae: 9730.514648, mean_q: 12047.996094\n",
      "Val: -48.330605 12147.2705\n",
      "wrong_move\n",
      "  1022/50000: episode: 1020, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: 137532512.000000, mae: 9730.464844, mean_q: 12062.446289\n",
      "Val: -38.67256 11937.426\n",
      "wrong_move\n",
      "  1023/50000: episode: 1021, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: 132378784.000000, mae: 9730.416016, mean_q: 12063.317383\n",
      "Val: -39.073864 12186.725\n",
      "wrong_move\n",
      "  1024/50000: episode: 1022, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1098.000 [1098.000, 1098.000],  loss: 130025160.000000, mae: 9730.366211, mean_q: 12091.322266\n",
      "Val: -41.522697 11933.802\n",
      "wrong_move\n",
      "  1025/50000: episode: 1023, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 135549984.000000, mae: 9730.316406, mean_q: 12061.503906\n",
      "Val: -40.00336 11966.102\n",
      "wrong_move\n",
      "  1026/50000: episode: 1024, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: 131888216.000000, mae: 9730.267578, mean_q: 12053.386719\n",
      "Val: -41.54352 11950.0625\n",
      "wrong_move\n",
      "  1027/50000: episode: 1025, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4007.000 [4007.000, 4007.000],  loss: 134634288.000000, mae: 9730.218750, mean_q: 12059.759766\n",
      "Val: -41.499756 11943.717\n",
      "wrong_move\n",
      "  1028/50000: episode: 1026, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 131990704.000000, mae: 9730.170898, mean_q: 12055.815430\n",
      "Val: -38.66141 12214.461\n",
      "wrong_move\n",
      "  1029/50000: episode: 1027, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2948.000 [2948.000, 2948.000],  loss: 135155904.000000, mae: 9730.122070, mean_q: 12049.365234\n",
      "Val: -40.310364 11986.228\n",
      "wrong_move\n",
      "  1030/50000: episode: 1028, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 896.000 [896.000, 896.000],  loss: 128324352.000000, mae: 9730.072266, mean_q: 12080.487305\n",
      "Val: -44.871502 12320.269\n",
      "wrong_move\n",
      "  1031/50000: episode: 1029, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2053.000 [2053.000, 2053.000],  loss: 134041488.000000, mae: 9730.022461, mean_q: 12046.018555\n",
      "Val: -40.811867 11967.829\n",
      "wrong_move\n",
      "  1032/50000: episode: 1030, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1162.000 [1162.000, 1162.000],  loss: 136485120.000000, mae: 9729.971680, mean_q: 12054.989258\n",
      "Val: -40.32721 12005.147\n",
      "wrong_move\n",
      "  1033/50000: episode: 1031, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 138146288.000000, mae: 9729.921875, mean_q: 12038.786133\n",
      "Val: -44.021057 12400.077\n",
      "wrong_move\n",
      "  1034/50000: episode: 1032, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: 129917592.000000, mae: 9729.873047, mean_q: 12041.191406\n",
      "Val: -40.96936 11953.01\n",
      "wrong_move\n",
      "  1035/50000: episode: 1033, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 931.000 [931.000, 931.000],  loss: 133180696.000000, mae: 9729.824219, mean_q: 12046.818359\n",
      "Val: -40.98819 11987.89\n",
      "wrong_move\n",
      "  1036/50000: episode: 1034, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 136343840.000000, mae: 9729.772461, mean_q: 12046.489258\n",
      "Val: -39.868702 11936.931\n",
      "wrong_move\n",
      "  1037/50000: episode: 1035, duration: 0.057s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: 129548720.000000, mae: 9729.722656, mean_q: 12036.912109\n",
      "Val: -41.5656 11939.373\n",
      "wrong_move\n",
      "  1038/50000: episode: 1036, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 135786112.000000, mae: 9729.669922, mean_q: 12051.640625\n",
      "Val: -40.720245 12000.895\n",
      "wrong_move\n",
      "  1039/50000: episode: 1037, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 131368656.000000, mae: 9729.621094, mean_q: 12056.134766\n",
      "Val: -44.92678 11849.737\n",
      "wrong_move\n",
      "  1040/50000: episode: 1038, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 139831872.000000, mae: 9729.568359, mean_q: 12042.222656\n",
      "Val: -37.24916 12017.094\n",
      "wrong_move\n",
      "  1041/50000: episode: 1039, duration: 0.075s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: 141067968.000000, mae: 9729.520508, mean_q: 12054.272461\n",
      "Val: -43.83132 12354.03\n",
      "wrong_move\n",
      "  1042/50000: episode: 1040, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: 135790976.000000, mae: 9729.469727, mean_q: 12034.355469\n",
      "Val: -39.871887 11960.829\n",
      "wrong_move\n",
      "  1043/50000: episode: 1041, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: 138443328.000000, mae: 9729.420898, mean_q: 12048.501953\n",
      "Val: -46.00981 12053.886\n",
      "wrong_move\n",
      "  1044/50000: episode: 1042, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 806.000 [806.000, 806.000],  loss: 124170016.000000, mae: 9729.369141, mean_q: 12032.996094\n",
      "Val: -41.387253 11911.064\n",
      "wrong_move\n",
      "  1045/50000: episode: 1043, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 130504160.000000, mae: 9729.318359, mean_q: 12034.969727\n",
      "Val: -41.745758 11871.859\n",
      "wrong_move\n",
      "  1046/50000: episode: 1044, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 131277304.000000, mae: 9729.146484, mean_q: 12041.369141\n",
      "Val: -41.240246 11917.946\n",
      "wrong_move\n",
      "  1047/50000: episode: 1045, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2057.000 [2057.000, 2057.000],  loss: 135197424.000000, mae: 9729.216797, mean_q: 12026.559570\n",
      "Val: -41.617096 11879.156\n",
      "wrong_move\n",
      "  1048/50000: episode: 1046, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 128542064.000000, mae: 9729.166016, mean_q: 12024.691406\n",
      "Val: -45.13335 11846.483\n",
      "wrong_move\n",
      "  1049/50000: episode: 1047, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: 135851456.000000, mae: 9729.114258, mean_q: 12018.920898\n",
      "Val: -44.795395 12088.515\n",
      "wrong_move\n",
      "  1050/50000: episode: 1048, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: 130426152.000000, mae: 9729.062500, mean_q: 12041.027344\n",
      "Val: -41.30996 11939.92\n",
      "wrong_move\n",
      "  1051/50000: episode: 1049, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 984.000 [984.000, 984.000],  loss: 130287344.000000, mae: 9729.009766, mean_q: 12038.206055\n",
      "Val: -42.724354 12383.601\n",
      "wrong_move\n",
      "  1052/50000: episode: 1050, duration: 0.052s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: 133841520.000000, mae: 9728.957031, mean_q: 12028.693359\n",
      "Val: -41.01139 11999.594\n",
      "wrong_move\n",
      "  1053/50000: episode: 1051, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3637.000 [3637.000, 3637.000],  loss: 136473504.000000, mae: 9728.906250, mean_q: 12025.327148\n",
      "Val: -47.037865 12024.966\n",
      "wrong_move\n",
      "  1054/50000: episode: 1052, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3699.000 [3699.000, 3699.000],  loss: 122172640.000000, mae: 9728.857422, mean_q: 12018.832031\n",
      "Val: -41.94393 12095.516\n",
      "wrong_move\n",
      "  1055/50000: episode: 1053, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2282.000 [2282.000, 2282.000],  loss: 134148112.000000, mae: 9728.804688, mean_q: 12008.614258\n",
      "Val: -38.89555 12269.57\n",
      "wrong_move\n",
      "  1056/50000: episode: 1054, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: 131066664.000000, mae: 9728.753906, mean_q: 12052.007812\n",
      "Val: -41.249485 11943.751\n",
      "wrong_move\n",
      "  1057/50000: episode: 1055, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3117.000 [3117.000, 3117.000],  loss: 132723232.000000, mae: 9728.703125, mean_q: 12016.261719\n",
      "Val: -41.59163 11864.458\n",
      "wrong_move\n",
      "  1058/50000: episode: 1056, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3036.000 [3036.000, 3036.000],  loss: 137952928.000000, mae: 9728.650391, mean_q: 12015.409180\n",
      "Val: -44.010845 12355.607\n",
      "wrong_move\n",
      "  1059/50000: episode: 1057, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: 133898264.000000, mae: 9728.600586, mean_q: 12020.421875\n",
      "Val: -44.62187 12233.481\n",
      "wrong_move\n",
      "  1060/50000: episode: 1058, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2053.000 [2053.000, 2053.000],  loss: 130845560.000000, mae: 9728.548828, mean_q: 12024.000000\n",
      "Val: -41.499783 11878.696\n",
      "wrong_move\n",
      "  1061/50000: episode: 1059, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1903.000 [1903.000, 1903.000],  loss: 128840032.000000, mae: 9728.384766, mean_q: 12012.863281\n",
      "Val: -41.559418 11850.392\n",
      "wrong_move\n",
      "  1062/50000: episode: 1060, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 29.000 [29.000, 29.000],  loss: 133817672.000000, mae: 9728.447266, mean_q: 12012.128906\n",
      "Val: -46.045845 11972.195\n",
      "wrong_move\n",
      "  1063/50000: episode: 1061, duration: 0.081s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: 129809008.000000, mae: 9728.394531, mean_q: 12028.867188\n",
      "Val: -46.45146 12185.483\n",
      "wrong_move\n",
      "  1064/50000: episode: 1062, duration: 0.085s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: 134574816.000000, mae: 9728.342773, mean_q: 12001.369141\n",
      "Val: -40.734844 11956.615\n",
      "wrong_move\n",
      "  1065/50000: episode: 1063, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 139546016.000000, mae: 9728.291016, mean_q: 11981.873047\n",
      "Val: -41.420948 11889.525\n",
      "wrong_move\n",
      "  1066/50000: episode: 1064, duration: 0.069s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 138729760.000000, mae: 9728.241211, mean_q: 12006.220703\n",
      "Val: -44.974003 11793.519\n",
      "wrong_move\n",
      "  1067/50000: episode: 1065, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2364.000 [2364.000, 2364.000],  loss: 133746280.000000, mae: 9728.191406, mean_q: 11995.657227\n",
      "Val: -38.257805 12179.493\n",
      "wrong_move\n",
      "  1068/50000: episode: 1066, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2835.000 [2835.000, 2835.000],  loss: 134963904.000000, mae: 9728.142578, mean_q: 12003.779297\n",
      "Val: -38.916847 11907.024\n",
      "wrong_move\n",
      "  1069/50000: episode: 1067, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: 130901312.000000, mae: 9728.092773, mean_q: 12008.658203\n",
      "Val: -41.388096 11862.313\n",
      "wrong_move\n",
      "  1070/50000: episode: 1068, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 125145344.000000, mae: 9727.925781, mean_q: 11989.156250\n",
      "Val: -41.232483 11898.245\n",
      "wrong_move\n",
      "  1071/50000: episode: 1069, duration: 0.080s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 134395760.000000, mae: 9727.991211, mean_q: 11993.199219\n",
      "Val: -41.06806 11898.712\n",
      "wrong_move\n",
      "  1072/50000: episode: 1070, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 135457952.000000, mae: 9727.941406, mean_q: 12004.199219\n",
      "Val: -42.13137 12257.356\n",
      "wrong_move\n",
      "  1073/50000: episode: 1071, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2104.000 [2104.000, 2104.000],  loss: 130338848.000000, mae: 9727.892578, mean_q: 12003.875977\n",
      "Val: -41.389637 11869.811\n",
      "wrong_move\n",
      "  1074/50000: episode: 1072, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 134570176.000000, mae: 9727.841797, mean_q: 12010.459961\n",
      "Val: -39.979305 11949.527\n",
      "wrong_move\n",
      "  1075/50000: episode: 1073, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3309.000 [3309.000, 3309.000],  loss: 138156064.000000, mae: 9727.791992, mean_q: 11991.405273\n",
      "Val: -45.166397 11932.182\n",
      "wrong_move\n",
      "  1076/50000: episode: 1074, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: 132627872.000000, mae: 9727.742188, mean_q: 11999.542969\n",
      "Val: -40.59863 11895.693\n",
      "wrong_move\n",
      "  1077/50000: episode: 1075, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 132541120.000000, mae: 9727.690430, mean_q: 11981.931641\n",
      "Val: -43.676445 12085.024\n",
      "wrong_move\n",
      "  1078/50000: episode: 1076, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3508.000 [3508.000, 3508.000],  loss: 135946928.000000, mae: 9727.639648, mean_q: 11994.478516\n",
      "Val: -48.072094 12230.546\n",
      "wrong_move\n",
      "  1079/50000: episode: 1077, duration: 0.080s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: 127611040.000000, mae: 9727.588867, mean_q: 11993.034180\n",
      "Val: -43.81777 12293.552\n",
      "wrong_move\n",
      "  1080/50000: episode: 1078, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1694.000 [1694.000, 1694.000],  loss: 135967280.000000, mae: 9727.536133, mean_q: 12008.082031\n",
      "Val: -41.698696 12011.223\n",
      "wrong_move\n",
      "  1081/50000: episode: 1079, duration: 0.112s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 812.000 [812.000, 812.000],  loss: 135276032.000000, mae: 9727.485352, mean_q: 11983.212891\n",
      "Val: -41.53616 11837.292\n",
      "wrong_move\n",
      "  1082/50000: episode: 1080, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3120.000 [3120.000, 3120.000],  loss: 138131776.000000, mae: 9727.433594, mean_q: 11993.471680\n",
      "Val: -40.615814 11913.912\n",
      "wrong_move\n",
      "  1083/50000: episode: 1081, duration: 0.086s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3284.000 [3284.000, 3284.000],  loss: 112894608.000000, mae: 9727.257812, mean_q: 12001.059570\n",
      "Val: -38.315346 12122.718\n",
      "wrong_move\n",
      "  1084/50000: episode: 1082, duration: 0.107s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1098.000 [1098.000, 1098.000],  loss: 135768208.000000, mae: 9727.328125, mean_q: 12001.401367\n",
      "Val: -40.775734 11887.387\n",
      "wrong_move\n",
      "  1085/50000: episode: 1083, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1003.000 [1003.000, 1003.000],  loss: 130174352.000000, mae: 9727.276367, mean_q: 11977.516602\n",
      "Val: -40.623634 11909.252\n",
      "wrong_move\n",
      "  1086/50000: episode: 1084, duration: 0.101s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 935.000 [935.000, 935.000],  loss: 138911808.000000, mae: 9727.225586, mean_q: 11980.016602\n",
      "Val: -44.79008 11904.762\n",
      "wrong_move\n",
      "  1087/50000: episode: 1085, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1035.000 [1035.000, 1035.000],  loss: 127119296.000000, mae: 9727.172852, mean_q: 12000.853516\n",
      "Val: -41.250957 11830.337\n",
      "wrong_move\n",
      "  1088/50000: episode: 1086, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: 132904448.000000, mae: 9727.120117, mean_q: 12001.675781\n",
      "Val: -43.78838 11682.409\n",
      "wrong_move\n",
      "  1089/50000: episode: 1087, duration: 0.134s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1314.000 [1314.000, 1314.000],  loss: 134125744.000000, mae: 9727.065430, mean_q: 11983.715820\n",
      "Val: -42.081978 11912.178\n",
      "wrong_move\n",
      "  1090/50000: episode: 1088, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: 139141584.000000, mae: 9727.012695, mean_q: 12002.681641\n",
      "Val: -46.168953 12187.594\n",
      "wrong_move\n",
      "  1091/50000: episode: 1089, duration: 0.102s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1098.000 [1098.000, 1098.000],  loss: 131960064.000000, mae: 9726.958984, mean_q: 11982.330078\n",
      "Val: -41.057613 11832.999\n",
      "wrong_move\n",
      "  1092/50000: episode: 1090, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 133909872.000000, mae: 9726.906250, mean_q: 11970.294922\n",
      "Val: -46.289097 11986.953\n",
      "wrong_move\n",
      "  1093/50000: episode: 1091, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1256.000 [1256.000, 1256.000],  loss: 135445840.000000, mae: 9726.855469, mean_q: 11987.458984\n",
      "Val: -42.307987 11807.708\n",
      "wrong_move\n",
      "  1094/50000: episode: 1092, duration: 0.092s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1939.000 [1939.000, 1939.000],  loss: 130993096.000000, mae: 9726.802734, mean_q: 11959.997070\n",
      "Val: -41.251263 11861.066\n",
      "wrong_move\n",
      "  1095/50000: episode: 1093, duration: 0.097s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 132422560.000000, mae: 9726.750000, mean_q: 11992.083984\n",
      "Val: -45.600994 12283.121\n",
      "wrong_move\n",
      "  1096/50000: episode: 1094, duration: 0.087s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: 133882432.000000, mae: 9726.698242, mean_q: 11988.679688\n",
      "Val: -41.504253 11827.881\n",
      "wrong_move\n",
      "  1097/50000: episode: 1095, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 138091616.000000, mae: 9726.645508, mean_q: 11952.496094\n",
      "Val: -41.16473 11872.572\n",
      "wrong_move\n",
      "  1098/50000: episode: 1096, duration: 0.114s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 134594336.000000, mae: 9726.593750, mean_q: 11979.903320\n",
      "Val: -41.923386 11809.623\n",
      "wrong_move\n",
      "  1099/50000: episode: 1097, duration: 0.111s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 136602048.000000, mae: 9726.541016, mean_q: 11986.630859\n",
      "Val: -45.89185 11790.611\n",
      "wrong_move\n",
      "  1100/50000: episode: 1098, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3226.000 [3226.000, 3226.000],  loss: 135712688.000000, mae: 9726.487305, mean_q: 11966.417969\n",
      "Val: -42.334866 11861.991\n",
      "wrong_move\n",
      "  1101/50000: episode: 1099, duration: 0.121s, episode steps:   1, steps per second:   8, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2053.000 [2053.000, 2053.000],  loss: 136073760.000000, mae: 9726.434570, mean_q: 11976.366211\n",
      "Val: -46.22656 12124.286\n",
      "wrong_move\n",
      "  1102/50000: episode: 1100, duration: 0.098s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1674.000 [1674.000, 1674.000],  loss: 133676456.000000, mae: 9726.382812, mean_q: 11983.552734\n",
      "Val: -42.84904 11795.192\n",
      "wrong_move\n",
      "  1103/50000: episode: 1101, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 136024384.000000, mae: 9726.330078, mean_q: 11972.814453\n",
      "Val: -45.687458 11732.36\n",
      "wrong_move\n",
      "  1104/50000: episode: 1102, duration: 0.113s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3297.000 [3297.000, 3297.000],  loss: 135270992.000000, mae: 9726.277344, mean_q: 11985.406250\n",
      "Val: -42.91422 11858.934\n",
      "wrong_move\n",
      "  1105/50000: episode: 1103, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 133465504.000000, mae: 9726.224609, mean_q: 11952.474609\n",
      "Val: -48.941833 12116.088\n",
      "wrong_move\n",
      "  1106/50000: episode: 1104, duration: 0.138s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3276.000 [3276.000, 3276.000],  loss: 134727488.000000, mae: 9726.171875, mean_q: 11944.776367\n",
      "Val: -43.64818 12008.861\n",
      "wrong_move\n",
      "  1107/50000: episode: 1105, duration: 0.147s, episode steps:   1, steps per second:   7, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1182.000 [1182.000, 1182.000],  loss: 129327504.000000, mae: 9726.119141, mean_q: 11964.247070\n",
      "Val: -50.99828 11973.855\n",
      "wrong_move\n",
      "  1108/50000: episode: 1106, duration: 0.109s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3637.000 [3637.000, 3637.000],  loss: 137413792.000000, mae: 9726.065430, mean_q: 11960.192383\n",
      "Val: -43.484943 11783.184\n",
      "wrong_move\n",
      "  1109/50000: episode: 1107, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 134091360.000000, mae: 9726.013672, mean_q: 11941.710938\n",
      "Val: -51.58194 11840.855\n",
      "wrong_move\n",
      "  1110/50000: episode: 1108, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 278.000 [278.000, 278.000],  loss: 124524288.000000, mae: 9725.959961, mean_q: 11971.179688\n",
      "Val: -43.891453 11883.09\n",
      "wrong_move\n",
      "  1111/50000: episode: 1109, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2053.000 [2053.000, 2053.000],  loss: 135688336.000000, mae: 9725.906250, mean_q: 11972.149414\n",
      "Val: -44.83547 11847.761\n",
      "wrong_move\n",
      "  1112/50000: episode: 1110, duration: 0.093s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 132516488.000000, mae: 9725.854492, mean_q: 11976.275391\n",
      "Val: -45.757767 11936.038\n",
      "wrong_move\n",
      "  1113/50000: episode: 1111, duration: 0.115s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3415.000 [3415.000, 3415.000],  loss: 139080000.000000, mae: 9725.800781, mean_q: 11938.337891\n",
      "Val: -52.1391 12017.757\n",
      "wrong_move\n",
      "  1114/50000: episode: 1112, duration: 0.117s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2832.000 [2832.000, 2832.000],  loss: 128725136.000000, mae: 9725.625000, mean_q: 11961.798828\n",
      "Val: -46.778366 12224.04\n",
      "wrong_move\n",
      "  1115/50000: episode: 1113, duration: 0.110s, episode steps:   1, steps per second:   9, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3415.000 [3415.000, 3415.000],  loss: 132115120.000000, mae: 9725.695312, mean_q: 11953.925781\n",
      "Val: -52.391506 12304.157\n",
      "wrong_move\n",
      "  1116/50000: episode: 1114, duration: 0.096s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3309.000 [3309.000, 3309.000],  loss: 137871488.000000, mae: 9725.642578, mean_q: 11945.096680\n",
      "Val: -50.25627 11813.344\n",
      "wrong_move\n",
      "  1117/50000: episode: 1115, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 774.000 [774.000, 774.000],  loss: 134113744.000000, mae: 9725.589844, mean_q: 11952.644531\n",
      "Val: -47.111877 11773.263\n",
      "wrong_move\n",
      "  1118/50000: episode: 1116, duration: 0.074s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 134625088.000000, mae: 9725.535156, mean_q: 11942.168945\n",
      "Val: -47.4829 11765.151\n",
      "wrong_move\n",
      "  1119/50000: episode: 1117, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2426.000 [2426.000, 2426.000],  loss: 135529616.000000, mae: 9725.480469, mean_q: 11941.478516\n",
      "Val: -47.7177 11805.867\n",
      "wrong_move\n",
      "  1120/50000: episode: 1118, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 480.000 [480.000, 480.000],  loss: 126225232.000000, mae: 9725.304688, mean_q: 11954.611328\n",
      "Val: -52.58083 12068.63\n",
      "wrong_move\n"
     ]
    }
   ],
   "source": [
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "for i in range (10):\n",
    "  policy = EpsGreedyQPolicy(0.01)\n",
    "  dqn = DQNAgent(model=model, nb_actions=NB_ACTIONS, memory=memory,\n",
    "                target_model_update=1e-2, policy=policy)\n",
    "  dqn.compile(Adam(lr=1e-4), metrics=['mae'])\n",
    "\n",
    "  # Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "  # slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "  # Ctrl + C.\n",
    "  his = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
    "  \n",
    "  model.save('chess_model_opt_init.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
