{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install keras-rl2\n",
    "# ! pip install chess\n",
    "# ! pip install python-chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 13:26:25.562941: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-14 13:26:25.562995: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten,\\\n",
    "     Input,BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# import gym_chess\n",
    "\n",
    "import chess\n",
    "# import sys\n",
    "# sys.path.insert(0, '../')\n",
    "# sys.path.insert(0, '../alpha_beta')\n",
    "# from MyChessBoard import MyChessBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14391377967476409416\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 13:26:26.971543: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-14 13:26:26.972435: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-14 13:26:26.972461: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-14 13:26:26.972487: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LoG): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SHAPE = (65, )\n",
    "NB_ACTIONS = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessEnv:\n",
    "    '''\n",
    "    state - obser: ndarray - (65,): [:65] is flatten from int_board; [65] is color of bot; 1 is white and -1 is black\n",
    "    step: int. step_range = (0, 4096) , is encoded from square A to square B (64 x 64 val)\n",
    "    reward: int\n",
    "    '''\n",
    "\n",
    "    mapped = {\n",
    "            'P': 10,     # White Pawn\n",
    "            'p': -10,    # Black Pawn\n",
    "            'N': 20,     # White Knight\n",
    "            'n': -20,    # Black Knight\n",
    "            'B': 30,     # White Bishop\n",
    "            'b': -30,    # Black Bishop\n",
    "            'R': 40,     # White Rook\n",
    "            'r': -40,    # Black Rook\n",
    "            'Q': 50,     # White Queen\n",
    "            'q': -50,    # Black Queen\n",
    "            'K': 900,     # White King\n",
    "            'k': -900     # Black King\n",
    "    }\n",
    "    # state_shape = (8, 8)\n",
    "    # nb_actions = 4096\n",
    "    model = None\n",
    "    \n",
    "    def __init__(self, model: Sequential, neg_r_each_step = -1) -> None:\n",
    "        self.env = chess.Board()\n",
    "        self.state = self.reset()\n",
    "        # [-1] = 1 -> white, -1 -> black\n",
    "        self.bot_color = self.env.turn * 2 - 1\n",
    "        self.neg_r_each_step = neg_r_each_step\n",
    "        self.model = model\n",
    "\n",
    "    def is_draw(self):\n",
    "        if self.env.is_stalemate():\n",
    "            print(\"statlemate\")\n",
    "            return True\n",
    "        if self.env.is_fivefold_repetition():\n",
    "            print(\"fivefold repetition\")\n",
    "            return True\n",
    "        if self.env.is_seventyfive_moves():\n",
    "            print(\"75 moves\")\n",
    "            return True\n",
    "        if self.env.is_insufficient_material():\n",
    "            print(\"Insufficient Material\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_checkmate(self):\n",
    "        # If There is checkmate then it will be TRUE else FALSE.It will be a boolean value.\n",
    "        return self.env.is_checkmate()\n",
    "\n",
    "    def convert_board_to_int(self):\n",
    "        epd_string = self.env.epd()\n",
    "        list_int = np.empty((0, ))\n",
    "        for i in epd_string:\n",
    "            if i == \" \":\n",
    "                list_int = list_int.reshape((8, 8))\n",
    "                return list_int\n",
    "            elif i != \"/\":\n",
    "                if i in self.mapped:\n",
    "                    list_int = np.append(list_int, self.mapped[i])\n",
    "                else:\n",
    "                    for counter in range(0, int(i)):\n",
    "                        list_int = np.append(list_int, 0)\n",
    "        list_int = list_int.reshape((8, 8))\n",
    "        return list_int\n",
    "\n",
    "    def get_state(self) -> np.ndarray:\n",
    "        return np.append(self.convert_board_to_int().reshape(64,), self.env.turn * 2 - 1)\n",
    "\n",
    "    def legal_moves(self):\n",
    "        return list(self.env.legal_moves)\n",
    "\n",
    "    def encodeMove(self, move_uci:str):\n",
    "        if len(move_uci) != 4:\n",
    "            raise ValueError()\n",
    "        a, b = chess.parse_square(move_uci[:2]), chess.parse_square(move_uci[2:])\n",
    "        return a * 64 + b\n",
    "\n",
    "    def decodeMove(self, move_int:int):\n",
    "        a, b = move_int//64, move_int%64\n",
    "        # a, b = chess.square_name(a), chess.square_name(b)\n",
    "\n",
    "        move = self.env.find_move(from_square= a,to_square= b)\n",
    "        return move\n",
    "\n",
    "    def render(self):\n",
    "        print(self.env.unicode())\n",
    "\n",
    "    def reset(self):\n",
    "        # random state\n",
    "        redo = True\n",
    "        num_sample_steps = 0\n",
    "        while redo:\n",
    "            redo = False\n",
    "            self.env = chess.Board()\n",
    "            num_sample_steps = np.random.randint(0, 50)\n",
    "            for i in range (num_sample_steps):\n",
    "                lg_move = self.legal_moves()\n",
    "                if len(lg_move) != 0:\n",
    "                    move = np.random.choice(self.legal_moves())\n",
    "                    self.env.push(move)\n",
    "                else:\n",
    "                    redo = True\n",
    "                    break\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action: int):\n",
    "        reward = 0\n",
    "        done = True\n",
    "\n",
    "        try:\n",
    "            # move in legal move\n",
    "            move = self.decodeMove(action)\n",
    "\n",
    "            # neg reward each step\n",
    "            reward = self.neg_r_each_step\n",
    "\n",
    "            # location to_square\n",
    "            to_r, to_c = move.to_square//8, move.to_square%8\n",
    "            reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
    "\n",
    "            # action\n",
    "            self.env.push(move)\n",
    "            self.state = self.get_state()\n",
    "\n",
    "            # check end game\n",
    "            if self.is_checkmate():\n",
    "                reward += self.mapped['K']\n",
    "                done = True\n",
    "                print('Win')\n",
    "            elif self.is_draw():\n",
    "                reward += 300\n",
    "                done = True\n",
    "\n",
    "            # opponent's turn   \n",
    "            else:\n",
    "                done = False\n",
    "                Q_val = self.model.predict(self.state.reshape((1, 1) + STATE_SHAPE)).reshape(-1, )\n",
    "                idx_sorted = np.argsort(Q_val)\n",
    "\n",
    "                for act in idx_sorted:\n",
    "                    try:\n",
    "                        move = self.decodeMove(act)\n",
    "\n",
    "                        # location to_square\n",
    "                        to_r, to_c = move.to_square//8, move.to_square%8\n",
    "                        reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
    "\n",
    "                        # action\n",
    "                        self.env.push(move)\n",
    "                        self.state = self.get_state()\n",
    "\n",
    "                        # check end game\n",
    "                        if self.is_checkmate():\n",
    "                            reward -= self.mapped['K']\n",
    "                            done = True\n",
    "                            print(\"Lose\")\n",
    "                        elif self.is_draw():\n",
    "                            reward += 300\n",
    "                            done = True\n",
    "                        \n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "        except:\n",
    "            # wrong move\n",
    "            reward = -5000\n",
    "            done = True\n",
    "            print('wrong_move')\n",
    "\n",
    "        return self.state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "WARNING:tensorflow:From /home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/layers/normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 65)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8448      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              528384    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4096)              0         \n",
      "=================================================================\n",
      "Total params: 554,368\n",
      "Trainable params: 553,856\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = Sequential()\n",
    "model.add(Input((1, ) + STATE_SHAPE))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(NB_ACTIONS))\n",
    "model.add(Activation('linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ChessEnv(model, neg_r_each_step=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('chess_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "2021-12-14 13:26:27.984771: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 1999965000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "wrong_move\n",
      "     1/50000: episode: 1, duration: 0.178s, episode steps:   1, steps per second:   6, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1940.000 [1940.000, 1940.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     2/50000: episode: 2, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     3/50000: episode: 3, duration: 0.006s, episode steps:   1, steps per second: 169, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     4/50000: episode: 4, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2015.000 [2015.000, 2015.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong_move\n",
      "     5/50000: episode: 5, duration: 0.006s, episode steps:   1, steps per second: 157, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     6/50000: episode: 6, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     7/50000: episode: 7, duration: 0.005s, episode steps:   1, steps per second: 183, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3475.000 [3475.000, 3475.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     8/50000: episode: 8, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 746.000 [746.000, 746.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     9/50000: episode: 9, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3420.000 [3420.000, 3420.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    10/50000: episode: 10, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    11/50000: episode: 11, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4094.000 [4094.000, 4094.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    12/50000: episode: 12, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 123.000 [123.000, 123.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    13/50000: episode: 13, duration: 0.005s, episode steps:   1, steps per second: 200, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    14/50000: episode: 14, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    15/50000: episode: 15, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2035.000 [2035.000, 2035.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    16/50000: episode: 16, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    17/50000: episode: 17, duration: 0.007s, episode steps:   1, steps per second: 150, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3896.000 [3896.000, 3896.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    18/50000: episode: 18, duration: 0.005s, episode steps:   1, steps per second: 219, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    19/50000: episode: 19, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    20/50000: episode: 20, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    21/50000: episode: 21, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1778.000 [1778.000, 1778.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    22/50000: episode: 22, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    23/50000: episode: 23, duration: 0.006s, episode steps:   1, steps per second: 178, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    24/50000: episode: 24, duration: 0.004s, episode steps:   1, steps per second: 280, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    25/50000: episode: 25, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    26/50000: episode: 26, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    27/50000: episode: 27, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    28/50000: episode: 28, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    29/50000: episode: 29, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    30/50000: episode: 30, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4013.000 [4013.000, 4013.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    31/50000: episode: 31, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    32/50000: episode: 32, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    33/50000: episode: 33, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    34/50000: episode: 34, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3392.000 [3392.000, 3392.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    35/50000: episode: 35, duration: 0.003s, episode steps:   1, steps per second: 293, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1495.000 [1495.000, 1495.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    36/50000: episode: 36, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2671.000 [2671.000, 2671.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    37/50000: episode: 37, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2219.000 [2219.000, 2219.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    38/50000: episode: 38, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2439.000 [2439.000, 2439.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    39/50000: episode: 39, duration: 0.005s, episode steps:   1, steps per second: 212, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2283.000 [2283.000, 2283.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    40/50000: episode: 40, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    41/50000: episode: 41, duration: 0.007s, episode steps:   1, steps per second: 150, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1005.000 [1005.000, 1005.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    42/50000: episode: 42, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    43/50000: episode: 43, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    44/50000: episode: 44, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    45/50000: episode: 45, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    46/50000: episode: 46, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    47/50000: episode: 47, duration: 0.005s, episode steps:   1, steps per second: 191, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3227.000 [3227.000, 3227.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    48/50000: episode: 48, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1330.000 [1330.000, 1330.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    49/50000: episode: 49, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3666.000 [3666.000, 3666.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    50/50000: episode: 50, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    51/50000: episode: 51, duration: 0.008s, episode steps:   1, steps per second: 133, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    52/50000: episode: 52, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    53/50000: episode: 53, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    54/50000: episode: 54, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    55/50000: episode: 55, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 136.000 [136.000, 136.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    56/50000: episode: 56, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    57/50000: episode: 57, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    58/50000: episode: 58, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2198.000 [2198.000, 2198.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    59/50000: episode: 59, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1259.000 [1259.000, 1259.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    60/50000: episode: 60, duration: 0.007s, episode steps:   1, steps per second: 150, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3358.000 [3358.000, 3358.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    61/50000: episode: 61, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1998.000 [1998.000, 1998.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    62/50000: episode: 62, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3409.000 [3409.000, 3409.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    63/50000: episode: 63, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 167.000 [167.000, 167.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    64/50000: episode: 64, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    65/50000: episode: 65, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2977.000 [2977.000, 2977.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    66/50000: episode: 66, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    67/50000: episode: 67, duration: 0.007s, episode steps:   1, steps per second: 134, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 829.000 [829.000, 829.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    68/50000: episode: 68, duration: 0.003s, episode steps:   1, steps per second: 394, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    69/50000: episode: 69, duration: 0.004s, episode steps:   1, steps per second: 263, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    70/50000: episode: 70, duration: 0.005s, episode steps:   1, steps per second: 216, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    71/50000: episode: 71, duration: 0.004s, episode steps:   1, steps per second: 269, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    72/50000: episode: 72, duration: 0.010s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    73/50000: episode: 73, duration: 0.005s, episode steps:   1, steps per second: 196, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    74/50000: episode: 74, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 973.000 [973.000, 973.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    75/50000: episode: 75, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    76/50000: episode: 76, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 458.000 [458.000, 458.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    77/50000: episode: 77, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    78/50000: episode: 78, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1465.000 [1465.000, 1465.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    79/50000: episode: 79, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 775.000 [775.000, 775.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    80/50000: episode: 80, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    81/50000: episode: 81, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2977.000 [2977.000, 2977.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    82/50000: episode: 82, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 465.000 [465.000, 465.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    83/50000: episode: 83, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3895.000 [3895.000, 3895.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    84/50000: episode: 84, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2891.000 [2891.000, 2891.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    85/50000: episode: 85, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    86/50000: episode: 86, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1948.000 [1948.000, 1948.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    87/50000: episode: 87, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    88/50000: episode: 88, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    89/50000: episode: 89, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    90/50000: episode: 90, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1872.000 [1872.000, 1872.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    91/50000: episode: 91, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1968.000 [1968.000, 1968.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    92/50000: episode: 92, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3706.000 [3706.000, 3706.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    93/50000: episode: 93, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    94/50000: episode: 94, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1532.000 [1532.000, 1532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    95/50000: episode: 95, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    96/50000: episode: 96, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    97/50000: episode: 97, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1872.000 [1872.000, 1872.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    98/50000: episode: 98, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    99/50000: episode: 99, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   100/50000: episode: 100, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   101/50000: episode: 101, duration: 0.004s, episode steps:   1, steps per second: 284, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   102/50000: episode: 102, duration: 0.003s, episode steps:   1, steps per second: 313, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   103/50000: episode: 103, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   104/50000: episode: 104, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   105/50000: episode: 105, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1138.000 [1138.000, 1138.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   106/50000: episode: 106, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1611.000 [1611.000, 1611.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   107/50000: episode: 107, duration: 0.005s, episode steps:   1, steps per second: 216, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   108/50000: episode: 108, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   109/50000: episode: 109, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2221.000 [2221.000, 2221.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   110/50000: episode: 110, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2334.000 [2334.000, 2334.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   111/50000: episode: 111, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   112/50000: episode: 112, duration: 0.005s, episode steps:   1, steps per second: 189, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   113/50000: episode: 113, duration: 0.006s, episode steps:   1, steps per second: 158, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   114/50000: episode: 114, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   115/50000: episode: 115, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   116/50000: episode: 116, duration: 0.006s, episode steps:   1, steps per second: 160, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3656.000 [3656.000, 3656.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   117/50000: episode: 117, duration: 0.007s, episode steps:   1, steps per second: 138, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   118/50000: episode: 118, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1640.000 [1640.000, 1640.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   119/50000: episode: 119, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   120/50000: episode: 120, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1465.000 [1465.000, 1465.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   121/50000: episode: 121, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1611.000 [1611.000, 1611.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   122/50000: episode: 122, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2827.000 [2827.000, 2827.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   123/50000: episode: 123, duration: 0.005s, episode steps:   1, steps per second: 215, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   124/50000: episode: 124, duration: 0.004s, episode steps:   1, steps per second: 251, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2421.000 [2421.000, 2421.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   125/50000: episode: 125, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   126/50000: episode: 126, duration: 0.005s, episode steps:   1, steps per second: 190, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   127/50000: episode: 127, duration: 0.004s, episode steps:   1, steps per second: 267, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3384.000 [3384.000, 3384.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   128/50000: episode: 128, duration: 0.006s, episode steps:   1, steps per second: 176, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2398.000 [2398.000, 2398.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   129/50000: episode: 129, duration: 0.017s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   130/50000: episode: 130, duration: 0.006s, episode steps:   1, steps per second: 165, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   131/50000: episode: 131, duration: 0.003s, episode steps:   1, steps per second: 290, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   132/50000: episode: 132, duration: 0.006s, episode steps:   1, steps per second: 169, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1170.000 [1170.000, 1170.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   133/50000: episode: 133, duration: 0.004s, episode steps:   1, steps per second: 236, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   134/50000: episode: 134, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   135/50000: episode: 135, duration: 0.006s, episode steps:   1, steps per second: 170, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   136/50000: episode: 136, duration: 0.004s, episode steps:   1, steps per second: 242, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   137/50000: episode: 137, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2010.000 [2010.000, 2010.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   138/50000: episode: 138, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1908.000 [1908.000, 1908.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   139/50000: episode: 139, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   140/50000: episode: 140, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   141/50000: episode: 141, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   142/50000: episode: 142, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3394.000 [3394.000, 3394.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   143/50000: episode: 143, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   144/50000: episode: 144, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3758.000 [3758.000, 3758.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   145/50000: episode: 145, duration: 0.003s, episode steps:   1, steps per second: 293, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2336.000 [2336.000, 2336.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   146/50000: episode: 146, duration: 0.007s, episode steps:   1, steps per second: 133, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2622.000 [2622.000, 2622.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   147/50000: episode: 147, duration: 0.006s, episode steps:   1, steps per second: 162, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3462.000 [3462.000, 3462.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   148/50000: episode: 148, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2680.000 [2680.000, 2680.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   149/50000: episode: 149, duration: 0.005s, episode steps:   1, steps per second: 182, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   150/50000: episode: 150, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   151/50000: episode: 151, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 775.000 [775.000, 775.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   152/50000: episode: 152, duration: 0.004s, episode steps:   1, steps per second: 251, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   153/50000: episode: 153, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   154/50000: episode: 154, duration: 0.005s, episode steps:   1, steps per second: 216, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3633.000 [3633.000, 3633.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   155/50000: episode: 155, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1730.000 [1730.000, 1730.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   156/50000: episode: 156, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3729.000 [3729.000, 3729.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   157/50000: episode: 157, duration: 0.005s, episode steps:   1, steps per second: 198, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   158/50000: episode: 158, duration: 0.004s, episode steps:   1, steps per second: 283, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1885.000 [1885.000, 1885.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   159/50000: episode: 159, duration: 0.005s, episode steps:   1, steps per second: 199, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   160/50000: episode: 160, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   162/50000: episode: 161, duration: 0.018s, episode steps:   2, steps per second: 110, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   163/50000: episode: 162, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2774.000 [2774.000, 2774.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   164/50000: episode: 163, duration: 0.005s, episode steps:   1, steps per second: 196, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2463.000 [2463.000, 2463.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   165/50000: episode: 164, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2010.000 [2010.000, 2010.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   166/50000: episode: 165, duration: 0.003s, episode steps:   1, steps per second: 380, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1414.000 [1414.000, 1414.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   167/50000: episode: 166, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3903.000 [3903.000, 3903.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   168/50000: episode: 167, duration: 0.004s, episode steps:   1, steps per second: 283, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   169/50000: episode: 168, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   170/50000: episode: 169, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3710.000 [3710.000, 3710.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   171/50000: episode: 170, duration: 0.005s, episode steps:   1, steps per second: 198, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   172/50000: episode: 171, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1786.000 [1786.000, 1786.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   173/50000: episode: 172, duration: 0.005s, episode steps:   1, steps per second: 201, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   174/50000: episode: 173, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   175/50000: episode: 174, duration: 0.004s, episode steps:   1, steps per second: 257, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   176/50000: episode: 175, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2463.000 [2463.000, 2463.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   177/50000: episode: 176, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   178/50000: episode: 177, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1668.000 [1668.000, 1668.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   179/50000: episode: 178, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 345.000 [345.000, 345.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   180/50000: episode: 179, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2560.000 [2560.000, 2560.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   181/50000: episode: 180, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   182/50000: episode: 181, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   183/50000: episode: 182, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2194.000 [2194.000, 2194.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   184/50000: episode: 183, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   185/50000: episode: 184, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3343.000 [3343.000, 3343.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   186/50000: episode: 185, duration: 0.024s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   187/50000: episode: 186, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1212.000 [1212.000, 1212.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   188/50000: episode: 187, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 963.000 [963.000, 963.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   189/50000: episode: 188, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   190/50000: episode: 189, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2956.000 [2956.000, 2956.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   191/50000: episode: 190, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   192/50000: episode: 191, duration: 0.004s, episode steps:   1, steps per second: 264, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   193/50000: episode: 192, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   194/50000: episode: 193, duration: 0.005s, episode steps:   1, steps per second: 187, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   195/50000: episode: 194, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1722.000 [1722.000, 1722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   196/50000: episode: 195, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3420.000 [3420.000, 3420.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   197/50000: episode: 196, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1872.000 [1872.000, 1872.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   198/50000: episode: 197, duration: 0.003s, episode steps:   1, steps per second: 316, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   199/50000: episode: 198, duration: 0.006s, episode steps:   1, steps per second: 160, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2842.000 [2842.000, 2842.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   200/50000: episode: 199, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 150.000 [150.000, 150.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   201/50000: episode: 200, duration: 0.006s, episode steps:   1, steps per second: 162, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   202/50000: episode: 201, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1786.000 [1786.000, 1786.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   203/50000: episode: 202, duration: 0.005s, episode steps:   1, steps per second: 188, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1117.000 [1117.000, 1117.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   204/50000: episode: 203, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   205/50000: episode: 204, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 681.000 [681.000, 681.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   206/50000: episode: 205, duration: 0.006s, episode steps:   1, steps per second: 161, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   207/50000: episode: 206, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   208/50000: episode: 207, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1153.000 [1153.000, 1153.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   209/50000: episode: 208, duration: 0.005s, episode steps:   1, steps per second: 182, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   210/50000: episode: 209, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2330.000 [2330.000, 2330.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   211/50000: episode: 210, duration: 0.005s, episode steps:   1, steps per second: 208, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   212/50000: episode: 211, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   214/50000: episode: 212, duration: 0.024s, episode steps:   2, steps per second:  84, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   215/50000: episode: 213, duration: 0.004s, episode steps:   1, steps per second: 232, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   216/50000: episode: 214, duration: 0.004s, episode steps:   1, steps per second: 278, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   217/50000: episode: 215, duration: 0.005s, episode steps:   1, steps per second: 186, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3330.000 [3330.000, 3330.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   218/50000: episode: 216, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3735.000 [3735.000, 3735.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   219/50000: episode: 217, duration: 0.006s, episode steps:   1, steps per second: 177, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   220/50000: episode: 218, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   221/50000: episode: 219, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   222/50000: episode: 220, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   223/50000: episode: 221, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3350.000 [3350.000, 3350.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   224/50000: episode: 222, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   225/50000: episode: 223, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   226/50000: episode: 224, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   227/50000: episode: 225, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   228/50000: episode: 226, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   229/50000: episode: 227, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   230/50000: episode: 228, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   231/50000: episode: 229, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2870.000 [2870.000, 2870.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   232/50000: episode: 230, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1697.000 [1697.000, 1697.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   233/50000: episode: 231, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3084.000 [3084.000, 3084.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   234/50000: episode: 232, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   235/50000: episode: 233, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   236/50000: episode: 234, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   237/50000: episode: 235, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3255.000 [3255.000, 3255.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   239/50000: episode: 236, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2145.500 [1504.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   240/50000: episode: 237, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 186.000 [186.000, 186.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   241/50000: episode: 238, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 270.000 [270.000, 270.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   242/50000: episode: 239, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   243/50000: episode: 240, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   244/50000: episode: 241, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   245/50000: episode: 242, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   246/50000: episode: 243, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   247/50000: episode: 244, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   248/50000: episode: 245, duration: 0.010s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1758.000 [1758.000, 1758.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   249/50000: episode: 246, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   250/50000: episode: 247, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   251/50000: episode: 248, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3831.000 [3831.000, 3831.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   252/50000: episode: 249, duration: 0.006s, episode steps:   1, steps per second: 163, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 238.000 [238.000, 238.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   253/50000: episode: 250, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2738.000 [2738.000, 2738.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   254/50000: episode: 251, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2010.000 [2010.000, 2010.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   255/50000: episode: 252, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2722.000 [2722.000, 2722.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   256/50000: episode: 253, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1916.000 [1916.000, 1916.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   257/50000: episode: 254, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   258/50000: episode: 255, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   259/50000: episode: 256, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3056.000 [3056.000, 3056.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   260/50000: episode: 257, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3436.000 [3436.000, 3436.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   261/50000: episode: 258, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   262/50000: episode: 259, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2110.000 [2110.000, 2110.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   263/50000: episode: 260, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2198.000 [2198.000, 2198.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   264/50000: episode: 261, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   265/50000: episode: 262, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 374.000 [374.000, 374.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   266/50000: episode: 263, duration: 0.004s, episode steps:   1, steps per second: 252, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   267/50000: episode: 264, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1138.000 [1138.000, 1138.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   268/50000: episode: 265, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1611.000 [1611.000, 1611.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   269/50000: episode: 266, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   270/50000: episode: 267, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3454.000 [3454.000, 3454.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   271/50000: episode: 268, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 302.000 [302.000, 302.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   272/50000: episode: 269, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   273/50000: episode: 270, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   274/50000: episode: 271, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   275/50000: episode: 272, duration: 0.005s, episode steps:   1, steps per second: 201, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   276/50000: episode: 273, duration: 0.004s, episode steps:   1, steps per second: 265, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   277/50000: episode: 274, duration: 0.003s, episode steps:   1, steps per second: 299, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3013.000 [3013.000, 3013.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   278/50000: episode: 275, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   279/50000: episode: 276, duration: 0.004s, episode steps:   1, steps per second: 230, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   280/50000: episode: 277, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3735.000 [3735.000, 3735.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   281/50000: episode: 278, duration: 0.006s, episode steps:   1, steps per second: 163, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1757.000 [1757.000, 1757.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   282/50000: episode: 279, duration: 0.005s, episode steps:   1, steps per second: 192, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   283/50000: episode: 280, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   284/50000: episode: 281, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1274.000 [1274.000, 1274.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   285/50000: episode: 282, duration: 0.005s, episode steps:   1, steps per second: 186, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   286/50000: episode: 283, duration: 0.005s, episode steps:   1, steps per second: 205, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 371.000 [371.000, 371.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   287/50000: episode: 284, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1175.000 [1175.000, 1175.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   288/50000: episode: 285, duration: 0.006s, episode steps:   1, steps per second: 159, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   289/50000: episode: 286, duration: 0.004s, episode steps:   1, steps per second: 241, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   290/50000: episode: 287, duration: 0.005s, episode steps:   1, steps per second: 203, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   291/50000: episode: 288, duration: 0.004s, episode steps:   1, steps per second: 253, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3802.000 [3802.000, 3802.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   292/50000: episode: 289, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1213.000 [1213.000, 1213.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   293/50000: episode: 290, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 526.000 [526.000, 526.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   294/50000: episode: 291, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3831.000 [3831.000, 3831.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   295/50000: episode: 292, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1446.000 [1446.000, 1446.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   296/50000: episode: 293, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 625.000 [625.000, 625.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   297/50000: episode: 294, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   298/50000: episode: 295, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 98.000 [98.000, 98.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   299/50000: episode: 296, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   300/50000: episode: 297, duration: 0.006s, episode steps:   1, steps per second: 182, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   301/50000: episode: 298, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   302/50000: episode: 299, duration: 0.004s, episode steps:   1, steps per second: 265, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3179.000 [3179.000, 3179.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   303/50000: episode: 300, duration: 0.003s, episode steps:   1, steps per second: 363, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   304/50000: episode: 301, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1611.000 [1611.000, 1611.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   305/50000: episode: 302, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   306/50000: episode: 303, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   307/50000: episode: 304, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1021.000 [1021.000, 1021.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   308/50000: episode: 305, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   309/50000: episode: 306, duration: 0.003s, episode steps:   1, steps per second: 298, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   310/50000: episode: 307, duration: 0.005s, episode steps:   1, steps per second: 190, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3240.000 [3240.000, 3240.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   311/50000: episode: 308, duration: 0.003s, episode steps:   1, steps per second: 340, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1404.000 [1404.000, 1404.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   312/50000: episode: 309, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3988.000 [3988.000, 3988.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   313/50000: episode: 310, duration: 0.007s, episode steps:   1, steps per second: 134, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   314/50000: episode: 311, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1197.000 [1197.000, 1197.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   315/50000: episode: 312, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2010.000 [2010.000, 2010.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   316/50000: episode: 313, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   317/50000: episode: 314, duration: 0.007s, episode steps:   1, steps per second: 153, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1849.000 [1849.000, 1849.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   318/50000: episode: 315, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1720.000 [1720.000, 1720.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   319/50000: episode: 316, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1057.000 [1057.000, 1057.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   320/50000: episode: 317, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   321/50000: episode: 318, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2011.000 [2011.000, 2011.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   322/50000: episode: 319, duration: 0.005s, episode steps:   1, steps per second: 182, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   323/50000: episode: 320, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3725.000 [3725.000, 3725.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   324/50000: episode: 321, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   325/50000: episode: 322, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 756.000 [756.000, 756.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   326/50000: episode: 323, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1872.000 [1872.000, 1872.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   327/50000: episode: 324, duration: 0.006s, episode steps:   1, steps per second: 165, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   328/50000: episode: 325, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   329/50000: episode: 326, duration: 0.006s, episode steps:   1, steps per second: 162, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   330/50000: episode: 327, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1611.000 [1611.000, 1611.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   331/50000: episode: 328, duration: 0.008s, episode steps:   1, steps per second: 133, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   332/50000: episode: 329, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2020.000 [2020.000, 2020.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   333/50000: episode: 330, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3431.000 [3431.000, 3431.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   334/50000: episode: 331, duration: 0.005s, episode steps:   1, steps per second: 216, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   335/50000: episode: 332, duration: 0.005s, episode steps:   1, steps per second: 207, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   336/50000: episode: 333, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2010.000 [2010.000, 2010.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   337/50000: episode: 334, duration: 0.006s, episode steps:   1, steps per second: 158, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3420.000 [3420.000, 3420.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   338/50000: episode: 335, duration: 0.004s, episode steps:   1, steps per second: 245, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   339/50000: episode: 336, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   340/50000: episode: 337, duration: 0.004s, episode steps:   1, steps per second: 261, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3076.000 [3076.000, 3076.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   341/50000: episode: 338, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   342/50000: episode: 339, duration: 0.007s, episode steps:   1, steps per second: 145, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1548.000 [1548.000, 1548.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   343/50000: episode: 340, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   344/50000: episode: 341, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3825.000 [3825.000, 3825.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   346/50000: episode: 342, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2364.500 [782.000, 3947.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   347/50000: episode: 343, duration: 0.003s, episode steps:   1, steps per second: 317, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   348/50000: episode: 344, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3831.000 [3831.000, 3831.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   349/50000: episode: 345, duration: 0.006s, episode steps:   1, steps per second: 173, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   350/50000: episode: 346, duration: 0.005s, episode steps:   1, steps per second: 192, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   351/50000: episode: 347, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   352/50000: episode: 348, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1993.000 [1993.000, 1993.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   353/50000: episode: 349, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1786.000 [1786.000, 1786.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   354/50000: episode: 350, duration: 0.004s, episode steps:   1, steps per second: 273, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   355/50000: episode: 351, duration: 0.004s, episode steps:   1, steps per second: 260, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   356/50000: episode: 352, duration: 0.006s, episode steps:   1, steps per second: 158, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   357/50000: episode: 353, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   358/50000: episode: 354, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2334.000 [2334.000, 2334.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   359/50000: episode: 355, duration: 0.003s, episode steps:   1, steps per second: 322, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   360/50000: episode: 356, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1786.000 [1786.000, 1786.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   361/50000: episode: 357, duration: 0.004s, episode steps:   1, steps per second: 275, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   362/50000: episode: 358, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2692.000 [2692.000, 2692.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   363/50000: episode: 359, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1742.000 [1742.000, 1742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   364/50000: episode: 360, duration: 0.004s, episode steps:   1, steps per second: 259, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 84.000 [84.000, 84.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   365/50000: episode: 361, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2719.000 [2719.000, 2719.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   366/50000: episode: 362, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2429.000 [2429.000, 2429.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   368/50000: episode: 363, duration: 0.017s, episode steps:   2, steps per second: 118, episode reward: -4971.000, mean reward: -2485.500 [-5000.000, 29.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   369/50000: episode: 364, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 31.000 [31.000, 31.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   370/50000: episode: 365, duration: 0.004s, episode steps:   1, steps per second: 240, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   371/50000: episode: 366, duration: 0.005s, episode steps:   1, steps per second: 216, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2676.000 [2676.000, 2676.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   372/50000: episode: 367, duration: 0.007s, episode steps:   1, steps per second: 140, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3773.000 [3773.000, 3773.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   373/50000: episode: 368, duration: 0.009s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   374/50000: episode: 369, duration: 0.006s, episode steps:   1, steps per second: 159, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   375/50000: episode: 370, duration: 0.007s, episode steps:   1, steps per second: 138, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   376/50000: episode: 371, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   377/50000: episode: 372, duration: 0.005s, episode steps:   1, steps per second: 193, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   378/50000: episode: 373, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   379/50000: episode: 374, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2010.000 [2010.000, 2010.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   380/50000: episode: 375, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   381/50000: episode: 376, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 27.000 [27.000, 27.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   382/50000: episode: 377, duration: 0.006s, episode steps:   1, steps per second: 165, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2085.000 [2085.000, 2085.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   383/50000: episode: 378, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3655.000 [3655.000, 3655.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   384/50000: episode: 379, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   385/50000: episode: 380, duration: 0.004s, episode steps:   1, steps per second: 261, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   386/50000: episode: 381, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3394.000 [3394.000, 3394.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   387/50000: episode: 382, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2391.000 [2391.000, 2391.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   388/50000: episode: 383, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 448.000 [448.000, 448.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   389/50000: episode: 384, duration: 0.004s, episode steps:   1, steps per second: 264, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   390/50000: episode: 385, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   391/50000: episode: 386, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3708.000 [3708.000, 3708.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   392/50000: episode: 387, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 224.000 [224.000, 224.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   393/50000: episode: 388, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   394/50000: episode: 389, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1325.000 [1325.000, 1325.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   395/50000: episode: 390, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   396/50000: episode: 391, duration: 0.004s, episode steps:   1, steps per second: 278, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   397/50000: episode: 392, duration: 0.007s, episode steps:   1, steps per second: 147, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 765.000 [765.000, 765.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   398/50000: episode: 393, duration: 0.007s, episode steps:   1, steps per second: 134, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   399/50000: episode: 394, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   400/50000: episode: 395, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   401/50000: episode: 396, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   402/50000: episode: 397, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   403/50000: episode: 398, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1230.000 [1230.000, 1230.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   404/50000: episode: 399, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1402.000 [1402.000, 1402.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   405/50000: episode: 400, duration: 0.006s, episode steps:   1, steps per second: 156, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   406/50000: episode: 401, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   407/50000: episode: 402, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   408/50000: episode: 403, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   409/50000: episode: 404, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   410/50000: episode: 405, duration: 0.007s, episode steps:   1, steps per second: 151, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   411/50000: episode: 406, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   412/50000: episode: 407, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1138.000 [1138.000, 1138.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   413/50000: episode: 408, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2010.000 [2010.000, 2010.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   414/50000: episode: 409, duration: 0.004s, episode steps:   1, steps per second: 251, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   415/50000: episode: 410, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3723.000 [3723.000, 3723.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   416/50000: episode: 411, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   417/50000: episode: 412, duration: 0.005s, episode steps:   1, steps per second: 199, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   418/50000: episode: 413, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2965.000 [2965.000, 2965.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   419/50000: episode: 414, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 727.000 [727.000, 727.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   420/50000: episode: 415, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   421/50000: episode: 416, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3653.000 [3653.000, 3653.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   422/50000: episode: 417, duration: 0.007s, episode steps:   1, steps per second: 140, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   423/50000: episode: 418, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   424/50000: episode: 419, duration: 0.006s, episode steps:   1, steps per second: 172, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3303.000 [3303.000, 3303.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   425/50000: episode: 420, duration: 0.004s, episode steps:   1, steps per second: 266, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   426/50000: episode: 421, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1450.000 [1450.000, 1450.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   427/50000: episode: 422, duration: 0.006s, episode steps:   1, steps per second: 175, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   428/50000: episode: 423, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   429/50000: episode: 424, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   430/50000: episode: 425, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   431/50000: episode: 426, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3106.000 [3106.000, 3106.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   432/50000: episode: 427, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   433/50000: episode: 428, duration: 0.004s, episode steps:   1, steps per second: 249, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   434/50000: episode: 429, duration: 0.007s, episode steps:   1, steps per second: 134, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2890.000 [2890.000, 2890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   435/50000: episode: 430, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1299.000 [1299.000, 1299.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   436/50000: episode: 431, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 284.000 [284.000, 284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   437/50000: episode: 432, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1871.000 [1871.000, 1871.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   438/50000: episode: 433, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2838.000 [2838.000, 2838.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   439/50000: episode: 434, duration: 0.003s, episode steps:   1, steps per second: 288, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   440/50000: episode: 435, duration: 0.005s, episode steps:   1, steps per second: 191, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   441/50000: episode: 436, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1908.000 [1908.000, 1908.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   442/50000: episode: 437, duration: 0.003s, episode steps:   1, steps per second: 336, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   443/50000: episode: 438, duration: 0.003s, episode steps:   1, steps per second: 297, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   444/50000: episode: 439, duration: 0.004s, episode steps:   1, steps per second: 258, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   445/50000: episode: 440, duration: 0.007s, episode steps:   1, steps per second: 146, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   446/50000: episode: 441, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   447/50000: episode: 442, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   448/50000: episode: 443, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   449/50000: episode: 444, duration: 0.004s, episode steps:   1, steps per second: 254, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 113.000 [113.000, 113.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   450/50000: episode: 445, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4073.000 [4073.000, 4073.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   451/50000: episode: 446, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   452/50000: episode: 447, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 752.000 [752.000, 752.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   453/50000: episode: 448, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   454/50000: episode: 449, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   455/50000: episode: 450, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   456/50000: episode: 451, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   457/50000: episode: 452, duration: 0.007s, episode steps:   1, steps per second: 140, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   458/50000: episode: 453, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   459/50000: episode: 454, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 643.000 [643.000, 643.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   460/50000: episode: 455, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1908.000 [1908.000, 1908.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   461/50000: episode: 456, duration: 0.006s, episode steps:   1, steps per second: 167, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   462/50000: episode: 457, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2198.000 [2198.000, 2198.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   463/50000: episode: 458, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   464/50000: episode: 459, duration: 0.005s, episode steps:   1, steps per second: 205, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   465/50000: episode: 460, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1778.000 [1778.000, 1778.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   466/50000: episode: 461, duration: 0.003s, episode steps:   1, steps per second: 331, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1289.000 [1289.000, 1289.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   467/50000: episode: 462, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   468/50000: episode: 463, duration: 0.006s, episode steps:   1, steps per second: 164, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3420.000 [3420.000, 3420.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   469/50000: episode: 464, duration: 0.007s, episode steps:   1, steps per second: 151, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   470/50000: episode: 465, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   471/50000: episode: 466, duration: 0.004s, episode steps:   1, steps per second: 223, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   472/50000: episode: 467, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   473/50000: episode: 468, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2275.000 [2275.000, 2275.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   474/50000: episode: 469, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2560.000 [2560.000, 2560.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   475/50000: episode: 470, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 689.000 [689.000, 689.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   476/50000: episode: 471, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   477/50000: episode: 472, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   478/50000: episode: 473, duration: 0.003s, episode steps:   1, steps per second: 303, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   480/50000: episode: 474, duration: 0.023s, episode steps:   2, steps per second:  88, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   481/50000: episode: 475, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1720.000 [1720.000, 1720.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   482/50000: episode: 476, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   483/50000: episode: 477, duration: 0.005s, episode steps:   1, steps per second: 206, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   484/50000: episode: 478, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   485/50000: episode: 479, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3247.000 [3247.000, 3247.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   486/50000: episode: 480, duration: 0.004s, episode steps:   1, steps per second: 258, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1590.000 [1590.000, 1590.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   487/50000: episode: 481, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2010.000 [2010.000, 2010.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   488/50000: episode: 482, duration: 0.004s, episode steps:   1, steps per second: 236, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   489/50000: episode: 483, duration: 0.005s, episode steps:   1, steps per second: 200, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1933.000 [1933.000, 1933.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   490/50000: episode: 484, duration: 0.006s, episode steps:   1, steps per second: 156, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   491/50000: episode: 485, duration: 0.004s, episode steps:   1, steps per second: 232, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   492/50000: episode: 486, duration: 0.004s, episode steps:   1, steps per second: 225, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   493/50000: episode: 487, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1541.000 [1541.000, 1541.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   494/50000: episode: 488, duration: 0.006s, episode steps:   1, steps per second: 162, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3358.000 [3358.000, 3358.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   495/50000: episode: 489, duration: 0.006s, episode steps:   1, steps per second: 168, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   496/50000: episode: 490, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   497/50000: episode: 491, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   498/50000: episode: 492, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1611.000 [1611.000, 1611.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   499/50000: episode: 493, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   500/50000: episode: 494, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3409.000 [3409.000, 3409.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   501/50000: episode: 495, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 462.000 [462.000, 462.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   502/50000: episode: 496, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   503/50000: episode: 497, duration: 0.005s, episode steps:   1, steps per second: 183, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   504/50000: episode: 498, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2541.000 [2541.000, 2541.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   505/50000: episode: 499, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   506/50000: episode: 500, duration: 0.005s, episode steps:   1, steps per second: 201, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2470.000 [2470.000, 2470.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   507/50000: episode: 501, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   508/50000: episode: 502, duration: 0.006s, episode steps:   1, steps per second: 181, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3998.000 [3998.000, 3998.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   509/50000: episode: 503, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   510/50000: episode: 504, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2144.000 [2144.000, 2144.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   511/50000: episode: 505, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3012.000 [3012.000, 3012.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   512/50000: episode: 506, duration: 0.005s, episode steps:   1, steps per second: 219, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3822.000 [3822.000, 3822.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   513/50000: episode: 507, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   514/50000: episode: 508, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   515/50000: episode: 509, duration: 0.004s, episode steps:   1, steps per second: 258, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   516/50000: episode: 510, duration: 0.004s, episode steps:   1, steps per second: 232, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   517/50000: episode: 511, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1720.000 [1720.000, 1720.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   518/50000: episode: 512, duration: 0.007s, episode steps:   1, steps per second: 154, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   519/50000: episode: 513, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   520/50000: episode: 514, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   521/50000: episode: 515, duration: 0.006s, episode steps:   1, steps per second: 155, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   522/50000: episode: 516, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 238.000 [238.000, 238.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   523/50000: episode: 517, duration: 0.007s, episode steps:   1, steps per second: 147, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   524/50000: episode: 518, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2694.000 [2694.000, 2694.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   525/50000: episode: 519, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 145.000 [145.000, 145.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   526/50000: episode: 520, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   527/50000: episode: 521, duration: 0.005s, episode steps:   1, steps per second: 220, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   528/50000: episode: 522, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 943.000 [943.000, 943.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   529/50000: episode: 523, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   530/50000: episode: 524, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   531/50000: episode: 525, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2250.000 [2250.000, 2250.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   532/50000: episode: 526, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3072.000 [3072.000, 3072.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   533/50000: episode: 527, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3544.000 [3544.000, 3544.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   534/50000: episode: 528, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3127.000 [3127.000, 3127.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   535/50000: episode: 529, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3729.000 [3729.000, 3729.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   536/50000: episode: 530, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   537/50000: episode: 531, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   538/50000: episode: 532, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   539/50000: episode: 533, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 512.000 [512.000, 512.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   540/50000: episode: 534, duration: 0.006s, episode steps:   1, steps per second: 173, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   541/50000: episode: 535, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   542/50000: episode: 536, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2010.000 [2010.000, 2010.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   543/50000: episode: 537, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3433.000 [3433.000, 3433.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   544/50000: episode: 538, duration: 0.007s, episode steps:   1, steps per second: 151, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   545/50000: episode: 539, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   546/50000: episode: 540, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   547/50000: episode: 541, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   548/50000: episode: 542, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2977.000 [2977.000, 2977.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   549/50000: episode: 543, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   550/50000: episode: 544, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   551/50000: episode: 545, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2883.000 [2883.000, 2883.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   552/50000: episode: 546, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   553/50000: episode: 547, duration: 0.007s, episode steps:   1, steps per second: 134, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   554/50000: episode: 548, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2010.000 [2010.000, 2010.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   555/50000: episode: 549, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   556/50000: episode: 550, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1778.000 [1778.000, 1778.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   557/50000: episode: 551, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 673.000 [673.000, 673.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   558/50000: episode: 552, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   559/50000: episode: 553, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 123.000 [123.000, 123.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   560/50000: episode: 554, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   561/50000: episode: 555, duration: 0.005s, episode steps:   1, steps per second: 203, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   562/50000: episode: 556, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1611.000 [1611.000, 1611.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   563/50000: episode: 557, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2010.000 [2010.000, 2010.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   564/50000: episode: 558, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   565/50000: episode: 559, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3078.000 [3078.000, 3078.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   566/50000: episode: 560, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   567/50000: episode: 561, duration: 0.004s, episode steps:   1, steps per second: 229, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   568/50000: episode: 562, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   569/50000: episode: 563, duration: 0.007s, episode steps:   1, steps per second: 145, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   570/50000: episode: 564, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 750.000 [750.000, 750.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   571/50000: episode: 565, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   572/50000: episode: 566, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   573/50000: episode: 567, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1720.000 [1720.000, 1720.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   574/50000: episode: 568, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1872.000 [1872.000, 1872.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   575/50000: episode: 569, duration: 0.004s, episode steps:   1, steps per second: 268, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   576/50000: episode: 570, duration: 0.004s, episode steps:   1, steps per second: 255, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   577/50000: episode: 571, duration: 0.005s, episode steps:   1, steps per second: 214, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   578/50000: episode: 572, duration: 0.007s, episode steps:   1, steps per second: 150, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1910.000 [1910.000, 1910.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   579/50000: episode: 573, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   580/50000: episode: 574, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   581/50000: episode: 575, duration: 0.006s, episode steps:   1, steps per second: 163, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   582/50000: episode: 576, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   583/50000: episode: 577, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   584/50000: episode: 578, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   585/50000: episode: 579, duration: 0.004s, episode steps:   1, steps per second: 261, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   586/50000: episode: 580, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   587/50000: episode: 581, duration: 0.003s, episode steps:   1, steps per second: 352, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1731.000 [1731.000, 1731.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   588/50000: episode: 582, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2977.000 [2977.000, 2977.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   589/50000: episode: 583, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   590/50000: episode: 584, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1373.000 [1373.000, 1373.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   591/50000: episode: 585, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3295.000 [3295.000, 3295.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   592/50000: episode: 586, duration: 0.004s, episode steps:   1, steps per second: 231, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2990.000 [2990.000, 2990.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   593/50000: episode: 587, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3871.000 [3871.000, 3871.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   594/50000: episode: 588, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   595/50000: episode: 589, duration: 0.005s, episode steps:   1, steps per second: 191, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   596/50000: episode: 590, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2232.000 [2232.000, 2232.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   597/50000: episode: 591, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   598/50000: episode: 592, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   599/50000: episode: 593, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   600/50000: episode: 594, duration: 0.003s, episode steps:   1, steps per second: 301, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1845.000 [1845.000, 1845.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   601/50000: episode: 595, duration: 0.006s, episode steps:   1, steps per second: 168, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3812.000 [3812.000, 3812.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   602/50000: episode: 596, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1611.000 [1611.000, 1611.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   603/50000: episode: 597, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 731.000 [731.000, 731.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   604/50000: episode: 598, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   605/50000: episode: 599, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   606/50000: episode: 600, duration: 0.005s, episode steps:   1, steps per second: 184, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   607/50000: episode: 601, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   608/50000: episode: 602, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3319.000 [3319.000, 3319.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   609/50000: episode: 603, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1444.000 [1444.000, 1444.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   610/50000: episode: 604, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   611/50000: episode: 605, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   612/50000: episode: 606, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   613/50000: episode: 607, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   614/50000: episode: 608, duration: 0.005s, episode steps:   1, steps per second: 193, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1891.000 [1891.000, 1891.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   615/50000: episode: 609, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2628.000 [2628.000, 2628.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   616/50000: episode: 610, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   617/50000: episode: 611, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   618/50000: episode: 612, duration: 0.007s, episode steps:   1, steps per second: 145, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2556.000 [2556.000, 2556.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   619/50000: episode: 613, duration: 0.004s, episode steps:   1, steps per second: 238, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1378.000 [1378.000, 1378.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   620/50000: episode: 614, duration: 0.003s, episode steps:   1, steps per second: 348, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2598.000 [2598.000, 2598.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   621/50000: episode: 615, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   622/50000: episode: 616, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   623/50000: episode: 617, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   624/50000: episode: 618, duration: 0.004s, episode steps:   1, steps per second: 234, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1091.000 [1091.000, 1091.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   625/50000: episode: 619, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   626/50000: episode: 620, duration: 0.003s, episode steps:   1, steps per second: 334, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   627/50000: episode: 621, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3024.000 [3024.000, 3024.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   628/50000: episode: 622, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2198.000 [2198.000, 2198.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   629/50000: episode: 623, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   630/50000: episode: 624, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   631/50000: episode: 625, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   632/50000: episode: 626, duration: 0.005s, episode steps:   1, steps per second: 218, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 890.000 [890.000, 890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   633/50000: episode: 627, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 226.000 [226.000, 226.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   634/50000: episode: 628, duration: 0.006s, episode steps:   1, steps per second: 167, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   635/50000: episode: 629, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1138.000 [1138.000, 1138.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   636/50000: episode: 630, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1063.000 [1063.000, 1063.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   637/50000: episode: 631, duration: 0.006s, episode steps:   1, steps per second: 181, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   638/50000: episode: 632, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   639/50000: episode: 633, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2724.000 [2724.000, 2724.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   640/50000: episode: 634, duration: 0.005s, episode steps:   1, steps per second: 205, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1243.000 [1243.000, 1243.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   641/50000: episode: 635, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   642/50000: episode: 636, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   643/50000: episode: 637, duration: 0.004s, episode steps:   1, steps per second: 272, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   644/50000: episode: 638, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   645/50000: episode: 639, duration: 0.006s, episode steps:   1, steps per second: 169, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   646/50000: episode: 640, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   647/50000: episode: 641, duration: 0.004s, episode steps:   1, steps per second: 231, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   648/50000: episode: 642, duration: 0.005s, episode steps:   1, steps per second: 215, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3125.000 [3125.000, 3125.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   649/50000: episode: 643, duration: 0.005s, episode steps:   1, steps per second: 183, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 195.000 [195.000, 195.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   650/50000: episode: 644, duration: 0.007s, episode steps:   1, steps per second: 145, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   651/50000: episode: 645, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   652/50000: episode: 646, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 893.000 [893.000, 893.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   653/50000: episode: 647, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   654/50000: episode: 648, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2042.000 [2042.000, 2042.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   655/50000: episode: 649, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3540.000 [3540.000, 3540.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   656/50000: episode: 650, duration: 0.005s, episode steps:   1, steps per second: 195, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3799.000 [3799.000, 3799.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   657/50000: episode: 651, duration: 0.005s, episode steps:   1, steps per second: 190, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 185.000 [185.000, 185.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   658/50000: episode: 652, duration: 0.004s, episode steps:   1, steps per second: 232, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   659/50000: episode: 653, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   660/50000: episode: 654, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1138.000 [1138.000, 1138.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   661/50000: episode: 655, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   662/50000: episode: 656, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1229.000 [1229.000, 1229.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   663/50000: episode: 657, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1460.000 [1460.000, 1460.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   664/50000: episode: 658, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   665/50000: episode: 659, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   666/50000: episode: 660, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   667/50000: episode: 661, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   668/50000: episode: 662, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   669/50000: episode: 663, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3586.000 [3586.000, 3586.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   670/50000: episode: 664, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2975.000 [2975.000, 2975.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   671/50000: episode: 665, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   672/50000: episode: 666, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   673/50000: episode: 667, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   674/50000: episode: 668, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1138.000 [1138.000, 1138.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   675/50000: episode: 669, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2850.000 [2850.000, 2850.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   676/50000: episode: 670, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 80.000 [80.000, 80.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   677/50000: episode: 671, duration: 0.004s, episode steps:   1, steps per second: 227, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   678/50000: episode: 672, duration: 0.006s, episode steps:   1, steps per second: 162, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 606.000 [606.000, 606.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   679/50000: episode: 673, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3161.000 [3161.000, 3161.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   680/50000: episode: 674, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   681/50000: episode: 675, duration: 0.004s, episode steps:   1, steps per second: 263, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   682/50000: episode: 676, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 305.000 [305.000, 305.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   683/50000: episode: 677, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2010.000 [2010.000, 2010.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   684/50000: episode: 678, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1611.000 [1611.000, 1611.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   685/50000: episode: 679, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3580.000 [3580.000, 3580.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   686/50000: episode: 680, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 411.000 [411.000, 411.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   687/50000: episode: 681, duration: 0.007s, episode steps:   1, steps per second: 151, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   688/50000: episode: 682, duration: 0.006s, episode steps:   1, steps per second: 157, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   689/50000: episode: 683, duration: 0.004s, episode steps:   1, steps per second: 280, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   690/50000: episode: 684, duration: 0.006s, episode steps:   1, steps per second: 160, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   691/50000: episode: 685, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2109.000 [2109.000, 2109.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   692/50000: episode: 686, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3549.000 [3549.000, 3549.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   693/50000: episode: 687, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   694/50000: episode: 688, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   695/50000: episode: 689, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   696/50000: episode: 690, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 84.000 [84.000, 84.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   697/50000: episode: 691, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   698/50000: episode: 692, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   699/50000: episode: 693, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   700/50000: episode: 694, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   701/50000: episode: 695, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1138.000 [1138.000, 1138.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   702/50000: episode: 696, duration: 0.004s, episode steps:   1, steps per second: 250, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3657.000 [3657.000, 3657.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   703/50000: episode: 697, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1850.000 [1850.000, 1850.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   704/50000: episode: 698, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   705/50000: episode: 699, duration: 0.004s, episode steps:   1, steps per second: 267, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3658.000 [3658.000, 3658.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   706/50000: episode: 700, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 375.000 [375.000, 375.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   707/50000: episode: 701, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   708/50000: episode: 702, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 242.000 [242.000, 242.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   709/50000: episode: 703, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   710/50000: episode: 704, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3192.000 [3192.000, 3192.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   711/50000: episode: 705, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   712/50000: episode: 706, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   713/50000: episode: 707, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   714/50000: episode: 708, duration: 0.006s, episode steps:   1, steps per second: 160, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   715/50000: episode: 709, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   716/50000: episode: 710, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1173.000 [1173.000, 1173.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   717/50000: episode: 711, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   718/50000: episode: 712, duration: 0.003s, episode steps:   1, steps per second: 301, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3362.000 [3362.000, 3362.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   719/50000: episode: 713, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2369.000 [2369.000, 2369.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   720/50000: episode: 714, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3028.000 [3028.000, 3028.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   721/50000: episode: 715, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 294.000 [294.000, 294.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   722/50000: episode: 716, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   723/50000: episode: 717, duration: 0.003s, episode steps:   1, steps per second: 286, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 205.000 [205.000, 205.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   724/50000: episode: 718, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   725/50000: episode: 719, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2301.000 [2301.000, 2301.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   726/50000: episode: 720, duration: 0.006s, episode steps:   1, steps per second: 176, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   727/50000: episode: 721, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1606.000 [1606.000, 1606.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   728/50000: episode: 722, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   729/50000: episode: 723, duration: 0.005s, episode steps:   1, steps per second: 188, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1060.000 [1060.000, 1060.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   730/50000: episode: 724, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   731/50000: episode: 725, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   732/50000: episode: 726, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1416.000 [1416.000, 1416.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   733/50000: episode: 727, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3082.000 [3082.000, 3082.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   734/50000: episode: 728, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   735/50000: episode: 729, duration: 0.005s, episode steps:   1, steps per second: 200, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3208.000 [3208.000, 3208.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   736/50000: episode: 730, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3735.000 [3735.000, 3735.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   737/50000: episode: 731, duration: 0.003s, episode steps:   1, steps per second: 296, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2074.000 [2074.000, 2074.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   738/50000: episode: 732, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   739/50000: episode: 733, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3223.000 [3223.000, 3223.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   740/50000: episode: 734, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   741/50000: episode: 735, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3631.000 [3631.000, 3631.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   742/50000: episode: 736, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1703.000 [1703.000, 1703.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   743/50000: episode: 737, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   744/50000: episode: 738, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 156.000 [156.000, 156.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   745/50000: episode: 739, duration: 0.005s, episode steps:   1, steps per second: 198, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2155.000 [2155.000, 2155.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   746/50000: episode: 740, duration: 0.005s, episode steps:   1, steps per second: 197, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   747/50000: episode: 741, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1720.000 [1720.000, 1720.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   748/50000: episode: 742, duration: 0.006s, episode steps:   1, steps per second: 173, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   749/50000: episode: 743, duration: 0.005s, episode steps:   1, steps per second: 198, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   750/50000: episode: 744, duration: 0.004s, episode steps:   1, steps per second: 240, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   751/50000: episode: 745, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 312.000 [312.000, 312.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   752/50000: episode: 746, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   753/50000: episode: 747, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1057.000 [1057.000, 1057.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   754/50000: episode: 748, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1561.000 [1561.000, 1561.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   755/50000: episode: 749, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   756/50000: episode: 750, duration: 0.005s, episode steps:   1, steps per second: 202, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2507.000 [2507.000, 2507.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   757/50000: episode: 751, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   758/50000: episode: 752, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   759/50000: episode: 753, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   760/50000: episode: 754, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 576.000 [576.000, 576.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   761/50000: episode: 755, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3831.000 [3831.000, 3831.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   762/50000: episode: 756, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   763/50000: episode: 757, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   764/50000: episode: 758, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   765/50000: episode: 759, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1720.000 [1720.000, 1720.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   766/50000: episode: 760, duration: 0.006s, episode steps:   1, steps per second: 168, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   767/50000: episode: 761, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1259.000 [1259.000, 1259.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   768/50000: episode: 762, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   769/50000: episode: 763, duration: 0.007s, episode steps:   1, steps per second: 151, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2752.000 [2752.000, 2752.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   770/50000: episode: 764, duration: 0.005s, episode steps:   1, steps per second: 218, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   771/50000: episode: 765, duration: 0.006s, episode steps:   1, steps per second: 174, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1707.000 [1707.000, 1707.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   772/50000: episode: 766, duration: 0.006s, episode steps:   1, steps per second: 174, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   773/50000: episode: 767, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1720.000 [1720.000, 1720.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   774/50000: episode: 768, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   775/50000: episode: 769, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2996.000 [2996.000, 2996.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   776/50000: episode: 770, duration: 0.004s, episode steps:   1, steps per second: 233, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 860.000 [860.000, 860.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   777/50000: episode: 771, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3569.000 [3569.000, 3569.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   778/50000: episode: 772, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   779/50000: episode: 773, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2385.000 [2385.000, 2385.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   780/50000: episode: 774, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   781/50000: episode: 775, duration: 0.005s, episode steps:   1, steps per second: 216, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 57.000 [57.000, 57.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   782/50000: episode: 776, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3420.000 [3420.000, 3420.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   783/50000: episode: 777, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1611.000 [1611.000, 1611.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   784/50000: episode: 778, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3713.000 [3713.000, 3713.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   785/50000: episode: 779, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   786/50000: episode: 780, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3420.000 [3420.000, 3420.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   787/50000: episode: 781, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3.000 [3.000, 3.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   788/50000: episode: 782, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1965.000 [1965.000, 1965.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   789/50000: episode: 783, duration: 0.006s, episode steps:   1, steps per second: 175, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3746.000 [3746.000, 3746.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   790/50000: episode: 784, duration: 0.005s, episode steps:   1, steps per second: 184, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1408.000 [1408.000, 1408.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   791/50000: episode: 785, duration: 0.005s, episode steps:   1, steps per second: 182, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2942.000 [2942.000, 2942.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   792/50000: episode: 786, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1778.000 [1778.000, 1778.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   793/50000: episode: 787, duration: 0.003s, episode steps:   1, steps per second: 296, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   794/50000: episode: 788, duration: 0.005s, episode steps:   1, steps per second: 201, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 356.000 [356.000, 356.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   795/50000: episode: 789, duration: 0.004s, episode steps:   1, steps per second: 239, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2977.000 [2977.000, 2977.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   796/50000: episode: 790, duration: 0.006s, episode steps:   1, steps per second: 169, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3522.000 [3522.000, 3522.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   797/50000: episode: 791, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2810.000 [2810.000, 2810.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   798/50000: episode: 792, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   799/50000: episode: 793, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 164.000 [164.000, 164.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   800/50000: episode: 794, duration: 0.005s, episode steps:   1, steps per second: 220, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   801/50000: episode: 795, duration: 0.004s, episode steps:   1, steps per second: 224, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 518.000 [518.000, 518.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   802/50000: episode: 796, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   803/50000: episode: 797, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   804/50000: episode: 798, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1009.000 [1009.000, 1009.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   805/50000: episode: 799, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   806/50000: episode: 800, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4000.000 [4000.000, 4000.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   807/50000: episode: 801, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2845.000 [2845.000, 2845.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   808/50000: episode: 802, duration: 0.005s, episode steps:   1, steps per second: 184, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   809/50000: episode: 803, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   810/50000: episode: 804, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2120.000 [2120.000, 2120.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   811/50000: episode: 805, duration: 0.003s, episode steps:   1, steps per second: 290, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3975.000 [3975.000, 3975.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   812/50000: episode: 806, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1173.000 [1173.000, 1173.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   813/50000: episode: 807, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   814/50000: episode: 808, duration: 0.007s, episode steps:   1, steps per second: 140, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   815/50000: episode: 809, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1611.000 [1611.000, 1611.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   816/50000: episode: 810, duration: 0.006s, episode steps:   1, steps per second: 157, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1901.000 [1901.000, 1901.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   817/50000: episode: 811, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2069.000 [2069.000, 2069.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   818/50000: episode: 812, duration: 0.007s, episode steps:   1, steps per second: 138, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   819/50000: episode: 813, duration: 0.005s, episode steps:   1, steps per second: 183, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2472.000 [2472.000, 2472.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   820/50000: episode: 814, duration: 0.006s, episode steps:   1, steps per second: 177, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   821/50000: episode: 815, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   822/50000: episode: 816, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   823/50000: episode: 817, duration: 0.006s, episode steps:   1, steps per second: 181, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2989.000 [2989.000, 2989.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   824/50000: episode: 818, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 238.000 [238.000, 238.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   825/50000: episode: 819, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3420.000 [3420.000, 3420.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   826/50000: episode: 820, duration: 0.004s, episode steps:   1, steps per second: 250, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   827/50000: episode: 821, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   828/50000: episode: 822, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   829/50000: episode: 823, duration: 0.005s, episode steps:   1, steps per second: 212, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3433.000 [3433.000, 3433.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   830/50000: episode: 824, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   831/50000: episode: 825, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   832/50000: episode: 826, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   833/50000: episode: 827, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   834/50000: episode: 828, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1611.000 [1611.000, 1611.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   835/50000: episode: 829, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2010.000 [2010.000, 2010.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   836/50000: episode: 830, duration: 0.006s, episode steps:   1, steps per second: 159, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   837/50000: episode: 831, duration: 0.004s, episode steps:   1, steps per second: 248, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 213.000 [213.000, 213.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   838/50000: episode: 832, duration: 0.005s, episode steps:   1, steps per second: 192, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1575.000 [1575.000, 1575.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   839/50000: episode: 833, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1335.000 [1335.000, 1335.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   840/50000: episode: 834, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1984.000 [1984.000, 1984.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   841/50000: episode: 835, duration: 0.006s, episode steps:   1, steps per second: 162, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4066.000 [4066.000, 4066.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   842/50000: episode: 836, duration: 0.004s, episode steps:   1, steps per second: 238, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2067.000 [2067.000, 2067.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   843/50000: episode: 837, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3818.000 [3818.000, 3818.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   844/50000: episode: 838, duration: 0.006s, episode steps:   1, steps per second: 165, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   845/50000: episode: 839, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   846/50000: episode: 840, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   847/50000: episode: 841, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   848/50000: episode: 842, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   849/50000: episode: 843, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3714.000 [3714.000, 3714.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   850/50000: episode: 844, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   851/50000: episode: 845, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3735.000 [3735.000, 3735.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   852/50000: episode: 846, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2475.000 [2475.000, 2475.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   853/50000: episode: 847, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   854/50000: episode: 848, duration: 0.007s, episode steps:   1, steps per second: 133, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   855/50000: episode: 849, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2739.000 [2739.000, 2739.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   856/50000: episode: 850, duration: 0.005s, episode steps:   1, steps per second: 217, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   857/50000: episode: 851, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   858/50000: episode: 852, duration: 0.004s, episode steps:   1, steps per second: 224, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   859/50000: episode: 853, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3267.000 [3267.000, 3267.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   860/50000: episode: 854, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   861/50000: episode: 855, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1057.000 [1057.000, 1057.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   862/50000: episode: 856, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1611.000 [1611.000, 1611.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   863/50000: episode: 857, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2735.000 [2735.000, 2735.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   864/50000: episode: 858, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   865/50000: episode: 859, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   866/50000: episode: 860, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1872.000 [1872.000, 1872.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   867/50000: episode: 861, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   868/50000: episode: 862, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2654.000 [2654.000, 2654.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   870/50000: episode: 863, duration: 0.018s, episode steps:   2, steps per second: 114, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 403.500 [76.000, 731.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   871/50000: episode: 864, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   872/50000: episode: 865, duration: 0.004s, episode steps:   1, steps per second: 245, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   873/50000: episode: 866, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   874/50000: episode: 867, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   875/50000: episode: 868, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2001.000 [2001.000, 2001.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   876/50000: episode: 869, duration: 0.007s, episode steps:   1, steps per second: 151, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   877/50000: episode: 870, duration: 0.005s, episode steps:   1, steps per second: 184, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   878/50000: episode: 871, duration: 0.005s, episode steps:   1, steps per second: 216, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   879/50000: episode: 872, duration: 0.004s, episode steps:   1, steps per second: 229, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   880/50000: episode: 873, duration: 0.009s, episode steps:   1, steps per second: 106, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   881/50000: episode: 874, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 599.000 [599.000, 599.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   882/50000: episode: 875, duration: 0.004s, episode steps:   1, steps per second: 274, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2990.000 [2990.000, 2990.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   883/50000: episode: 876, duration: 0.010s, episode steps:   1, steps per second: 100, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   884/50000: episode: 877, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   886/50000: episode: 878, duration: 0.021s, episode steps:   2, steps per second:  95, episode reward: -4981.000, mean reward: -2490.500 [-5000.000, 19.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   887/50000: episode: 879, duration: 0.004s, episode steps:   1, steps per second: 226, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   888/50000: episode: 880, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   889/50000: episode: 881, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 24.000 [24.000, 24.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   890/50000: episode: 882, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   891/50000: episode: 883, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   892/50000: episode: 884, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   893/50000: episode: 885, duration: 0.006s, episode steps:   1, steps per second: 169, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   894/50000: episode: 886, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   895/50000: episode: 887, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2472.000 [2472.000, 2472.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   896/50000: episode: 888, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   897/50000: episode: 889, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   898/50000: episode: 890, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   899/50000: episode: 891, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   900/50000: episode: 892, duration: 0.006s, episode steps:   1, steps per second: 164, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2469.000 [2469.000, 2469.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   901/50000: episode: 893, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   902/50000: episode: 894, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   903/50000: episode: 895, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   904/50000: episode: 896, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   905/50000: episode: 897, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2669.000 [2669.000, 2669.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   907/50000: episode: 898, duration: 0.025s, episode steps:   2, steps per second:  81, episode reward: -4961.000, mean reward: -2480.500 [-5000.000, 39.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   908/50000: episode: 899, duration: 0.004s, episode steps:   1, steps per second: 232, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   909/50000: episode: 900, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 61.000 [61.000, 61.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   910/50000: episode: 901, duration: 0.010s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 62.000 [62.000, 62.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   911/50000: episode: 902, duration: 0.004s, episode steps:   1, steps per second: 241, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   913/50000: episode: 903, duration: 0.019s, episode steps:   2, steps per second: 106, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 3097.500 [2453.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   914/50000: episode: 904, duration: 0.005s, episode steps:   1, steps per second: 186, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   915/50000: episode: 905, duration: 0.007s, episode steps:   1, steps per second: 150, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2380.000 [2380.000, 2380.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   916/50000: episode: 906, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   917/50000: episode: 907, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   918/50000: episode: 908, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1318.000 [1318.000, 1318.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   919/50000: episode: 909, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   920/50000: episode: 910, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2300.000 [2300.000, 2300.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   921/50000: episode: 911, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3476.000 [3476.000, 3476.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   922/50000: episode: 912, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   923/50000: episode: 913, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   924/50000: episode: 914, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   925/50000: episode: 915, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   926/50000: episode: 916, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2821.000 [2821.000, 2821.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   927/50000: episode: 917, duration: 0.006s, episode steps:   1, steps per second: 167, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   928/50000: episode: 918, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1748.000 [1748.000, 1748.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   929/50000: episode: 919, duration: 0.004s, episode steps:   1, steps per second: 274, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   930/50000: episode: 920, duration: 0.007s, episode steps:   1, steps per second: 151, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   931/50000: episode: 921, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   932/50000: episode: 922, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 5.000 [5.000, 5.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   933/50000: episode: 923, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   934/50000: episode: 924, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3981.000 [3981.000, 3981.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   935/50000: episode: 925, duration: 0.006s, episode steps:   1, steps per second: 164, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   936/50000: episode: 926, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   937/50000: episode: 927, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3289.000 [3289.000, 3289.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   938/50000: episode: 928, duration: 0.005s, episode steps:   1, steps per second: 183, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1720.000 [1720.000, 1720.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   939/50000: episode: 929, duration: 0.003s, episode steps:   1, steps per second: 293, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   940/50000: episode: 930, duration: 0.006s, episode steps:   1, steps per second: 172, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   941/50000: episode: 931, duration: 0.003s, episode steps:   1, steps per second: 305, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1756.000 [1756.000, 1756.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   942/50000: episode: 932, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1908.000 [1908.000, 1908.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   943/50000: episode: 933, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2850.000 [2850.000, 2850.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   944/50000: episode: 934, duration: 0.004s, episode steps:   1, steps per second: 259, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3595.000 [3595.000, 3595.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   945/50000: episode: 935, duration: 0.004s, episode steps:   1, steps per second: 248, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1966.000 [1966.000, 1966.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   946/50000: episode: 936, duration: 0.004s, episode steps:   1, steps per second: 226, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 177.000 [177.000, 177.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   947/50000: episode: 937, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 643.000 [643.000, 643.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   948/50000: episode: 938, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   949/50000: episode: 939, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3767.000 [3767.000, 3767.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   950/50000: episode: 940, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   951/50000: episode: 941, duration: 0.027s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1872.000 [1872.000, 1872.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   952/50000: episode: 942, duration: 0.008s, episode steps:   1, steps per second: 127, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   953/50000: episode: 943, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   954/50000: episode: 944, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   955/50000: episode: 945, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   956/50000: episode: 946, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   957/50000: episode: 947, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   958/50000: episode: 948, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1259.000 [1259.000, 1259.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   959/50000: episode: 949, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2198.000 [2198.000, 2198.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   960/50000: episode: 950, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2850.000 [2850.000, 2850.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   961/50000: episode: 951, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4039.000 [4039.000, 4039.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   962/50000: episode: 952, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   963/50000: episode: 953, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   964/50000: episode: 954, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3035.000 [3035.000, 3035.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   965/50000: episode: 955, duration: 0.009s, episode steps:   1, steps per second: 112, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3416.000 [3416.000, 3416.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   966/50000: episode: 956, duration: 0.007s, episode steps:   1, steps per second: 149, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   967/50000: episode: 957, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   968/50000: episode: 958, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   969/50000: episode: 959, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1801.000 [1801.000, 1801.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   970/50000: episode: 960, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2010.000 [2010.000, 2010.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   971/50000: episode: 961, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   972/50000: episode: 962, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   973/50000: episode: 963, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   974/50000: episode: 964, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 157.000 [157.000, 157.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   975/50000: episode: 965, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2647.000 [2647.000, 2647.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   976/50000: episode: 966, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 642.000 [642.000, 642.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   977/50000: episode: 967, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3040.000 [3040.000, 3040.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   978/50000: episode: 968, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   979/50000: episode: 969, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3742.000 [3742.000, 3742.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   980/50000: episode: 970, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   981/50000: episode: 971, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3286.000 [3286.000, 3286.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   982/50000: episode: 972, duration: 0.006s, episode steps:   1, steps per second: 157, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   983/50000: episode: 973, duration: 0.006s, episode steps:   1, steps per second: 169, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   984/50000: episode: 974, duration: 0.006s, episode steps:   1, steps per second: 163, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   985/50000: episode: 975, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 532.000 [532.000, 532.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   986/50000: episode: 976, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   987/50000: episode: 977, duration: 0.008s, episode steps:   1, steps per second: 126, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3649.000 [3649.000, 3649.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   988/50000: episode: 978, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   989/50000: episode: 979, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   990/50000: episode: 980, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1787.000 [1787.000, 1787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   991/50000: episode: 981, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1872.000 [1872.000, 1872.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   992/50000: episode: 982, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2207.000 [2207.000, 2207.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   993/50000: episode: 983, duration: 0.007s, episode steps:   1, steps per second: 150, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   994/50000: episode: 984, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   995/50000: episode: 985, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   996/50000: episode: 986, duration: 0.003s, episode steps:   1, steps per second: 294, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3961.000 [3961.000, 3961.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   997/50000: episode: 987, duration: 0.005s, episode steps:   1, steps per second: 207, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   998/50000: episode: 988, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3624.000 [3624.000, 3624.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   999/50000: episode: 989, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1778.000 [1778.000, 1778.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "  1000/50000: episode: 990, duration: 0.004s, episode steps:   1, steps per second: 276, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1001/50000: episode: 991, duration: 1.167s, episode steps:   1, steps per second:   1, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "  1002/50000: episode: 992, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3735.000 [3735.000, 3735.000],  loss: 12500288.000000, mae: 1.413537, mean_q: 0.854278\n",
      "wrong_move\n",
      "  1003/50000: episode: 993, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: 11719268.000000, mae: 1.333815, mean_q: 0.831497\n",
      "wrong_move\n",
      "  1004/50000: episode: 994, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: 12500940.000000, mae: 1.412140, mean_q: 0.863199\n",
      "wrong_move\n",
      "  1005/50000: episode: 995, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2961.000 [2961.000, 2961.000],  loss: 12500651.000000, mae: 1.408355, mean_q: 0.819376\n",
      "wrong_move\n",
      "  1006/50000: episode: 996, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1611.000 [1611.000, 1611.000],  loss: 12500328.000000, mae: 1.408794, mean_q: 0.816648\n",
      "wrong_move\n",
      "  1007/50000: episode: 997, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2010.000 [2010.000, 2010.000],  loss: 12500612.000000, mae: 1.408356, mean_q: 0.849569\n",
      "wrong_move\n",
      "  1008/50000: episode: 998, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: 12501028.000000, mae: 1.411134, mean_q: 0.848095\n",
      "wrong_move\n",
      "  1009/50000: episode: 999, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 76.000 [76.000, 76.000],  loss: 12499970.000000, mae: 1.404053, mean_q: 0.806266\n",
      "wrong_move\n",
      "  1010/50000: episode: 1000, duration: 0.043s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: 12500907.000000, mae: 1.413360, mean_q: 0.857930\n",
      "wrong_move\n",
      "  1011/50000: episode: 1001, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2010.000 [2010.000, 2010.000],  loss: 12500720.000000, mae: 1.407595, mean_q: 0.805189\n",
      "wrong_move\n",
      "  1012/50000: episode: 1002, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1668.000 [1668.000, 1668.000],  loss: 12109637.000000, mae: 1.369105, mean_q: 0.823013\n",
      "wrong_move\n",
      "  1013/50000: episode: 1003, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1778.000 [1778.000, 1778.000],  loss: 12500444.000000, mae: 1.407255, mean_q: 0.821265\n",
      "wrong_move\n",
      "  1014/50000: episode: 1004, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2971.000 [2971.000, 2971.000],  loss: 12500525.000000, mae: 1.407722, mean_q: 0.833516\n",
      "wrong_move\n",
      "  1015/50000: episode: 1005, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 178.000 [178.000, 178.000],  loss: 12500271.000000, mae: 1.409780, mean_q: 0.843845\n",
      "wrong_move\n",
      "  1016/50000: episode: 1006, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1778.000 [1778.000, 1778.000],  loss: 12109846.000000, mae: 1.370154, mean_q: 0.833424\n",
      "wrong_move\n",
      "  1017/50000: episode: 1007, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1778.000 [1778.000, 1778.000],  loss: 12500163.000000, mae: 1.407103, mean_q: 0.811378\n",
      "wrong_move\n",
      "  1018/50000: episode: 1008, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2285.000 [2285.000, 2285.000],  loss: 12500105.000000, mae: 1.404512, mean_q: 0.829348\n",
      "wrong_move\n",
      "  1019/50000: episode: 1009, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1031.000 [1031.000, 1031.000],  loss: 12500576.000000, mae: 1.411136, mean_q: 0.860196\n",
      "wrong_move\n",
      "  1020/50000: episode: 1010, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 676.000 [676.000, 676.000],  loss: 12500632.000000, mae: 1.409509, mean_q: 0.836400\n",
      "wrong_move\n",
      "  1021/50000: episode: 1011, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1778.000 [1778.000, 1778.000],  loss: 12500456.000000, mae: 1.408603, mean_q: 0.859566\n",
      "wrong_move\n",
      "  1022/50000: episode: 1012, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3238.000 [3238.000, 3238.000],  loss: 12499598.000000, mae: 1.409748, mean_q: 0.834406\n",
      "wrong_move\n",
      "  1023/50000: episode: 1013, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2838.000 [2838.000, 2838.000],  loss: 12499784.000000, mae: 1.408368, mean_q: 0.843293\n",
      "wrong_move\n",
      "  1024/50000: episode: 1014, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 940.000 [940.000, 940.000],  loss: 11718826.000000, mae: 1.333281, mean_q: 0.834040\n",
      "wrong_move\n",
      "  1025/50000: episode: 1015, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3349.000 [3349.000, 3349.000],  loss: 12500034.000000, mae: 1.407862, mean_q: 0.837105\n",
      "wrong_move\n",
      "  1026/50000: episode: 1016, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 810.000 [810.000, 810.000],  loss: 12500315.000000, mae: 1.407259, mean_q: 0.802213\n",
      "wrong_move\n",
      "  1027/50000: episode: 1017, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1611.000 [1611.000, 1611.000],  loss: 12500674.000000, mae: 1.407681, mean_q: 0.810588\n",
      "wrong_move\n",
      "  1028/50000: episode: 1018, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1778.000 [1778.000, 1778.000],  loss: 12109170.000000, mae: 1.369129, mean_q: 0.833038\n",
      "wrong_move\n",
      "  1029/50000: episode: 1019, duration: 0.068s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 595.000 [595.000, 595.000],  loss: 12499772.000000, mae: 1.407707, mean_q: 0.826252\n",
      "wrong_move\n",
      "  1030/50000: episode: 1020, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1778.000 [1778.000, 1778.000],  loss: 12500134.000000, mae: 1.408208, mean_q: 0.835439\n",
      "wrong_move\n",
      "  1031/50000: episode: 1021, duration: 0.090s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 788.000 [788.000, 788.000],  loss: 12499863.000000, mae: 1.406865, mean_q: 0.848017\n",
      "wrong_move\n",
      "  1033/50000: episode: 1022, duration: 0.156s, episode steps:   2, steps per second:  13, episode reward: -4961.000, mean reward: -2480.500 [-5000.000, 39.000], mean action: 3849.000 [3742.000, 3956.000],  loss: 12304799.000000, mae: 1.390936, mean_q: 0.837878\n",
      "wrong_move\n",
      "  1034/50000: episode: 1023, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1778.000 [1778.000, 1778.000],  loss: 12500003.000000, mae: 1.409063, mean_q: 0.816917\n",
      "wrong_move\n",
      "  1035/50000: episode: 1024, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3371.000 [3371.000, 3371.000],  loss: 12499874.000000, mae: 1.407974, mean_q: 0.820114\n",
      "wrong_move\n",
      "  1036/50000: episode: 1025, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1778.000 [1778.000, 1778.000],  loss: 12499746.000000, mae: 1.409763, mean_q: 0.833984\n",
      "wrong_move\n",
      "  1037/50000: episode: 1026, duration: 0.083s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2391.000 [2391.000, 2391.000],  loss: 12108698.000000, mae: 1.369166, mean_q: 0.836472\n",
      "wrong_move\n",
      "  1038/50000: episode: 1027, duration: 0.069s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3349.000 [3349.000, 3349.000],  loss: 12499627.000000, mae: 1.410295, mean_q: 0.827805\n",
      "wrong_move\n",
      "  1039/50000: episode: 1028, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: 12499986.000000, mae: 1.411203, mean_q: 0.832078\n",
      "wrong_move\n",
      "  1040/50000: episode: 1029, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3412.000 [3412.000, 3412.000],  loss: 12499864.000000, mae: 1.410331, mean_q: 0.850844\n",
      "wrong_move\n",
      "  1041/50000: episode: 1030, duration: 0.067s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1778.000 [1778.000, 1778.000],  loss: 12109105.000000, mae: 1.371875, mean_q: 0.826932\n",
      "wrong_move\n",
      "  1042/50000: episode: 1031, duration: 0.051s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1818.000 [1818.000, 1818.000],  loss: 12500082.000000, mae: 1.408088, mean_q: 0.844965\n",
      "wrong_move\n",
      "  1043/50000: episode: 1032, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 45.000 [45.000, 45.000],  loss: 12499442.000000, mae: 1.407853, mean_q: 0.829009\n",
      "wrong_move\n",
      "  1044/50000: episode: 1033, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3750.000 [3750.000, 3750.000],  loss: 12500041.000000, mae: 1.409738, mean_q: 0.844717\n",
      "wrong_move\n",
      "  1045/50000: episode: 1034, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1010.000 [1010.000, 1010.000],  loss: 12499740.000000, mae: 1.411279, mean_q: 0.848486\n",
      "wrong_move\n",
      "  1046/50000: episode: 1035, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 178.000 [178.000, 178.000],  loss: 12499395.000000, mae: 1.410424, mean_q: 0.836322\n",
      "wrong_move\n",
      "  1047/50000: episode: 1036, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3412.000 [3412.000, 3412.000],  loss: 12499435.000000, mae: 1.410682, mean_q: 0.836034\n",
      "wrong_move\n",
      "  1048/50000: episode: 1037, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2905.000 [2905.000, 2905.000],  loss: 12500102.000000, mae: 1.407564, mean_q: 0.825213\n",
      "wrong_move\n",
      "  1049/50000: episode: 1038, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1579.000 [1579.000, 1579.000],  loss: 11718337.000000, mae: 1.334597, mean_q: 0.835114\n",
      "wrong_move\n",
      "  1050/50000: episode: 1039, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: 12499246.000000, mae: 1.409948, mean_q: 0.819097\n",
      "wrong_move\n",
      "  1051/50000: episode: 1040, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2429.000 [2429.000, 2429.000],  loss: 12108759.000000, mae: 1.371007, mean_q: 0.824162\n",
      "wrong_move\n",
      "  1052/50000: episode: 1041, duration: 0.077s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3961.000 [3961.000, 3961.000],  loss: 12498714.000000, mae: 1.411586, mean_q: 0.843578\n",
      "wrong_move\n",
      "  1053/50000: episode: 1042, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3807.000 [3807.000, 3807.000],  loss: 11718168.000000, mae: 1.333800, mean_q: 0.850756\n",
      "wrong_move\n",
      "  1054/50000: episode: 1043, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: 12108915.000000, mae: 1.370866, mean_q: 0.836359\n",
      "wrong_move\n",
      "  1055/50000: episode: 1044, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1948.000 [1948.000, 1948.000],  loss: 12499288.000000, mae: 1.410323, mean_q: 0.848356\n",
      "wrong_move\n",
      "  1056/50000: episode: 1045, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3163.000 [3163.000, 3163.000],  loss: 12499648.000000, mae: 1.407716, mean_q: 0.842581\n",
      "wrong_move\n",
      "  1057/50000: episode: 1046, duration: 0.076s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2231.000 [2231.000, 2231.000],  loss: 12499230.000000, mae: 1.411136, mean_q: 0.826738\n",
      "wrong_move\n",
      "  1058/50000: episode: 1047, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 146.000 [146.000, 146.000],  loss: 12499348.000000, mae: 1.405433, mean_q: 0.806259\n",
      "wrong_move\n",
      "  1059/50000: episode: 1048, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3940.000 [3940.000, 3940.000],  loss: 12499538.000000, mae: 1.410178, mean_q: 0.862816\n",
      "wrong_move\n",
      "  1060/50000: episode: 1049, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3412.000 [3412.000, 3412.000],  loss: 12499556.000000, mae: 1.406257, mean_q: 0.843098\n",
      "wrong_move\n",
      "  1061/50000: episode: 1050, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 759.000 [759.000, 759.000],  loss: 12499645.000000, mae: 1.409682, mean_q: 0.847301\n",
      "wrong_move\n",
      "  1062/50000: episode: 1051, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3409.000 [3409.000, 3409.000],  loss: 12498785.000000, mae: 1.408609, mean_q: 0.827119\n",
      "wrong_move\n",
      "  1063/50000: episode: 1052, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: 12498694.000000, mae: 1.408009, mean_q: 0.848226\n",
      "wrong_move\n",
      "  1064/50000: episode: 1053, duration: 0.057s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2717.000 [2717.000, 2717.000],  loss: 12498914.000000, mae: 1.408036, mean_q: 0.842208\n",
      "wrong_move\n",
      "  1065/50000: episode: 1054, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2885.000 [2885.000, 2885.000],  loss: 12499169.000000, mae: 1.409766, mean_q: 0.834509\n",
      "wrong_move\n",
      "  1066/50000: episode: 1055, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2732.000 [2732.000, 2732.000],  loss: 12498528.000000, mae: 1.408064, mean_q: 0.856214\n",
      "wrong_move\n",
      "  1067/50000: episode: 1056, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 45.000 [45.000, 45.000],  loss: 12498806.000000, mae: 1.411528, mean_q: 0.846323\n",
      "wrong_move\n",
      "  1068/50000: episode: 1057, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1566.000 [1566.000, 1566.000],  loss: 12108836.000000, mae: 1.371313, mean_q: 0.876480\n",
      "wrong_move\n",
      "  1069/50000: episode: 1058, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2717.000 [2717.000, 2717.000],  loss: 12498754.000000, mae: 1.408704, mean_q: 0.824437\n",
      "wrong_move\n",
      "  1070/50000: episode: 1059, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: 12499110.000000, mae: 1.410370, mean_q: 0.852520\n",
      "wrong_move\n",
      "  1071/50000: episode: 1060, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 178.000 [178.000, 178.000],  loss: 12498867.000000, mae: 1.409906, mean_q: 0.837382\n",
      "wrong_move\n",
      "  1072/50000: episode: 1061, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 288.000 [288.000, 288.000],  loss: 12498572.000000, mae: 1.410911, mean_q: 0.868176\n",
      "wrong_move\n",
      "  1073/50000: episode: 1062, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1566.000 [1566.000, 1566.000],  loss: 11717390.000000, mae: 1.330863, mean_q: 0.861053\n",
      "wrong_move\n",
      "  1074/50000: episode: 1063, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1941.000 [1941.000, 1941.000],  loss: 12498876.000000, mae: 1.410445, mean_q: 0.893322\n",
      "wrong_move\n",
      "  1075/50000: episode: 1064, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2405.000 [2405.000, 2405.000],  loss: 12498662.000000, mae: 1.412564, mean_q: 0.873294\n",
      "wrong_move\n",
      "  1076/50000: episode: 1065, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1651.000 [1651.000, 1651.000],  loss: 12499214.000000, mae: 1.409535, mean_q: 0.847446\n",
      "wrong_move\n",
      "  1077/50000: episode: 1066, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3704.000 [3704.000, 3704.000],  loss: 12107045.000000, mae: 1.371778, mean_q: 0.876090\n",
      "wrong_move\n",
      "  1078/50000: episode: 1067, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1651.000 [1651.000, 1651.000],  loss: 12498756.000000, mae: 1.413644, mean_q: 0.848894\n",
      "wrong_move\n",
      "  1079/50000: episode: 1068, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3860.000 [3860.000, 3860.000],  loss: 12107906.000000, mae: 1.373150, mean_q: 0.863626\n",
      "wrong_move\n",
      "  1080/50000: episode: 1069, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4090.000 [4090.000, 4090.000],  loss: 12499179.000000, mae: 1.410549, mean_q: 0.865194\n",
      "wrong_move\n",
      "  1081/50000: episode: 1070, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1022.000 [1022.000, 1022.000],  loss: 12108554.000000, mae: 1.373031, mean_q: 0.877160\n",
      "wrong_move\n",
      "  1082/50000: episode: 1071, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1566.000 [1566.000, 1566.000],  loss: 12499466.000000, mae: 1.411338, mean_q: 0.865474\n",
      "wrong_move\n",
      "  1083/50000: episode: 1072, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1010.000 [1010.000, 1010.000],  loss: 12108012.000000, mae: 1.369935, mean_q: 0.857829\n",
      "wrong_move\n",
      "  1084/50000: episode: 1073, duration: 0.058s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: 12498680.000000, mae: 1.410055, mean_q: 0.908841\n",
      "wrong_move\n",
      "  1085/50000: episode: 1074, duration: 0.060s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1651.000 [1651.000, 1651.000],  loss: 12107266.000000, mae: 1.375542, mean_q: 0.898639\n",
      "wrong_move\n",
      "  1086/50000: episode: 1075, duration: 0.064s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1433.000 [1433.000, 1433.000],  loss: 12498261.000000, mae: 1.409890, mean_q: 0.885424\n",
      "wrong_move\n",
      "  1087/50000: episode: 1076, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1665.000 [1665.000, 1665.000],  loss: 11716867.000000, mae: 1.334859, mean_q: 0.895121\n",
      "wrong_move\n",
      "  1088/50000: episode: 1077, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1651.000 [1651.000, 1651.000],  loss: 12498714.000000, mae: 1.411499, mean_q: 0.883226\n",
      "wrong_move\n",
      "  1089/50000: episode: 1078, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3860.000 [3860.000, 3860.000],  loss: 12107953.000000, mae: 1.372620, mean_q: 0.897380\n",
      "wrong_move\n",
      "  1090/50000: episode: 1079, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1689.000 [1689.000, 1689.000],  loss: 12498764.000000, mae: 1.410712, mean_q: 0.913914\n",
      "wrong_move\n",
      "  1091/50000: episode: 1080, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3860.000 [3860.000, 3860.000],  loss: 12498844.000000, mae: 1.406441, mean_q: 0.897671\n",
      "wrong_move\n",
      "  1092/50000: episode: 1081, duration: 0.055s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1401.000 [1401.000, 1401.000],  loss: 12498353.000000, mae: 1.412784, mean_q: 0.910943\n",
      "wrong_move\n",
      "  1093/50000: episode: 1082, duration: 0.079s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3526.000 [3526.000, 3526.000],  loss: 12498418.000000, mae: 1.411983, mean_q: 0.901864\n",
      "wrong_move\n",
      "  1094/50000: episode: 1083, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2300.000 [2300.000, 2300.000],  loss: 12498186.000000, mae: 1.410648, mean_q: 0.937529\n",
      "wrong_move\n",
      "  1095/50000: episode: 1084, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3860.000 [3860.000, 3860.000],  loss: 12107992.000000, mae: 1.375034, mean_q: 0.917247\n",
      "wrong_move\n",
      "  1096/50000: episode: 1085, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3185.000 [3185.000, 3185.000],  loss: 12498329.000000, mae: 1.408939, mean_q: 0.911781\n",
      "wrong_move\n",
      "  1097/50000: episode: 1086, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3860.000 [3860.000, 3860.000],  loss: 12497890.000000, mae: 1.412821, mean_q: 0.922379\n",
      "wrong_move\n",
      "  1098/50000: episode: 1087, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1280.000 [1280.000, 1280.000],  loss: 12497784.000000, mae: 1.411841, mean_q: 0.888889\n",
      "wrong_move\n",
      "  1099/50000: episode: 1088, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3860.000 [3860.000, 3860.000],  loss: 12107593.000000, mae: 1.372214, mean_q: 0.917181\n",
      "wrong_move\n",
      "  1100/50000: episode: 1089, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3790.000 [3790.000, 3790.000],  loss: 12107606.000000, mae: 1.371766, mean_q: 0.981792\n",
      "wrong_move\n",
      "  1101/50000: episode: 1090, duration: 0.091s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1849.000 [1849.000, 1849.000],  loss: 12497409.000000, mae: 1.409046, mean_q: 0.910954\n",
      "wrong_move\n",
      "  1102/50000: episode: 1091, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3860.000 [3860.000, 3860.000],  loss: 12497614.000000, mae: 1.413099, mean_q: 0.909807\n",
      "wrong_move\n",
      "  1103/50000: episode: 1092, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3969.000 [3969.000, 3969.000],  loss: 12497701.000000, mae: 1.412347, mean_q: 0.886595\n",
      "wrong_move\n",
      "  1104/50000: episode: 1093, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: 12498430.000000, mae: 1.411897, mean_q: 0.921909\n",
      "wrong_move\n",
      "  1105/50000: episode: 1094, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3790.000 [3790.000, 3790.000],  loss: 12498320.000000, mae: 1.410027, mean_q: 0.936342\n",
      "wrong_move\n",
      "  1106/50000: episode: 1095, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3592.000 [3592.000, 3592.000],  loss: 12497854.000000, mae: 1.413370, mean_q: 1.007100\n",
      "wrong_move\n",
      "  1107/50000: episode: 1096, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3451.000 [3451.000, 3451.000],  loss: 12108176.000000, mae: 1.377107, mean_q: 0.910629\n",
      "wrong_move\n",
      "  1108/50000: episode: 1097, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 681.000 [681.000, 681.000],  loss: 12497528.000000, mae: 1.414869, mean_q: 0.940331\n",
      "wrong_move\n",
      "  1109/50000: episode: 1098, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1018.000 [1018.000, 1018.000],  loss: 12497684.000000, mae: 1.413231, mean_q: 0.976643\n",
      "wrong_move\n",
      "  1110/50000: episode: 1099, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: 12498040.000000, mae: 1.410704, mean_q: 0.993565\n",
      "wrong_move\n",
      "  1111/50000: episode: 1100, duration: 0.099s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4082.000 [4082.000, 4082.000],  loss: 12497005.000000, mae: 1.410934, mean_q: 1.052351\n",
      "wrong_move\n",
      "  1112/50000: episode: 1101, duration: 0.078s, episode steps:   1, steps per second:  13, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: 12497168.000000, mae: 1.412163, mean_q: 1.019712\n",
      "wrong_move\n",
      "  1113/50000: episode: 1102, duration: 0.066s, episode steps:   1, steps per second:  15, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 681.000 [681.000, 681.000],  loss: 11716368.000000, mae: 1.338417, mean_q: 0.946715\n",
      "wrong_move\n",
      "  1114/50000: episode: 1103, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 681.000 [681.000, 681.000],  loss: 12498402.000000, mae: 1.412298, mean_q: 0.974306\n",
      "wrong_move\n",
      "  1115/50000: episode: 1104, duration: 0.061s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: 12106620.000000, mae: 1.374327, mean_q: 0.971923\n",
      "wrong_move\n",
      "  1116/50000: episode: 1105, duration: 0.088s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: 12107564.000000, mae: 1.376020, mean_q: 1.031640\n",
      "wrong_move\n",
      "  1117/50000: episode: 1106, duration: 0.105s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 681.000 [681.000, 681.000],  loss: 12496815.000000, mae: 1.409937, mean_q: 1.073849\n",
      "wrong_move\n",
      "  1118/50000: episode: 1107, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4059.000 [4059.000, 4059.000],  loss: 12497723.000000, mae: 1.413952, mean_q: 0.966384\n",
      "wrong_move\n",
      "  1119/50000: episode: 1108, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3497.000 [3497.000, 3497.000],  loss: 12497382.000000, mae: 1.412026, mean_q: 1.045694\n",
      "wrong_move\n",
      "  1120/50000: episode: 1109, duration: 0.073s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 681.000 [681.000, 681.000],  loss: 12106545.000000, mae: 1.377626, mean_q: 0.986744\n",
      "wrong_move\n",
      "  1121/50000: episode: 1110, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2409.000 [2409.000, 2409.000],  loss: 12498109.000000, mae: 1.416331, mean_q: 0.988498\n",
      "wrong_move\n",
      "  1122/50000: episode: 1111, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3302.000 [3302.000, 3302.000],  loss: 12497954.000000, mae: 1.413775, mean_q: 1.037250\n",
      "wrong_move\n",
      "  1123/50000: episode: 1112, duration: 0.100s, episode steps:   1, steps per second:  10, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3930.000 [3930.000, 3930.000],  loss: 12497340.000000, mae: 1.414249, mean_q: 0.958983\n",
      "wrong_move\n",
      "  1124/50000: episode: 1113, duration: 0.094s, episode steps:   1, steps per second:  11, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 920.000 [920.000, 920.000],  loss: 12497350.000000, mae: 1.414221, mean_q: 1.011547\n",
      "wrong_move\n",
      "  1125/50000: episode: 1114, duration: 0.072s, episode steps:   1, steps per second:  14, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 681.000 [681.000, 681.000],  loss: 12496518.000000, mae: 1.414536, mean_q: 1.013307\n",
      "wrong_move\n",
      "  1126/50000: episode: 1115, duration: 0.084s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 125.000 [125.000, 125.000],  loss: 12497406.000000, mae: 1.414554, mean_q: 1.071477\n",
      "wrong_move\n",
      "  1127/50000: episode: 1116, duration: 0.048s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3245.000 [3245.000, 3245.000],  loss: 12496486.000000, mae: 1.414103, mean_q: 0.991304\n",
      "wrong_move\n",
      "  1128/50000: episode: 1117, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3790.000 [3790.000, 3790.000],  loss: 12107532.000000, mae: 1.375408, mean_q: 1.021749\n",
      "wrong_move\n",
      "  1129/50000: episode: 1118, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: 12497873.000000, mae: 1.415448, mean_q: 1.058318\n",
      "wrong_move\n",
      "  1130/50000: episode: 1119, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: 12107428.000000, mae: 1.377100, mean_q: 1.066036\n",
      "wrong_move\n",
      "  1131/50000: episode: 1120, duration: 0.059s, episode steps:   1, steps per second:  17, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: 12498306.000000, mae: 1.413760, mean_q: 1.069478\n",
      "wrong_move\n",
      "  1132/50000: episode: 1121, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1016.000 [1016.000, 1016.000],  loss: 12496882.000000, mae: 1.411638, mean_q: 1.058439\n",
      "wrong_move\n",
      "  1133/50000: episode: 1122, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 681.000 [681.000, 681.000],  loss: 12497443.000000, mae: 1.412489, mean_q: 1.140877\n",
      "wrong_move\n",
      "  1134/50000: episode: 1123, duration: 0.043s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: 12496214.000000, mae: 1.414746, mean_q: 1.119095\n",
      "wrong_move\n",
      "  1135/50000: episode: 1124, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3489.000 [3489.000, 3489.000],  loss: 12106764.000000, mae: 1.374581, mean_q: 1.146836\n",
      "wrong_move\n",
      "  1136/50000: episode: 1125, duration: 0.053s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: 12106276.000000, mae: 1.376052, mean_q: 1.115738\n",
      "wrong_move\n",
      "  1137/50000: episode: 1126, duration: 0.038s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: 12496564.000000, mae: 1.413929, mean_q: 1.137409\n",
      "wrong_move\n",
      "  1138/50000: episode: 1127, duration: 0.054s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: 12105770.000000, mae: 1.377278, mean_q: 1.076546\n",
      "wrong_move\n",
      "  1139/50000: episode: 1128, duration: 0.050s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: 12498390.000000, mae: 1.413241, mean_q: 1.064452\n",
      "wrong_move\n",
      "  1140/50000: episode: 1129, duration: 0.063s, episode steps:   1, steps per second:  16, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2750.000 [2750.000, 2750.000],  loss: 12496634.000000, mae: 1.415024, mean_q: 1.127594\n",
      "wrong_move\n",
      "  1141/50000: episode: 1130, duration: 0.082s, episode steps:   1, steps per second:  12, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1675.000 [1675.000, 1675.000],  loss: 12495928.000000, mae: 1.411157, mean_q: 1.166072\n",
      "wrong_move\n",
      "done, took 20.048 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong_move\n",
      "     1/50000: episode: 1, duration: 0.592s, episode steps:   1, steps per second:   2, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 678.000 [678.000, 678.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     2/50000: episode: 2, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     3/50000: episode: 3, duration: 0.008s, episode steps:   1, steps per second: 133, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2076.000 [2076.000, 2076.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     4/50000: episode: 4, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     5/50000: episode: 5, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     6/50000: episode: 6, duration: 0.009s, episode steps:   1, steps per second: 117, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     7/50000: episode: 7, duration: 0.007s, episode steps:   1, steps per second: 143, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     8/50000: episode: 8, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 768.000 [768.000, 768.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "     9/50000: episode: 9, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    10/50000: episode: 10, duration: 0.006s, episode steps:   1, steps per second: 177, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2060.000 [2060.000, 2060.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    11/50000: episode: 11, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    12/50000: episode: 12, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 961.000 [961.000, 961.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    13/50000: episode: 13, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    14/50000: episode: 14, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    15/50000: episode: 15, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    16/50000: episode: 16, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    17/50000: episode: 17, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    18/50000: episode: 18, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2584.000 [2584.000, 2584.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    19/50000: episode: 19, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    20/50000: episode: 20, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    21/50000: episode: 21, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    22/50000: episode: 22, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    23/50000: episode: 23, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    24/50000: episode: 24, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    25/50000: episode: 25, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1002.000 [1002.000, 1002.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    26/50000: episode: 26, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2575.000 [2575.000, 2575.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    27/50000: episode: 27, duration: 0.003s, episode steps:   1, steps per second: 299, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    28/50000: episode: 28, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    29/50000: episode: 29, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    30/50000: episode: 30, duration: 0.009s, episode steps:   1, steps per second: 115, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    31/50000: episode: 31, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    32/50000: episode: 32, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    33/50000: episode: 33, duration: 0.004s, episode steps:   1, steps per second: 250, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    34/50000: episode: 34, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1606.000 [1606.000, 1606.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    35/50000: episode: 35, duration: 0.007s, episode steps:   1, steps per second: 147, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    36/50000: episode: 36, duration: 0.008s, episode steps:   1, steps per second: 133, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    37/50000: episode: 37, duration: 0.007s, episode steps:   1, steps per second: 147, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    38/50000: episode: 38, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    39/50000: episode: 39, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    40/50000: episode: 40, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    41/50000: episode: 41, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3401.000 [3401.000, 3401.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    42/50000: episode: 42, duration: 0.008s, episode steps:   1, steps per second: 132, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3188.000 [3188.000, 3188.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    43/50000: episode: 43, duration: 0.007s, episode steps:   1, steps per second: 133, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2593.000 [2593.000, 2593.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    44/50000: episode: 44, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    45/50000: episode: 45, duration: 0.007s, episode steps:   1, steps per second: 134, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    46/50000: episode: 46, duration: 0.007s, episode steps:   1, steps per second: 146, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    47/50000: episode: 47, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    48/50000: episode: 48, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    49/50000: episode: 49, duration: 0.008s, episode steps:   1, steps per second: 133, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3594.000 [3594.000, 3594.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    50/50000: episode: 50, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4037.000 [4037.000, 4037.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    51/50000: episode: 51, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3534.000 [3534.000, 3534.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    52/50000: episode: 52, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    53/50000: episode: 53, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2575.000 [2575.000, 2575.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    54/50000: episode: 54, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2575.000 [2575.000, 2575.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    55/50000: episode: 55, duration: 0.006s, episode steps:   1, steps per second: 176, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    56/50000: episode: 56, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    57/50000: episode: 57, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    58/50000: episode: 58, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    59/50000: episode: 59, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 992.000 [992.000, 992.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    60/50000: episode: 60, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    61/50000: episode: 61, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3322.000 [3322.000, 3322.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    62/50000: episode: 62, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    63/50000: episode: 63, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 845.000 [845.000, 845.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    64/50000: episode: 64, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    65/50000: episode: 65, duration: 0.005s, episode steps:   1, steps per second: 207, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    66/50000: episode: 66, duration: 0.005s, episode steps:   1, steps per second: 199, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    67/50000: episode: 67, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    68/50000: episode: 68, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3049.000 [3049.000, 3049.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    69/50000: episode: 69, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2228.000 [2228.000, 2228.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    70/50000: episode: 70, duration: 0.005s, episode steps:   1, steps per second: 198, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    71/50000: episode: 71, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3879.000 [3879.000, 3879.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    72/50000: episode: 72, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3251.000 [3251.000, 3251.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    73/50000: episode: 73, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 372.000 [372.000, 372.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    74/50000: episode: 74, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3950.000 [3950.000, 3950.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    76/50000: episode: 75, duration: 0.030s, episode steps:   2, steps per second:  67, episode reward: -4981.000, mean reward: -2490.500 [-5000.000, 19.000], mean action: 2870.000 [2787.000, 2953.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    77/50000: episode: 76, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    78/50000: episode: 77, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3241.000 [3241.000, 3241.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    79/50000: episode: 78, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    81/50000: episode: 79, duration: 0.030s, episode steps:   2, steps per second:  68, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    84/50000: episode: 80, duration: 0.043s, episode steps:   3, steps per second:  70, episode reward: -5002.000, mean reward: -1667.333 [-5000.000, -1.000], mean action: 2973.333 [2316.000, 3817.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    85/50000: episode: 81, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    86/50000: episode: 82, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    87/50000: episode: 83, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    88/50000: episode: 84, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    89/50000: episode: 85, duration: 0.009s, episode steps:   1, steps per second: 110, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    90/50000: episode: 86, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    91/50000: episode: 87, duration: 0.005s, episode steps:   1, steps per second: 186, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    92/50000: episode: 88, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    93/50000: episode: 89, duration: 0.005s, episode steps:   1, steps per second: 196, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    94/50000: episode: 90, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3788.000 [3788.000, 3788.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    95/50000: episode: 91, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    96/50000: episode: 92, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1579.000 [1579.000, 1579.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    97/50000: episode: 93, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    98/50000: episode: 94, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "    99/50000: episode: 95, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2693.000 [2693.000, 2693.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   100/50000: episode: 96, duration: 0.007s, episode steps:   1, steps per second: 138, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   101/50000: episode: 97, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   102/50000: episode: 98, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   103/50000: episode: 99, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2520.000 [2520.000, 2520.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   104/50000: episode: 100, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1469.000 [1469.000, 1469.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   105/50000: episode: 101, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1862.000 [1862.000, 1862.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   107/50000: episode: 102, duration: 0.032s, episode steps:   2, steps per second:  63, episode reward: -4951.000, mean reward: -2475.500 [-5000.000, 49.000], mean action: 2726.000 [1635.000, 3817.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   108/50000: episode: 103, duration: 0.009s, episode steps:   1, steps per second: 105, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 132.000 [132.000, 132.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   109/50000: episode: 104, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   110/50000: episode: 105, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   111/50000: episode: 106, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   112/50000: episode: 107, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   113/50000: episode: 108, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   114/50000: episode: 109, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   115/50000: episode: 110, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   116/50000: episode: 111, duration: 0.006s, episode steps:   1, steps per second: 161, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 469.000 [469.000, 469.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   117/50000: episode: 112, duration: 0.003s, episode steps:   1, steps per second: 297, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   118/50000: episode: 113, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1862.000 [1862.000, 1862.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   120/50000: episode: 114, duration: 0.021s, episode steps:   2, steps per second:  95, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   121/50000: episode: 115, duration: 0.010s, episode steps:   1, steps per second: 101, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   122/50000: episode: 116, duration: 0.006s, episode steps:   1, steps per second: 164, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2157.000 [2157.000, 2157.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   123/50000: episode: 117, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   124/50000: episode: 118, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1797.000 [1797.000, 1797.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   125/50000: episode: 119, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   126/50000: episode: 120, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   127/50000: episode: 121, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   128/50000: episode: 122, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   129/50000: episode: 123, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   130/50000: episode: 124, duration: 0.006s, episode steps:   1, steps per second: 173, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   131/50000: episode: 125, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   132/50000: episode: 126, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   133/50000: episode: 127, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3693.000 [3693.000, 3693.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   134/50000: episode: 128, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   135/50000: episode: 129, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   136/50000: episode: 130, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3493.000 [3493.000, 3493.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   137/50000: episode: 131, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3879.000 [3879.000, 3879.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   138/50000: episode: 132, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 475.000 [475.000, 475.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   139/50000: episode: 133, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   140/50000: episode: 134, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   141/50000: episode: 135, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   142/50000: episode: 136, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 333.000 [333.000, 333.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   143/50000: episode: 137, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   144/50000: episode: 138, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1833.000 [1833.000, 1833.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   145/50000: episode: 139, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   146/50000: episode: 140, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 401.000 [401.000, 401.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   147/50000: episode: 141, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   148/50000: episode: 142, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   149/50000: episode: 143, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2384.000 [2384.000, 2384.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   150/50000: episode: 144, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   151/50000: episode: 145, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   152/50000: episode: 146, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   153/50000: episode: 147, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3426.000 [3426.000, 3426.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   154/50000: episode: 148, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   155/50000: episode: 149, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3657.000 [3657.000, 3657.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   156/50000: episode: 150, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   157/50000: episode: 151, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1112.000 [1112.000, 1112.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   158/50000: episode: 152, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3108.000 [3108.000, 3108.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   159/50000: episode: 153, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3879.000 [3879.000, 3879.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   160/50000: episode: 154, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   161/50000: episode: 155, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   162/50000: episode: 156, duration: 0.027s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2154.000 [2154.000, 2154.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   163/50000: episode: 157, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3611.000 [3611.000, 3611.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   164/50000: episode: 158, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   165/50000: episode: 159, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   166/50000: episode: 160, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   167/50000: episode: 161, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   168/50000: episode: 162, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   169/50000: episode: 163, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2914.000 [2914.000, 2914.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   170/50000: episode: 164, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 569.000 [569.000, 569.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   171/50000: episode: 165, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   172/50000: episode: 166, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   173/50000: episode: 167, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   174/50000: episode: 168, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   175/50000: episode: 169, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3482.000 [3482.000, 3482.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   176/50000: episode: 170, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3269.000 [3269.000, 3269.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   177/50000: episode: 171, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   178/50000: episode: 172, duration: 0.011s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   179/50000: episode: 173, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   180/50000: episode: 174, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   181/50000: episode: 175, duration: 0.004s, episode steps:   1, steps per second: 257, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 171.000 [171.000, 171.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   182/50000: episode: 176, duration: 0.009s, episode steps:   1, steps per second: 107, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   183/50000: episode: 177, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   184/50000: episode: 178, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   185/50000: episode: 179, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 200.000 [200.000, 200.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   186/50000: episode: 180, duration: 0.008s, episode steps:   1, steps per second: 118, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   187/50000: episode: 181, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   188/50000: episode: 182, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   189/50000: episode: 183, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2520.000 [2520.000, 2520.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   190/50000: episode: 184, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   191/50000: episode: 185, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1604.000 [1604.000, 1604.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   192/50000: episode: 186, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   193/50000: episode: 187, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   194/50000: episode: 188, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   195/50000: episode: 189, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   196/50000: episode: 190, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   198/50000: episode: 191, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   199/50000: episode: 192, duration: 0.008s, episode steps:   1, steps per second: 124, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   200/50000: episode: 193, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3930.000 [3930.000, 3930.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   201/50000: episode: 194, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   202/50000: episode: 195, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   203/50000: episode: 196, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   204/50000: episode: 197, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   205/50000: episode: 198, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   206/50000: episode: 199, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1457.000 [1457.000, 1457.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   207/50000: episode: 200, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3768.000 [3768.000, 3768.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   208/50000: episode: 201, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 313.000 [313.000, 313.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   209/50000: episode: 202, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2638.000 [2638.000, 2638.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   210/50000: episode: 203, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   211/50000: episode: 204, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3604.000 [3604.000, 3604.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   212/50000: episode: 205, duration: 0.007s, episode steps:   1, steps per second: 151, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   213/50000: episode: 206, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1977.000 [1977.000, 1977.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   214/50000: episode: 207, duration: 0.009s, episode steps:   1, steps per second: 116, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   215/50000: episode: 208, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   216/50000: episode: 209, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   217/50000: episode: 210, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   218/50000: episode: 211, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   219/50000: episode: 212, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 149.000 [149.000, 149.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   220/50000: episode: 213, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 401.000 [401.000, 401.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   221/50000: episode: 214, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   222/50000: episode: 215, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   223/50000: episode: 216, duration: 0.006s, episode steps:   1, steps per second: 161, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   224/50000: episode: 217, duration: 0.008s, episode steps:   1, steps per second: 130, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   225/50000: episode: 218, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   226/50000: episode: 219, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1698.000 [1698.000, 1698.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   227/50000: episode: 220, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3340.000 [3340.000, 3340.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   228/50000: episode: 221, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 887.000 [887.000, 887.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   229/50000: episode: 222, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2574.000 [2574.000, 2574.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   230/50000: episode: 223, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   231/50000: episode: 224, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   232/50000: episode: 225, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   233/50000: episode: 226, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2770.000 [2770.000, 2770.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   234/50000: episode: 227, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   235/50000: episode: 228, duration: 0.006s, episode steps:   1, steps per second: 165, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   236/50000: episode: 229, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 234.000 [234.000, 234.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   237/50000: episode: 230, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   238/50000: episode: 231, duration: 0.005s, episode steps:   1, steps per second: 196, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3151.000 [3151.000, 3151.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   239/50000: episode: 232, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   240/50000: episode: 233, duration: 0.003s, episode steps:   1, steps per second: 291, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 631.000 [631.000, 631.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   241/50000: episode: 234, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   242/50000: episode: 235, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3455.000 [3455.000, 3455.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   243/50000: episode: 236, duration: 0.006s, episode steps:   1, steps per second: 167, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   244/50000: episode: 237, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3521.000 [3521.000, 3521.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   245/50000: episode: 238, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 456.000 [456.000, 456.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   246/50000: episode: 239, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   247/50000: episode: 240, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   248/50000: episode: 241, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1802.000 [1802.000, 1802.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   249/50000: episode: 242, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   250/50000: episode: 243, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2714.000 [2714.000, 2714.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   251/50000: episode: 244, duration: 0.010s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   252/50000: episode: 245, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   253/50000: episode: 246, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   254/50000: episode: 247, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   255/50000: episode: 248, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3521.000 [3521.000, 3521.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   256/50000: episode: 249, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3688.000 [3688.000, 3688.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   257/50000: episode: 250, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4005.000 [4005.000, 4005.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   258/50000: episode: 251, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1545.000 [1545.000, 1545.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   259/50000: episode: 252, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1233.000 [1233.000, 1233.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   260/50000: episode: 253, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1849.000 [1849.000, 1849.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   261/50000: episode: 254, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   262/50000: episode: 255, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   263/50000: episode: 256, duration: 0.005s, episode steps:   1, steps per second: 192, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   264/50000: episode: 257, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   265/50000: episode: 258, duration: 0.007s, episode steps:   1, steps per second: 135, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2112.000 [2112.000, 2112.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   266/50000: episode: 259, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1336.000 [1336.000, 1336.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   267/50000: episode: 260, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2769.000 [2769.000, 2769.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   268/50000: episode: 261, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2520.000 [2520.000, 2520.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   269/50000: episode: 262, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1545.000 [1545.000, 1545.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   270/50000: episode: 263, duration: 0.005s, episode steps:   1, steps per second: 200, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2890.000 [2890.000, 2890.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   271/50000: episode: 264, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3858.000 [3858.000, 3858.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   272/50000: episode: 265, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3930.000 [3930.000, 3930.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   273/50000: episode: 266, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   274/50000: episode: 267, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3147.000 [3147.000, 3147.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   275/50000: episode: 268, duration: 0.006s, episode steps:   1, steps per second: 182, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   276/50000: episode: 269, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2403.000 [2403.000, 2403.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   277/50000: episode: 270, duration: 0.011s, episode steps:   1, steps per second:  94, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   278/50000: episode: 271, duration: 0.008s, episode steps:   1, steps per second: 129, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   279/50000: episode: 272, duration: 0.007s, episode steps:   1, steps per second: 145, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   280/50000: episode: 273, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   281/50000: episode: 274, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   282/50000: episode: 275, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   283/50000: episode: 276, duration: 0.009s, episode steps:   1, steps per second: 109, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1577.000 [1577.000, 1577.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   284/50000: episode: 277, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3046.000 [3046.000, 3046.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   285/50000: episode: 278, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   286/50000: episode: 279, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   287/50000: episode: 280, duration: 0.007s, episode steps:   1, steps per second: 138, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   288/50000: episode: 281, duration: 0.010s, episode steps:   1, steps per second:  95, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   289/50000: episode: 282, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 101.000 [101.000, 101.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   290/50000: episode: 283, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   291/50000: episode: 284, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3152.000 [3152.000, 3152.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   292/50000: episode: 285, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1578.000 [1578.000, 1578.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   293/50000: episode: 286, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   294/50000: episode: 287, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   295/50000: episode: 288, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   296/50000: episode: 289, duration: 0.006s, episode steps:   1, steps per second: 171, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3825.000 [3825.000, 3825.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   297/50000: episode: 290, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   298/50000: episode: 291, duration: 0.011s, episode steps:   1, steps per second:  93, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 78.000 [78.000, 78.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   299/50000: episode: 292, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   300/50000: episode: 293, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2800.000 [2800.000, 2800.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   301/50000: episode: 294, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   302/50000: episode: 295, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 741.000 [741.000, 741.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   303/50000: episode: 296, duration: 0.007s, episode steps:   1, steps per second: 136, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   304/50000: episode: 297, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   305/50000: episode: 298, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   306/50000: episode: 299, duration: 0.007s, episode steps:   1, steps per second: 139, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3376.000 [3376.000, 3376.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   307/50000: episode: 300, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   308/50000: episode: 301, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1286.000 [1286.000, 1286.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   309/50000: episode: 302, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   310/50000: episode: 303, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2860.000 [2860.000, 2860.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   311/50000: episode: 304, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   312/50000: episode: 305, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1637.000 [1637.000, 1637.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   313/50000: episode: 306, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   314/50000: episode: 307, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   315/50000: episode: 308, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 340.000 [340.000, 340.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   316/50000: episode: 309, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   317/50000: episode: 310, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   318/50000: episode: 311, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 799.000 [799.000, 799.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   319/50000: episode: 312, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   320/50000: episode: 313, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1495.000 [1495.000, 1495.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   321/50000: episode: 314, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   322/50000: episode: 315, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   323/50000: episode: 316, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 124.000 [124.000, 124.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   324/50000: episode: 317, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   325/50000: episode: 318, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   327/50000: episode: 319, duration: 0.029s, episode steps:   2, steps per second:  68, episode reward: -4961.000, mean reward: -2480.500 [-5000.000, 39.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   328/50000: episode: 320, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   329/50000: episode: 321, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   330/50000: episode: 322, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2011.000 [2011.000, 2011.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   331/50000: episode: 323, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   332/50000: episode: 324, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   333/50000: episode: 325, duration: 0.040s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   334/50000: episode: 326, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   335/50000: episode: 327, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   336/50000: episode: 328, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   337/50000: episode: 329, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 701.000 [701.000, 701.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   338/50000: episode: 330, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3879.000 [3879.000, 3879.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   339/50000: episode: 331, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3937.000 [3937.000, 3937.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   340/50000: episode: 332, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3961.000 [3961.000, 3961.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   341/50000: episode: 333, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1114.000 [1114.000, 1114.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   342/50000: episode: 334, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   343/50000: episode: 335, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   345/50000: episode: 336, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   346/50000: episode: 337, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   347/50000: episode: 338, duration: 0.005s, episode steps:   1, steps per second: 216, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   348/50000: episode: 339, duration: 0.008s, episode steps:   1, steps per second: 123, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 972.000 [972.000, 972.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   349/50000: episode: 340, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3879.000 [3879.000, 3879.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   350/50000: episode: 341, duration: 0.007s, episode steps:   1, steps per second: 148, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3992.000 [3992.000, 3992.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   351/50000: episode: 342, duration: 0.010s, episode steps:   1, steps per second:  97, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2872.000 [2872.000, 2872.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   352/50000: episode: 343, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   353/50000: episode: 344, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   354/50000: episode: 345, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   355/50000: episode: 346, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   356/50000: episode: 347, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   357/50000: episode: 348, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 129.000 [129.000, 129.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   358/50000: episode: 349, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   359/50000: episode: 350, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2103.000 [2103.000, 2103.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   360/50000: episode: 351, duration: 0.008s, episode steps:   1, steps per second: 128, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   361/50000: episode: 352, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   362/50000: episode: 353, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2575.000 [2575.000, 2575.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   363/50000: episode: 354, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   364/50000: episode: 355, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3879.000 [3879.000, 3879.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   365/50000: episode: 356, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   366/50000: episode: 357, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 176.000 [176.000, 176.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   367/50000: episode: 358, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 101.000 [101.000, 101.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   368/50000: episode: 359, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   369/50000: episode: 360, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   370/50000: episode: 361, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2024.000 [2024.000, 2024.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   371/50000: episode: 362, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   372/50000: episode: 363, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3930.000 [3930.000, 3930.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   374/50000: episode: 364, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2621.000 [1284.000, 3958.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   375/50000: episode: 365, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3395.000 [3395.000, 3395.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   376/50000: episode: 366, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   377/50000: episode: 367, duration: 0.006s, episode steps:   1, steps per second: 170, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4044.000 [4044.000, 4044.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   378/50000: episode: 368, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   379/50000: episode: 369, duration: 0.008s, episode steps:   1, steps per second: 121, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4014.000 [4014.000, 4014.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   380/50000: episode: 370, duration: 0.009s, episode steps:   1, steps per second: 113, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   381/50000: episode: 371, duration: 0.009s, episode steps:   1, steps per second: 114, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   382/50000: episode: 372, duration: 0.008s, episode steps:   1, steps per second: 120, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   383/50000: episode: 373, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   384/50000: episode: 374, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   385/50000: episode: 375, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2059.000 [2059.000, 2059.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   386/50000: episode: 376, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   387/50000: episode: 377, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 951.000 [951.000, 951.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   388/50000: episode: 378, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2520.000 [2520.000, 2520.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   389/50000: episode: 379, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   390/50000: episode: 380, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   392/50000: episode: 381, duration: 0.031s, episode steps:   2, steps per second:  64, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 3950.000 [3817.000, 4083.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   393/50000: episode: 382, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 101.000 [101.000, 101.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   394/50000: episode: 383, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   395/50000: episode: 384, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 225.000 [225.000, 225.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   396/50000: episode: 385, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1066.000 [1066.000, 1066.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   397/50000: episode: 386, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   398/50000: episode: 387, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   399/50000: episode: 388, duration: 0.006s, episode steps:   1, steps per second: 174, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   400/50000: episode: 389, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   401/50000: episode: 390, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   402/50000: episode: 391, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1587.000 [1587.000, 1587.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   403/50000: episode: 392, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   404/50000: episode: 393, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   405/50000: episode: 394, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   406/50000: episode: 395, duration: 0.010s, episode steps:   1, steps per second: 103, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 678.000 [678.000, 678.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   407/50000: episode: 396, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   409/50000: episode: 397, duration: 0.020s, episode steps:   2, steps per second:  98, episode reward: -5001.000, mean reward: -2500.500 [-5000.000, -1.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   410/50000: episode: 398, duration: 0.005s, episode steps:   1, steps per second: 213, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 779.000 [779.000, 779.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   411/50000: episode: 399, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1252.000 [1252.000, 1252.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   412/50000: episode: 400, duration: 0.008s, episode steps:   1, steps per second: 125, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   413/50000: episode: 401, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 101.000 [101.000, 101.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   414/50000: episode: 402, duration: 0.008s, episode steps:   1, steps per second: 131, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 546.000 [546.000, 546.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   415/50000: episode: 403, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 401.000 [401.000, 401.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   417/50000: episode: 404, duration: 0.030s, episode steps:   2, steps per second:  66, episode reward: -4981.000, mean reward: -2490.500 [-5000.000, 19.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   418/50000: episode: 405, duration: 0.010s, episode steps:   1, steps per second: 104, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2731.000 [2731.000, 2731.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   419/50000: episode: 406, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   420/50000: episode: 407, duration: 0.008s, episode steps:   1, steps per second: 119, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1448.000 [1448.000, 1448.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   421/50000: episode: 408, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   422/50000: episode: 409, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   423/50000: episode: 410, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4008.000 [4008.000, 4008.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   424/50000: episode: 411, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2140.000 [2140.000, 2140.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   425/50000: episode: 412, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1199.000 [1199.000, 1199.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   426/50000: episode: 413, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   427/50000: episode: 414, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 401.000 [401.000, 401.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   428/50000: episode: 415, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   429/50000: episode: 416, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   430/50000: episode: 417, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3743.000 [3743.000, 3743.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   431/50000: episode: 418, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2575.000 [2575.000, 2575.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   432/50000: episode: 419, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   433/50000: episode: 420, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 550.000 [550.000, 550.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   434/50000: episode: 421, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2575.000 [2575.000, 2575.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   435/50000: episode: 422, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   436/50000: episode: 423, duration: 0.044s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1315.000 [1315.000, 1315.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   437/50000: episode: 424, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2504.000 [2504.000, 2504.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   438/50000: episode: 425, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3879.000 [3879.000, 3879.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   439/50000: episode: 426, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1068.000 [1068.000, 1068.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   440/50000: episode: 427, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2172.000 [2172.000, 2172.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   441/50000: episode: 428, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   442/50000: episode: 429, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   443/50000: episode: 430, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 521.000 [521.000, 521.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   444/50000: episode: 431, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   445/50000: episode: 432, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   446/50000: episode: 433, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2966.000 [2966.000, 2966.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   447/50000: episode: 434, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3879.000 [3879.000, 3879.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   448/50000: episode: 435, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   450/50000: episode: 436, duration: 0.044s, episode steps:   2, steps per second:  45, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   451/50000: episode: 437, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1339.000 [1339.000, 1339.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   452/50000: episode: 438, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   453/50000: episode: 439, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1262.000 [1262.000, 1262.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   454/50000: episode: 440, duration: 0.007s, episode steps:   1, steps per second: 137, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   455/50000: episode: 441, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   456/50000: episode: 442, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   457/50000: episode: 443, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1177.000 [1177.000, 1177.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   458/50000: episode: 444, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   459/50000: episode: 445, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   460/50000: episode: 446, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   461/50000: episode: 447, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   462/50000: episode: 448, duration: 0.010s, episode steps:   1, steps per second:  96, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   463/50000: episode: 449, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   464/50000: episode: 450, duration: 0.010s, episode steps:   1, steps per second:  98, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1092.000 [1092.000, 1092.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   465/50000: episode: 451, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1893.000 [1893.000, 1893.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   466/50000: episode: 452, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 101.000 [101.000, 101.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   467/50000: episode: 453, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2575.000 [2575.000, 2575.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   468/50000: episode: 454, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3251.000 [3251.000, 3251.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   469/50000: episode: 455, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2575.000 [2575.000, 2575.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   470/50000: episode: 456, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 944.000 [944.000, 944.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   471/50000: episode: 457, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3817.000 [3817.000, 3817.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   472/50000: episode: 458, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   474/50000: episode: 459, duration: 0.064s, episode steps:   2, steps per second:  31, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2836.500 [2787.000, 2886.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   475/50000: episode: 460, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3310.000 [3310.000, 3310.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   476/50000: episode: 461, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   477/50000: episode: 462, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3930.000 [3930.000, 3930.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   478/50000: episode: 463, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 401.000 [401.000, 401.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   479/50000: episode: 464, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   480/50000: episode: 465, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   481/50000: episode: 466, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 401.000 [401.000, 401.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   482/50000: episode: 467, duration: 0.010s, episode steps:   1, steps per second:  99, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   483/50000: episode: 468, duration: 0.010s, episode steps:   1, steps per second: 102, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   484/50000: episode: 469, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1112.000 [1112.000, 1112.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   485/50000: episode: 470, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1545.000 [1545.000, 1545.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   486/50000: episode: 471, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   487/50000: episode: 472, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   488/50000: episode: 473, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2504.000 [2504.000, 2504.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   489/50000: episode: 474, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1194.000 [1194.000, 1194.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   490/50000: episode: 475, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   491/50000: episode: 476, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   492/50000: episode: 477, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3773.000 [3773.000, 3773.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   493/50000: episode: 478, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 790.000 [790.000, 790.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   494/50000: episode: 479, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1357.000 [1357.000, 1357.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   495/50000: episode: 480, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   496/50000: episode: 481, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   497/50000: episode: 482, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   498/50000: episode: 483, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   499/50000: episode: 484, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2914.000 [2914.000, 2914.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   500/50000: episode: 485, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   501/50000: episode: 486, duration: 0.027s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   502/50000: episode: 487, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2942.000 [2942.000, 2942.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   503/50000: episode: 488, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   504/50000: episode: 489, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   505/50000: episode: 490, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   506/50000: episode: 491, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2617.000 [2617.000, 2617.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   507/50000: episode: 492, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   508/50000: episode: 493, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   509/50000: episode: 494, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1493.000 [1493.000, 1493.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   510/50000: episode: 495, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1637.000 [1637.000, 1637.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   511/50000: episode: 496, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2575.000 [2575.000, 2575.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   512/50000: episode: 497, duration: 0.007s, episode steps:   1, steps per second: 142, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   513/50000: episode: 498, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   514/50000: episode: 499, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1495.000 [1495.000, 1495.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   515/50000: episode: 500, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   516/50000: episode: 501, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   517/50000: episode: 502, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3930.000 [3930.000, 3930.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   518/50000: episode: 503, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3427.000 [3427.000, 3427.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   519/50000: episode: 504, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3898.000 [3898.000, 3898.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   520/50000: episode: 505, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   521/50000: episode: 506, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3444.000 [3444.000, 3444.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   523/50000: episode: 507, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   524/50000: episode: 508, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   525/50000: episode: 509, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   526/50000: episode: 510, duration: 0.034s, episode steps:   1, steps per second:  29, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3417.000 [3417.000, 3417.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   527/50000: episode: 511, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   528/50000: episode: 512, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   529/50000: episode: 513, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   530/50000: episode: 514, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1545.000 [1545.000, 1545.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   531/50000: episode: 515, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   532/50000: episode: 516, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   533/50000: episode: 517, duration: 0.027s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   534/50000: episode: 518, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   535/50000: episode: 519, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   536/50000: episode: 520, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   537/50000: episode: 521, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1865.000 [1865.000, 1865.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   538/50000: episode: 522, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   539/50000: episode: 523, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1495.000 [1495.000, 1495.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   540/50000: episode: 524, duration: 0.005s, episode steps:   1, steps per second: 206, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   541/50000: episode: 525, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3251.000 [3251.000, 3251.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   542/50000: episode: 526, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   544/50000: episode: 527, duration: 0.067s, episode steps:   2, steps per second:  30, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   545/50000: episode: 528, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   546/50000: episode: 529, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   547/50000: episode: 530, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   549/50000: episode: 531, duration: 0.043s, episode steps:   2, steps per second:  46, episode reward: -4991.000, mean reward: -2495.500 [-5000.000,  9.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   550/50000: episode: 532, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 570.000 [570.000, 570.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   551/50000: episode: 533, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3110.000 [3110.000, 3110.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   552/50000: episode: 534, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   553/50000: episode: 535, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3419.000 [3419.000, 3419.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   554/50000: episode: 536, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 101.000 [101.000, 101.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   555/50000: episode: 537, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   556/50000: episode: 538, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3369.000 [3369.000, 3369.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   557/50000: episode: 539, duration: 0.036s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   558/50000: episode: 540, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 401.000 [401.000, 401.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   559/50000: episode: 541, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   560/50000: episode: 542, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   561/50000: episode: 543, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   562/50000: episode: 544, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 461.000 [461.000, 461.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   563/50000: episode: 545, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   564/50000: episode: 546, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   565/50000: episode: 547, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3251.000 [3251.000, 3251.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   566/50000: episode: 548, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   568/50000: episode: 549, duration: 0.076s, episode steps:   2, steps per second:  26, episode reward: -4981.000, mean reward: -2490.500 [-5000.000, 19.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   569/50000: episode: 550, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   570/50000: episode: 551, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   571/50000: episode: 552, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   572/50000: episode: 553, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   573/50000: episode: 554, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1052.000 [1052.000, 1052.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   574/50000: episode: 555, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   575/50000: episode: 556, duration: 0.027s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   576/50000: episode: 557, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   577/50000: episode: 558, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   578/50000: episode: 559, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1725.000 [1725.000, 1725.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   579/50000: episode: 560, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   580/50000: episode: 561, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3922.000 [3922.000, 3922.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   581/50000: episode: 562, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4034.000 [4034.000, 4034.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   582/50000: episode: 563, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   583/50000: episode: 564, duration: 0.049s, episode steps:   1, steps per second:  20, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   584/50000: episode: 565, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   585/50000: episode: 566, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   586/50000: episode: 567, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   587/50000: episode: 568, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   588/50000: episode: 569, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   589/50000: episode: 570, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   590/50000: episode: 571, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1271.000 [1271.000, 1271.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   591/50000: episode: 572, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2229.000 [2229.000, 2229.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   592/50000: episode: 573, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   593/50000: episode: 574, duration: 0.034s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1137.000 [1137.000, 1137.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   594/50000: episode: 575, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   595/50000: episode: 576, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1912.000 [1912.000, 1912.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   596/50000: episode: 577, duration: 0.009s, episode steps:   1, steps per second: 111, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   597/50000: episode: 578, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3531.000 [3531.000, 3531.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   598/50000: episode: 579, duration: 0.041s, episode steps:   1, steps per second:  25, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3137.000 [3137.000, 3137.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   599/50000: episode: 580, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   600/50000: episode: 581, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   601/50000: episode: 582, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   602/50000: episode: 583, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   603/50000: episode: 584, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   604/50000: episode: 585, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3930.000 [3930.000, 3930.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   605/50000: episode: 586, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2463.000 [2463.000, 2463.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   606/50000: episode: 587, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2611.000 [2611.000, 2611.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   607/50000: episode: 588, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   608/50000: episode: 589, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   609/50000: episode: 590, duration: 0.033s, episode steps:   1, steps per second:  30, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2911.000 [2911.000, 2911.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   610/50000: episode: 591, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   611/50000: episode: 592, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   612/50000: episode: 593, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3848.000 [3848.000, 3848.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   613/50000: episode: 594, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1446.000 [1446.000, 1446.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   614/50000: episode: 595, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1226.000 [1226.000, 1226.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   615/50000: episode: 596, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   616/50000: episode: 597, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   617/50000: episode: 598, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   618/50000: episode: 599, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   619/50000: episode: 600, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   620/50000: episode: 601, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   621/50000: episode: 602, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 757.000 [757.000, 757.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   622/50000: episode: 603, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   623/50000: episode: 604, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2277.000 [2277.000, 2277.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   624/50000: episode: 605, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2193.000 [2193.000, 2193.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   625/50000: episode: 606, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   626/50000: episode: 607, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   627/50000: episode: 608, duration: 0.035s, episode steps:   1, steps per second:  28, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 392.000 [392.000, 392.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   629/50000: episode: 609, duration: 0.046s, episode steps:   2, steps per second:  44, episode reward: -5011.000, mean reward: -2505.500 [-5000.000, -11.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   630/50000: episode: 610, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1478.000 [1478.000, 1478.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   631/50000: episode: 611, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   632/50000: episode: 612, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   633/50000: episode: 613, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3980.000 [3980.000, 3980.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   634/50000: episode: 614, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1577.000 [1577.000, 1577.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   635/50000: episode: 615, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3879.000 [3879.000, 3879.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   636/50000: episode: 616, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3952.000 [3952.000, 3952.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   637/50000: episode: 617, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2033.000 [2033.000, 2033.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   638/50000: episode: 618, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   639/50000: episode: 619, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   640/50000: episode: 620, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   641/50000: episode: 621, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2947.000 [2947.000, 2947.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   642/50000: episode: 622, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3942.000 [3942.000, 3942.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   643/50000: episode: 623, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   644/50000: episode: 624, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   645/50000: episode: 625, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   646/50000: episode: 626, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   647/50000: episode: 627, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   648/50000: episode: 628, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   649/50000: episode: 629, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   650/50000: episode: 630, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   651/50000: episode: 631, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1314.000 [1314.000, 1314.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   652/50000: episode: 632, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 3167.000 [3167.000, 3167.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   653/50000: episode: 633, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   654/50000: episode: 634, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 189.000 [189.000, 189.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   655/50000: episode: 635, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 62.000 [62.000, 62.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   656/50000: episode: 636, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 4003.000 [4003.000, 4003.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   657/50000: episode: 637, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   658/50000: episode: 638, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1545.000 [1545.000, 1545.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   659/50000: episode: 639, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 2787.000 [2787.000, 2787.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n",
      "   660/50000: episode: 640, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: -5000.000, mean reward: -5000.000 [-5000.000, -5000.000], mean action: 1284.000 [1284.000, 1284.000],  loss: --, mae: --, mean_q: --\n",
      "wrong_move\n"
     ]
    }
   ],
   "source": [
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "for i in range (10):\n",
    "  policy = EpsGreedyQPolicy(0.01)\n",
    "  dqn = DQNAgent(model=model, nb_actions=NB_ACTIONS, memory=memory,\n",
    "                target_model_update=1e-2, policy=policy)\n",
    "  dqn.compile(Adam(lr=1e-4), metrics=['mae'])\n",
    "\n",
    "  # Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "  # slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "  # Ctrl + C.\n",
    "  his = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
    "  \n",
    "  model.save('chess_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save dqn\n",
    "# dqn.save_weights('dqn_{}_weights.h5f'.format('chess'), overwrite=True)\n",
    "\n",
    "# # save model\n",
    "model.save('chess_model.h5')\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "files.download('chess_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# model = keras.models.load_model('chess_model.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
