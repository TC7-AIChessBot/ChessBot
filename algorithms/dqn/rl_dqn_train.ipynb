{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install keras-rl2\n",
    "# ! pip install chess\n",
    "# ! pip install python-chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 21:38:19.472552: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-15 21:38:19.472591: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten,\\\n",
    "     Input,BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# import gym_chess\n",
    "\n",
    "import chess\n",
    "# import sys\n",
    "# sys.path.insert(0, '../')\n",
    "# sys.path.insert(0, '../alpha_beta')\n",
    "# from MyChessBoard import MyChessBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16524551451354734588\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 21:38:23.012893: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-15 21:38:23.014806: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-15 21:38:23.014841: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-15 21:38:23.014877: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (LoG): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SHAPE = (65, )\n",
    "NB_ACTIONS = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessEnv:\n",
    "    '''\n",
    "    state - obser: ndarray - (65,): [:65] is flatten from int_board; [65] is color of bot; 1 is white and -1 is black\n",
    "    step: int. step_range = (0, 4096) , is encoded from square A to square B (64 x 64 val)\n",
    "    reward: int\n",
    "    '''\n",
    "\n",
    "    mapped = {\n",
    "            'P': 10,     # White Pawn\n",
    "            'p': -10,    # Black Pawn\n",
    "            'N': 20,     # White Knight\n",
    "            'n': -20,    # Black Knight\n",
    "            'B': 30,     # White Bishop\n",
    "            'b': -30,    # Black Bishop\n",
    "            'R': 40,     # White Rook\n",
    "            'r': -40,    # Black Rook\n",
    "            'Q': 50,     # White Queen\n",
    "            'q': -50,    # Black Queen\n",
    "            'K': 900,     # White King\n",
    "            'k': -900     # Black King\n",
    "    }\n",
    "    # state_shape = (8, 8)\n",
    "    # nb_actions = 4096\n",
    "    model = None\n",
    "    \n",
    "    def __init__(self, model: Sequential, neg_r_each_step = -1) -> None:\n",
    "        self.env = chess.Board()\n",
    "        self.model = model\n",
    "        self.state = self.reset()\n",
    "        # [-1] = 1 -> white, -1 -> black\n",
    "        self.bot_color = self.env.turn * 2 - 1\n",
    "        self.neg_r_each_step = neg_r_each_step\n",
    "\n",
    "    def is_draw(self):\n",
    "        if self.env.is_stalemate():\n",
    "            print(\"statlemate\")\n",
    "            return True\n",
    "        if self.env.is_fivefold_repetition():\n",
    "            print(\"fivefold repetition\")\n",
    "            return True\n",
    "        if self.env.is_seventyfive_moves():\n",
    "            print(\"75 moves\")\n",
    "            return True\n",
    "        if self.env.is_insufficient_material():\n",
    "            print(\"Insufficient Material\")\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_checkmate(self):\n",
    "        # If There is checkmate then it will be TRUE else FALSE.It will be a boolean value.\n",
    "        return self.env.is_checkmate()\n",
    "\n",
    "    def convert_board_to_int(self):\n",
    "        epd_string = self.env.epd()\n",
    "        list_int = np.empty((0, ))\n",
    "        for i in epd_string:\n",
    "            if i == \" \":\n",
    "                list_int = list_int.reshape((8, 8))\n",
    "                return list_int\n",
    "            elif i != \"/\":\n",
    "                if i in self.mapped:\n",
    "                    list_int = np.append(list_int, self.mapped[i])\n",
    "                else:\n",
    "                    for counter in range(0, int(i)):\n",
    "                        list_int = np.append(list_int, 0)\n",
    "        list_int = list_int.reshape((8, 8))\n",
    "        return list_int\n",
    "\n",
    "    def get_state(self) -> np.ndarray:\n",
    "        return np.append(self.convert_board_to_int().reshape(64,), self.env.turn * 2 - 1)\n",
    "\n",
    "    def legal_moves(self):\n",
    "        return list(self.env.legal_moves)\n",
    "\n",
    "    def encodeMove(self, move_uci:str):\n",
    "        if len(move_uci) != 4:\n",
    "            raise ValueError()\n",
    "        a, b = chess.parse_square(move_uci[:2]), chess.parse_square(move_uci[2:])\n",
    "        return a * 64 + b\n",
    "\n",
    "    def decodeMove(self, move_int:int):\n",
    "        a, b = move_int//64, move_int%64\n",
    "        # a, b = chess.square_name(a), chess.square_name(b)\n",
    "\n",
    "        move = self.env.find_move(from_square= a,to_square= b)\n",
    "        return move\n",
    "\n",
    "    def render(self):\n",
    "        print(self.env.unicode())\n",
    "\n",
    "    def reset(self):\n",
    "        # random state\n",
    "        redo = True\n",
    "        num_sample_steps = 0\n",
    "        while redo:\n",
    "            redo = False\n",
    "            self.env = chess.Board()\n",
    "            num_sample_steps = np.random.randint(0, 15)\n",
    "            for i in range (num_sample_steps):\n",
    "                lg_move = self.legal_moves()\n",
    "                if len(lg_move) != 0:\n",
    "                    move = np.random.choice(self.legal_moves())\n",
    "                    self.env.push(move)\n",
    "                else:\n",
    "                    redo = True\n",
    "                    break\n",
    "\n",
    "        Q_val = self.model.predict(self.get_state().reshape((1, 1) + STATE_SHAPE)).reshape(-1, )\n",
    "        print('Val:', min(Q_val), max(Q_val))\n",
    "        return self.get_state()\n",
    "\n",
    "    def step(self, action: int):\n",
    "        reward = 0\n",
    "        done = True\n",
    "        \n",
    "        try:\n",
    "            # move in legal move\n",
    "            move = self.decodeMove(action)\n",
    "\n",
    "            # neg reward each step\n",
    "            reward = self.neg_r_each_step\n",
    "\n",
    "            # location to_square\n",
    "            to_r, to_c = move.to_square//8, move.to_square%8\n",
    "            reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
    "\n",
    "            # action\n",
    "            self.env.push(move)\n",
    "            self.state = self.get_state()\n",
    "\n",
    "            # check end game\n",
    "            if self.is_checkmate():\n",
    "                reward += self.mapped['K']\n",
    "                done = True\n",
    "                print('Win')\n",
    "            elif self.is_draw():\n",
    "                reward += 300\n",
    "                done = True\n",
    "\n",
    "            # opponent's turn   \n",
    "            else:\n",
    "                done = False\n",
    "                Q_val = self.model.predict(self.state.reshape((1, 1) + STATE_SHAPE)).reshape(-1, )\n",
    "                idx_sorted = np.argsort(Q_val)\n",
    "\n",
    "                for act in idx_sorted:\n",
    "                    try:\n",
    "                        move = self.decodeMove(act)\n",
    "\n",
    "                        # location to_square\n",
    "                        to_r, to_c = move.to_square//8, move.to_square%8\n",
    "                        reward -= self.state[(7 - to_r)*8 + to_c ] * self.bot_color\n",
    "\n",
    "                        # action\n",
    "                        self.env.push(move)\n",
    "                        self.state = self.get_state()\n",
    "\n",
    "                        # check end game\n",
    "                        if self.is_checkmate():\n",
    "                            reward -= self.mapped['K']\n",
    "                            done = True\n",
    "                            print(\"Lose\")\n",
    "                        elif self.is_draw():\n",
    "                            reward += 300\n",
    "                            done = True\n",
    "                        \n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "        except:\n",
    "            # wrong move\n",
    "            reward = -5000\n",
    "            done = True\n",
    "            print('wrong_move')\n",
    "\n",
    "        return self.state, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "WARNING:tensorflow:From /home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/layers/normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 65)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               8448      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              528384    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 4096)              0         \n",
      "=================================================================\n",
      "Total params: 554,368\n",
      "Trainable params: 553,856\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = Sequential()\n",
    "model.add(Input((1, ) + STATE_SHAPE))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(NB_ACTIONS))\n",
    "model.add(Activation('linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kienanh/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "2021-12-15 21:38:25.119939: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 1999965000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val: -60.424118 73.745285\n"
     ]
    }
   ],
   "source": [
    "env = ChessEnv(model, neg_r_each_step=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 09:52:08.271799: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 1999965000 Hz\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('chess_model_dqn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SequentialMemory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18135/489546443.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# even the metrics!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpsGreedyQPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SequentialMemory' is not defined"
     ]
    }
   ],
   "source": [
    "# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n",
    "# even the metrics!\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "for i in range (10):\n",
    "  policy = EpsGreedyQPolicy(0.01)\n",
    "  dqn = DQNAgent(model=model, nb_actions=NB_ACTIONS, memory=memory,\n",
    "                target_model_update=1e-2, policy=policy)\n",
    "  dqn.compile(Adam(lr=1e-4), metrics=['mae'])\n",
    "\n",
    "  # Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "  # slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "  # Ctrl + C.\n",
    "  his = dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
    "  \n",
    "  model.save('chess_model_dqn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save dqn\n",
    "# dqn.save_weights('dqn_{}_weights.h5f'.format('chess'), overwrite=True)\n",
    "\n",
    "# # save model\n",
    "# model.save('chess_model.h5')\n",
    "\n",
    "# from google.colab import files\n",
    "\n",
    "# files.download('chess_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# model = keras.models.load_model('chess_model.h5')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
